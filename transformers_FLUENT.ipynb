{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:27.682419Z","iopub.status.busy":"2024-07-02T20:17:27.682115Z","iopub.status.idle":"2024-07-02T20:17:30.597078Z","shell.execute_reply":"2024-07-02T20:17:30.596113Z","shell.execute_reply.started":"2024-07-02T20:17:27.682391Z"},"trusted":true},"outputs":[],"source":["from collections import Counter\n","import json\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset\n","import torch.utils.data\n","import math\n","import torch.nn.functional as F\n","import neptune\n","import pandas as pd"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:30.598900Z","iopub.status.busy":"2024-07-02T20:17:30.598458Z","iopub.status.idle":"2024-07-02T20:17:32.694354Z","shell.execute_reply":"2024-07-02T20:17:32.693406Z","shell.execute_reply.started":"2024-07-02T20:17:30.598873Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Pertanyaan</th>\n","      <th>Jawaban</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>email Fitra A. Bachtiar</td>\n","      <td>fitra.bachtiar[at]ub.ac.id</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>NIK/NIP Fitra A. Bachtiar</td>\n","      <td>198406282019031006</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>nama lengkap Fitra A. Bachtiar</td>\n","      <td>Dr.Eng. Fitra A. Bachtiar</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Departemen Fitra A. Bachtiar</td>\n","      <td>Departemen Teknik Informatika</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Program Studi Fitra A. Bachtiar</td>\n","      <td>S2 Ilmu Komputer</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1229</th>\n","      <td>Apa Manfaat Konseling FILKOM ?</td>\n","      <td>1. Masalah ditangani oleh ahli yang kompeten d...</td>\n","    </tr>\n","    <tr>\n","      <th>1230</th>\n","      <td>Berikan informasi mengenai Layanan Konseling</td>\n","      <td>Informasi mengenai Layanan Konseling dapat dia...</td>\n","    </tr>\n","    <tr>\n","      <th>1231</th>\n","      <td>Siapa Konselor Bimbingan dan Konseling di FILK...</td>\n","      <td>Ada 2 konselor Bimbingan dan Konseling di FILK...</td>\n","    </tr>\n","    <tr>\n","      <th>1232</th>\n","      <td>Siapa Koordinator Konselor Sebaya ?</td>\n","      <td>Koordinator Konselor Sebaya adalah Muhammad Da...</td>\n","    </tr>\n","    <tr>\n","      <th>1233</th>\n","      <td>Berikan Rincian Layanan ULTKSP</td>\n","      <td>Rincian Layanan ULTKSP dapat diakses pada taut...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1198 rows Ã— 2 columns</p>\n","</div>"],"text/plain":["                                             Pertanyaan  \\\n","0                               email Fitra A. Bachtiar   \n","1                             NIK/NIP Fitra A. Bachtiar   \n","2                        nama lengkap Fitra A. Bachtiar   \n","3                          Departemen Fitra A. Bachtiar   \n","4                       Program Studi Fitra A. Bachtiar   \n","...                                                 ...   \n","1229                     Apa Manfaat Konseling FILKOM ?   \n","1230       Berikan informasi mengenai Layanan Konseling   \n","1231  Siapa Konselor Bimbingan dan Konseling di FILK...   \n","1232                Siapa Koordinator Konselor Sebaya ?   \n","1233                     Berikan Rincian Layanan ULTKSP   \n","\n","                                                Jawaban  \n","0                            fitra.bachtiar[at]ub.ac.id  \n","1                                    198406282019031006  \n","2                             Dr.Eng. Fitra A. Bachtiar  \n","3                         Departemen Teknik Informatika  \n","4                                      S2 Ilmu Komputer  \n","...                                                 ...  \n","1229  1. Masalah ditangani oleh ahli yang kompeten d...  \n","1230  Informasi mengenai Layanan Konseling dapat dia...  \n","1231  Ada 2 konselor Bimbingan dan Konseling di FILK...  \n","1232  Koordinator Konselor Sebaya adalah Muhammad Da...  \n","1233  Rincian Layanan ULTKSP dapat diakses pada taut...  \n","\n","[1198 rows x 2 columns]"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["knowledgebase_url = 'https://github.com/AndiAlifs/FLUENT-Chatbot-2023/raw/main/KnowledgeBaseFilkom.xlsx'\n","knowledgebase = pd.read_excel(knowledgebase_url)\n","\n","qa_paired = knowledgebase.drop(columns=knowledgebase.columns.drop(['Pertanyaan', 'Jawaban']))\n","qa_paired.dropna(inplace=True)\n","qa_paired"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:32.697066Z","iopub.status.busy":"2024-07-02T20:17:32.696633Z","iopub.status.idle":"2024-07-02T20:17:32.702322Z","shell.execute_reply":"2024-07-02T20:17:32.701334Z","shell.execute_reply.started":"2024-07-02T20:17:32.697038Z"},"trusted":true},"outputs":[],"source":["def remove_punc(string):\n","    punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n","    no_punct = \"\"\n","    for char in string:\n","        if char not in punctuations:\n","            no_punct = no_punct + char  # space is also a character\n","    return no_punct.lower()"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:32.703938Z","iopub.status.busy":"2024-07-02T20:17:32.703658Z","iopub.status.idle":"2024-07-02T20:17:32.827025Z","shell.execute_reply":"2024-07-02T20:17:32.825935Z","shell.execute_reply.started":"2024-07-02T20:17:32.703915Z"},"trusted":true},"outputs":[],"source":["pairs = []\n","max_len = 90\n","\n","for line in qa_paired.iterrows():\n","    pertanyaan = line[1]['Pertanyaan']\n","    jawaban = line[1]['Jawaban']\n","    qa_pairs = []\n","    first = remove_punc(pertanyaan.strip())      \n","    second = remove_punc(jawaban.strip())\n","    qa_pairs.append(first.split()[:max_len])\n","    qa_pairs.append(second.split()[:max_len])\n","    pairs.append(qa_pairs)"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:32.828565Z","iopub.status.busy":"2024-07-02T20:17:32.828255Z","iopub.status.idle":"2024-07-02T20:17:32.838911Z","shell.execute_reply":"2024-07-02T20:17:32.837848Z","shell.execute_reply.started":"2024-07-02T20:17:32.828539Z"},"trusted":true},"outputs":[],"source":["word_freq = Counter()\n","for pair in pairs:\n","    word_freq.update(pair[0])\n","    word_freq.update(pair[1])"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:32.840576Z","iopub.status.busy":"2024-07-02T20:17:32.840244Z","iopub.status.idle":"2024-07-02T20:17:32.850265Z","shell.execute_reply":"2024-07-02T20:17:32.849352Z","shell.execute_reply.started":"2024-07-02T20:17:32.840548Z"},"trusted":true},"outputs":[],"source":["min_word_freq = 2\n","words = [w for w in word_freq.keys() if word_freq[w] > min_word_freq]\n","word_map = {k: v + 1 for v, k in enumerate(words)}\n","word_map['<unk>'] = len(word_map) + 1\n","word_map['<start>'] = len(word_map) + 1\n","word_map['<end>'] = len(word_map) + 1\n","word_map['<pad>'] = 0"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:32.851802Z","iopub.status.busy":"2024-07-02T20:17:32.851465Z","iopub.status.idle":"2024-07-02T20:17:32.860482Z","shell.execute_reply":"2024-07-02T20:17:32.859625Z","shell.execute_reply.started":"2024-07-02T20:17:32.851770Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Total words are: 1079\n"]}],"source":["print(\"Total words are: {}\".format(len(word_map)))"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:32.861919Z","iopub.status.busy":"2024-07-02T20:17:32.861579Z","iopub.status.idle":"2024-07-02T20:17:32.872138Z","shell.execute_reply":"2024-07-02T20:17:32.871399Z","shell.execute_reply.started":"2024-07-02T20:17:32.861895Z"},"trusted":true},"outputs":[],"source":["with open('WORDMAP_corpus_KBFILKOM.json', 'w') as j:\n","    json.dump(word_map, j)"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:32.877074Z","iopub.status.busy":"2024-07-02T20:17:32.876749Z","iopub.status.idle":"2024-07-02T20:17:32.884325Z","shell.execute_reply":"2024-07-02T20:17:32.883430Z","shell.execute_reply.started":"2024-07-02T20:17:32.877048Z"},"trusted":true},"outputs":[],"source":["def encode_question(words, word_map):\n","    enc_c = [word_map.get(word, word_map['<unk>']) for word in words] + [word_map['<pad>']] * (max_len - len(words))\n","    return enc_c\n","\n","def encode_reply(words, word_map):\n","    enc_c = [word_map['<start>']] + [word_map.get(word, word_map['<unk>']) for word in words] + \\\n","    [word_map['<end>']] + [word_map['<pad>']] * (max_len - len(words))\n","    return enc_c"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:32.885632Z","iopub.status.busy":"2024-07-02T20:17:32.885341Z","iopub.status.idle":"2024-07-02T20:17:33.099456Z","shell.execute_reply":"2024-07-02T20:17:33.098486Z","shell.execute_reply.started":"2024-07-02T20:17:32.885608Z"},"trusted":true},"outputs":[],"source":["pairs_encoded = []\n","for pair in pairs:\n","    qus = encode_question(pair[0], word_map)\n","    ans = encode_reply(pair[1], word_map)\n","    pairs_encoded.append([qus, ans])\n","\n","with open('pairs_encoded_kbfilkom.json', 'w') as p:\n","    json.dump(pairs_encoded, p)"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:33.310393Z","iopub.status.busy":"2024-07-02T20:17:33.310107Z","iopub.status.idle":"2024-07-02T20:17:33.317262Z","shell.execute_reply":"2024-07-02T20:17:33.316249Z","shell.execute_reply.started":"2024-07-02T20:17:33.310368Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'apa tujuan filkom <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>'"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["rev_word_map = {v: k for k, v in word_map.items()}\n","' '.join([rev_word_map[v] for v in pairs_encoded[15][0]])"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:33.318791Z","iopub.status.busy":"2024-07-02T20:17:33.318483Z","iopub.status.idle":"2024-07-02T20:17:33.325793Z","shell.execute_reply":"2024-07-02T20:17:33.325035Z","shell.execute_reply.started":"2024-07-02T20:17:33.318756Z"},"trusted":true},"outputs":[],"source":["class Dataset(Dataset):\n","\n","    def __init__(self):\n","\n","        self.pairs = json.load(open('pairs_encoded_kbfilkom.json'))\n","        self.dataset_size = len(self.pairs)\n","\n","    def __getitem__(self, i):\n","        \n","        question = torch.LongTensor(self.pairs[i][0])\n","        reply = torch.LongTensor(self.pairs[i][1])\n","            \n","        return question, reply\n","\n","    def __len__(self):\n","        return self.dataset_size"]},{"cell_type":"markdown","metadata":{},"source":["## Train Loader"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:33.337842Z","iopub.status.busy":"2024-07-02T20:17:33.337481Z","iopub.status.idle":"2024-07-02T20:17:33.379910Z","shell.execute_reply":"2024-07-02T20:17:33.378865Z","shell.execute_reply.started":"2024-07-02T20:17:33.337813Z"},"trusted":true},"outputs":[],"source":["train_loader = torch.utils.data.DataLoader(Dataset(),\n","                                           batch_size = 100, \n","                                           shuffle=True, \n","                                           pin_memory=True)"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:33.381467Z","iopub.status.busy":"2024-07-02T20:17:33.381114Z","iopub.status.idle":"2024-07-02T20:17:33.388963Z","shell.execute_reply":"2024-07-02T20:17:33.388016Z","shell.execute_reply.started":"2024-07-02T20:17:33.381434Z"},"trusted":true},"outputs":[],"source":["def create_masks(question, reply_input, reply_target):\n","    \n","    def subsequent_mask(size):\n","        mask = torch.triu(torch.ones(size, size)).transpose(0, 1).type(dtype=torch.uint8)\n","        return mask.unsqueeze(0)\n","    \n","    question_mask = question!=0\n","    question_mask = question_mask.to(device)\n","    question_mask = question_mask.unsqueeze(1).unsqueeze(1)         # (batch_size, 1, 1, max_words)\n","     \n","    reply_input_mask = reply_input!=0\n","    reply_input_mask = reply_input_mask.unsqueeze(1)  # (batch_size, 1, max_words)\n","    reply_input_mask = reply_input_mask & subsequent_mask(reply_input.size(-1)).type_as(reply_input_mask.data) \n","    reply_input_mask = reply_input_mask.unsqueeze(1) # (batch_size, 1, max_words, max_words)\n","    reply_target_mask = reply_target!=0              # (batch_size, max_words)\n","    \n","    return question_mask, reply_input_mask, reply_target_mask"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:33.390611Z","iopub.status.busy":"2024-07-02T20:17:33.390255Z","iopub.status.idle":"2024-07-02T20:17:33.397673Z","shell.execute_reply":"2024-07-02T20:17:33.396839Z","shell.execute_reply.started":"2024-07-02T20:17:33.390578Z"},"trusted":true},"outputs":[],"source":["import os\n","\n","def create_directory(path):\n","    if not os.path.exists(path):\n","        os.makedirs(path)\n","        print(f\"Directory created at {path}\")\n","    else:\n","        print(f\"Directory already exists at {path}\")"]},{"cell_type":"markdown","metadata":{},"source":["# Architecture"]},{"cell_type":"markdown","metadata":{},"source":["## Embeddings"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:33.399270Z","iopub.status.busy":"2024-07-02T20:17:33.398914Z","iopub.status.idle":"2024-07-02T20:17:33.421634Z","shell.execute_reply":"2024-07-02T20:17:33.420718Z","shell.execute_reply.started":"2024-07-02T20:17:33.399239Z"},"trusted":true},"outputs":[],"source":["class Embeddings(nn.Module):\n","    \"\"\"\n","    Implements embeddings of the words and adds their positional encodings. \n","    \"\"\"\n","    def __init__(self, vocab_size, d_model, max_len = 50, num_layers = 6):\n","        super(Embeddings, self).__init__()\n","        self.d_model = d_model\n","        self.dropout = nn.Dropout(0.1)\n","        self.embed = nn.Embedding(vocab_size, d_model)\n","        self.pe = self.create_positinal_encoding(max_len, self.d_model)     # (1, max_len, d_model)\n","        self.te = self.create_positinal_encoding(num_layers, self.d_model)  # (1, num_layers, d_model)\n","        self.dropout = nn.Dropout(0.1)\n","        \n","    def create_positinal_encoding(self, max_len, d_model):\n","        pe = torch.zeros(max_len, d_model).to(device)\n","        for pos in range(max_len):   # for each position of the word\n","            for i in range(0, d_model, 2):   # for each dimension of the each position\n","                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/d_model)))\n","                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n","        pe = pe.unsqueeze(0)   # include the batch size\n","        return pe\n","        \n","    def forward(self, embedding, layer_idx):\n","        if layer_idx == 0:\n","            embedding = self.embed(embedding) * math.sqrt(self.d_model)\n","        embedding += self.pe[:, :embedding.size(1)]   # pe will automatically be expanded with the same batch size as encoded_words\n","        # embedding: (batch_size, max_len, d_model), te: (batch_size, 1, d_model)\n","        embedding += self.te[:, layer_idx, :].unsqueeze(1).repeat(1, embedding.size(1), 1)\n","        embedding = self.dropout(embedding)\n","        return embedding\n"]},{"cell_type":"markdown","metadata":{},"source":["## Multi-Head Attention"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:33.423711Z","iopub.status.busy":"2024-07-02T20:17:33.423056Z","iopub.status.idle":"2024-07-02T20:17:33.435766Z","shell.execute_reply":"2024-07-02T20:17:33.435028Z","shell.execute_reply.started":"2024-07-02T20:17:33.423675Z"},"trusted":true},"outputs":[],"source":["class MultiHeadAttention(nn.Module):\n","    \n","    def __init__(self, heads, d_model):\n","        super(MultiHeadAttention, self).__init__()\n","        assert d_model % heads == 0\n","        self.d_k = d_model // heads\n","        self.heads = heads\n","        self.dropout = nn.Dropout(0.1)\n","        self.query = nn.Linear(d_model, d_model)\n","        self.key = nn.Linear(d_model, d_model)\n","        self.value = nn.Linear(d_model, d_model)\n","        self.concat = nn.Linear(d_model, d_model)\n","        \n","    def forward(self, query, key, value, mask):\n","        \"\"\"\n","        query, key, value of shape: (batch_size, max_len, 512)\n","        mask of shape: (batch_size, 1, 1, max_words)\n","        \"\"\"\n","        # (batch_size, max_len, 512)\n","        query = self.query(query)\n","        key = self.key(key)        \n","        value = self.value(value)   \n","        \n","        # (batch_size, max_len, 512) --> (batch_size, max_len, h, d_k) --> (batch_size, h, max_len, d_k)\n","        query = query.view(query.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)   \n","        key = key.view(key.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)  \n","        value = value.view(value.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)  \n","        \n","        # (batch_size, h, max_len, d_k) matmul (batch_size, h, d_k, max_len) --> (batch_size, h, max_len, max_len)\n","        scores = torch.matmul(query, key.permute(0,1,3,2)) / math.sqrt(query.size(-1))\n","        scores = scores.masked_fill(mask == 0, -1e9)    # (batch_size, h, max_len, max_len)\n","        weights = F.softmax(scores, dim = -1)           # (batch_size, h, max_len, max_len)\n","        weights = self.dropout(weights)\n","        # (batch_size, h, max_len, max_len) matmul (batch_size, h, max_len, d_k) --> (batch_size, h, max_len, d_k)\n","        context = torch.matmul(weights, value)\n","        # (batch_size, h, max_len, d_k) --> (batch_size, max_len, h, d_k) --> (batch_size, max_len, h * d_k)\n","        context = context.permute(0,2,1,3).contiguous().view(context.shape[0], -1, self.heads * self.d_k)\n","        # (batch_size, max_len, h * d_k)\n","        interacted = self.concat(context)\n","        return interacted "]},{"cell_type":"markdown","metadata":{},"source":["## Feed Forward Neural Network"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:33.437331Z","iopub.status.busy":"2024-07-02T20:17:33.437039Z","iopub.status.idle":"2024-07-02T20:17:33.450588Z","shell.execute_reply":"2024-07-02T20:17:33.449684Z","shell.execute_reply.started":"2024-07-02T20:17:33.437306Z"},"trusted":true},"outputs":[],"source":["class FeedForward(nn.Module):\n","    def __init__(self, d_model, middle_dim = 2048):\n","        super(FeedForward, self).__init__()\n","        \n","        self.fc1 = nn.Linear(d_model, middle_dim)\n","        self.fc2 = nn.Linear(middle_dim, d_model)\n","        self.dropout = nn.Dropout(0.1)\n","\n","    def forward(self, x):\n","        out = F.relu(self.fc1(x))\n","        out = self.fc2(self.dropout(out))\n","        return out\n"]},{"cell_type":"markdown","metadata":{},"source":["## Encoder"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:33.452046Z","iopub.status.busy":"2024-07-02T20:17:33.451709Z","iopub.status.idle":"2024-07-02T20:17:33.461741Z","shell.execute_reply":"2024-07-02T20:17:33.460847Z","shell.execute_reply.started":"2024-07-02T20:17:33.452019Z"},"trusted":true},"outputs":[],"source":["class EncoderLayer(nn.Module):\n","    def __init__(self, d_model, heads):\n","        super(EncoderLayer, self).__init__()\n","        self.layernorm = nn.LayerNorm(d_model)\n","        self.self_multihead = MultiHeadAttention(heads, d_model)\n","        self.feed_forward = FeedForward(d_model)\n","        self.dropout = nn.Dropout(0.1)\n","\n","    def forward(self, embeddings, mask):\n","        interacted = self.dropout(self.self_multihead(embeddings, embeddings, embeddings, mask))\n","        interacted = self.layernorm(interacted + embeddings)\n","        feed_forward_out = self.dropout(self.feed_forward(interacted))\n","        encoded = self.layernorm(feed_forward_out + interacted)\n","        return encoded\n","        "]},{"cell_type":"markdown","metadata":{},"source":["## Decoder"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:33.463365Z","iopub.status.busy":"2024-07-02T20:17:33.463025Z","iopub.status.idle":"2024-07-02T20:17:33.475859Z","shell.execute_reply":"2024-07-02T20:17:33.475008Z","shell.execute_reply.started":"2024-07-02T20:17:33.463339Z"},"trusted":true},"outputs":[],"source":["class DecoderLayer(nn.Module):\n","    \n","    def __init__(self, d_model, heads):\n","        super(DecoderLayer, self).__init__()\n","        self.layernorm = nn.LayerNorm(d_model)\n","        self.self_multihead = MultiHeadAttention(heads, d_model)\n","        self.src_multihead = MultiHeadAttention(heads, d_model)\n","        self.feed_forward = FeedForward(d_model)\n","        self.dropout = nn.Dropout(0.1)\n","        \n","    def forward(self, embeddings, encoded, src_mask, target_mask):\n","        query = self.dropout(self.self_multihead(embeddings, embeddings, embeddings, target_mask))\n","        query = self.layernorm(query + embeddings)\n","        interacted = self.dropout(self.src_multihead(query, encoded, encoded, src_mask))\n","        interacted = self.layernorm(interacted + query)\n","        feed_forward_out = self.dropout(self.feed_forward(interacted))\n","        decoded = self.layernorm(feed_forward_out + interacted)\n","        return decoded"]},{"cell_type":"markdown","metadata":{},"source":["## Transformer Architecture"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:33.478079Z","iopub.status.busy":"2024-07-02T20:17:33.477782Z","iopub.status.idle":"2024-07-02T20:17:33.514932Z","shell.execute_reply":"2024-07-02T20:17:33.514023Z","shell.execute_reply.started":"2024-07-02T20:17:33.478055Z"},"trusted":true},"outputs":[],"source":["class Transformer(nn.Module):\n","    \n","    def __init__(self, d_model, heads, num_layers, word_map, max_len = 50):\n","        super(Transformer, self).__init__()\n","        \n","        self.d_model = d_model\n","        self.num_layers = num_layers\n","        self.vocab_size = len(word_map)\n","        self.embed = Embeddings(self.vocab_size, d_model, num_layers = num_layers, max_len = max_len)\n","        self.encoder = EncoderLayer(d_model, heads) \n","        self.decoder = DecoderLayer(d_model, heads)\n","        self.logit = nn.Linear(d_model, self.vocab_size)\n","        \n","    def encode(self, src_embeddings, src_mask):\n","        for i in range(self.num_layers):\n","            src_embeddings = self.embed(src_embeddings, i)\n","            src_embeddings = self.encoder(src_embeddings, src_mask)\n","        return src_embeddings\n","    \n","    def decode(self, tgt_embeddings, target_mask, src_embeddings, src_mask):\n","        for i in range(self.num_layers):\n","            tgt_embeddings = self.embed(tgt_embeddings, i)\n","            tgt_embeddings = self.decoder(tgt_embeddings, src_embeddings, src_mask, target_mask)\n","        return tgt_embeddings\n","        \n","    def forward(self, src_words, src_mask, target_words, target_mask):\n","        encoded = self.encode(src_words, src_mask)\n","        decoded = self.decode(target_words, target_mask, encoded, src_mask)\n","        out = F.log_softmax(self.logit(decoded), dim = 2)\n","        return out\n"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:33.516425Z","iopub.status.busy":"2024-07-02T20:17:33.516094Z","iopub.status.idle":"2024-07-02T20:17:33.527522Z","shell.execute_reply":"2024-07-02T20:17:33.526734Z","shell.execute_reply.started":"2024-07-02T20:17:33.516394Z"},"trusted":true},"outputs":[],"source":["class AdamWarmup:\n","    \n","    def __init__(self, model_size, warmup_steps, optimizer):\n","        \n","        self.model_size = model_size\n","        self.warmup_steps = warmup_steps\n","        self.optimizer = optimizer\n","        self.current_step = 0\n","        self.lr = 0\n","        \n","    def get_lr(self):\n","        return self.model_size ** (-0.5) * min(self.current_step ** (-0.5), self.current_step * self.warmup_steps ** (-1.5))\n","        \n","    def step(self):\n","        # Increment the number of steps each time we call the step function\n","        self.current_step += 1\n","        lr = self.get_lr()\n","        for param_group in self.optimizer.param_groups:\n","            param_group['lr'] = lr\n","        # update the learning rate\n","        self.lr = lr\n","        self.optimizer.step()       \n","        "]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:33.529363Z","iopub.status.busy":"2024-07-02T20:17:33.528622Z","iopub.status.idle":"2024-07-02T20:17:33.540886Z","shell.execute_reply":"2024-07-02T20:17:33.540153Z","shell.execute_reply.started":"2024-07-02T20:17:33.529331Z"},"trusted":true},"outputs":[],"source":["class LossWithLS(nn.Module):\n","\n","    def __init__(self, size, smooth):\n","        super(LossWithLS, self).__init__()\n","        self.criterion = nn.KLDivLoss(size_average=False, reduce=False)\n","        self.confidence = 1.0 - smooth\n","        self.smooth = smooth\n","        self.size = size\n","        \n","    def forward(self, prediction, target, mask):\n","        \"\"\"\n","        prediction of shape: (batch_size, max_words, vocab_size)\n","        target and mask of shape: (batch_size, max_words)\n","        \"\"\"\n","        prediction = prediction.view(-1, prediction.size(-1))   # (batch_size * max_words, vocab_size)\n","        target = target.contiguous().view(-1)   # (batch_size * max_words)\n","        mask = mask.float()\n","        mask = mask.view(-1)       # (batch_size * max_words)\n","        labels = prediction.data.clone()\n","        labels.fill_(self.smooth / (self.size - 1))\n","        labels.scatter_(1, target.data.unsqueeze(1), self.confidence)\n","        loss = self.criterion(prediction, labels)    # (batch_size * max_words, vocab_size)\n","        loss = (loss.sum(1) * mask).sum() / mask.sum()\n","        return loss\n","        "]},{"cell_type":"markdown","metadata":{},"source":["# Define Neptune Experiment"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:33.542496Z","iopub.status.busy":"2024-07-02T20:17:33.542228Z","iopub.status.idle":"2024-07-02T20:17:33.550662Z","shell.execute_reply":"2024-07-02T20:17:33.549840Z","shell.execute_reply.started":"2024-07-02T20:17:33.542473Z"},"trusted":true},"outputs":[],"source":["project = \"andialifs/fluent-tesis-24\"\n","api_token = \"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJjZTY2YWQ3My04OTBkLTQ2OWUtYTc1Ni1jYjk0MGZhMWFiNGEifQ==\"\n","\n","def neptune_init(name):\n","    run = neptune.init_run(\n","        project=project,\n","        api_token=api_token,\n","        name=name\n","    )\n","    return run"]},{"cell_type":"markdown","metadata":{},"source":["# Function"]},{"cell_type":"markdown","metadata":{},"source":["## Train"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:33.564996Z","iopub.status.busy":"2024-07-02T20:17:33.564656Z","iopub.status.idle":"2024-07-02T20:17:33.574590Z","shell.execute_reply":"2024-07-02T20:17:33.573653Z","shell.execute_reply.started":"2024-07-02T20:17:33.564947Z"},"trusted":true},"outputs":[],"source":["def train(train_loader, transformer, criterion, epoch):\n","    transformer.train()\n","    sum_loss = 0\n","    count = 0\n","\n","    for i, (question, reply) in enumerate(train_loader):\n","        \n","        samples = question.shape[0]\n","\n","        # Move to device\n","        question = question.to(device)\n","        reply = reply.to(device)\n","\n","        # Prepare Target Data\n","        reply_input = reply[:, :-1]\n","        reply_target = reply[:, 1:]\n","\n","        # Create mask and add dimensions\n","        question_mask, reply_input_mask, reply_target_mask = create_masks(question, reply_input, reply_target)\n","\n","        # Get the transformer outputs\n","        out = transformer(question, question_mask, reply_input, reply_input_mask)\n","\n","        # Compute the loss\n","        loss = criterion(out, reply_target, reply_target_mask)\n","        \n","        # Backprop\n","        transformer_optimizer.optimizer.zero_grad()\n","        loss.backward()\n","        transformer_optimizer.step()\n","        \n","        sum_loss += loss.item() * samples\n","        count += samples\n","        \n","        if i % 100 == 0:\n","            print(\"Epoch [{}][{}/{}]\\tLoss: {:.3f}\".format(epoch, i, len(train_loader), sum_loss/count))\n","    \n","    return sum_loss/count"]},{"cell_type":"markdown","metadata":{},"source":["## Evaluate"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:33.576127Z","iopub.status.busy":"2024-07-02T20:17:33.575799Z","iopub.status.idle":"2024-07-02T20:17:33.587390Z","shell.execute_reply":"2024-07-02T20:17:33.586545Z","shell.execute_reply.started":"2024-07-02T20:17:33.576103Z"},"trusted":true},"outputs":[],"source":["def evaluate(transformer, question, question_mask, max_len, word_map):\n","    \"\"\"\n","    Performs Greedy Decoding with a batch size of 1\n","    \"\"\"\n","    rev_word_map = {v: k for k, v in word_map.items()}\n","    transformer.eval()\n","    start_token = word_map['<start>']\n","    encoded = transformer.encode(question, question_mask)\n","    words = torch.LongTensor([[start_token]]).to(device)\n","    \n","    for step in range(max_len - 1):\n","        size = words.shape[1]\n","        target_mask = torch.triu(torch.ones(size, size)).transpose(0, 1).type(dtype=torch.uint8)\n","        target_mask = target_mask.to(device).unsqueeze(0).unsqueeze(0)\n","        decoded = transformer.decode(words, target_mask, encoded, question_mask)\n","        predictions = transformer.logit(decoded[:, -1])\n","        _, next_word = torch.max(predictions, dim = 1)\n","        next_word = next_word.item()\n","        if next_word == word_map['<end>']:\n","            break\n","        words = torch.cat([words, torch.LongTensor([[next_word]]).to(device)], dim = 1)   # (1,step+2)\n","        \n","    # Construct Sentence\n","    if words.dim() == 2:\n","        words = words.squeeze(0)\n","        words = words.tolist()\n","        \n","    sen_idx = [w for w in words if w not in {word_map['<start>']}]\n","    sentence = ' '.join([rev_word_map[sen_idx[k]] for k in range(len(sen_idx))])\n","    \n","    return sentence"]},{"cell_type":"markdown","metadata":{},"source":["# Experiment"]},{"cell_type":"markdown","metadata":{},"source":["## Transformers without reg"]},{"cell_type":"code","execution_count":40,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:33.588788Z","iopub.status.busy":"2024-07-02T20:17:33.588528Z","iopub.status.idle":"2024-07-02T20:17:47.395240Z","shell.execute_reply":"2024-07-02T20:17:47.393722Z","shell.execute_reply.started":"2024-07-02T20:17:33.588766Z"},"scrolled":true,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Skipping experiment 0 with d_model 512, heads8, num_layers5\n","\n","\n","Skipping experiment 1 with d_model 512, heads8, num_layers10\n","\n","\n","Skipping experiment 2 with d_model 512, heads16, num_layers5\n","\n","\n","Skipping experiment 3 with d_model 512, heads16, num_layers10\n","\n","\n","Skipping experiment 4 with d_model 512, heads32, num_layers5\n","\n","\n","Skipping experiment 5 with d_model 512, heads32, num_layers10\n","\n","\n","Skipping experiment 6 with d_model 1024, heads8, num_layers5\n","\n","\n","Skipping experiment 7 with d_model 1024, heads8, num_layers10\n","\n","\n","Skipping experiment 8 with d_model 1024, heads16, num_layers5\n","\n","\n","Skipping experiment 9 with d_model 1024, heads16, num_layers10\n","\n","\n","Skipping experiment 10 with d_model 1024, heads32, num_layers5\n","\n","\n","Skipping experiment 11 with d_model 1024, heads32, num_layers10\n","\n","\n","Skipping experiment 12 with d_model 2048, heads8, num_layers5\n","\n","\n","Skipping experiment 13 with d_model 2048, heads8, num_layers10\n","\n","\n","Skipping experiment 14 with d_model 2048, heads16, num_layers5\n","\n","\n","Running for experiment 15 with d_model 2048, heads16, num_layers10\n","\n","[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/andialifs/fluent-tesis-24/e/FLUENT24-79\n"]},{"name":"stderr","output_type":"stream","text":["/home/andyalyfsyah/.conda/envs/myenv/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [0][0/12]\tLoss: 5.156\n","Epoch [1][0/12]\tLoss: 5.049\n","Epoch [2][0/12]\tLoss: 4.608\n","Epoch [3][0/12]\tLoss: 4.526\n","Epoch [4][0/12]\tLoss: 4.342\n","Epoch [5][0/12]\tLoss: 4.071\n","Epoch [6][0/12]\tLoss: 4.036\n","Epoch [7][0/12]\tLoss: 4.092\n","Epoch [8][0/12]\tLoss: 4.076\n","Epoch [10][0/12]\tLoss: 4.040\n","Epoch [11][0/12]\tLoss: 3.927\n","Epoch [12][0/12]\tLoss: 4.045\n","Epoch [13][0/12]\tLoss: 3.989\n","Epoch [14][0/12]\tLoss: 3.873\n","Epoch [15][0/12]\tLoss: 3.832\n","Epoch [16][0/12]\tLoss: 4.009\n","Epoch [17][0/12]\tLoss: 3.784\n","Epoch [18][0/12]\tLoss: 3.777\n","Epoch [19][0/12]\tLoss: 3.729\n","Epoch [20][0/12]\tLoss: 3.806\n","Epoch [21][0/12]\tLoss: 3.710\n","Epoch [22][0/12]\tLoss: 3.549\n","Epoch [23][0/12]\tLoss: 3.696\n","Epoch [24][0/12]\tLoss: 3.566\n","Epoch [25][0/12]\tLoss: 3.625\n","Epoch [26][0/12]\tLoss: 3.291\n","Epoch [27][0/12]\tLoss: 3.621\n","Epoch [28][0/12]\tLoss: 3.374\n","Epoch [29][0/12]\tLoss: 3.372\n","Epoch [30][0/12]\tLoss: 3.402\n","Epoch [31][0/12]\tLoss: 3.352\n","Epoch [32][0/12]\tLoss: 3.220\n","Epoch [33][0/12]\tLoss: 3.341\n","Epoch [34][0/12]\tLoss: 3.450\n","Epoch [35][0/12]\tLoss: 3.313\n","Epoch [36][0/12]\tLoss: 3.215\n","Epoch [37][0/12]\tLoss: 3.150\n","Epoch [38][0/12]\tLoss: 3.073\n","Epoch [39][0/12]\tLoss: 3.236\n","Epoch [40][0/12]\tLoss: 3.079\n","Epoch [41][0/12]\tLoss: 3.015\n","Epoch [42][0/12]\tLoss: 3.096\n","Epoch [43][0/12]\tLoss: 2.833\n","Epoch [44][0/12]\tLoss: 2.826\n","Epoch [45][0/12]\tLoss: 2.926\n","Epoch [46][0/12]\tLoss: 2.852\n","Epoch [47][0/12]\tLoss: 2.992\n","Epoch [48][0/12]\tLoss: 2.755\n","Epoch [49][0/12]\tLoss: 2.800\n","Epoch [50][0/12]\tLoss: 2.844\n","Epoch [51][0/12]\tLoss: 2.690\n","Epoch [52][0/12]\tLoss: 2.430\n","Epoch [53][0/12]\tLoss: 2.623\n","Epoch [54][0/12]\tLoss: 2.358\n","Epoch [55][0/12]\tLoss: 2.793\n","Epoch [56][0/12]\tLoss: 2.614\n","Epoch [57][0/12]\tLoss: 2.677\n","Epoch [58][0/12]\tLoss: 2.433\n","Epoch [59][0/12]\tLoss: 2.587\n","Epoch [60][0/12]\tLoss: 2.452\n","Epoch [61][0/12]\tLoss: 2.386\n","Epoch [62][0/12]\tLoss: 2.514\n","Epoch [63][0/12]\tLoss: 2.421\n","Epoch [64][0/12]\tLoss: 2.406\n","Epoch [65][0/12]\tLoss: 2.303\n","Epoch [66][0/12]\tLoss: 2.357\n","Epoch [67][0/12]\tLoss: 2.380\n","Epoch [68][0/12]\tLoss: 2.248\n","Epoch [69][0/12]\tLoss: 2.223\n","Epoch [70][0/12]\tLoss: 2.252\n","Epoch [71][0/12]\tLoss: 2.187\n","Epoch [72][0/12]\tLoss: 2.083\n","Epoch [73][0/12]\tLoss: 2.149\n","Epoch [74][0/12]\tLoss: 2.138\n","Epoch [75][0/12]\tLoss: 2.081\n","Epoch [76][0/12]\tLoss: 2.023\n","Epoch [77][0/12]\tLoss: 1.927\n","Epoch [78][0/12]\tLoss: 1.831\n","Epoch [79][0/12]\tLoss: 1.895\n","Epoch [80][0/12]\tLoss: 1.996\n","Epoch [81][0/12]\tLoss: 1.795\n","Epoch [82][0/12]\tLoss: 1.748\n","Epoch [83][0/12]\tLoss: 1.919\n","Epoch [84][0/12]\tLoss: 1.687\n","Epoch [85][0/12]\tLoss: 1.766\n","Epoch [86][0/12]\tLoss: 1.706\n","Epoch [87][0/12]\tLoss: 1.754\n","Epoch [88][0/12]\tLoss: 1.608\n","Epoch [89][0/12]\tLoss: 1.318\n","Epoch [90][0/12]\tLoss: 1.326\n","Epoch [91][0/12]\tLoss: 1.410\n","Epoch [92][0/12]\tLoss: 1.348\n","Epoch [93][0/12]\tLoss: 1.350\n","Epoch [94][0/12]\tLoss: 1.651\n","Epoch [95][0/12]\tLoss: 1.399\n","Epoch [96][0/12]\tLoss: 1.349\n","Epoch [97][0/12]\tLoss: 1.313\n","Epoch [98][0/12]\tLoss: 1.172\n","Epoch [99][0/12]\tLoss: 1.056\n","[neptune] [info   ] Shutting down background jobs, please wait a moment...\n","[neptune] [info   ] Done!\n","[neptune] [info   ] Waiting for the remaining 1 operations to synchronize with Neptune. Do not kill this process.\n","[neptune] [info   ] All 1 operations synced, thanks for waiting!\n","[neptune] [info   ] Explore the metadata in the Neptune app: https://app.neptune.ai/andialifs/fluent-tesis-24/e/FLUENT24-79/metadata\n","\n","Running for experiment 16 with d_model 2048, heads32, num_layers5\n","\n","[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/andialifs/fluent-tesis-24/e/FLUENT24-80\n"]},{"name":"stderr","output_type":"stream","text":["/home/andyalyfsyah/.conda/envs/myenv/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [0][0/12]\tLoss: 5.202\n","Epoch [1][0/12]\tLoss: 5.062\n","Epoch [2][0/12]\tLoss: 4.734\n","Epoch [3][0/12]\tLoss: 4.371\n","Epoch [4][0/12]\tLoss: 4.236\n","Epoch [5][0/12]\tLoss: 4.167\n","Epoch [6][0/12]\tLoss: 4.178\n","Epoch [7][0/12]\tLoss: 3.889\n","Epoch [8][0/12]\tLoss: 4.102\n","Epoch [9][0/12]\tLoss: 4.067\n","Epoch [10][0/12]\tLoss: 4.009\n","Epoch [11][0/12]\tLoss: 3.869\n","Epoch [12][0/12]\tLoss: 3.987\n","Epoch [13][0/12]\tLoss: 3.984\n","Epoch [14][0/12]\tLoss: 3.879\n","Epoch [15][0/12]\tLoss: 3.601\n","Epoch [16][0/12]\tLoss: 3.539\n","Epoch [17][0/12]\tLoss: 3.507\n","Epoch [18][0/12]\tLoss: 3.415\n","Epoch [19][0/12]\tLoss: 3.446\n","Epoch [20][0/12]\tLoss: 3.041\n","Epoch [21][0/12]\tLoss: 3.090\n","Epoch [22][0/12]\tLoss: 3.004\n","Epoch [23][0/12]\tLoss: 2.906\n","Epoch [24][0/12]\tLoss: 3.072\n","Epoch [25][0/12]\tLoss: 2.878\n","Epoch [26][0/12]\tLoss: 2.795\n","Epoch [27][0/12]\tLoss: 2.622\n","Epoch [28][0/12]\tLoss: 2.341\n","Epoch [29][0/12]\tLoss: 2.367\n","Epoch [30][0/12]\tLoss: 2.489\n","Epoch [31][0/12]\tLoss: 2.395\n","Epoch [32][0/12]\tLoss: 2.238\n","Epoch [33][0/12]\tLoss: 2.099\n","Epoch [34][0/12]\tLoss: 1.988\n","Epoch [35][0/12]\tLoss: 1.993\n","Epoch [36][0/12]\tLoss: 1.909\n","Epoch [37][0/12]\tLoss: 2.032\n","Epoch [38][0/12]\tLoss: 1.927\n","Epoch [39][0/12]\tLoss: 1.988\n","Epoch [40][0/12]\tLoss: 1.965\n","Epoch [41][0/12]\tLoss: 1.662\n","Epoch [42][0/12]\tLoss: 1.531\n","Epoch [43][0/12]\tLoss: 1.570\n","Epoch [44][0/12]\tLoss: 1.602\n","Epoch [45][0/12]\tLoss: 1.381\n","Epoch [46][0/12]\tLoss: 1.510\n","Epoch [47][0/12]\tLoss: 1.471\n","Epoch [48][0/12]\tLoss: 1.307\n","Epoch [49][0/12]\tLoss: 1.339\n","Epoch [50][0/12]\tLoss: 1.392\n","Epoch [51][0/12]\tLoss: 1.306\n","Epoch [52][0/12]\tLoss: 1.121\n","Epoch [53][0/12]\tLoss: 1.202\n","Epoch [54][0/12]\tLoss: 1.350\n","Epoch [55][0/12]\tLoss: 1.175\n","Epoch [56][0/12]\tLoss: 1.198\n","Epoch [57][0/12]\tLoss: 1.011\n","Epoch [58][0/12]\tLoss: 1.118\n","Epoch [59][0/12]\tLoss: 1.034\n","Epoch [60][0/12]\tLoss: 0.926\n","Epoch [61][0/12]\tLoss: 1.111\n","Epoch [62][0/12]\tLoss: 0.784\n","Epoch [63][0/12]\tLoss: 0.815\n","Epoch [64][0/12]\tLoss: 0.840\n","Epoch [65][0/12]\tLoss: 0.768\n","Epoch [66][0/12]\tLoss: 0.771\n","Epoch [67][0/12]\tLoss: 0.658\n","Epoch [68][0/12]\tLoss: 0.812\n","Epoch [69][0/12]\tLoss: 0.702\n","Epoch [70][0/12]\tLoss: 0.730\n","Epoch [71][0/12]\tLoss: 0.724\n","Epoch [72][0/12]\tLoss: 0.807\n","Epoch [73][0/12]\tLoss: 0.674\n","Epoch [74][0/12]\tLoss: 0.601\n","Epoch [75][0/12]\tLoss: 0.589\n","Epoch [76][0/12]\tLoss: 0.576\n","Epoch [77][0/12]\tLoss: 0.529\n","Epoch [78][0/12]\tLoss: 0.443\n","Epoch [79][0/12]\tLoss: 0.514\n","Epoch [80][0/12]\tLoss: 0.542\n","Epoch [81][0/12]\tLoss: 0.477\n","Epoch [82][0/12]\tLoss: 0.458\n","Epoch [83][0/12]\tLoss: 0.475\n","Epoch [84][0/12]\tLoss: 0.467\n","Epoch [85][0/12]\tLoss: 0.461\n","Epoch [86][0/12]\tLoss: 0.364\n","Epoch [87][0/12]\tLoss: 0.450\n","Epoch [88][0/12]\tLoss: 0.365\n","Epoch [89][0/12]\tLoss: 0.408\n","Epoch [90][0/12]\tLoss: 0.331\n","Epoch [91][0/12]\tLoss: 0.373\n","Epoch [92][0/12]\tLoss: 0.382\n","Epoch [93][0/12]\tLoss: 0.348\n","Epoch [94][0/12]\tLoss: 0.379\n","Epoch [95][0/12]\tLoss: 0.328\n","Epoch [96][0/12]\tLoss: 0.324\n","Epoch [97][0/12]\tLoss: 0.351\n","Epoch [98][0/12]\tLoss: 0.310\n","Epoch [99][0/12]\tLoss: 0.287\n","[neptune] [info   ] Shutting down background jobs, please wait a moment...\n","[neptune] [info   ] Done!\n","[neptune] [info   ] Waiting for the remaining 1 operations to synchronize with Neptune. Do not kill this process.\n","[neptune] [info   ] All 1 operations synced, thanks for waiting!\n","[neptune] [info   ] Explore the metadata in the Neptune app: https://app.neptune.ai/andialifs/fluent-tesis-24/e/FLUENT24-80/metadata\n","\n","Running for experiment 17 with d_model 2048, heads32, num_layers10\n","\n","[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/andialifs/fluent-tesis-24/e/FLUENT24-81\n"]},{"name":"stderr","output_type":"stream","text":["/home/andyalyfsyah/.conda/envs/myenv/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [0][0/12]\tLoss: 5.305\n","Epoch [1][0/12]\tLoss: 5.135\n","Epoch [2][0/12]\tLoss: 4.741\n","Epoch [3][0/12]\tLoss: 4.543\n","Epoch [4][0/12]\tLoss: 4.287\n","Epoch [5][0/12]\tLoss: 4.217\n","Epoch [6][0/12]\tLoss: 4.077\n","Epoch [7][0/12]\tLoss: 4.044\n","Epoch [8][0/12]\tLoss: 4.000\n","Epoch [9][0/12]\tLoss: 4.013\n","Epoch [10][0/12]\tLoss: 4.043\n","Epoch [11][0/12]\tLoss: 4.042\n","Epoch [12][0/12]\tLoss: 3.944\n","Epoch [13][0/12]\tLoss: 3.967\n","Epoch [14][0/12]\tLoss: 3.917\n","Epoch [15][0/12]\tLoss: 3.865\n","Epoch [16][0/12]\tLoss: 3.986\n","Epoch [17][0/12]\tLoss: 3.857\n","Epoch [18][0/12]\tLoss: 3.783\n","Epoch [19][0/12]\tLoss: 3.795\n","Epoch [20][0/12]\tLoss: 3.743\n","Epoch [21][0/12]\tLoss: 3.673\n","Epoch [22][0/12]\tLoss: 3.752\n","Epoch [23][0/12]\tLoss: 3.691\n","Epoch [24][0/12]\tLoss: 3.647\n","Epoch [25][0/12]\tLoss: 3.610\n","Epoch [26][0/12]\tLoss: 3.487\n","Epoch [27][0/12]\tLoss: 3.622\n","Epoch [28][0/12]\tLoss: 3.374\n","Epoch [29][0/12]\tLoss: 3.396\n","Epoch [30][0/12]\tLoss: 3.317\n","Epoch [31][0/12]\tLoss: 3.426\n","Epoch [32][0/12]\tLoss: 3.413\n","Epoch [33][0/12]\tLoss: 3.393\n","Epoch [34][0/12]\tLoss: 3.228\n","Epoch [35][0/12]\tLoss: 3.093\n","Epoch [36][0/12]\tLoss: 3.052\n","Epoch [37][0/12]\tLoss: 3.220\n","Epoch [38][0/12]\tLoss: 3.218\n","Epoch [39][0/12]\tLoss: 3.080\n","Epoch [40][0/12]\tLoss: 2.977\n","Epoch [41][0/12]\tLoss: 2.940\n","Epoch [42][0/12]\tLoss: 2.896\n","Epoch [43][0/12]\tLoss: 3.079\n","Epoch [44][0/12]\tLoss: 2.881\n","Epoch [45][0/12]\tLoss: 3.001\n","Epoch [46][0/12]\tLoss: 2.811\n","Epoch [47][0/12]\tLoss: 2.855\n","Epoch [48][0/12]\tLoss: 2.644\n","Epoch [49][0/12]\tLoss: 2.708\n","Epoch [50][0/12]\tLoss: 2.783\n","Epoch [51][0/12]\tLoss: 2.823\n","Epoch [52][0/12]\tLoss: 2.476\n","Epoch [53][0/12]\tLoss: 2.595\n","Epoch [54][0/12]\tLoss: 2.694\n","Epoch [55][0/12]\tLoss: 2.631\n","Epoch [56][0/12]\tLoss: 2.455\n","Epoch [57][0/12]\tLoss: 2.441\n","Epoch [58][0/12]\tLoss: 2.464\n","Epoch [59][0/12]\tLoss: 2.498\n","Epoch [60][0/12]\tLoss: 2.597\n","Epoch [61][0/12]\tLoss: 2.546\n","Epoch [62][0/12]\tLoss: 2.499\n","Epoch [63][0/12]\tLoss: 2.282\n","Epoch [64][0/12]\tLoss: 2.218\n","Epoch [65][0/12]\tLoss: 2.016\n","Epoch [66][0/12]\tLoss: 2.438\n","Epoch [67][0/12]\tLoss: 2.139\n","Epoch [68][0/12]\tLoss: 2.094\n","Epoch [69][0/12]\tLoss: 2.236\n","Epoch [70][0/12]\tLoss: 1.997\n","Epoch [71][0/12]\tLoss: 2.103\n","Epoch [72][0/12]\tLoss: 1.934\n","Epoch [73][0/12]\tLoss: 2.166\n","Epoch [74][0/12]\tLoss: 2.049\n","Epoch [75][0/12]\tLoss: 2.011\n","Epoch [76][0/12]\tLoss: 1.968\n","Epoch [77][0/12]\tLoss: 1.810\n","Epoch [78][0/12]\tLoss: 2.016\n","Epoch [79][0/12]\tLoss: 1.960\n","Epoch [80][0/12]\tLoss: 1.731\n","Epoch [81][0/12]\tLoss: 1.669\n","Epoch [82][0/12]\tLoss: 1.734\n","Epoch [83][0/12]\tLoss: 1.591\n","Epoch [84][0/12]\tLoss: 1.711\n","Epoch [85][0/12]\tLoss: 1.498\n","Epoch [86][0/12]\tLoss: 1.657\n","Epoch [87][0/12]\tLoss: 1.552\n","Epoch [88][0/12]\tLoss: 1.631\n","Epoch [89][0/12]\tLoss: 1.503\n","Epoch [90][0/12]\tLoss: 1.258\n","Epoch [91][0/12]\tLoss: 1.475\n","Epoch [92][0/12]\tLoss: 1.373\n","Epoch [93][0/12]\tLoss: 1.336\n","Epoch [94][0/12]\tLoss: 1.105\n","Epoch [95][0/12]\tLoss: 1.380\n","Epoch [96][0/12]\tLoss: 1.190\n","Epoch [97][0/12]\tLoss: 1.081\n","Epoch [98][0/12]\tLoss: 1.020\n","Epoch [99][0/12]\tLoss: 1.027\n","[neptune] [info   ] Shutting down background jobs, please wait a moment...\n","[neptune] [info   ] Done!\n","[neptune] [info   ] Waiting for the remaining 1 operations to synchronize with Neptune. Do not kill this process.\n","[neptune] [info   ] All 1 operations synced, thanks for waiting!\n","[neptune] [info   ] Explore the metadata in the Neptune app: https://app.neptune.ai/andialifs/fluent-tesis-24/e/FLUENT24-81/metadata\n","\n","Running for experiment 18 with d_model 4096, heads8, num_layers5\n","\n","[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/andialifs/fluent-tesis-24/e/FLUENT24-82\n"]},{"name":"stderr","output_type":"stream","text":["/home/andyalyfsyah/.conda/envs/myenv/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [0][0/12]\tLoss: 5.245\n","Epoch [1][0/12]\tLoss: 4.997\n","Epoch [2][0/12]\tLoss: 4.554\n","Epoch [3][0/12]\tLoss: 4.162\n","Epoch [4][0/12]\tLoss: 4.080\n","Epoch [5][0/12]\tLoss: 4.062\n","Epoch [6][0/12]\tLoss: 4.088\n","Epoch [7][0/12]\tLoss: 4.035\n","Epoch [8][0/12]\tLoss: 3.864\n","Epoch [9][0/12]\tLoss: 4.037\n","Epoch [10][0/12]\tLoss: 3.855\n","Epoch [11][0/12]\tLoss: 3.762\n","Epoch [12][0/12]\tLoss: 3.771\n","Epoch [13][0/12]\tLoss: 3.803\n","Epoch [14][0/12]\tLoss: 3.614\n","Epoch [15][0/12]\tLoss: 3.478\n","Epoch [16][0/12]\tLoss: 3.437\n","Epoch [17][0/12]\tLoss: 3.238\n","Epoch [18][0/12]\tLoss: 3.247\n","Epoch [19][0/12]\tLoss: 3.001\n","Epoch [20][0/12]\tLoss: 3.000\n","Epoch [21][0/12]\tLoss: 2.792\n","Epoch [22][0/12]\tLoss: 2.444\n","Epoch [23][0/12]\tLoss: 2.828\n","Epoch [24][0/12]\tLoss: 2.557\n","Epoch [25][0/12]\tLoss: 2.459\n","Epoch [26][0/12]\tLoss: 2.480\n","Epoch [27][0/12]\tLoss: 2.088\n","Epoch [28][0/12]\tLoss: 2.233\n","Epoch [29][0/12]\tLoss: 2.347\n","Epoch [30][0/12]\tLoss: 2.040\n","Epoch [31][0/12]\tLoss: 2.020\n","Epoch [32][0/12]\tLoss: 1.896\n","Epoch [33][0/12]\tLoss: 1.625\n","Epoch [34][0/12]\tLoss: 1.695\n","Epoch [35][0/12]\tLoss: 1.752\n","Epoch [36][0/12]\tLoss: 1.688\n","Epoch [37][0/12]\tLoss: 1.393\n","Epoch [38][0/12]\tLoss: 1.523\n","Epoch [39][0/12]\tLoss: 1.231\n","Epoch [40][0/12]\tLoss: 1.479\n","Epoch [41][0/12]\tLoss: 1.490\n","Epoch [42][0/12]\tLoss: 1.258\n","Epoch [43][0/12]\tLoss: 1.278\n","Epoch [44][0/12]\tLoss: 1.304\n","Epoch [45][0/12]\tLoss: 1.064\n","Epoch [46][0/12]\tLoss: 0.996\n","Epoch [47][0/12]\tLoss: 0.996\n","Epoch [48][0/12]\tLoss: 1.079\n","Epoch [49][0/12]\tLoss: 1.046\n","Epoch [50][0/12]\tLoss: 0.769\n","Epoch [51][0/12]\tLoss: 0.808\n","Epoch [52][0/12]\tLoss: 0.777\n","Epoch [53][0/12]\tLoss: 0.814\n","Epoch [54][0/12]\tLoss: 0.874\n","Epoch [55][0/12]\tLoss: 0.815\n","Epoch [56][0/12]\tLoss: 0.647\n","Epoch [57][0/12]\tLoss: 0.693\n","Epoch [58][0/12]\tLoss: 0.626\n","Epoch [59][0/12]\tLoss: 0.626\n","Epoch [60][0/12]\tLoss: 0.680\n","Epoch [61][0/12]\tLoss: 0.695\n","Epoch [62][0/12]\tLoss: 0.594\n","Epoch [63][0/12]\tLoss: 0.511\n","Epoch [64][0/12]\tLoss: 0.570\n","Epoch [65][0/12]\tLoss: 0.477\n","Epoch [66][0/12]\tLoss: 0.540\n","Epoch [67][0/12]\tLoss: 0.542\n","Epoch [68][0/12]\tLoss: 0.456\n","Epoch [69][0/12]\tLoss: 0.485\n","Epoch [70][0/12]\tLoss: 0.427\n","Epoch [71][0/12]\tLoss: 0.431\n","Epoch [72][0/12]\tLoss: 0.426\n","Epoch [73][0/12]\tLoss: 0.354\n","Epoch [74][0/12]\tLoss: 0.373\n","Epoch [75][0/12]\tLoss: 0.413\n","Epoch [76][0/12]\tLoss: 0.341\n","Epoch [77][0/12]\tLoss: 0.408\n","Epoch [78][0/12]\tLoss: 0.346\n","Epoch [79][0/12]\tLoss: 0.352\n","Epoch [80][0/12]\tLoss: 0.337\n","Epoch [81][0/12]\tLoss: 0.351\n","Epoch [82][0/12]\tLoss: 0.352\n","Epoch [83][0/12]\tLoss: 0.305\n","Epoch [84][0/12]\tLoss: 0.275\n","Epoch [85][0/12]\tLoss: 0.304\n","Epoch [86][0/12]\tLoss: 0.282\n","Epoch [87][0/12]\tLoss: 0.278\n","Epoch [88][0/12]\tLoss: 0.264\n","Epoch [89][0/12]\tLoss: 0.275\n","Epoch [90][0/12]\tLoss: 0.299\n","Epoch [91][0/12]\tLoss: 0.298\n","Epoch [92][0/12]\tLoss: 0.266\n","Epoch [93][0/12]\tLoss: 0.248\n","Epoch [94][0/12]\tLoss: 0.258\n","Epoch [95][0/12]\tLoss: 0.251\n","Epoch [96][0/12]\tLoss: 0.250\n","Epoch [97][0/12]\tLoss: 0.205\n","Epoch [98][0/12]\tLoss: 0.266\n","Epoch [99][0/12]\tLoss: 0.235\n","[neptune] [info   ] Shutting down background jobs, please wait a moment...\n","[neptune] [info   ] Done!\n","[neptune] [info   ] Waiting for the remaining 1 operations to synchronize with Neptune. Do not kill this process.\n","[neptune] [info   ] All 1 operations synced, thanks for waiting!\n","[neptune] [info   ] Explore the metadata in the Neptune app: https://app.neptune.ai/andialifs/fluent-tesis-24/e/FLUENT24-82/metadata\n","\n","Running for experiment 19 with d_model 4096, heads8, num_layers10\n","\n","[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/andialifs/fluent-tesis-24/e/FLUENT24-83\n"]},{"name":"stderr","output_type":"stream","text":["/home/andyalyfsyah/.conda/envs/myenv/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [0][0/12]\tLoss: 5.212\n","Epoch [1][0/12]\tLoss: 5.006\n","Epoch [2][0/12]\tLoss: 4.476\n","Epoch [3][0/12]\tLoss: 4.137\n","Epoch [4][0/12]\tLoss: 4.158\n","Epoch [5][0/12]\tLoss: 4.043\n","Epoch [6][0/12]\tLoss: 4.092\n","Epoch [7][0/12]\tLoss: 4.070\n","Epoch [8][0/12]\tLoss: 3.994\n","Epoch [9][0/12]\tLoss: 3.931\n","Epoch [10][0/12]\tLoss: 4.079\n","Epoch [11][0/12]\tLoss: 4.009\n","Epoch [12][0/12]\tLoss: 3.951\n","Epoch [13][0/12]\tLoss: 3.858\n","Epoch [14][0/12]\tLoss: 3.801\n","Epoch [15][0/12]\tLoss: 3.749\n","Epoch [16][0/12]\tLoss: 3.854\n","Epoch [17][0/12]\tLoss: 3.660\n","Epoch [18][0/12]\tLoss: 3.801\n","Epoch [19][0/12]\tLoss: 3.621\n","Epoch [20][0/12]\tLoss: 3.695\n","Epoch [21][0/12]\tLoss: 3.556\n","Epoch [22][0/12]\tLoss: 3.574\n","Epoch [23][0/12]\tLoss: 3.519\n","Epoch [24][0/12]\tLoss: 3.421\n","Epoch [25][0/12]\tLoss: 3.348\n","Epoch [26][0/12]\tLoss: 3.515\n","Epoch [27][0/12]\tLoss: 3.439\n","Epoch [28][0/12]\tLoss: 3.519\n","Epoch [29][0/12]\tLoss: 3.381\n","Epoch [30][0/12]\tLoss: 3.212\n","Epoch [31][0/12]\tLoss: 3.406\n","Epoch [32][0/12]\tLoss: 3.111\n","Epoch [33][0/12]\tLoss: 3.181\n","Epoch [34][0/12]\tLoss: 3.225\n","Epoch [35][0/12]\tLoss: 2.917\n","Epoch [36][0/12]\tLoss: 3.080\n","Epoch [37][0/12]\tLoss: 3.067\n","Epoch [38][0/12]\tLoss: 3.156\n","Epoch [39][0/12]\tLoss: 2.822\n","Epoch [40][0/12]\tLoss: 2.986\n"]}],"source":["d_model = [512, 1024, 2048, 4096]\n","heads = [8, 16, 32]\n","num_layers = [5, 10]\n","epochs = 100\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","with open('WORDMAP_corpus_KBFILKOM.json', 'r') as j:\n","    word_map = json.load(j)\n","\n","transformer_experiment = pd.DataFrame(columns = ['experiment_id', 'd_model', 'heads', 'num_layers', 'train_loss'])\n","loss_history = {}\n","\n","experiment_id = -1\n","\n","for d_m in d_model:\n","    for h in heads:\n","        for n_l in num_layers: \n","            parameters = {\n","                'd_model': d_m,\n","                'heads': h,\n","                'num_layers': n_l\n","            }\n","            \n","            experiment_id += 1\n","            \n","            if experiment_id <= 14:\n","                print('\\nSkipping experiment {} with d_model {}, heads{}, num_layers{}\\n'.format(experiment_id, d_m, h, n_l))\n","                continue\n","            \n","            print('\\nRunning for experiment {} with d_model {}, heads{}, num_layers{}\\n'.format(experiment_id, d_m, h, n_l))\n","            name = \"experiment_2724_noreg_\" + str(experiment_id)\n","\n","            run = neptune_init(name)\n","            run['parameters'] = parameters\n","            run['tags'] = \"transformers-vanilla-no-reg\"\n","            \n","            transformer_experiment.loc[experiment_id, 'experiment_id'] = 'experiment_{}'.format(str(experiment_id))\n","            transformer_experiment.loc[experiment_id, 'd_model'] = d_m\n","            transformer_experiment.loc[experiment_id, 'heads'] = h\n","            transformer_experiment.loc[experiment_id, 'num_layers'] = n_l\n","\n","            transformer = Transformer(d_model = d_m, heads = h, num_layers = n_l, word_map = word_map, max_len=95)\n","            transformer = transformer.to(device)\n","            adam_optimizer = torch.optim.Adam(transformer.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-9)\n","            transformer_optimizer = AdamWarmup(model_size = d_m, warmup_steps = 4000, optimizer = adam_optimizer)\n","            criterion = LossWithLS(len(word_map), 0.2)\n","\n","            loss_list_experiment = []\n","            for epoch in range(epochs):\n","                loss_train = train(train_loader, transformer, criterion, epoch)\n","\n","                state = {'epoch': epoch, 'transformer': transformer, 'transformer_optimizer': transformer_optimizer}\n","                # torch.save(state, 'checkpoint_experiment_' + str(epoch) + 'id_'+ str(experiment_id) +'.pth.tar')\n","\n","                loss_list_experiment.append(loss_train)\n","\n","                run['train/loss'].append(loss_train)\n","\n","            transformer_experiment.loc[experiment_id, 'train_loss'] = loss_train\n","            loss_history['experiment_{}'.format(str(experiment_id))] = loss_list_experiment\n","            \n","            # torch.cuda.empty_cache() \n","            run.stop()\n"]},{"cell_type":"markdown","metadata":{},"source":["## Transformers With Reg"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"trusted":true},"outputs":[],"source":["d_model = [512, 1024, 2048, 4096]\n","heads = [8, 16, 32]\n","num_layers = [5, 10]\n","epochs = 100\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","with open('WORDMAP_corpus_KBFILKOM.json', 'r') as j:\n","    word_map = json.load(j)\n","\n","transformer_experiment = pd.DataFrame(columns = ['experiment_id', 'd_model', 'heads', 'num_layers', 'train_loss'])\n","loss_history = {}\n","\n","experiment_id = -1\n","\n","for d_m in d_model:\n","    for h in heads:\n","        for n_l in num_layers: \n","            experiment_id += 1\n","            print('\\nRunning for experiment {} with d_model {}, heads{}, num_layers{}\\n'.format(experiment_id, d_m, h, n_l))\n","\n","            run = neptune.init_run(\n","                project=project,\n","                api_token=api_token,\n","                name=\"experiment_1724_\" + str(experiment_id)\n","            ) \n","            run['parameters'] = {\n","                'd_model': d_m,\n","                'heads': h,\n","                'num_layers': n_l\n","            }\n","\n","            transformer_experiment.loc[experiment_id, 'experiment_id'] = 'experiment_{}'.format(str(experiment_id))\n","            transformer_experiment.loc[experiment_id, 'd_model'] = d_m\n","            transformer_experiment.loc[experiment_id, 'heads'] = h\n","            transformer_experiment.loc[experiment_id, 'num_layers'] = n_l\n","\n","            transformer = Transformer(d_model = d_m, heads = h, num_layers = n_l, word_map = word_map, max_len=95)\n","            transformer = transformer.to(device)\n","            adam_optimizer = torch.optim.Adam(transformer.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-9)\n","            transformer_optimizer = AdamWarmup(model_size = d_m, warmup_steps = 4000, optimizer = adam_optimizer)\n","            criterion = LossWithLS(len(word_map), 0.2)\n","\n","            loss_list_experiment = []\n","            for epoch in range(epochs):\n","                loss_train = train(train_loader, transformer, criterion, epoch)\n","\n","                state = {'epoch': epoch, 'transformer': transformer, 'transformer_optimizer': transformer_optimizer}\n","                torch.save(state, 'checkpoint_experiment_' + str(epoch) + 'id_'+ str(experiment_id) +'.pth.tar')\n","\n","                loss_list_experiment.append(loss_train)\n","\n","                run['train/loss'].append(loss_train)\n","\n","            transformer_experiment.loc[experiment_id, 'train_loss'] = loss_train\n","            loss_history['experiment_{}'.format(str(experiment_id))] = loss_list_experiment\n","            \n","            run.stop()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import yaml\n","\n","with open('loss_history_transformer_experiment_KBFILKOM.yaml', 'w') as file:\n","    documents = yaml.dump(loss_history, file)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["transformer_experiment.dropna(inplace=True)\n","transformer_experiment"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import yaml\n","\n","with open('history_rnn_150524.yaml', 'r') as file:\n","    history_rnn = yaml.load(file, Loader=yaml.FullLoader)"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true},"outputs":[],"source":["import matplotlib \n","import matplotlib.pyplot as plt\n","\n","loss_history_key = list(loss_history.keys())\n","\n","plt.figure(figsize=(15,10))\n","plt.title(\"Training loss vs. Number of Epochs\")\n","plt.xlabel(\"Number of Epochs\")\n","plt.ylabel(\"Training Loss\")\n","z\n","\n","for key in loss_history_key:\n","    loss_list = loss_history[key]\n","    labels = f'd_model: {transformer_experiment[transformer_experiment[\"experiment_id\"] == key][\"d_model\"].values[0]}, heads: {transformer_experiment[transformer_experiment[\"experiment_id\"] == key][\"heads\"].values[0]}, num_layers: {transformer_experiment[transformer_experiment[\"experiment_id\"] == key][\"num_layers\"].values[0]}'\n","    plt.plot(loss_list, label = labels)\n","\n","    \n","plt.plot(history_rnn['loss'], \n","                label = 'LSTM (Baseline FLUENT 2023)', \n","                linestyle='dashed', \n","                color='black', \n","                linewidth=2.5, \n","                alpha=0.7, \n","                marker='o', \n","                markerfacecolor='black', \n","                markersize=5\n","        )\n","\n","plt.legend()\n","torch.cuda.is_available()\n","plt.grid()"]},{"cell_type":"markdown","metadata":{},"source":["# Training"]},{"cell_type":"markdown","metadata":{},"source":["## Vanilla"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Directory created at experiment_vanilla_cobain\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [0][0/12]\tLoss: 5.249\n","Epoch [1][0/12]\tLoss: 4.999\n","Epoch [2][0/12]\tLoss: 4.573\n","Epoch [3][0/12]\tLoss: 4.163\n","Epoch [4][0/12]\tLoss: 4.276\n","Epoch [5][0/12]\tLoss: 4.165\n","Epoch [6][0/12]\tLoss: 4.073\n","Epoch [7][0/12]\tLoss: 3.987\n","Epoch [8][0/12]\tLoss: 3.915\n","Epoch [9][0/12]\tLoss: 3.911\n","Epoch [10][0/12]\tLoss: 3.958\n","Epoch [11][0/12]\tLoss: 3.939\n","Epoch [12][0/12]\tLoss: 3.963\n","Epoch [13][0/12]\tLoss: 3.779\n","Epoch [14][0/12]\tLoss: 3.815\n","Epoch [15][0/12]\tLoss: 3.809\n","Epoch [16][0/12]\tLoss: 3.736\n","Epoch [17][0/12]\tLoss: 3.794\n","Epoch [18][0/12]\tLoss: 3.850\n","Epoch [19][0/12]\tLoss: 3.747\n","Epoch [20][0/12]\tLoss: 3.595\n","Epoch [21][0/12]\tLoss: 3.698\n","Epoch [22][0/12]\tLoss: 3.593\n","Epoch [23][0/12]\tLoss: 3.713\n","Epoch [24][0/12]\tLoss: 3.434\n","Epoch [25][0/12]\tLoss: 3.599\n","Epoch [26][0/12]\tLoss: 3.577\n","Epoch [27][0/12]\tLoss: 3.455\n","Epoch [28][0/12]\tLoss: 3.368\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Input \u001b[0;32mIn [27]\u001b[0m, in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m criterion \u001b[38;5;241m=\u001b[39m LossWithLS(\u001b[38;5;28mlen\u001b[39m(word_map), \u001b[38;5;241m0.2\u001b[39m)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m---> 23\u001b[0m     loss_train \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     state \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m: epoch, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer\u001b[39m\u001b[38;5;124m'\u001b[39m: transformer, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer_optimizer\u001b[39m\u001b[38;5;124m'\u001b[39m: transformer_optimizer}\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m# torch.save(state, directory + '/checkpoint_' + str(epoch) +'.pth.tar')\u001b[39;00m\n","Input \u001b[0;32mIn [25]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(train_loader, transformer, criterion, epoch)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Backprop\u001b[39;00m\n\u001b[1;32m     28\u001b[0m transformer_optimizer\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 29\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m transformer_optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     32\u001b[0m sum_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m samples\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["directory = 'experiment_vanilla_cobain'\n","create_directory(directory)\n","\n","d_model = 4096\n","heads = 16\n","num_layers = 10\n","epochs = 100\n","\n","loss_history_vanilla_transformer = []\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","with open('WORDMAP_corpus_KBFILKOM.json', 'r') as j:\n","    word_map = json.load(j)\n","    \n","transformer = Transformer(d_model = d_model, heads = heads, num_layers = num_layers, word_map = word_map, max_len=95)\n","transformer = transformer.to(device)\n","adam_optimizer = torch.optim.Adam(transformer.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-9)\n","transformer_optimizer = AdamWarmup(model_size = d_model, warmup_steps = 4000, optimizer = adam_optimizer)\n","criterion = LossWithLS(len(word_map), 0.2)\n","\n","for epoch in range(epochs):\n","    loss_train = train(train_loader, transformer, criterion, epoch)\n","\n","    state = {'epoch': epoch, 'transformer': transformer, 'transformer_optimizer': transformer_optimizer}\n","    # torch.save(state, directory + '/checkpoint_' + str(epoch) +'.pth.tar')\n","\n","    loss_history_vanilla_transformer.append(loss_train)\n","\n","import yaml \n","\n","with open(directory + '/loss_history_vanilla_transformer.yaml', 'w') as file:\n","    yaml.dump(loss_history_vanilla_transformer, file)"]},{"cell_type":"markdown","metadata":{},"source":["## Vanilla without Regularization"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Directory already exists at transformers_vanillanoreg_so_13824\n","[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/andialifs/fluent-tesis-24/e/FLUENT24-113\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [0][0/12]\tLoss: 5.324\n","Epoch [1][0/12]\tLoss: 5.026\n","Epoch [2][0/12]\tLoss: 4.673\n","Epoch [3][0/12]\tLoss: 4.191\n","Epoch [4][0/12]\tLoss: 4.270\n","Epoch [5][0/12]\tLoss: 4.240\n","Epoch [6][0/12]\tLoss: 3.993\n","Epoch [7][0/12]\tLoss: 4.007\n","Epoch [8][0/12]\tLoss: 4.073\n","Epoch [9][0/12]\tLoss: 3.871\n","Epoch [10][0/12]\tLoss: 3.847\n","Epoch [11][0/12]\tLoss: 3.718\n","Epoch [12][0/12]\tLoss: 3.753\n","Epoch [13][0/12]\tLoss: 3.843\n","Epoch [14][0/12]\tLoss: 3.661\n","Epoch [15][0/12]\tLoss: 3.553\n","Epoch [16][0/12]\tLoss: 3.316\n","Epoch [17][0/12]\tLoss: 3.262\n","Epoch [18][0/12]\tLoss: 3.122\n","Epoch [19][0/12]\tLoss: 3.103\n","Epoch [20][0/12]\tLoss: 3.082\n","Epoch [21][0/12]\tLoss: 2.794\n","Epoch [22][0/12]\tLoss: 2.717\n","Epoch [23][0/12]\tLoss: 2.575\n","Epoch [24][0/12]\tLoss: 2.574\n","Epoch [25][0/12]\tLoss: 2.477\n","Epoch [26][0/12]\tLoss: 2.228\n","Epoch [27][0/12]\tLoss: 2.300\n","Epoch [28][0/12]\tLoss: 1.882\n","Epoch [29][0/12]\tLoss: 2.025\n","Epoch [30][0/12]\tLoss: 2.018\n","Epoch [31][0/12]\tLoss: 2.172\n","Epoch [32][0/12]\tLoss: 1.993\n","Epoch [33][0/12]\tLoss: 2.086\n","Epoch [34][0/12]\tLoss: 1.917\n","Epoch [35][0/12]\tLoss: 1.525\n","Epoch [36][0/12]\tLoss: 1.841\n","Epoch [37][0/12]\tLoss: 1.425\n","Epoch [38][0/12]\tLoss: 1.494\n","Epoch [39][0/12]\tLoss: 1.478\n","Epoch [40][0/12]\tLoss: 1.416\n","Epoch [41][0/12]\tLoss: 1.477\n","Epoch [42][0/12]\tLoss: 1.353\n","Epoch [43][0/12]\tLoss: 1.331\n","Epoch [44][0/12]\tLoss: 1.216\n","Epoch [45][0/12]\tLoss: 1.121\n","Epoch [46][0/12]\tLoss: 1.068\n","Epoch [47][0/12]\tLoss: 0.971\n","Epoch [48][0/12]\tLoss: 0.991\n","Epoch [49][0/12]\tLoss: 1.096\n","Epoch [50][0/12]\tLoss: 1.080\n","Epoch [51][0/12]\tLoss: 0.759\n","Epoch [52][0/12]\tLoss: 0.951\n","Epoch [53][0/12]\tLoss: 0.839\n","Epoch [54][0/12]\tLoss: 0.766\n","Epoch [55][0/12]\tLoss: 0.688\n","Epoch [71][0/12]\tLoss: 0.386\n","Epoch [72][0/12]\tLoss: 0.381\n","Epoch [73][0/12]\tLoss: 0.361\n","Epoch [74][0/12]\tLoss: 0.403\n","Epoch [75][0/12]\tLoss: 0.309\n","Epoch [76][0/12]\tLoss: 0.400\n","Epoch [77][0/12]\tLoss: 0.356\n","Epoch [78][0/12]\tLoss: 0.305\n","Epoch [79][0/12]\tLoss: 0.303\n","Epoch [80][0/12]\tLoss: 0.339\n","Epoch [81][0/12]\tLoss: 0.328\n","Epoch [82][0/12]\tLoss: 0.294\n","Epoch [83][0/12]\tLoss: 0.334\n","Epoch [84][0/12]\tLoss: 0.305\n","Epoch [85][0/12]\tLoss: 0.339\n","Epoch [86][0/12]\tLoss: 0.287\n","Epoch [87][0/12]\tLoss: 0.291\n","Epoch [88][0/12]\tLoss: 0.283\n","Epoch [89][0/12]\tLoss: 0.237\n","Epoch [90][0/12]\tLoss: 0.238\n","Epoch [91][0/12]\tLoss: 0.241\n","Epoch [92][0/12]\tLoss: 0.239\n","Epoch [93][0/12]\tLoss: 0.264\n","Epoch [94][0/12]\tLoss: 0.263\n","Epoch [95][0/12]\tLoss: 0.230\n","Epoch [96][0/12]\tLoss: 0.250\n","Epoch [97][0/12]\tLoss: 0.266\n","Epoch [98][0/12]\tLoss: 0.237\n","Epoch [99][0/12]\tLoss: 0.233\n"]},{"ename":"NameError","evalue":"name 'yaml' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Input \u001b[0;32mIn [28]\u001b[0m, in \u001b[0;36m<cell line: 42>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m run[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmemory_used\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmemory_reserved(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m1000000\u001b[39m)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(directory \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/loss_history_vanillanoreg_transformer.yaml\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m---> 43\u001b[0m     \u001b[43myaml\u001b[49m\u001b[38;5;241m.\u001b[39mdump(loss_history_vanilla_transformer, file)\n\u001b[1;32m     45\u001b[0m run\u001b[38;5;241m.\u001b[39mstop()\n","\u001b[0;31mNameError\u001b[0m: name 'yaml' is not defined"]}],"source":["directory = 'transformers_vanillanoreg_so_13824'\n","create_directory(directory)\n","\n","run = neptune_init(directory)\n","\n","parameters = {\n","    'd_model': 4096,\n","    'heads': 32,\n","    'num_layers': 5,\n","    'epochs': 100,\n","}\n","run['parameters'] = parameters\n","\n","loss_history_vanillanoreg_transformer = []\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","with open('WORDMAP_corpus_KBFILKOM.json', 'r') as j:\n","    word_map = json.load(j)\n","    \n","    \n","transformer = Transformer(d_model = parameters['d_model'], heads = parameters['heads'], num_layers = parameters['num_layers'], word_map = word_map, max_len=95)\n","transformer = transformer.to(device)\n","adam_optimizer = torch.optim.Adam(transformer.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-9)\n","transformer_optimizer = AdamWarmup(model_size = parameters['d_model'], warmup_steps = 4000, optimizer = adam_optimizer)\n","criterion = LossWithLS(len(word_map), 0.2)\n","\n","for epoch in range(parameters['epochs']):\n","    loss_train = train(train_loader, transformer, criterion, epoch)\n","\n","    state = {'epoch': epoch, 'transformer': transformer, 'transformer_optimizer': transformer_optimizer}\n","\n","    loss_history_vanillanoreg_transformer.append(loss_train)\n","    run['train/loss'].append(loss_train)\n","\n","    if epoch == parameters['epochs'] - 1:\n","        torch.save(state, directory + '/checkpoint_' + str(epoch) +'.pth.tar')\n","        run['model_checkpoint'].upload(directory + '/checkpoint_' + str(epoch) +'.pth.tar')\n","\n","run['memory_used'] = float(torch.cuda.memory_reserved(0)/1000000)\n","\n","with open(directory + '/loss_history_vanillanoreg_transformer.yaml', 'w') as file:\n","    yaml.dump(loss_history_vanilla_transformer, file)\n","\n","run.stop()"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[neptune] [info   ] Shutting down background jobs, please wait a moment...\n","[neptune] [info   ] Done!\n","[neptune] [info   ] All 0 operations synced, thanks for waiting!\n","[neptune] [info   ] Explore the metadata in the Neptune app: https://app.neptune.ai/andialifs/fluent-tesis-24/e/FLUENT24-103/metadata\n"]}],"source":["run.stop()"]},{"cell_type":"markdown","metadata":{},"source":["## Decoder Only"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["directory = 'experiment_deconly_17624'\n","create_directory(directory)\n","\n","d_model = 2048\n","heads = 16\n","num_layers = 5\n","epochs = 100\n","\n","loss_history_decoder_transformer = []\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","with open('WORDMAP_corpus_KBFILKOM.json', 'r') as j:\n","    word_map = json.load(j)\n","    \n","transformer = TransformerDecoderOnly(d_model = d_model, heads = heads, num_layers = num_layers, word_map = word_map, max_len=95)\n","transformer = transformer.to(device)\n","adam_optimizer = torch.optim.Adam(transformer.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-9)\n","transformer_optimizer = AdamWarmup(model_size = d_model, warmup_steps = 4000, optimizer = adam_optimizer)\n","criterion = LossWithLS(len(word_map), 0.2)\n","\n","for epoch in range(epochs):\n","    loss_train = train(train_loader, transformer, criterion, epoch)\n","\n","    state = {'epoch': epoch, 'transformer': transformer, 'transformer_optimizer': transformer_optimizer}\n","    torch.save(state, directory + '/checkpoint_' + str(epoch) +'.pth.tar')\n","\n","    loss_history_decoder_transformer.append(loss_train)"]},{"cell_type":"markdown","metadata":{},"source":["## LSTM_Transformer"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["directory = 'experiment_2_lstm'\n","\n","d_model = 1024\n","heads = 32\n","num_layers = 10\n","epochs = 100\n","\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","loss_history_lstm_transformer = []\n","\n","with open('WORDMAP_corpus_KBFILKOM.json', 'r') as j:\n","    word_map = json.load(j)\n","    \n","transformer = TransformerLSTM(d_model = d_model, heads = heads, num_layers = num_layers, word_map = word_map, max_len=90)\n","transformer = transformer.to(device)\n","adam_optimizer = torch.optim.Adam(transformer.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-9)\n","transformer_optimizer = AdamWarmup(model_size = d_model, warmup_steps = 4000, optimizer = adam_optimizer)\n","criterion = LossWithLS(len(word_map), 0.2)\n","\n","\n","for epoch in range(epochs):\n","    loss_train = train(train_loader, transformer, criterion, epoch)\n","\n","    state = {'epoch': epoch, 'transformer': transformer, 'transformer_optimizer': transformer_optimizer}\n","    torch.save(state, directory + '/checkpoint_' + str(epoch) +'.pth.tar')\n","\n","    loss_history_lstm_transformer.append(loss_train)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import yaml\n","\n","with open(directory + '/loss_history_lstm_transformer.yaml', 'w') as file:\n","    yaml.dump(loss_history_lstm_transformer, file)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","directory = 'transformers_vanillanoreg_so_17724'\n","checkpoint = torch.load(directory + '/checkpoint_99.pth.tar')\n","transformer = checkpoint['transformer']\n"]},{"cell_type":"code","execution_count":55,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["1 menghasilkan lulusan yang kompeten profesional <unk> <unk> luhur berjiwa <unk> dan berdaya saing internasional menghasilkan <unk> akademika yang mampu mengembangkan <unk> dan mampu mengembangkan <unk> dan teknologi informasi yang bermanfaat dan <unk> <unk> akademik yang <unk> dalam bidang pendidikan penelitian dan <unk> organisasi melalui pembangunan bangsa melalui integrasi\n"]}],"source":["question = \"tujuan punya filkom itu apa\" \n","max_len = 50\n","enc_qus = [word_map.get(word, word_map['<unk>']) for word in question.split()]\n","question = torch.LongTensor(enc_qus).to(device).unsqueeze(0)\n","question_mask = (question!=0).to(device).unsqueeze(1).unsqueeze(1)  \n","sentence = evaluate(transformer, question, question_mask, int(max_len), word_map)\n","print(sentence)"]},{"cell_type":"code","execution_count":59,"metadata":{},"outputs":[],"source":["del transformer"]},{"cell_type":"code","execution_count":57,"metadata":{},"outputs":[{"data":{"text/plain":["tensor([[  88, 1076,   25,  993,   87]], device='cuda:0')"]},"execution_count":57,"metadata":{},"output_type":"execute_result"}],"source":["question"]},{"cell_type":"code","execution_count":58,"metadata":{},"outputs":[{"data":{"text/plain":["tensor([[[[True, True, True, True, True]]]], device='cuda:0')"]},"execution_count":58,"metadata":{},"output_type":"execute_result"}],"source":["question_mask"]},{"cell_type":"code","execution_count":56,"metadata":{},"outputs":[{"data":{"text/plain":["[88, 1076, 25, 993, 87]"]},"execution_count":56,"metadata":{},"output_type":"execute_result"}],"source":["enc_qus"]},{"cell_type":"code","execution_count":40,"metadata":{},"outputs":[{"data":{"text/plain":["OrderedDict([('embed.embed.weight',\n","              tensor([[-1.9928, -0.7004,  0.7193,  ..., -1.6292,  1.5850,  0.2723],\n","                      [-0.3583,  0.6155,  0.0369,  ...,  1.8031, -0.4863,  0.8909],\n","                      [ 1.1024, -1.2518, -0.0069,  ...,  1.7611, -0.7846, -1.1839],\n","                      ...,\n","                      [-0.2117,  0.6299, -0.6148,  ..., -0.0408, -0.5580, -0.8067],\n","                      [ 0.0884, -0.1558, -1.5991,  ..., -1.6785,  0.0616, -0.3889],\n","                      [ 1.1627, -0.2376, -1.3329,  ...,  0.1444,  1.2254,  0.7990]],\n","                     device='cuda:0')),\n","             ('encoder.layernorm.weight',\n","              tensor([1.0070, 1.0067, 1.0042,  ..., 0.9963, 1.0000, 0.9963], device='cuda:0')),\n","             ('encoder.layernorm.bias',\n","              tensor([ 0.0002,  0.0005,  0.0001,  ..., -0.0012,  0.0002, -0.0009],\n","                     device='cuda:0')),\n","             ('encoder.self_multihead.query.weight',\n","              tensor([[-0.0031,  0.0102,  0.0126,  ...,  0.0018,  0.0086, -0.0046],\n","                      [ 0.0132, -0.0050, -0.0094,  ...,  0.0116, -0.0071, -0.0045],\n","                      [-0.0149,  0.0011,  0.0069,  ..., -0.0134, -0.0159,  0.0032],\n","                      ...,\n","                      [ 0.0158, -0.0152, -0.0012,  ...,  0.0016, -0.0126, -0.0015],\n","                      [-0.0081, -0.0061,  0.0127,  ...,  0.0082,  0.0103, -0.0026],\n","                      [-0.0084,  0.0104,  0.0096,  ..., -0.0147, -0.0102, -0.0023]],\n","                     device='cuda:0')),\n","             ('encoder.self_multihead.query.bias',\n","              tensor([ 0.0042, -0.0004, -0.0119,  ...,  0.0102, -0.0103,  0.0010],\n","                     device='cuda:0')),\n","             ('encoder.self_multihead.key.weight',\n","              tensor([[ 1.0109e-02, -6.5532e-03,  1.2352e-02,  ...,  1.6414e-02,\n","                       -1.4065e-02, -1.5211e-02],\n","                      [-8.2038e-03, -5.8626e-05,  4.6679e-03,  ..., -3.6972e-03,\n","                       -4.1696e-03,  9.3405e-03],\n","                      [ 2.2977e-03, -6.7920e-03, -1.0518e-02,  ...,  8.0617e-03,\n","                       -6.6053e-04,  7.4824e-03],\n","                      ...,\n","                      [-1.2220e-02,  2.7840e-03, -1.3633e-02,  ..., -5.3522e-03,\n","                       -2.4934e-03, -9.3002e-03],\n","                      [-1.4678e-02, -4.2314e-03,  6.2354e-03,  ..., -1.0081e-03,\n","                        7.1206e-03, -7.0433e-03],\n","                      [ 1.7540e-02,  1.8881e-03,  1.2356e-02,  ...,  5.9729e-03,\n","                       -6.3011e-04,  7.4333e-04]], device='cuda:0')),\n","             ('encoder.self_multihead.key.bias',\n","              tensor([-0.0041, -0.0034,  0.0112,  ...,  0.0107, -0.0063, -0.0099],\n","                     device='cuda:0')),\n","             ('encoder.self_multihead.value.weight',\n","              tensor([[-0.0075,  0.0045, -0.0072,  ..., -0.0066, -0.0099,  0.0036],\n","                      [ 0.0106, -0.0010,  0.0157,  ...,  0.0132, -0.0094,  0.0010],\n","                      [-0.0137,  0.0099,  0.0001,  ...,  0.0024,  0.0042, -0.0040],\n","                      ...,\n","                      [-0.0141,  0.0050, -0.0105,  ...,  0.0042, -0.0090,  0.0010],\n","                      [-0.0139,  0.0112, -0.0064,  ...,  0.0029, -0.0031,  0.0112],\n","                      [-0.0115,  0.0059, -0.0021,  ..., -0.0111, -0.0089,  0.0124]],\n","                     device='cuda:0')),\n","             ('encoder.self_multihead.value.bias',\n","              tensor([-0.0116,  0.0083, -0.0021,  ...,  0.0119,  0.0134, -0.0109],\n","                     device='cuda:0')),\n","             ('encoder.self_multihead.concat.weight',\n","              tensor([[ 0.0065, -0.0028,  0.0116,  ...,  0.0123, -0.0141,  0.0052],\n","                      [-0.0076, -0.0097, -0.0073,  ...,  0.0098, -0.0083,  0.0108],\n","                      [ 0.0041, -0.0016, -0.0030,  ..., -0.0085,  0.0092, -0.0123],\n","                      ...,\n","                      [ 0.0057,  0.0147,  0.0079,  ..., -0.0136, -0.0036, -0.0126],\n","                      [ 0.0078,  0.0013,  0.0095,  ...,  0.0013,  0.0138,  0.0069],\n","                      [ 0.0038, -0.0050,  0.0053,  ..., -0.0012, -0.0073, -0.0070]],\n","                     device='cuda:0')),\n","             ('encoder.self_multihead.concat.bias',\n","              tensor([ 0.0065,  0.0038, -0.0139,  ..., -0.0006, -0.0027,  0.0092],\n","                     device='cuda:0')),\n","             ('encoder.feed_forward.fc1.weight',\n","              tensor([[-0.0036, -0.0054, -0.0120,  ...,  0.0125, -0.0081,  0.0046],\n","                      [-0.0113, -0.0131, -0.0145,  ...,  0.0083, -0.0125, -0.0201],\n","                      [-0.0041, -0.0088, -0.0018,  ..., -0.0085, -0.0019,  0.0021],\n","                      ...,\n","                      [ 0.0009, -0.0032, -0.0150,  ...,  0.0156,  0.0118, -0.0176],\n","                      [ 0.0093,  0.0106, -0.0045,  ..., -0.0011, -0.0031,  0.0108],\n","                      [ 0.0017, -0.0077, -0.0055,  ...,  0.0037, -0.0084,  0.0016]],\n","                     device='cuda:0')),\n","             ('encoder.feed_forward.fc1.bias',\n","              tensor([ 0.0136, -0.0160, -0.0147,  ...,  0.0101,  0.0137,  0.0136],\n","                     device='cuda:0')),\n","             ('encoder.feed_forward.fc2.weight',\n","              tensor([[ 0.0211,  0.0159,  0.0100,  ...,  0.0159, -0.0207, -0.0132],\n","                      [ 0.0219, -0.0054,  0.0016,  ..., -0.0138, -0.0026,  0.0196],\n","                      [ 0.0074, -0.0034, -0.0215,  ..., -0.0035,  0.0083, -0.0172],\n","                      ...,\n","                      [-0.0070, -0.0016, -0.0063,  ..., -0.0050, -0.0055, -0.0137],\n","                      [-0.0066,  0.0202,  0.0091,  ..., -0.0186, -0.0154, -0.0055],\n","                      [ 0.0076,  0.0152, -0.0186,  ...,  0.0153, -0.0096, -0.0066]],\n","                     device='cuda:0')),\n","             ('encoder.feed_forward.fc2.bias',\n","              tensor([-0.0081, -0.0095,  0.0148,  ..., -0.0128,  0.0095, -0.0089],\n","                     device='cuda:0')),\n","             ('decoder.layernorm.weight',\n","              tensor([1.0100, 1.0170, 1.0158,  ..., 0.9956, 1.0089, 0.9987], device='cuda:0')),\n","             ('decoder.layernorm.bias',\n","              tensor([ 6.8728e-05,  2.3146e-04,  4.5524e-04,  ..., -1.1563e-03,\n","                       7.4412e-04, -8.4300e-04], device='cuda:0')),\n","             ('decoder.self_multihead.query.weight',\n","              tensor([[-2.1661e-04,  3.5624e-03, -1.7765e-03,  ...,  4.8836e-03,\n","                        9.7879e-04, -1.0917e-02],\n","                      [ 6.6353e-03,  1.1326e-02,  4.1226e-03,  ..., -1.9615e-03,\n","                        1.3620e-02,  5.2403e-03],\n","                      [-1.4315e-02,  1.0830e-03, -4.5501e-03,  ..., -5.0407e-03,\n","                       -3.4112e-04, -1.0869e-02],\n","                      ...,\n","                      [-1.3500e-03,  6.4467e-05,  1.5571e-02,  ...,  5.1110e-03,\n","                       -9.9973e-03, -7.9595e-03],\n","                      [ 1.1301e-02,  9.1592e-04, -1.1403e-02,  ..., -7.4405e-03,\n","                        5.7100e-03,  1.1438e-02],\n","                      [-7.7716e-04,  1.6625e-03, -1.2358e-02,  ...,  3.6082e-03,\n","                       -3.3888e-04, -5.3696e-03]], device='cuda:0')),\n","             ('decoder.self_multihead.query.bias',\n","              tensor([ 0.0183, -0.0118, -0.0071,  ..., -0.0183,  0.0111,  0.0007],\n","                     device='cuda:0')),\n","             ('decoder.self_multihead.key.weight',\n","              tensor([[ 0.0044,  0.0142, -0.0099,  ...,  0.0018, -0.0094, -0.0031],\n","                      [ 0.0044, -0.0091, -0.0152,  ..., -0.0083,  0.0097,  0.0030],\n","                      [-0.0149,  0.0118,  0.0032,  ...,  0.0113, -0.0004, -0.0119],\n","                      ...,\n","                      [ 0.0058, -0.0111, -0.0187,  ...,  0.0017, -0.0086,  0.0039],\n","                      [-0.0025,  0.0016, -0.0023,  ..., -0.0139,  0.0072, -0.0027],\n","                      [-0.0058, -0.0146, -0.0093,  ..., -0.0042,  0.0166, -0.0006]],\n","                     device='cuda:0')),\n","             ('decoder.self_multihead.key.bias',\n","              tensor([ 0.0092, -0.0136,  0.0136,  ..., -0.0111,  0.0148, -0.0034],\n","                     device='cuda:0')),\n","             ('decoder.self_multihead.value.weight',\n","              tensor([[-0.0027, -0.0117,  0.0010,  ...,  0.0076, -0.0028,  0.0052],\n","                      [ 0.0103,  0.0042,  0.0078,  ..., -0.0062,  0.0086, -0.0035],\n","                      [ 0.0102, -0.0088,  0.0135,  ...,  0.0132,  0.0132, -0.0007],\n","                      ...,\n","                      [-0.0071, -0.0082, -0.0028,  ..., -0.0095,  0.0089, -0.0092],\n","                      [-0.0126,  0.0053,  0.0115,  ...,  0.0119, -0.0136, -0.0121],\n","                      [ 0.0122, -0.0033, -0.0137,  ...,  0.0084,  0.0126, -0.0081]],\n","                     device='cuda:0')),\n","             ('decoder.self_multihead.value.bias',\n","              tensor([ 0.0155,  0.0038, -0.0139,  ...,  0.0132,  0.0013, -0.0149],\n","                     device='cuda:0')),\n","             ('decoder.self_multihead.concat.weight',\n","              tensor([[ 0.0153, -0.0114,  0.0070,  ..., -0.0078,  0.0063, -0.0018],\n","                      [-0.0048, -0.0119,  0.0061,  ..., -0.0136,  0.0095,  0.0060],\n","                      [-0.0077,  0.0028, -0.0128,  ...,  0.0012,  0.0021, -0.0163],\n","                      ...,\n","                      [ 0.0088,  0.0062, -0.0039,  ..., -0.0002, -0.0067, -0.0140],\n","                      [-0.0134, -0.0154,  0.0113,  ..., -0.0096,  0.0028, -0.0009],\n","                      [ 0.0030, -0.0028, -0.0019,  ..., -0.0054,  0.0108,  0.0086]],\n","                     device='cuda:0')),\n","             ('decoder.self_multihead.concat.bias',\n","              tensor([-0.0095,  0.0018,  0.0025,  ...,  0.0135, -0.0117,  0.0074],\n","                     device='cuda:0')),\n","             ('decoder.src_multihead.query.weight',\n","              tensor([[ 1.2623e-02,  7.5130e-05, -5.4338e-03,  ..., -1.4365e-02,\n","                        5.1989e-03,  4.5605e-03],\n","                      [-1.4487e-02,  1.5683e-02,  7.7116e-04,  ..., -6.1338e-03,\n","                       -6.1896e-03, -1.0288e-02],\n","                      [ 2.5185e-03,  1.9203e-02,  1.4414e-03,  ..., -6.1762e-03,\n","                        4.9363e-03, -1.1200e-02],\n","                      ...,\n","                      [-7.8316e-03,  6.5798e-03, -4.0680e-03,  ..., -1.1064e-02,\n","                        6.8138e-03,  4.6212e-03],\n","                      [ 1.6875e-02, -6.5700e-03, -4.5269e-03,  ..., -1.4273e-02,\n","                       -4.8249e-03, -1.2021e-02],\n","                      [ 6.9608e-03,  1.7812e-02, -1.4734e-02,  ..., -7.8012e-03,\n","                        1.3350e-02, -7.0971e-03]], device='cuda:0')),\n","             ('decoder.src_multihead.query.bias',\n","              tensor([ 0.0128, -0.0073, -0.0148,  ..., -0.0024, -0.0054,  0.0005],\n","                     device='cuda:0')),\n","             ('decoder.src_multihead.key.weight',\n","              tensor([[-2.5773e-03,  1.2932e-02,  4.1779e-04,  ..., -1.5033e-03,\n","                        9.1824e-03, -3.0159e-03],\n","                      [ 1.1409e-03,  9.8396e-03,  8.3350e-05,  ...,  5.9906e-03,\n","                       -1.7060e-02,  1.4246e-02],\n","                      [ 1.1250e-02,  9.1557e-03, -1.1160e-02,  ...,  1.4740e-02,\n","                        5.8261e-03, -8.8155e-04],\n","                      ...,\n","                      [-3.9944e-03, -1.8200e-02,  1.5230e-02,  ..., -7.4379e-03,\n","                        5.5598e-03,  1.1866e-02],\n","                      [ 1.2286e-02, -1.8818e-02,  1.0146e-02,  ...,  3.1332e-03,\n","                       -1.5398e-02, -6.5490e-03],\n","                      [ 1.5273e-02, -7.7076e-03,  7.7275e-03,  ..., -5.3025e-03,\n","                        1.4483e-03, -6.9521e-03]], device='cuda:0')),\n","             ('decoder.src_multihead.key.bias',\n","              tensor([-0.0052,  0.0012,  0.0027,  ..., -0.0020, -0.0049,  0.0126],\n","                     device='cuda:0')),\n","             ('decoder.src_multihead.value.weight',\n","              tensor([[-0.0073, -0.0132,  0.0073,  ..., -0.0149,  0.0098,  0.0122],\n","                      [ 0.0005,  0.0115, -0.0040,  ...,  0.0102,  0.0060,  0.0050],\n","                      [ 0.0087,  0.0026, -0.0125,  ...,  0.0138, -0.0055,  0.0046],\n","                      ...,\n","                      [-0.0095,  0.0022, -0.0082,  ...,  0.0013, -0.0044,  0.0083],\n","                      [-0.0119,  0.0010,  0.0018,  ..., -0.0097, -0.0079,  0.0096],\n","                      [-0.0027, -0.0109,  0.0087,  ..., -0.0083,  0.0136,  0.0118]],\n","                     device='cuda:0')),\n","             ('decoder.src_multihead.value.bias',\n","              tensor([-0.0129,  0.0107,  0.0111,  ...,  0.0025,  0.0037,  0.0019],\n","                     device='cuda:0')),\n","             ('decoder.src_multihead.concat.weight',\n","              tensor([[ 0.0053, -0.0032, -0.0018,  ...,  0.0110,  0.0063,  0.0022],\n","                      [-0.0140, -0.0040, -0.0019,  ..., -0.0076, -0.0024,  0.0001],\n","                      [-0.0044, -0.0010, -0.0010,  ...,  0.0058,  0.0047, -0.0120],\n","                      ...,\n","                      [-0.0102, -0.0107,  0.0142,  ..., -0.0001,  0.0017,  0.0023],\n","                      [ 0.0118,  0.0073,  0.0089,  ..., -0.0103,  0.0061, -0.0084],\n","                      [ 0.0003,  0.0092,  0.0028,  ...,  0.0076, -0.0084, -0.0014]],\n","                     device='cuda:0')),\n","             ('decoder.src_multihead.concat.bias',\n","              tensor([-0.0123,  0.0120, -0.0061,  ..., -0.0036,  0.0016, -0.0039],\n","                     device='cuda:0')),\n","             ('decoder.feed_forward.fc1.weight',\n","              tensor([[ 0.0048, -0.0148, -0.0080,  ...,  0.0134,  0.0028,  0.0059],\n","                      [-0.0055, -0.0135,  0.0047,  ..., -0.0069,  0.0060,  0.0059],\n","                      [ 0.0135,  0.0206,  0.0055,  ...,  0.0039, -0.0023,  0.0103],\n","                      ...,\n","                      [-0.0074, -0.0177,  0.0053,  ...,  0.0123,  0.0132,  0.0107],\n","                      [ 0.0081,  0.0142, -0.0094,  ..., -0.0143, -0.0108, -0.0141],\n","                      [-0.0009,  0.0108,  0.0134,  ...,  0.0031, -0.0051,  0.0205]],\n","                     device='cuda:0')),\n","             ('decoder.feed_forward.fc1.bias',\n","              tensor([-0.0114,  0.0138, -0.0047,  ...,  0.0112,  0.0062, -0.0107],\n","                     device='cuda:0')),\n","             ('decoder.feed_forward.fc2.weight',\n","              tensor([[-5.5558e-03, -8.6607e-04,  1.6650e-02,  ...,  1.2085e-02,\n","                        1.3394e-05, -1.9291e-02],\n","                      [-6.2738e-03, -1.0796e-03,  9.7020e-04,  ...,  8.3856e-03,\n","                        8.5384e-03,  1.6576e-02],\n","                      [-1.4398e-02, -1.5290e-03, -1.6680e-02,  ..., -6.3344e-03,\n","                        1.0143e-02, -2.2244e-02],\n","                      ...,\n","                      [ 1.4680e-02,  9.5699e-03,  4.2634e-03,  ...,  6.0847e-03,\n","                       -1.1116e-02, -2.5637e-03],\n","                      [ 1.7389e-02, -1.3612e-02, -1.6255e-02,  ...,  3.1703e-03,\n","                        2.0608e-03,  1.6723e-02],\n","                      [-6.7239e-03,  2.1272e-02, -1.9903e-02,  ..., -6.9105e-03,\n","                       -2.0819e-02,  2.1178e-02]], device='cuda:0')),\n","             ('decoder.feed_forward.fc2.bias',\n","              tensor([ 0.0004,  0.0187,  0.0005,  ..., -0.0037, -0.0115, -0.0122],\n","                     device='cuda:0')),\n","             ('logit.weight',\n","              tensor([[ 0.0048, -0.0070, -0.0026,  ..., -0.0013, -0.0098, -0.0021],\n","                      [ 0.0064, -0.0115, -0.0051,  ...,  0.0080, -0.0031,  0.0010],\n","                      [ 0.0011,  0.0157, -0.0150,  ..., -0.0094,  0.0143,  0.0138],\n","                      ...,\n","                      [-0.0125,  0.0028, -0.0021,  ...,  0.0111,  0.0161,  0.0021],\n","                      [ 0.0088, -0.0067,  0.0022,  ..., -0.0109, -0.0072,  0.0039],\n","                      [ 0.0107,  0.0109,  0.0054,  ..., -0.0123,  0.0125, -0.0037]],\n","                     device='cuda:0')),\n","             ('logit.bias',\n","              tensor([-0.0150,  0.0014, -0.0118,  ..., -0.0023, -0.0027,  0.0031],\n","                     device='cuda:0'))])"]},"execution_count":40,"metadata":{},"output_type":"execute_result"}],"source":["transformer.state_dict()"]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[{"data":{"text/plain":["{'epoch': 99,\n"," 'transformer': TransformerNoReg(\n","   (embed): Embeddings(\n","     (dropout): Dropout(p=0.1, inplace=False)\n","     (embed): Embedding(1079, 4096)\n","   )\n","   (encoder): EncoderLayerNoReg(\n","     (layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n","     (self_multihead): MultiHeadAttention(\n","       (dropout): Dropout(p=0.1, inplace=False)\n","       (query): Linear(in_features=4096, out_features=4096, bias=True)\n","       (key): Linear(in_features=4096, out_features=4096, bias=True)\n","       (value): Linear(in_features=4096, out_features=4096, bias=True)\n","       (concat): Linear(in_features=4096, out_features=4096, bias=True)\n","     )\n","     (feed_forward): FeedForward(\n","       (fc1): Linear(in_features=4096, out_features=2048, bias=True)\n","       (fc2): Linear(in_features=2048, out_features=4096, bias=True)\n","       (dropout): Dropout(p=0.1, inplace=False)\n","     )\n","   )\n","   (decoder): DecoderLayerNoReg(\n","     (layernorm): LayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n","     (self_multihead): MultiHeadAttention(\n","       (dropout): Dropout(p=0.1, inplace=False)\n","       (query): Linear(in_features=4096, out_features=4096, bias=True)\n","       (key): Linear(in_features=4096, out_features=4096, bias=True)\n","       (value): Linear(in_features=4096, out_features=4096, bias=True)\n","       (concat): Linear(in_features=4096, out_features=4096, bias=True)\n","     )\n","     (src_multihead): MultiHeadAttention(\n","       (dropout): Dropout(p=0.1, inplace=False)\n","       (query): Linear(in_features=4096, out_features=4096, bias=True)\n","       (key): Linear(in_features=4096, out_features=4096, bias=True)\n","       (value): Linear(in_features=4096, out_features=4096, bias=True)\n","       (concat): Linear(in_features=4096, out_features=4096, bias=True)\n","     )\n","     (feed_forward): FeedForward(\n","       (fc1): Linear(in_features=4096, out_features=2048, bias=True)\n","       (fc2): Linear(in_features=2048, out_features=4096, bias=True)\n","       (dropout): Dropout(p=0.1, inplace=False)\n","     )\n","   )\n","   (logit): Linear(in_features=4096, out_features=1079, bias=True)\n"," ),\n"," 'transformer_optimizer': <__main__.AdamWarmup at 0x7f11550c96a0>}"]},"execution_count":39,"metadata":{},"output_type":"execute_result"}],"source":["checkpoint"]},{"cell_type":"markdown","metadata":{},"source":["# Menuhin GPU"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Directory created at experiment_vanilla_cobain\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [0][0/12]\tLoss: 5.367\n","Epoch [1][0/12]\tLoss: 5.142\n","Epoch [2][0/12]\tLoss: 4.596\n","Epoch [3][0/12]\tLoss: 4.335\n","Epoch [4][0/12]\tLoss: 4.187\n","Epoch [5][0/12]\tLoss: 4.106\n","Epoch [6][0/12]\tLoss: 4.048\n","Epoch [7][0/12]\tLoss: 4.074\n","Epoch [8][0/12]\tLoss: 3.929\n","Epoch [9][0/12]\tLoss: 4.063\n","Epoch [10][0/12]\tLoss: 4.011\n","Epoch [11][0/12]\tLoss: 3.724\n","Epoch [12][0/12]\tLoss: 3.933\n","Epoch [13][0/12]\tLoss: 3.902\n","Epoch [14][0/12]\tLoss: 3.858\n","Epoch [15][0/12]\tLoss: 3.855\n","Epoch [16][0/12]\tLoss: 3.801\n","Epoch [17][0/12]\tLoss: 3.756\n","Epoch [18][0/12]\tLoss: 3.875\n","Epoch [19][0/12]\tLoss: 3.675\n","Epoch [20][0/12]\tLoss: 3.634\n","Epoch [21][0/12]\tLoss: 3.571\n","Epoch [22][0/12]\tLoss: 3.561\n","Epoch [23][0/12]\tLoss: 3.574\n","Epoch [24][0/12]\tLoss: 3.406\n","Epoch [25][0/12]\tLoss: 3.563\n","Epoch [26][0/12]\tLoss: 3.382\n","Epoch [27][0/12]\tLoss: 3.332\n","Epoch [28][0/12]\tLoss: 3.316\n","Epoch [29][0/12]\tLoss: 3.341\n","Epoch [30][0/12]\tLoss: 3.131\n","Epoch [31][0/12]\tLoss: 3.199\n","Epoch [32][0/12]\tLoss: 3.143\n","Epoch [33][0/12]\tLoss: 3.102\n","Epoch [34][0/12]\tLoss: 3.045\n","Epoch [35][0/12]\tLoss: 3.139\n","Epoch [36][0/12]\tLoss: 3.043\n","Epoch [37][0/12]\tLoss: 2.975\n","Epoch [38][0/12]\tLoss: 2.880\n","Epoch [39][0/12]\tLoss: 3.009\n","Epoch [40][0/12]\tLoss: 2.889\n","Epoch [41][0/12]\tLoss: 2.801\n","Epoch [42][0/12]\tLoss: 2.545\n","Epoch [43][0/12]\tLoss: 2.723\n","Epoch [44][0/12]\tLoss: 2.746\n","Epoch [45][0/12]\tLoss: 2.651\n","Epoch [46][0/12]\tLoss: 2.669\n","Epoch [47][0/12]\tLoss: 2.714\n","Epoch [48][0/12]\tLoss: 2.806\n","Epoch [49][0/12]\tLoss: 2.641\n","Epoch [50][0/12]\tLoss: 2.486\n","Epoch [51][0/12]\tLoss: 2.487\n","Epoch [52][0/12]\tLoss: 2.632\n","Epoch [53][0/12]\tLoss: 2.579\n","Epoch [54][0/12]\tLoss: 2.304\n","Epoch [55][0/12]\tLoss: 2.505\n","Epoch [56][0/12]\tLoss: 2.310\n","Epoch [57][0/12]\tLoss: 2.567\n","Epoch [58][0/12]\tLoss: 2.280\n"]}],"source":["while True:\n","    torch.cuda.empty_cache()\n","\n","    directory = 'experiment_vanilla_cobain'\n","    create_directory(directory)\n","\n","    d_model = 4096\n","    heads = 16\n","    num_layers = 10\n","    epochs = 100\n","\n","    loss_history_vanilla_transformer = []\n","\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","    with open('WORDMAP_corpus_KBFILKOM.json', 'r') as j:\n","        word_map = json.load(j)\n","        \n","    transformer = Transformer(d_model = d_model, heads = heads, num_layers = num_layers, word_map = word_map, max_len=95)\n","    transformer = transformer.to(device)\n","    adam_optimizer = torch.optim.Adam(transformer.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-9)\n","    transformer_optimizer = AdamWarmup(model_size = d_model, warmup_steps = 4000, optimizer = adam_optimizer)\n","    criterion = LossWithLS(len(word_map), 0.2)\n","\n","    for epoch in range(epochs):\n","        loss_train = train(train_loader, transformer, criterion, epoch)\n","\n","        state = {'epoch': epoch, 'transformer': transformer, 'transformer_optimizer': transformer_optimizer}\n","        # torch.save(state, directory + '/checkpoint_' + str(epoch) +'.pth.tar')\n","\n","        loss_history_vanilla_transformer.append(loss_train)\n","\n","    import yaml \n","\n","    with open(directory + '/loss_history_vanilla_transformer.yaml', 'w') as file:\n","        yaml.dump(loss_history_vanilla_transformer, file)"]},{"cell_type":"markdown","metadata":{},"source":["# Reset Cache"]},{"cell_type":"code","execution_count":87,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["|===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 2            |        cudaMalloc retries: 4         |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |  51970 MiB |  51970 MiB | 446291 MiB | 394320 MiB |\n","|       from large pool |  51963 MiB |  51963 MiB | 445553 MiB | 393589 MiB |\n","|       from small pool |      7 MiB |      8 MiB |    738 MiB |    730 MiB |\n","|---------------------------------------------------------------------------|\n","| Active memory         |  51970 MiB |  51970 MiB | 446291 MiB | 394320 MiB |\n","|       from large pool |  51963 MiB |  51963 MiB | 445553 MiB | 393589 MiB |\n","|       from small pool |      7 MiB |      8 MiB |    738 MiB |    730 MiB |\n","|---------------------------------------------------------------------------|\n","| Requested memory      |  51930 MiB |  51930 MiB | 445290 MiB | 393360 MiB |\n","|       from large pool |  51923 MiB |  51923 MiB | 444553 MiB | 392629 MiB |\n","|       from small pool |      7 MiB |      8 MiB |    737 MiB |    730 MiB |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |  53592 MiB |  53592 MiB |  67474 MiB |  13882 MiB |\n","|       from large pool |  53582 MiB |  53582 MiB |  67460 MiB |  13878 MiB |\n","|       from small pool |     10 MiB |     10 MiB |     14 MiB |      4 MiB |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |   1621 MiB |   3254 MiB |  52913 MiB |  51292 MiB |\n","|       from large pool |   1618 MiB |   3253 MiB |  52169 MiB |  50551 MiB |\n","|       from small pool |      2 MiB |      5 MiB |    744 MiB |    741 MiB |\n","|---------------------------------------------------------------------------|\n","| Allocations           |     576    |     701    |    7988    |    7412    |\n","|       from large pool |     364    |     398    |    4387    |    4023    |\n","|       from small pool |     212    |     468    |    3601    |    3389    |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |     576    |     701    |    7988    |    7412    |\n","|       from large pool |     364    |     398    |    4387    |    4023    |\n","|       from small pool |     212    |     468    |    3601    |    3389    |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |     305    |     515    |     585    |     280    |\n","|       from large pool |     300    |     510    |     578    |     278    |\n","|       from small pool |       5    |       5    |       7    |       2    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |     204    |     239    |    3779    |    3575    |\n","|       from large pool |     186    |     213    |    2276    |    2090    |\n","|       from small pool |      18    |      29    |    1503    |    1485    |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n"]},{"name":"stdout","output_type":"stream","text":["[neptune] [info   ] Shutting down background jobs, please wait a moment...\n","[neptune] [info   ] Done!\n","[neptune] [info   ] All 0 operations synced, thanks for waiting!\n","[neptune] [info   ] Explore the metadata in the Neptune app: https://app.neptune.ai/andialifs/fluent-tesis-24/e/FLUENT24-101/metadata\n","[neptune] [info   ] Shutting down background jobs, please wait a moment...\n","[neptune] [info   ] Done!\n","[neptune] [info   ] All 0 operations synced, thanks for waiting!\n","[neptune] [info   ] Explore the metadata in the Neptune app: https://app.neptune.ai/andialifs/fluent-tesis-24/e/FLUENT24-100/metadata\n","[neptune] [info   ] Shutting down background jobs, please wait a moment...\n","[neptune] [info   ] Done!\n","[neptune] [info   ] Explore the metadata in the Neptune app: https://app.neptune.ai/andialifs/fluent-tesis-24/e/FLUENT24-99/metadata\n"]}],"source":["import torch\n","\n","torch.cuda.empty_cache()\n","\n","print(torch.cuda.memory_summary(device=None, abbreviated=False))"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30733,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"kernel-alip","language":"python","name":"kernel-alip"}},"nbformat":4,"nbformat_minor":4}
