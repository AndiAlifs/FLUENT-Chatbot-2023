{"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install neptune","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-07-02T20:17:03.272867Z","iopub.execute_input":"2024-07-02T20:17:03.273239Z","iopub.status.idle":"2024-07-02T20:17:27.680245Z","shell.execute_reply.started":"2024-07-02T20:17:03.273209Z","shell.execute_reply":"2024-07-02T20:17:27.679127Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting neptune\n  Downloading neptune-1.10.4-py3-none-any.whl.metadata (16 kB)\nRequirement already satisfied: GitPython>=2.0.8 in /opt/conda/lib/python3.10/site-packages (from neptune) (3.1.41)\nRequirement already satisfied: Pillow>=1.1.6 in /opt/conda/lib/python3.10/site-packages (from neptune) (9.5.0)\nRequirement already satisfied: PyJWT in /opt/conda/lib/python3.10/site-packages (from neptune) (2.8.0)\nCollecting boto3>=1.28.0 (from neptune)\n  Downloading boto3-1.34.138-py3-none-any.whl.metadata (6.6 kB)\nCollecting bravado<12.0.0,>=11.0.0 (from neptune)\n  Downloading bravado-11.0.3-py2.py3-none-any.whl.metadata (5.9 kB)\nRequirement already satisfied: click>=7.0 in /opt/conda/lib/python3.10/site-packages (from neptune) (8.1.7)\nRequirement already satisfied: future>=0.17.1 in /opt/conda/lib/python3.10/site-packages (from neptune) (1.0.0)\nRequirement already satisfied: oauthlib>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from neptune) (3.2.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from neptune) (21.3)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from neptune) (2.2.1)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from neptune) (5.9.3)\nRequirement already satisfied: requests>=2.20.0 in /opt/conda/lib/python3.10/site-packages (from neptune) (2.32.3)\nRequirement already satisfied: requests-oauthlib>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from neptune) (1.3.1)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from neptune) (1.16.0)\nCollecting swagger-spec-validator>=2.7.4 (from neptune)\n  Downloading swagger_spec_validator-3.0.4-py2.py3-none-any.whl.metadata (2.5 kB)\nRequirement already satisfied: typing-extensions>=3.10.0 in /opt/conda/lib/python3.10/site-packages (from neptune) (4.9.0)\nRequirement already satisfied: urllib3 in /opt/conda/lib/python3.10/site-packages (from neptune) (1.26.18)\nRequirement already satisfied: websocket-client!=1.0.0,>=0.35.0 in /opt/conda/lib/python3.10/site-packages (from neptune) (1.7.0)\nCollecting botocore<1.35.0,>=1.34.138 (from boto3>=1.28.0->neptune)\n  Downloading botocore-1.34.138-py3-none-any.whl.metadata (5.7 kB)\nRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from boto3>=1.28.0->neptune) (1.0.1)\nCollecting s3transfer<0.11.0,>=0.10.0 (from boto3>=1.28.0->neptune)\n  Downloading s3transfer-0.10.2-py3-none-any.whl.metadata (1.7 kB)\nCollecting bravado-core>=5.16.1 (from bravado<12.0.0,>=11.0.0->neptune)\n  Downloading bravado-core-6.1.1.tar.gz (63 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.9/63.9 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: msgpack in /opt/conda/lib/python3.10/site-packages (from bravado<12.0.0,>=11.0.0->neptune) (1.0.7)\nRequirement already satisfied: python-dateutil in /opt/conda/lib/python3.10/site-packages (from bravado<12.0.0,>=11.0.0->neptune) (2.9.0.post0)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from bravado<12.0.0,>=11.0.0->neptune) (6.0.1)\nRequirement already satisfied: simplejson in /opt/conda/lib/python3.10/site-packages (from bravado<12.0.0,>=11.0.0->neptune) (3.19.2)\nCollecting monotonic (from bravado<12.0.0,>=11.0.0->neptune)\n  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from GitPython>=2.0.8->neptune) (4.0.11)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.20.0->neptune) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.20.0->neptune) (3.6)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.20.0->neptune) (2024.2.2)\nRequirement already satisfied: jsonschema in /opt/conda/lib/python3.10/site-packages (from swagger-spec-validator>=2.7.4->neptune) (4.20.0)\nRequirement already satisfied: importlib-resources>=1.3 in /opt/conda/lib/python3.10/site-packages (from swagger-spec-validator>=2.7.4->neptune) (6.1.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->neptune) (3.1.1)\nRequirement already satisfied: numpy<2,>=1.22.4 in /opt/conda/lib/python3.10/site-packages (from pandas->neptune) (1.26.4)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->neptune) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->neptune) (2023.4)\nCollecting jsonref (from bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune)\n  Downloading jsonref-1.1.0-py3-none-any.whl.metadata (2.7 kB)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython>=2.0.8->neptune) (5.0.1)\nRequirement already satisfied: attrs>=22.2.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune) (23.2.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune) (2023.12.1)\nRequirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune) (0.32.1)\nRequirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune) (0.16.2)\nRequirement already satisfied: fqdn in /opt/conda/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune) (1.5.1)\nRequirement already satisfied: isoduration in /opt/conda/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune) (20.11.0)\nRequirement already satisfied: jsonpointer>1.13 in /opt/conda/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune) (2.4)\nRequirement already satisfied: rfc3339-validator in /opt/conda/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune) (0.1.4)\nRequirement already satisfied: rfc3986-validator>0.1.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune) (0.1.1)\nRequirement already satisfied: uri-template in /opt/conda/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune) (1.3.0)\nRequirement already satisfied: webcolors>=1.11 in /opt/conda/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune) (1.13)\nRequirement already satisfied: arrow>=0.15.0 in /opt/conda/lib/python3.10/site-packages (from isoduration->jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune) (1.3.0)\nRequirement already satisfied: types-python-dateutil>=2.8.10 in /opt/conda/lib/python3.10/site-packages (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune) (2.8.19.20240106)\nDownloading neptune-1.10.4-py3-none-any.whl (502 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m502.6/502.6 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading boto3-1.34.138-py3-none-any.whl (139 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.2/139.2 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading bravado-11.0.3-py2.py3-none-any.whl (38 kB)\nDownloading swagger_spec_validator-3.0.4-py2.py3-none-any.whl (28 kB)\nDownloading botocore-1.34.138-py3-none-any.whl (12.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m83.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading s3transfer-0.10.2-py3-none-any.whl (82 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.7/82.7 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\nDownloading jsonref-1.1.0-py3-none-any.whl (9.4 kB)\nBuilding wheels for collected packages: bravado-core\n  Building wheel for bravado-core (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for bravado-core: filename=bravado_core-6.1.1-py2.py3-none-any.whl size=67673 sha256=b538ea72ebc5cbdf85fb36fe965e471d9f66720f253d0b62693a6c2ccb6b4507\n  Stored in directory: /root/.cache/pip/wheels/42/35/4a/44ec4c358db21a5d63ed4e40f0f0012a438106f220bce4ccba\nSuccessfully built bravado-core\nInstalling collected packages: monotonic, jsonref, botocore, s3transfer, swagger-spec-validator, boto3, bravado-core, bravado, neptune\n  Attempting uninstall: botocore\n    Found existing installation: botocore 1.34.106\n    Uninstalling botocore-1.34.106:\n      Successfully uninstalled botocore-1.34.106\n  Attempting uninstall: s3transfer\n    Found existing installation: s3transfer 0.6.2\n    Uninstalling s3transfer-0.6.2:\n      Successfully uninstalled s3transfer-0.6.2\n  Attempting uninstall: boto3\n    Found existing installation: boto3 1.26.100\n    Uninstalling boto3-1.26.100:\n      Successfully uninstalled boto3-1.26.100\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\naiobotocore 2.13.0 requires aiohttp<4.0.0,>=3.9.2, but you have aiohttp 3.9.1 which is incompatible.\naiobotocore 2.13.0 requires botocore<1.34.107,>=1.34.70, but you have botocore 1.34.138 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed boto3-1.34.138 botocore-1.34.138 bravado-11.0.3 bravado-core-6.1.1 jsonref-1.1.0 monotonic-1.6 neptune-1.10.4 s3transfer-0.10.2 swagger-spec-validator-3.0.4\n","output_type":"stream"}]},{"cell_type":"code","source":"from collections import Counter\nimport json\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset\nimport torch.utils.data\nimport math\nimport torch.nn.functional as F\nimport neptune\nimport pandas as pd","metadata":{"execution":{"iopub.status.busy":"2024-07-02T20:17:27.682115Z","iopub.execute_input":"2024-07-02T20:17:27.682419Z","iopub.status.idle":"2024-07-02T20:17:30.597078Z","shell.execute_reply.started":"2024-07-02T20:17:27.682391Z","shell.execute_reply":"2024-07-02T20:17:30.596113Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"knowledgebase_url = 'https://github.com/AndiAlifs/FLUENT-Chatbot-2023/raw/main/KnowledgeBaseFilkom.xlsx'\nknowledgebase = pd.read_excel(knowledgebase_url)\n\nqa_paired = knowledgebase.drop(columns=knowledgebase.columns.drop(['Pertanyaan', 'Jawaban']))\nqa_paired.dropna(inplace=True)\nqa_paired","metadata":{"execution":{"iopub.status.busy":"2024-07-02T20:17:30.598458Z","iopub.execute_input":"2024-07-02T20:17:30.598900Z","iopub.status.idle":"2024-07-02T20:17:32.694354Z","shell.execute_reply.started":"2024-07-02T20:17:30.598873Z","shell.execute_reply":"2024-07-02T20:17:32.693406Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"                                             Pertanyaan  \\\n0                               email Fitra A. Bachtiar   \n1                             NIK/NIP Fitra A. Bachtiar   \n2                        nama lengkap Fitra A. Bachtiar   \n3                          Departemen Fitra A. Bachtiar   \n4                       Program Studi Fitra A. Bachtiar   \n...                                                 ...   \n1229                     Apa Manfaat Konseling FILKOM ?   \n1230       Berikan informasi mengenai Layanan Konseling   \n1231  Siapa Konselor Bimbingan dan Konseling di FILK...   \n1232                Siapa Koordinator Konselor Sebaya ?   \n1233                     Berikan Rincian Layanan ULTKSP   \n\n                                                Jawaban  \n0                            fitra.bachtiar[at]ub.ac.id  \n1                                    198406282019031006  \n2                             Dr.Eng. Fitra A. Bachtiar  \n3                         Departemen Teknik Informatika  \n4                                      S2 Ilmu Komputer  \n...                                                 ...  \n1229  1. Masalah ditangani oleh ahli yang kompeten d...  \n1230  Informasi mengenai Layanan Konseling dapat dia...  \n1231  Ada 2 konselor Bimbingan dan Konseling di FILK...  \n1232  Koordinator Konselor Sebaya adalah Muhammad Da...  \n1233  Rincian Layanan ULTKSP dapat diakses pada taut...  \n\n[1198 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Pertanyaan</th>\n      <th>Jawaban</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>email Fitra A. Bachtiar</td>\n      <td>fitra.bachtiar[at]ub.ac.id</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>NIK/NIP Fitra A. Bachtiar</td>\n      <td>198406282019031006</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>nama lengkap Fitra A. Bachtiar</td>\n      <td>Dr.Eng. Fitra A. Bachtiar</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Departemen Fitra A. Bachtiar</td>\n      <td>Departemen Teknik Informatika</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Program Studi Fitra A. Bachtiar</td>\n      <td>S2 Ilmu Komputer</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1229</th>\n      <td>Apa Manfaat Konseling FILKOM ?</td>\n      <td>1. Masalah ditangani oleh ahli yang kompeten d...</td>\n    </tr>\n    <tr>\n      <th>1230</th>\n      <td>Berikan informasi mengenai Layanan Konseling</td>\n      <td>Informasi mengenai Layanan Konseling dapat dia...</td>\n    </tr>\n    <tr>\n      <th>1231</th>\n      <td>Siapa Konselor Bimbingan dan Konseling di FILK...</td>\n      <td>Ada 2 konselor Bimbingan dan Konseling di FILK...</td>\n    </tr>\n    <tr>\n      <th>1232</th>\n      <td>Siapa Koordinator Konselor Sebaya ?</td>\n      <td>Koordinator Konselor Sebaya adalah Muhammad Da...</td>\n    </tr>\n    <tr>\n      <th>1233</th>\n      <td>Berikan Rincian Layanan ULTKSP</td>\n      <td>Rincian Layanan ULTKSP dapat diakses pada taut...</td>\n    </tr>\n  </tbody>\n</table>\n<p>1198 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"def remove_punc(string):\n    punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n    no_punct = \"\"\n    for char in string:\n        if char not in punctuations:\n            no_punct = no_punct + char  # space is also a character\n    return no_punct.lower()","metadata":{"execution":{"iopub.status.busy":"2024-07-02T20:17:32.696633Z","iopub.execute_input":"2024-07-02T20:17:32.697066Z","iopub.status.idle":"2024-07-02T20:17:32.702322Z","shell.execute_reply.started":"2024-07-02T20:17:32.697038Z","shell.execute_reply":"2024-07-02T20:17:32.701334Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"pairs = []\nmax_len = 90\n\nfor line in qa_paired.iterrows():\n    pertanyaan = line[1]['Pertanyaan']\n    jawaban = line[1]['Jawaban']\n    qa_pairs = []\n    first = remove_punc(pertanyaan.strip())      \n    second = remove_punc(jawaban.strip())\n    qa_pairs.append(first.split()[:max_len])\n    qa_pairs.append(second.split()[:max_len])\n    pairs.append(qa_pairs)","metadata":{"execution":{"iopub.status.busy":"2024-07-02T20:17:32.703658Z","iopub.execute_input":"2024-07-02T20:17:32.703938Z","iopub.status.idle":"2024-07-02T20:17:32.827025Z","shell.execute_reply.started":"2024-07-02T20:17:32.703915Z","shell.execute_reply":"2024-07-02T20:17:32.825935Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"word_freq = Counter()\nfor pair in pairs:\n    word_freq.update(pair[0])\n    word_freq.update(pair[1])","metadata":{"execution":{"iopub.status.busy":"2024-07-02T20:17:32.828255Z","iopub.execute_input":"2024-07-02T20:17:32.828565Z","iopub.status.idle":"2024-07-02T20:17:32.838911Z","shell.execute_reply.started":"2024-07-02T20:17:32.828539Z","shell.execute_reply":"2024-07-02T20:17:32.837848Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"min_word_freq = 2\nwords = [w for w in word_freq.keys() if word_freq[w] > min_word_freq]\nword_map = {k: v + 1 for v, k in enumerate(words)}\nword_map['<unk>'] = len(word_map) + 1\nword_map['<start>'] = len(word_map) + 1\nword_map['<end>'] = len(word_map) + 1\nword_map['<pad>'] = 0","metadata":{"execution":{"iopub.status.busy":"2024-07-02T20:17:32.840244Z","iopub.execute_input":"2024-07-02T20:17:32.840576Z","iopub.status.idle":"2024-07-02T20:17:32.850265Z","shell.execute_reply.started":"2024-07-02T20:17:32.840548Z","shell.execute_reply":"2024-07-02T20:17:32.849352Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"print(\"Total words are: {}\".format(len(word_map)))","metadata":{"execution":{"iopub.status.busy":"2024-07-02T20:17:32.851465Z","iopub.execute_input":"2024-07-02T20:17:32.851802Z","iopub.status.idle":"2024-07-02T20:17:32.860482Z","shell.execute_reply.started":"2024-07-02T20:17:32.851770Z","shell.execute_reply":"2024-07-02T20:17:32.859625Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Total words are: 1079\n","output_type":"stream"}]},{"cell_type":"code","source":"with open('WORDMAP_corpus_KBFILKOM.json', 'w') as j:\n    json.dump(word_map, j)","metadata":{"execution":{"iopub.status.busy":"2024-07-02T20:17:32.861579Z","iopub.execute_input":"2024-07-02T20:17:32.861919Z","iopub.status.idle":"2024-07-02T20:17:32.872138Z","shell.execute_reply.started":"2024-07-02T20:17:32.861895Z","shell.execute_reply":"2024-07-02T20:17:32.871399Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def encode_question(words, word_map):\n    enc_c = [word_map.get(word, word_map['<unk>']) for word in words] + [word_map['<pad>']] * (max_len - len(words))\n    return enc_c\n\ndef encode_reply(words, word_map):\n    enc_c = [word_map['<start>']] + [word_map.get(word, word_map['<unk>']) for word in words] + \\\n    [word_map['<end>']] + [word_map['<pad>']] * (max_len - len(words))\n    return enc_c\n\ndef encode_reply_with_maxlen(words, word_map):\n    enc_c = [word_map['<start>']] + [word_map.get(word, word_map['<unk>']) for word in words] + [word_map['<end>']] + [word_map['<pad>']] * (max_len-2 - len(words))\n    return enc_c","metadata":{"execution":{"iopub.status.busy":"2024-07-02T20:17:32.876749Z","iopub.execute_input":"2024-07-02T20:17:32.877074Z","iopub.status.idle":"2024-07-02T20:17:32.884325Z","shell.execute_reply.started":"2024-07-02T20:17:32.877048Z","shell.execute_reply":"2024-07-02T20:17:32.883430Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"pairs_encoded = []\nfor pair in pairs:\n    qus = encode_question(pair[0], word_map)\n    ans = encode_reply(pair[1], word_map)\n    pairs_encoded.append([qus, ans])\n\nwith open('pairs_encoded_kbfilkom.json', 'w') as p:\n    json.dump(pairs_encoded, p)","metadata":{"execution":{"iopub.status.busy":"2024-07-02T20:17:32.885341Z","iopub.execute_input":"2024-07-02T20:17:32.885632Z","iopub.status.idle":"2024-07-02T20:17:33.099456Z","shell.execute_reply.started":"2024-07-02T20:17:32.885608Z","shell.execute_reply":"2024-07-02T20:17:33.098486Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"pairs_encoded_same_length = []\nfor pair in pairs:\n    qus = encode_question(pair[0], word_map)\n    ans = encode_reply_with_maxlen(pair[1], word_map)\n    pairs_encoded_same_length.append([qus, ans])\n\nwith open('pairs_encoded_kbfilkom_same_len.json', 'w') as p:\n    json.dump(pairs_encoded_same_length, p)","metadata":{"execution":{"iopub.status.busy":"2024-07-02T20:17:33.100609Z","iopub.execute_input":"2024-07-02T20:17:33.100894Z","iopub.status.idle":"2024-07-02T20:17:33.308867Z","shell.execute_reply.started":"2024-07-02T20:17:33.100868Z","shell.execute_reply":"2024-07-02T20:17:33.307678Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"rev_word_map = {v: k for k, v in word_map.items()}\n' '.join([rev_word_map[v] for v in pairs_encoded[15][0]])","metadata":{"execution":{"iopub.status.busy":"2024-07-02T20:17:33.310107Z","iopub.execute_input":"2024-07-02T20:17:33.310393Z","iopub.status.idle":"2024-07-02T20:17:33.317262Z","shell.execute_reply.started":"2024-07-02T20:17:33.310368Z","shell.execute_reply":"2024-07-02T20:17:33.316249Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"'apa tujuan filkom <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>'"},"metadata":{}}]},{"cell_type":"code","source":"class Dataset(Dataset):\n\n    def __init__(self):\n\n        self.pairs = json.load(open('pairs_encoded_kbfilkom.json'))\n        self.dataset_size = len(self.pairs)\n\n    def __getitem__(self, i):\n        \n        question = torch.LongTensor(self.pairs[i][0])\n        reply = torch.LongTensor(self.pairs[i][1])\n            \n        return question, reply\n\n    def __len__(self):\n        return self.dataset_size","metadata":{"execution":{"iopub.status.busy":"2024-07-02T20:17:33.318483Z","iopub.execute_input":"2024-07-02T20:17:33.318791Z","iopub.status.idle":"2024-07-02T20:17:33.325793Z","shell.execute_reply.started":"2024-07-02T20:17:33.318756Z","shell.execute_reply":"2024-07-02T20:17:33.325035Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"class Dataset_Same_Len(Dataset):\n\n    def __init__(self):\n\n        self.pairs = json.load(open('pairs_encoded_kbfilkom_same_len.json'))\n        self.dataset_size = len(self.pairs)\n\n    def __getitem__(self, i):\n        \n        question = torch.LongTensor(self.pairs[i][0])\n        reply = torch.LongTensor(self.pairs[i][1])\n            \n        return question, reply\n\n    def __len__(self):\n        return self.dataset_size","metadata":{"execution":{"iopub.status.busy":"2024-07-02T20:17:33.326939Z","iopub.execute_input":"2024-07-02T20:17:33.327276Z","iopub.status.idle":"2024-07-02T20:17:33.336359Z","shell.execute_reply.started":"2024-07-02T20:17:33.327245Z","shell.execute_reply":"2024-07-02T20:17:33.335543Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"train_loader = torch.utils.data.DataLoader(Dataset(),\n                                           batch_size = 100, \n                                           shuffle=True, \n                                           pin_memory=True)","metadata":{"execution":{"iopub.status.busy":"2024-07-02T20:17:33.337481Z","iopub.execute_input":"2024-07-02T20:17:33.337842Z","iopub.status.idle":"2024-07-02T20:17:33.379910Z","shell.execute_reply.started":"2024-07-02T20:17:33.337813Z","shell.execute_reply":"2024-07-02T20:17:33.378865Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def create_masks(question, reply_input, reply_target):\n    \n    def subsequent_mask(size):\n        mask = torch.triu(torch.ones(size, size)).transpose(0, 1).type(dtype=torch.uint8)\n        return mask.unsqueeze(0)\n    \n    question_mask = question!=0\n    question_mask = question_mask.to(device)\n    question_mask = question_mask.unsqueeze(1).unsqueeze(1)         # (batch_size, 1, 1, max_words)\n     \n    reply_input_mask = reply_input!=0\n    reply_input_mask = reply_input_mask.unsqueeze(1)  # (batch_size, 1, max_words)\n    reply_input_mask = reply_input_mask & subsequent_mask(reply_input.size(-1)).type_as(reply_input_mask.data) \n    reply_input_mask = reply_input_mask.unsqueeze(1) # (batch_size, 1, max_words, max_words)\n    reply_target_mask = reply_target!=0              # (batch_size, max_words)\n    \n    return question_mask, reply_input_mask, reply_target_mask","metadata":{"execution":{"iopub.status.busy":"2024-07-02T20:17:33.381114Z","iopub.execute_input":"2024-07-02T20:17:33.381467Z","iopub.status.idle":"2024-07-02T20:17:33.388963Z","shell.execute_reply.started":"2024-07-02T20:17:33.381434Z","shell.execute_reply":"2024-07-02T20:17:33.388016Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"import os\n\ndef create_directory(path):\n    if not os.path.exists(path):\n        os.makedirs(path)\n        print(f\"Directory created at {path}\")\n    else:\n        print(f\"Directory already exists at {path}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-02T20:17:33.390255Z","iopub.execute_input":"2024-07-02T20:17:33.390611Z","iopub.status.idle":"2024-07-02T20:17:33.397673Z","shell.execute_reply.started":"2024-07-02T20:17:33.390578Z","shell.execute_reply":"2024-07-02T20:17:33.396839Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"# Architecture","metadata":{}},{"cell_type":"markdown","source":"## Embeddings","metadata":{}},{"cell_type":"code","source":"class Embeddings(nn.Module):\n    \"\"\"\n    Implements embeddings of the words and adds their positional encodings. \n    \"\"\"\n    def __init__(self, vocab_size, d_model, max_len = 50, num_layers = 6):\n        super(Embeddings, self).__init__()\n        self.d_model = d_model\n        self.dropout = nn.Dropout(0.1)\n        self.embed = nn.Embedding(vocab_size, d_model)\n        self.pe = self.create_positinal_encoding(max_len, self.d_model)     # (1, max_len, d_model)\n        self.te = self.create_positinal_encoding(num_layers, self.d_model)  # (1, num_layers, d_model)\n        self.dropout = nn.Dropout(0.1)\n        \n    def create_positinal_encoding(self, max_len, d_model):\n        pe = torch.zeros(max_len, d_model).to(device)\n        for pos in range(max_len):   # for each position of the word\n            for i in range(0, d_model, 2):   # for each dimension of the each position\n                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/d_model)))\n                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n        pe = pe.unsqueeze(0)   # include the batch size\n        return pe\n        \n    def forward(self, embedding, layer_idx):\n        if layer_idx == 0:\n            embedding = self.embed(embedding) * math.sqrt(self.d_model)\n        embedding += self.pe[:, :embedding.size(1)]   # pe will automatically be expanded with the same batch size as encoded_words\n        # embedding: (batch_size, max_len, d_model), te: (batch_size, 1, d_model)\n        embedding += self.te[:, layer_idx, :].unsqueeze(1).repeat(1, embedding.size(1), 1)\n        embedding = self.dropout(embedding)\n        return embedding\n\nclass EmbeddingsLSTM(nn.Module):\n    \"\"\"\n    Implements embeddingsLSTM of the words and adds their positional encodings. \n    \"\"\"\n    def __init__(self, vocab_size, d_model, num_layers = 6):\n        super(EmbeddingsLSTM, self).__init__()\n        self.d_model = d_model\n        self.dropout = nn.Dropout(0.1)\n        self.embed = nn.Embedding(vocab_size, d_model)\n        self.lstm = nn.LSTM(d_model, d_model, num_layers=num_layers)\n        \n    def forward(self, embedding, layer_idx, hidden, cell_state):\n        if layer_idx == 0:\n            embedding = self.embed(embedding) * math.sqrt(self.d_model)\n        # Pass the embeddings through the LSTM\\\n        embedding, (hidden, cell_state) = self.lstm(embedding, (hidden, cell_state))\n        print()\n        print(embedding.size())\n        print(embedding)\n        return embedding, hidden, cell_state\n\nclass PretrainedEmbedding(nn.Module):\n    \"\"\"\n    Implements embeddingsLSTM of the words and adds their positional encodings. \n    \"\"\"\n    def __init__(self, vocab_size, d_model, max_len = 50, num_layers = 6):\n        super(PretrainedEmbedding, self).__init__()\n        self.d_model = d_model\n        self.dropout = nn.Dropout(0.1)\n        self.embed = nn.Embedding(vocab_size, d_model)\n        self.pe = self.create_positinal_encoding(max_len, self.d_model)     # (1, max_len, d_model)\n        self.te = self.create_positinal_encoding(num_layers, self.d_model)  # (1, num_layers, d_model)\n        self.dropout = nn.Dropout(0.1)\n        \n    def create_positinal_encoding(self, max_len, d_model):\n        pe = torch.zeros(max_len, d_model).to(device)\n        for pos in range(max_len):   # for each position of the word\n            for i in range(0, d_model, 2):   # for each dimension of the each position\n                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/d_model)))\n                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n        pe = pe.unsqueeze(0)   # include the batch size\n        return pe\n        \n    def forward(self, embedding, layer_idx):\n        if layer_idx == 0:\n            embedding = self.embed(embedding) * math.sqrt(self.d_model)\n        embedding += self.pe[:, :embedding.size(1)]   # pe will automatically be expanded with the same batch size as encoded_words\n        # embedding: (batch_size, max_len, d_model), te: (batch_size, 1, d_model)\n        embedding += self.te[:, layer_idx, :].unsqueeze(1).repeat(1, embedding.size(1), 1)\n        embedding = self.dropout(embedding)\n        print()\n        print(embedding.size())\n        print(embedding)\n        return embedding","metadata":{"execution":{"iopub.status.busy":"2024-07-02T20:17:33.398914Z","iopub.execute_input":"2024-07-02T20:17:33.399270Z","iopub.status.idle":"2024-07-02T20:17:33.421634Z","shell.execute_reply.started":"2024-07-02T20:17:33.399239Z","shell.execute_reply":"2024-07-02T20:17:33.420718Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"## Multi-Head Attention","metadata":{}},{"cell_type":"code","source":"class MultiHeadAttention(nn.Module):\n    \n    def __init__(self, heads, d_model):\n        super(MultiHeadAttention, self).__init__()\n        assert d_model % heads == 0\n        self.d_k = d_model // heads\n        self.heads = heads\n        self.dropout = nn.Dropout(0.1)\n        self.query = nn.Linear(d_model, d_model)\n        self.key = nn.Linear(d_model, d_model)\n        self.value = nn.Linear(d_model, d_model)\n        self.concat = nn.Linear(d_model, d_model)\n        \n    def forward(self, query, key, value, mask):\n        \"\"\"\n        query, key, value of shape: (batch_size, max_len, 512)\n        mask of shape: (batch_size, 1, 1, max_words)\n        \"\"\"\n        # (batch_size, max_len, 512)\n        query = self.query(query)\n        key = self.key(key)        \n        value = self.value(value)   \n        \n        # (batch_size, max_len, 512) --> (batch_size, max_len, h, d_k) --> (batch_size, h, max_len, d_k)\n        query = query.view(query.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)   \n        key = key.view(key.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)  \n        value = value.view(value.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)  \n        \n        # (batch_size, h, max_len, d_k) matmul (batch_size, h, d_k, max_len) --> (batch_size, h, max_len, max_len)\n        scores = torch.matmul(query, key.permute(0,1,3,2)) / math.sqrt(query.size(-1))\n        scores = scores.masked_fill(mask == 0, -1e9)    # (batch_size, h, max_len, max_len)\n        weights = F.softmax(scores, dim = -1)           # (batch_size, h, max_len, max_len)\n        weights = self.dropout(weights)\n        # (batch_size, h, max_len, max_len) matmul (batch_size, h, max_len, d_k) --> (batch_size, h, max_len, d_k)\n        context = torch.matmul(weights, value)\n        # (batch_size, h, max_len, d_k) --> (batch_size, max_len, h, d_k) --> (batch_size, max_len, h * d_k)\n        context = context.permute(0,2,1,3).contiguous().view(context.shape[0], -1, self.heads * self.d_k)\n        # (batch_size, max_len, h * d_k)\n        interacted = self.concat(context)\n        return interacted ","metadata":{"execution":{"iopub.status.busy":"2024-07-02T20:17:33.423056Z","iopub.execute_input":"2024-07-02T20:17:33.423711Z","iopub.status.idle":"2024-07-02T20:17:33.435766Z","shell.execute_reply.started":"2024-07-02T20:17:33.423675Z","shell.execute_reply":"2024-07-02T20:17:33.435028Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"## Feed Forward Neural Network","metadata":{}},{"cell_type":"code","source":"class FeedForward(nn.Module):\n    def __init__(self, d_model, middle_dim = 2048):\n        super(FeedForward, self).__init__()\n        \n        self.fc1 = nn.Linear(d_model, middle_dim)\n        self.fc2 = nn.Linear(middle_dim, d_model)\n        self.dropout = nn.Dropout(0.1)\n\n    def forward(self, x):\n        out = F.relu(self.fc1(x))\n        out = self.fc2(self.dropout(out))\n        return out\n\nclass FeedForwardLSTM(nn.Module):\n    def __init__(self, d_model, middle_dim = 2048):\n        super(FeedForwardLSTM, self).__init__()\n        \n        self.fc1 = nn.Linear(d_model, middle_dim)\n        self.lstm = nn.LSTM(middle_dim, middle_dim)\n        self.fc2 = nn.Linear(middle_dim, d_model)\n        self.dropout = nn.Dropout(0.1)\n\n    def forward(self, x):\n        out = F.relu(self.fc1(x))\n        out, _ = self.lstm(out)\n        out = self.fc2(self.dropout(out))\n        return out\n\nclass FeedForwardLayerNoReg(nn.Module):\n    def __init__(self, d_model, middle_dim = 2048):\n        super(FeedForward, self).__init__()\n        \n        self.fc1 = nn.Linear(d_model, middle_dim)\n        self.fc2 = nn.Linear(middle_dim, d_model)\n\n    def forward(self, x):\n        out = F.relu(self.fc1(x))\n        out = self.fc2(out)\n        return out","metadata":{"execution":{"iopub.status.busy":"2024-07-02T20:17:33.437039Z","iopub.execute_input":"2024-07-02T20:17:33.437331Z","iopub.status.idle":"2024-07-02T20:17:33.450588Z","shell.execute_reply.started":"2024-07-02T20:17:33.437306Z","shell.execute_reply":"2024-07-02T20:17:33.449684Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"## Encoder","metadata":{}},{"cell_type":"code","source":"class EncoderLayer(nn.Module):\n    def __init__(self, d_model, heads):\n        super(EncoderLayer, self).__init__()\n        self.layernorm = nn.LayerNorm(d_model)\n        self.self_multihead = MultiHeadAttention(heads, d_model)\n        self.feed_forward = FeedForward(d_model)\n        self.dropout = nn.Dropout(0.1)\n\n    def forward(self, embeddings, mask):\n        interacted = self.dropout(self.self_multihead(embeddings, embeddings, embeddings, mask))\n        interacted = self.layernorm(interacted + embeddings)\n        feed_forward_out = self.dropout(self.feed_forward(interacted))\n        encoded = self.layernorm(feed_forward_out + interacted)\n        return encoded\n\nclass EncoderLayerNoReg(nn.Module):\n    def __init__(self, d_model, heads):\n        super(EncoderLayerNoReg, self).__init__()\n        self.layernorm = nn.LayerNorm(d_model)\n        self.self_multihead = MultiHeadAttention(heads, d_model)\n        self.feed_forward = FeedForward(d_model)\n\n    def forward(self, embeddings, mask):\n        interacted = self.self_multihead(embeddings, embeddings, embeddings, mask)\n        interacted = self.layernorm(interacted + embeddings)\n        feed_forward_out = self.feed_forward(interacted)\n        encoded = self.layernorm(feed_forward_out + interacted)\n        return encoded\n        \n        ","metadata":{"execution":{"iopub.status.busy":"2024-07-02T20:17:33.451709Z","iopub.execute_input":"2024-07-02T20:17:33.452046Z","iopub.status.idle":"2024-07-02T20:17:33.461741Z","shell.execute_reply.started":"2024-07-02T20:17:33.452019Z","shell.execute_reply":"2024-07-02T20:17:33.460847Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"## Decoder","metadata":{}},{"cell_type":"code","source":"class DecoderLayer(nn.Module):\n    \n    def __init__(self, d_model, heads):\n        super(DecoderLayer, self).__init__()\n        self.layernorm = nn.LayerNorm(d_model)\n        self.self_multihead = MultiHeadAttention(heads, d_model)\n        self.src_multihead = MultiHeadAttention(heads, d_model)\n        self.feed_forward = FeedForward(d_model)\n        self.dropout = nn.Dropout(0.1)\n        \n    def forward(self, embeddings, encoded, src_mask, target_mask):\n        query = self.dropout(self.self_multihead(embeddings, embeddings, embeddings, target_mask))\n        query = self.layernorm(query + embeddings)\n        interacted = self.dropout(self.src_multihead(query, encoded, encoded, src_mask))\n        interacted = self.layernorm(interacted + query)\n        feed_forward_out = self.dropout(self.feed_forward(interacted))\n        decoded = self.layernorm(feed_forward_out + interacted)\n        return decoded\n\nclass DecoderLayerNoReg(nn.Module):\n    def __init__(self, d_model, heads):\n        super(DecoderLayerNoReg, self).__init__()\n        self.layernorm = nn.LayerNorm(d_model)\n        self.self_multihead = MultiHeadAttention(heads, d_model)\n        self.src_multihead = MultiHeadAttention(heads, d_model)\n        self.feed_forward = FeedForward(d_model)\n        \n    def forward(self, embeddings, encoded, src_mask, target_mask):\n        query = self.self_multihead(embeddings, embeddings, embeddings, target_mask)\n        query = self.layernorm(query + embeddings)\n        interacted = self.src_multihead(query, encoded, encoded, src_mask)\n        interacted = self.layernorm(interacted + query)\n        feed_forward_out = self.feed_forward(interacted)\n        decoded = self.layernorm(feed_forward_out + interacted)\n        return decoded","metadata":{"execution":{"iopub.status.busy":"2024-07-02T20:17:33.463025Z","iopub.execute_input":"2024-07-02T20:17:33.463365Z","iopub.status.idle":"2024-07-02T20:17:33.475859Z","shell.execute_reply.started":"2024-07-02T20:17:33.463339Z","shell.execute_reply":"2024-07-02T20:17:33.475008Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"## Transformer Architecture","metadata":{}},{"cell_type":"code","source":"class Transformer(nn.Module):\n    \n    def __init__(self, d_model, heads, num_layers, word_map, max_len = 50):\n        super(Transformer, self).__init__()\n        \n        self.d_model = d_model\n        self.num_layers = num_layers\n        self.vocab_size = len(word_map)\n        self.embed = Embeddings(self.vocab_size, d_model, num_layers = num_layers, max_len = max_len)\n        self.encoder = EncoderLayer(d_model, heads) \n        self.decoder = DecoderLayer(d_model, heads)\n        self.logit = nn.Linear(d_model, self.vocab_size)\n        \n    def encode(self, src_embeddings, src_mask):\n        for i in range(self.num_layers):\n            src_embeddings = self.embed(src_embeddings, i)\n            src_embeddings = self.encoder(src_embeddings, src_mask)\n        return src_embeddings\n    \n    def decode(self, tgt_embeddings, target_mask, src_embeddings, src_mask):\n        for i in range(self.num_layers):\n            tgt_embeddings = self.embed(tgt_embeddings, i)\n            tgt_embeddings = self.decoder(tgt_embeddings, src_embeddings, src_mask, target_mask)\n        return tgt_embeddings\n        \n    def forward(self, src_words, src_mask, target_words, target_mask):\n        encoded = self.encode(src_words, src_mask)\n        decoded = self.decode(target_words, target_mask, encoded, src_mask)\n        out = F.log_softmax(self.logit(decoded), dim = 2)\n        return out\n\nclass TransformerNoReg(nn.Module):\n    \n    def __init__(self, d_model, heads, num_layers, word_map, max_len = 50):\n        super(TransformerNoReg, self).__init__()\n        \n        self.d_model = d_model\n        self.num_layers = num_layers\n        self.vocab_size = len(word_map)\n        self.embed = Embeddings(self.vocab_size, d_model, num_layers = num_layers, max_len = max_len)\n        self.encoder = EncoderLayerNoReg(d_model, heads) \n        self.decoder = DecoderLayerNoReg(d_model, heads)\n        self.logit = nn.Linear(d_model, self.vocab_size)\n        \n    def encode(self, src_embeddings, src_mask):\n        for i in range(self.num_layers):\n            src_embeddings = self.embed(src_embeddings, i)\n            src_embeddings = self.encoder(src_embeddings, src_mask)\n        return src_embeddings\n    \n    def decode(self, tgt_embeddings, target_mask, src_embeddings, src_mask):\n        for i in range(self.num_layers):\n            tgt_embeddings = self.embed(tgt_embeddings, i)\n            tgt_embeddings = self.decoder(tgt_embeddings, src_embeddings, src_mask, target_mask)\n        return tgt_embeddings\n        \n    def forward(self, src_words, src_mask, target_words, target_mask):\n        encoded = self.encode(src_words, src_mask)\n        decoded = self.decode(target_words, target_mask, encoded, src_mask)\n        out = F.log_softmax(self.logit(decoded), dim = 2)\n        return out\n\nclass TransformerLSTM(nn.Module):\n    \n    def __init__(self, d_model, heads, num_layers, word_map, max_len = 50):\n        super(TransformerLSTM, self).__init__()\n        \n        self.d_model = d_model\n        self.num_layers = num_layers\n        self.vocab_size = len(word_map)\n        self.embed = EmbeddingsLSTM(self.vocab_size, d_model, num_layers = num_layers)\n        self.encoder = EncoderLayer(d_model, heads) \n        self.decoder = DecoderLayer(d_model, heads)\n        self.logit = nn.Linear(d_model, self.vocab_size)\n        self.max_len = max_len\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        \n    def encode(self, src_embeddings, src_mask):\n        encoder_hidden = torch.zeros([self.num_layers, self.max_len, self.d_model]).to(self.device) # 1 = number of LSTM layers\n        cell_state = torch.zeros([self.num_layers, self.max_len, self.d_model]).to(self.device)\n        for i in range(self.num_layers):\n            src_embeddings, encoder_hidden, cell_state = self.embed(src_embeddings, i, encoder_hidden, cell_state)\n            src_embeddings = self.encoder(src_embeddings, src_mask)\n        return src_embeddings, encoder_hidden, cell_state\n    \n    def decode(self, tgt_embeddings, target_mask, src_embeddings, src_mask, encoder_hidden, cell_state):\n        zeros = torch.zeros((self.num_layers, 1, self.d_model), device=encoder_hidden.device)\n        encoder_hidden = torch.cat((encoder_hidden, zeros), dim=1)\n        decoder_input = torch.Tensor([[0]]).long().to(self.device) # 0 = SOS_token\n        decoder_hidden = encoder_hidden\n        print(decoder_hidden.size())\n        for i in range(self.num_layers):\n            tgt_embeddings, decoder_hidden, cell_state = self.embed(tgt_embeddings, i, decoder_hidden, cell_state)\n            tgt_embeddings = self.decoder(tgt_embeddings, src_embeddings, src_mask, target_mask)\n        return tgt_embeddings\n        \n    def forward(self, src_words, src_mask, target_words, target_mask):\n        encoded, encoder_hidden, cell_state = self.encode(src_words, src_mask)\n        decoded = self.decode(target_words, target_mask, encoded, src_mask, encoder_hidden, cell_state)\n        out = F.log_softmax(self.logit(decoded), dim = 2)\n        return out\n\nclass TransformerPreTrainedEmbedding(nn.Module):\n    def __init__(self, d_model, heads, num_layers, word_map, max_len = 50):\n        super(TransformerPreTrainedEmbedding, self).__init__()\n        \n        self.d_model = d_model\n        self.num_layers = num_layers\n        self.vocab_size = len(word_map)\n        self.embed = PretrainedEmbedding(self.vocab_size, d_model, num_layers = num_layers, max_len = max_len)\n        self.encoder = EncoderLayer(d_model, heads) \n        self.decoder = DecoderLayer(d_model, heads)\n        self.logit = nn.Linear(d_model, self.vocab_size)\n        \n    def encode(self, src_embeddings, src_mask):\n        for i in range(self.num_layers):\n            src_embeddings = self.embed(src_embeddings, i)\n            src_embeddings = self.encoder(src_embeddings, src_mask)\n        return src_embeddings\n    \n    def decode(self, tgt_embeddings, target_mask, src_embeddings, src_mask):\n        for i in range(self.num_layers):\n            tgt_embeddings = self.embed(tgt_embeddings, i)\n            tgt_embeddings = self.decoder(tgt_embeddings, src_embeddings, src_mask, target_mask)\n        return tgt_embeddings\n        \n    def forward(self, src_words, src_mask, target_words, target_mask):\n        encoded = self.encode(src_words, src_mask)\n        decoded = self.decode(target_words, target_mask, encoded, src_mask)\n        out = F.log_softmax(self.logit(decoded), dim = 2)\n        return out\n\nclass TransformerDecoderOnly(nn.Module):    \n    def __init__(self, d_model, heads, num_layers, word_map, max_len = 50):\n        super(TransformerDecoderOnly, self).__init__()\n        \n        self.d_model = d_model\n        self.num_layers = num_layers\n        self.vocab_size = len(word_map)\n        self.embed = Embeddings(self.vocab_size, d_model, num_layers = num_layers, max_len = max_len)\n        self.encoder = EncoderLayer(d_model, heads) \n        self.decoder = DecoderLayer(d_model, heads)\n        self.logit = nn.Linear(d_model, self.vocab_size)\n    \n    def decode(self, tgt_embeddings, target_mask, src_embeddings, src_mask):\n        for i in range(self.num_layers):\n            tgt_embeddings = self.embed(tgt_embeddings, i)\n            tgt_embeddings = self.decoder(tgt_embeddings, src_embeddings, src_mask, target_mask)\n        return tgt_embeddings\n        \n    def forward(self, src_words, src_mask, target_words, target_mask):\n        encoded = self.embed(src_words, 0)\n        decoded = self.decode(target_words, target_mask, encoded, src_mask)\n        out = F.log_softmax(self.logit(decoded), dim = 2)\n        return out","metadata":{"execution":{"iopub.status.busy":"2024-07-02T20:17:33.477782Z","iopub.execute_input":"2024-07-02T20:17:33.478079Z","iopub.status.idle":"2024-07-02T20:17:33.514932Z","shell.execute_reply.started":"2024-07-02T20:17:33.478055Z","shell.execute_reply":"2024-07-02T20:17:33.514023Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"class AdamWarmup:\n    \n    def __init__(self, model_size, warmup_steps, optimizer):\n        \n        self.model_size = model_size\n        self.warmup_steps = warmup_steps\n        self.optimizer = optimizer\n        self.current_step = 0\n        self.lr = 0\n        \n    def get_lr(self):\n        return self.model_size ** (-0.5) * min(self.current_step ** (-0.5), self.current_step * self.warmup_steps ** (-1.5))\n        \n    def step(self):\n        # Increment the number of steps each time we call the step function\n        self.current_step += 1\n        lr = self.get_lr()\n        for param_group in self.optimizer.param_groups:\n            param_group['lr'] = lr\n        # update the learning rate\n        self.lr = lr\n        self.optimizer.step()       \n        ","metadata":{"execution":{"iopub.status.busy":"2024-07-02T20:17:33.516094Z","iopub.execute_input":"2024-07-02T20:17:33.516425Z","iopub.status.idle":"2024-07-02T20:17:33.527522Z","shell.execute_reply.started":"2024-07-02T20:17:33.516394Z","shell.execute_reply":"2024-07-02T20:17:33.526734Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"class LossWithLS(nn.Module):\n\n    def __init__(self, size, smooth):\n        super(LossWithLS, self).__init__()\n        self.criterion = nn.KLDivLoss(size_average=False, reduce=False)\n        self.confidence = 1.0 - smooth\n        self.smooth = smooth\n        self.size = size\n        \n    def forward(self, prediction, target, mask):\n        \"\"\"\n        prediction of shape: (batch_size, max_words, vocab_size)\n        target and mask of shape: (batch_size, max_words)\n        \"\"\"\n        prediction = prediction.view(-1, prediction.size(-1))   # (batch_size * max_words, vocab_size)\n        target = target.contiguous().view(-1)   # (batch_size * max_words)\n        mask = mask.float()\n        mask = mask.view(-1)       # (batch_size * max_words)\n        labels = prediction.data.clone()\n        labels.fill_(self.smooth / (self.size - 1))\n        labels.scatter_(1, target.data.unsqueeze(1), self.confidence)\n        loss = self.criterion(prediction, labels)    # (batch_size * max_words, vocab_size)\n        loss = (loss.sum(1) * mask).sum() / mask.sum()\n        return loss\n        ","metadata":{"execution":{"iopub.status.busy":"2024-07-02T20:17:33.528622Z","iopub.execute_input":"2024-07-02T20:17:33.529363Z","iopub.status.idle":"2024-07-02T20:17:33.540886Z","shell.execute_reply.started":"2024-07-02T20:17:33.529331Z","shell.execute_reply":"2024-07-02T20:17:33.540153Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"# Define Neptune Experiment","metadata":{}},{"cell_type":"code","source":"project = \"andialifs/fluent-tesis-24\"\napi_token = \"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJjZTY2YWQ3My04OTBkLTQ2OWUtYTc1Ni1jYjk0MGZhMWFiNGEifQ==\"\n\ndef neptune_init(name):\n    run = neptune.init_run(\n        project=project,\n        api_token=api_token,\n        name=name\n    )\n    return run","metadata":{"execution":{"iopub.status.busy":"2024-07-02T20:17:33.542228Z","iopub.execute_input":"2024-07-02T20:17:33.542496Z","iopub.status.idle":"2024-07-02T20:17:33.550662Z","shell.execute_reply.started":"2024-07-02T20:17:33.542473Z","shell.execute_reply":"2024-07-02T20:17:33.549840Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"import os\n\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:8000\"\nos.getenv(\"PYTORCH_CUDA_ALLOC_CONF\")","metadata":{"execution":{"iopub.status.busy":"2024-07-02T20:17:33.555608Z","iopub.execute_input":"2024-07-02T20:17:33.555852Z","iopub.status.idle":"2024-07-02T20:17:33.563546Z","shell.execute_reply.started":"2024-07-02T20:17:33.555831Z","shell.execute_reply":"2024-07-02T20:17:33.562660Z"},"trusted":true},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"'max_split_size_mb:8000'"},"metadata":{}}]},{"cell_type":"markdown","source":"# Function","metadata":{}},{"cell_type":"markdown","source":"## Train","metadata":{}},{"cell_type":"code","source":"def train(train_loader, transformer, criterion, epoch):\n    transformer.train()\n    sum_loss = 0\n    count = 0\n\n    for i, (question, reply) in enumerate(train_loader):\n        \n        samples = question.shape[0]\n\n        # Move to device\n        question = question.to(device)\n        reply = reply.to(device)\n\n        # Prepare Target Data\n        reply_input = reply[:, :-1]\n        reply_target = reply[:, 1:]\n\n        # Create mask and add dimensions\n        question_mask, reply_input_mask, reply_target_mask = create_masks(question, reply_input, reply_target)\n\n        # Get the transformer outputs\n        out = transformer(question, question_mask, reply_input, reply_input_mask)\n\n        # Compute the loss\n        loss = criterion(out, reply_target, reply_target_mask)\n        \n        # Backprop\n        transformer_optimizer.optimizer.zero_grad()\n        loss.backward()\n        transformer_optimizer.step()\n        \n        sum_loss += loss.item() * samples\n        count += samples\n        \n        if i % 100 == 0:\n            print(\"Epoch [{}][{}/{}]\\tLoss: {:.3f}\".format(epoch, i, len(train_loader), sum_loss/count))\n            \n        torch.cuda.empty_cache() \n    \n    return sum_loss/count","metadata":{"execution":{"iopub.status.busy":"2024-07-02T20:17:33.564656Z","iopub.execute_input":"2024-07-02T20:17:33.564996Z","iopub.status.idle":"2024-07-02T20:17:33.574590Z","shell.execute_reply.started":"2024-07-02T20:17:33.564947Z","shell.execute_reply":"2024-07-02T20:17:33.573653Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"## Evaluate","metadata":{}},{"cell_type":"code","source":"def evaluate(transformer, question, question_mask, max_len, word_map):\n    \"\"\"\n    Performs Greedy Decoding with a batch size of 1\n    \"\"\"\n    rev_word_map = {v: k for k, v in word_map.items()}\n    transformer.eval()\n    start_token = word_map['<start>']\n    encoded = transformer.encode(question, question_mask)\n    words = torch.LongTensor([[start_token]]).to(device)\n    \n    for step in range(max_len - 1):\n        size = words.shape[1]\n        target_mask = torch.triu(torch.ones(size, size)).transpose(0, 1).type(dtype=torch.uint8)\n        target_mask = target_mask.to(device).unsqueeze(0).unsqueeze(0)\n        decoded = transformer.decode(words, target_mask, encoded, question_mask)\n        predictions = transformer.logit(decoded[:, -1])\n        _, next_word = torch.max(predictions, dim = 1)\n        next_word = next_word.item()\n        if next_word == word_map['<end>']:\n            break\n        words = torch.cat([words, torch.LongTensor([[next_word]]).to(device)], dim = 1)   # (1,step+2)\n        \n    # Construct Sentence\n    if words.dim() == 2:\n        words = words.squeeze(0)\n        words = words.tolist()\n        \n    sen_idx = [w for w in words if w not in {word_map['<start>']}]\n    sentence = ' '.join([rev_word_map[sen_idx[k]] for k in range(len(sen_idx))])\n    \n    return sentence","metadata":{"execution":{"iopub.status.busy":"2024-07-02T20:17:33.575799Z","iopub.execute_input":"2024-07-02T20:17:33.576127Z","iopub.status.idle":"2024-07-02T20:17:33.587390Z","shell.execute_reply.started":"2024-07-02T20:17:33.576103Z","shell.execute_reply":"2024-07-02T20:17:33.586545Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"# Experiment","metadata":{}},{"cell_type":"markdown","source":"## Transformers without reg","metadata":{}},{"cell_type":"code","source":"d_model = [512, 1024, 2048, 4096]\nheads = [8, 16, 32]\nnum_layers = [5, 10]\nepochs = 100\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nwith open('WORDMAP_corpus_KBFILKOM.json', 'r') as j:\n    word_map = json.load(j)\n\ntransformer_experiment = pd.DataFrame(columns = ['experiment_id', 'd_model', 'heads', 'num_layers', 'train_loss'])\nloss_history = {}\n\nexperiment_id = -1\n\nfor d_m in d_model:\n    for h in heads:\n        for n_l in num_layers: \n            parameters = {\n                'd_model': d_m,\n                'heads': h,\n                'num_layers': n_l\n            }\n            \n            experiment_id += 1\n            \n            if experiment_id < 9:\n                print('\\Skipping experiment {} with d_model {}, heads{}, num_layers{}\\n'.format(experiment_id, d_m, h, n_l))\n                continue\n            \n            print('\\nRunning for experiment {} with d_model {}, heads{}, num_layers{}\\n'.format(experiment_id, d_m, h, n_l))\n            name = \"experiment_2724_noreg_\" + str(experiment_id)\n\n            run = neptune_init(name)\n            run['parameters'] = parameters\n            \n            transformer_experiment.loc[experiment_id, 'experiment_id'] = 'experiment_{}'.format(str(experiment_id))\n            transformer_experiment.loc[experiment_id, 'd_model'] = d_m\n            transformer_experiment.loc[experiment_id, 'heads'] = h\n            transformer_experiment.loc[experiment_id, 'num_layers'] = n_l\n\n            transformer = Transformer(d_model = d_m, heads = h, num_layers = n_l, word_map = word_map, max_len=95)\n            transformer = transformer.to(device)\n            adam_optimizer = torch.optim.Adam(transformer.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-9)\n            transformer_optimizer = AdamWarmup(model_size = d_m, warmup_steps = 4000, optimizer = adam_optimizer)\n            criterion = LossWithLS(len(word_map), 0.2)\n\n            loss_list_experiment = []\n            for epoch in range(epochs):\n                loss_train = train(train_loader, transformer, criterion, epoch)\n\n                state = {'epoch': epoch, 'transformer': transformer, 'transformer_optimizer': transformer_optimizer}\n                # torch.save(state, 'checkpoint_experiment_' + str(epoch) + 'id_'+ str(experiment_id) +'.pth.tar')\n\n                loss_list_experiment.append(loss_train)\n\n                run['train/loss'].append(loss_train)\n\n            transformer_experiment.loc[experiment_id, 'train_loss'] = loss_train\n            loss_history['experiment_{}'.format(str(experiment_id))] = loss_list_experiment\n            \n            del transformer\n            del loss_train\n            del state\n            torch.cuda.empty_cache() \n            run.stop()\n","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-07-02T20:17:33.588528Z","iopub.execute_input":"2024-07-02T20:17:33.588788Z","iopub.status.idle":"2024-07-02T20:17:47.395240Z","shell.execute_reply.started":"2024-07-02T20:17:33.588766Z","shell.execute_reply":"2024-07-02T20:17:47.393722Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"\\Skipping experiment 0 with d_model 512, heads8, num_layers5\n\n\\Skipping experiment 1 with d_model 512, heads8, num_layers10\n\n\\Skipping experiment 2 with d_model 512, heads16, num_layers5\n\n\\Skipping experiment 3 with d_model 512, heads16, num_layers10\n\n\\Skipping experiment 4 with d_model 512, heads32, num_layers5\n\n\\Skipping experiment 5 with d_model 512, heads32, num_layers10\n\n\\Skipping experiment 6 with d_model 1024, heads8, num_layers5\n\n\\Skipping experiment 7 with d_model 1024, heads8, num_layers10\n\n\\Skipping experiment 8 with d_model 1024, heads16, num_layers5\n\n\nRunning for experiment 9 with d_model 1024, heads16, num_layers10\n\n","output_type":"stream"},{"name":"stderr","text":"[neptune] [warning] NeptuneWarning: The following monitoring options are disabled by default in interactive sessions: 'capture_stdout', 'capture_stderr', 'capture_traceback', and 'capture_hardware_metrics'. To enable them, set each parameter to 'True' when initializing the run. The monitoring will continue until you call run.stop() or the kernel stops. Also note: Your source files can only be tracked if you pass the path(s) to the 'source_code' argument. For help, see the Neptune docs: https://docs.neptune.ai/logging/source_code/\n","output_type":"stream"},{"name":"stdout","text":"[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/andialifs/fluent-tesis-24/e/FLUENT24-59\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n  warnings.warn(warning.format(ret))\n","output_type":"stream"},{"name":"stdout","text":"Epoch [0][0/12]\tLoss: 5.255\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","Cell \u001b[0;32mIn[31], line 50\u001b[0m\n\u001b[1;32m     48\u001b[0m loss_list_experiment \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m---> 50\u001b[0m     loss_train \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m     state \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m: epoch, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer\u001b[39m\u001b[38;5;124m'\u001b[39m: transformer, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer_optimizer\u001b[39m\u001b[38;5;124m'\u001b[39m: transformer_optimizer}\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;66;03m# torch.save(state, 'checkpoint_experiment_' + str(epoch) + 'id_'+ str(experiment_id) +'.pth.tar')\u001b[39;00m\n","Cell \u001b[0;32mIn[29], line 22\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(train_loader, transformer, criterion, epoch)\u001b[0m\n\u001b[1;32m     19\u001b[0m question_mask, reply_input_mask, reply_target_mask \u001b[38;5;241m=\u001b[39m create_masks(question, reply_input, reply_target)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Get the transformer outputs\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreply_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreply_input_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Compute the loss\u001b[39;00m\n\u001b[1;32m     25\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(out, reply_target, reply_target_mask)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[24], line 28\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, src_words, src_mask, target_words, target_mask)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, src_words, src_mask, target_words, target_mask):\n\u001b[1;32m     27\u001b[0m     encoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode(src_words, src_mask)\n\u001b[0;32m---> 28\u001b[0m     decoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_words\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m     out \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mlog_softmax(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogit(decoded), dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n","Cell \u001b[0;32mIn[24], line 23\u001b[0m, in \u001b[0;36mTransformer.decode\u001b[0;34m(self, tgt_embeddings, target_mask, src_embeddings, src_mask)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers):\n\u001b[1;32m     22\u001b[0m     tgt_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed(tgt_embeddings, i)\n\u001b[0;32m---> 23\u001b[0m     tgt_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtgt_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tgt_embeddings\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[23], line 16\u001b[0m, in \u001b[0;36mDecoderLayer.forward\u001b[0;34m(self, embeddings, encoded, src_mask, target_mask)\u001b[0m\n\u001b[1;32m     14\u001b[0m interacted \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msrc_multihead(query, encoded, encoded, src_mask))\n\u001b[1;32m     15\u001b[0m interacted \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayernorm(interacted \u001b[38;5;241m+\u001b[39m query)\n\u001b[0;32m---> 16\u001b[0m feed_forward_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minteracted\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m decoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayernorm(feed_forward_out \u001b[38;5;241m+\u001b[39m interacted)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m decoded\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/dropout.py:58\u001b[0m, in \u001b[0;36mDropout.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:1266\u001b[0m, in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[1;32m   1265\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1266\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mdropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 36.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 26.12 MiB is free. Process 2014 has 15.87 GiB memory in use. Of the allocated memory 15.21 GiB is allocated by PyTorch, and 369.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 36.00 MiB. GPU 0 has a total capacty of 15.89 GiB of which 26.12 MiB is free. Process 2014 has 15.87 GiB memory in use. Of the allocated memory 15.21 GiB is allocated by PyTorch, and 369.81 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF","output_type":"error"},{"name":"stdout","text":"[neptune] [error  ] Unexpected error occurred in Neptune background thread: Killing Neptune ping thread. Your run's status will not be updated and the run will be shown as inactive.\n","output_type":"stream"},{"name":"stderr","text":"Exception in thread NeptunePing:\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/neptune/internal/backends/swagger_client_wrapper.py\", line 111, in __call__\n    return FinishedApiResponseFuture(future.response())  # wait synchronously\n  File \"/opt/conda/lib/python3.10/site-packages/bravado/http_future.py\", line 200, in response\n    swagger_result = self._get_swagger_result(incoming_response)\n  File \"/opt/conda/lib/python3.10/site-packages/bravado/http_future.py\", line 124, in wrapper\n    return func(self, *args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/bravado/http_future.py\", line 300, in _get_swagger_result\n    unmarshal_response(\n  File \"/opt/conda/lib/python3.10/site-packages/bravado/http_future.py\", line 353, in unmarshal_response\n    raise_on_expected(incoming_response)\n  File \"/opt/conda/lib/python3.10/site-packages/bravado/http_future.py\", line 420, in raise_on_expected\n    raise make_http_exception(\nbravado.exception.HTTPNotFound: 404 Not Found\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n    self.run()\n  File \"/opt/conda/lib/python3.10/site-packages/neptune/internal/threading/daemon.py\", line 96, in run\n    self.work()\n  File \"/opt/conda/lib/python3.10/site-packages/neptune/internal/threading/daemon.py\", line 121, in wrapper\n    result = func(self_, *args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/neptune/internal/utils/ping_background_job.py\", line 72, in work\n    self._container.ping()\n  File \"/opt/conda/lib/python3.10/site-packages/neptune/metadata_containers/metadata_container.py\", line 412, in ping\n    self._backend.ping(self._id, self.container_type)\n  File \"/opt/conda/lib/python3.10/site-packages/neptune/common/backends/utils.py\", line 79, in wrapper\n    return func(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/neptune/internal/backends/hosted_neptune_backend.py\", line 466, in ping\n    self.leaderboard_client.api.ping(\n  File \"/opt/conda/lib/python3.10/site-packages/neptune/internal/backends/swagger_client_wrapper.py\", line 113, in __call__\n    self.handle_neptune_http_errors(e.response, exception=e)\n  File \"/opt/conda/lib/python3.10/site-packages/neptune/internal/backends/swagger_client_wrapper.py\", line 102, in handle_neptune_http_errors\n    raise error_processor(body) from exception\nneptune.management.exceptions.ObjectNotFound: Object not found. (code: 22)\n","output_type":"stream"}]},{"cell_type":"code","source":"torch.cuda.empty_cache() ","metadata":{"execution":{"iopub.status.busy":"2024-07-02T20:16:32.297969Z","iopub.execute_input":"2024-07-02T20:16:32.298394Z","iopub.status.idle":"2024-07-02T20:16:32.305442Z","shell.execute_reply.started":"2024-07-02T20:16:32.298351Z","shell.execute_reply":"2024-07-02T20:16:32.304012Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"## Transformers","metadata":{}},{"cell_type":"code","source":"d_model = [512, 1024, 2048, 4096]\nheads = [8, 16, 32]\nnum_layers = [5, 10]\nepochs = 100\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nwith open('WORDMAP_corpus_KBFILKOM.json', 'r') as j:\n    word_map = json.load(j)\n\ntransformer_experiment = pd.DataFrame(columns = ['experiment_id', 'd_model', 'heads', 'num_layers', 'train_loss'])\nloss_history = {}\n\nexperiment_id = -1\n\nfor d_m in d_model:\n    for h in heads:\n        for n_l in num_layers: \n            experiment_id += 1\n            print('\\nRunning for experiment {} with d_model {}, heads{}, num_layers{}\\n'.format(experiment_id, d_m, h, n_l))\n\n            run = neptune.init_run(\n                project=project,\n                api_token=api_token,\n                name=\"experiment_1724_\" + str(experiment_id)\n            ) \n            run['parameters'] = {\n                'd_model': d_m,\n                'heads': h,\n                'num_layers': n_l\n            }\n\n            transformer_experiment.loc[experiment_id, 'experiment_id'] = 'experiment_{}'.format(str(experiment_id))\n            transformer_experiment.loc[experiment_id, 'd_model'] = d_m\n            transformer_experiment.loc[experiment_id, 'heads'] = h\n            transformer_experiment.loc[experiment_id, 'num_layers'] = n_l\n\n            transformer = Transformer(d_model = d_m, heads = h, num_layers = n_l, word_map = word_map, max_len=95)\n            transformer = transformer.to(device)\n            adam_optimizer = torch.optim.Adam(transformer.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-9)\n            transformer_optimizer = AdamWarmup(model_size = d_m, warmup_steps = 4000, optimizer = adam_optimizer)\n            criterion = LossWithLS(len(word_map), 0.2)\n\n            loss_list_experiment = []\n            for epoch in range(epochs):\n                loss_train = train(train_loader, transformer, criterion, epoch)\n\n                state = {'epoch': epoch, 'transformer': transformer, 'transformer_optimizer': transformer_optimizer}\n                torch.save(state, 'checkpoint_experiment_' + str(epoch) + 'id_'+ str(experiment_id) +'.pth.tar')\n\n                loss_list_experiment.append(loss_train)\n\n                run['train/loss'].append(loss_train)\n\n            transformer_experiment.loc[experiment_id, 'train_loss'] = loss_train\n            loss_history['experiment_{}'.format(str(experiment_id))] = loss_list_experiment\n            \n            run.stop()\n","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import yaml\n\nwith open('loss_history_transformer_experiment_KBFILKOM.yaml', 'w') as file:\n    documents = yaml.dump(loss_history, file)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"transformer_experiment.dropna(inplace=True)\ntransformer_experiment","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import yaml\n\nwith open('history_rnn_150524.yaml', 'r') as file:\n    history_rnn = yaml.load(file, Loader=yaml.FullLoader)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib \nimport matplotlib.pyplot as plt\n\nloss_history_key = list(loss_history.keys())\n\nplt.figure(figsize=(15,10))\nplt.title(\"Training loss vs. Number of Epochs\")\nplt.xlabel(\"Number of Epochs\")\nplt.ylabel(\"Training Loss\")\nz\n\nfor key in loss_history_key:\n    loss_list = loss_history[key]\n    labels = f'd_model: {transformer_experiment[transformer_experiment[\"experiment_id\"] == key][\"d_model\"].values[0]}, heads: {transformer_experiment[transformer_experiment[\"experiment_id\"] == key][\"heads\"].values[0]}, num_layers: {transformer_experiment[transformer_experiment[\"experiment_id\"] == key][\"num_layers\"].values[0]}'\n    plt.plot(loss_list, label = labels)\n\n    \nplt.plot(history_rnn['loss'], \n                label = 'LSTM (Baseline FLUENT 2023)', \n                linestyle='dashed', \n                color='black', \n                linewidth=2.5, \n                alpha=0.7, \n                marker='o', \n                markerfacecolor='black', \n                markersize=5\n        )\n\nplt.legend()\ntorch.cuda.is_available()\nplt.grid()","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"markdown","source":"## Pretrained","metadata":{}},{"cell_type":"code","source":"directory = 'experiment_pretrained_1'\n\nd_model = 2048\nheads = 16\nnum_layers = 5\nepochs = 100\n\nloss_history_pratrained_embed_transformer = []\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nwith open('WORDMAP_corpus_KBFILKOM.json', 'r') as j:\n    word_map = json.load(j)\n    \ntransformer = TransformerPreTrainedEmbedding(d_model = d_model, heads = heads, num_layers = num_layers, word_map = word_map, max_len=95)\ntransformer = transformer.to(device)\nadam_optimizer = torch.optim.Adam(transformer.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-9)\ntransformer_optimizer = AdamWarmup(model_size = d_model, warmup_steps = 4000, optimizer = adam_optimizer)\ncriterion = LossWithLS(len(word_map), 0.2)\n\nfor epoch in range(epochs):\n    loss_train = train(train_loader, transformer, criterion, epoch)\n\n    state = {'epoch': epoch, 'transformer': transformer, 'transformer_optimizer': transformer_optimizer}\n    torch.save(state, directory + '/checkpoint_' + str(epoch) +'.pth.tar')\n\n    loss_history_pratrained_embed_transformer.append(loss_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Vanilla","metadata":{}},{"cell_type":"code","source":"directory = 'experiment_vanilla_30624'\ncreate_directory(directory)\n\nd_model = 2048\nheads = 16\nnum_layers = 5\nepochs = 100\n\nloss_history_vanilla_transformer = []\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nwith open('WORDMAP_corpus_KBFILKOM.json', 'r') as j:\n    word_map = json.load(j)\n    \ntransformer = Transformer(d_model = d_model, heads = heads, num_layers = num_layers, word_map = word_map, max_len=95)\ntransformer = transformer.to(device)\nadam_optimizer = torch.optim.Adam(transformer.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-9)\ntransformer_optimizer = AdamWarmup(model_size = d_model, warmup_steps = 4000, optimizer = adam_optimizer)\ncriterion = LossWithLS(len(word_map), 0.2)\n\nfor epoch in range(epochs):\n    loss_train = train(train_loader, transformer, criterion, epoch)\n\n    state = {'epoch': epoch, 'transformer': transformer, 'transformer_optimizer': transformer_optimizer}\n    torch.save(state, directory + '/checkpoint_' + str(epoch) +'.pth.tar')\n\n    loss_history_vanilla_transformer.append(loss_train)\n\nimport yaml \n\nwith open(directory + '/loss_history_vanilla_transformer.yaml', 'w') as file:\n    yaml.dump(loss_history_vanilla_transformer, file)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Vanilla without Regularization","metadata":{}},{"cell_type":"code","source":"directory = 'experiment_vanillanoreg_01724'\ncreate_directory(directory)\n\nrun = neptune_init(directory)\n\nparameters = {\n    'd_model': 2048,\n    'heads': 16,\n    'num_layers': 5,\n    'epochs': 100,\n}\nrun['parameters'] = parameters\n\nloss_history_vanillanoreg_transformer = []\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n# device = \"cpu\"\n\nwith open('WORDMAP_corpus_KBFILKOM.json', 'r') as j:\n    word_map = json.load(j)\n    \ntransformer = TransformerNoReg(d_model = parameters['d_model'], heads = parameters['heads'], num_layers = parameters['num_layers'], word_map = word_map, max_len=95)\ntransformer = transformer.to(device)\nadam_optimizer = torch.optim.Adam(transformer.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-9)\ntransformer_optimizer = AdamWarmup(model_size = parameters['d_model'], warmup_steps = 4000, optimizer = adam_optimizer)\ncriterion = LossWithLS(len(word_map), 0.2)\n\nfor epoch in range(parameters['epochs']):\n    loss_train = train(train_loader, transformer, criterion, epoch)\n\n    state = {'epoch': epoch, 'transformer': transformer, 'transformer_optimizer': transformer_optimizer}\n    # torch.save(state, directory + '/checkpoint_' + str(epoch) +'.pth.tar')\n    # run['model_checkpoint'].upload(directory + '/checkpoint_' + str(epoch) +'.pth.tar')\n\n    loss_history_vanillanoreg_transformer.append(loss_train)\n    run['train/loss'].append(loss_train)\n\nwith open(directory + '/loss_history_vanillanoreg_transformer.yaml', 'w') as file:\n    yaml.dump(loss_history_vanilla_transformer, file)\n\nrun.stop()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Decoder Only","metadata":{}},{"cell_type":"code","source":"directory = 'experiment_deconly_17624'\ncreate_directory(directory)\n\nd_model = 2048\nheads = 16\nnum_layers = 5\nepochs = 100\n\nloss_history_decoder_transformer = []\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nwith open('WORDMAP_corpus_KBFILKOM.json', 'r') as j:\n    word_map = json.load(j)\n    \ntransformer = TransformerDecoderOnly(d_model = d_model, heads = heads, num_layers = num_layers, word_map = word_map, max_len=95)\ntransformer = transformer.to(device)\nadam_optimizer = torch.optim.Adam(transformer.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-9)\ntransformer_optimizer = AdamWarmup(model_size = d_model, warmup_steps = 4000, optimizer = adam_optimizer)\ncriterion = LossWithLS(len(word_map), 0.2)\n\nfor epoch in range(epochs):\n    loss_train = train(train_loader, transformer, criterion, epoch)\n\n    state = {'epoch': epoch, 'transformer': transformer, 'transformer_optimizer': transformer_optimizer}\n    torch.save(state, directory + '/checkpoint_' + str(epoch) +'.pth.tar')\n\n    loss_history_decoder_transformer.append(loss_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## LSTM_Transformer","metadata":{}},{"cell_type":"code","source":"directory = 'experiment_2_lstm'\n\nd_model = 1024\nheads = 32\nnum_layers = 10\nepochs = 100\n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nloss_history_lstm_transformer = []\n\nwith open('WORDMAP_corpus_KBFILKOM.json', 'r') as j:\n    word_map = json.load(j)\n    \ntransformer = TransformerLSTM(d_model = d_model, heads = heads, num_layers = num_layers, word_map = word_map, max_len=90)\ntransformer = transformer.to(device)\nadam_optimizer = torch.optim.Adam(transformer.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-9)\ntransformer_optimizer = AdamWarmup(model_size = d_model, warmup_steps = 4000, optimizer = adam_optimizer)\ncriterion = LossWithLS(len(word_map), 0.2)\n\n\nfor epoch in range(epochs):\n    loss_train = train(train_loader, transformer, criterion, epoch)\n\n    state = {'epoch': epoch, 'transformer': transformer, 'transformer_optimizer': transformer_optimizer}\n    torch.save(state, directory + '/checkpoint_' + str(epoch) +'.pth.tar')\n\n    loss_history_lstm_transformer.append(loss_train)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import yaml\n\nwith open(directory + '/loss_history_lstm_transformer.yaml', 'w') as file:\n    yaml.dump(loss_history_lstm_transformer, file)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"directory = 'experiment_1'\ncheckpoint = torch.load(directory + '/checkpoint_99.pth.tar')\ntransformer = checkpoint['transformer']\n\nquestion = \"Visi FILKOM\" \nmax_len = 50\nenc_qus = [word_map.get(word, word_map['<unk>']) for word in question.split()]\nquestion = torch.LongTensor(enc_qus).to(device).unsqueeze(0)\nquestion_mask = (question!=0).to(device).unsqueeze(1).unsqueeze(1)  \nsentence = evaluate(transformer, question, question_mask, int(max_len), word_map)\nprint(sentence)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checkpoint","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Reset Cache","metadata":{}},{"cell_type":"code","source":"import torch\n\ntorch.cuda.empty_cache()\n\nprint(torch.cuda.memory_summary(device=None, abbreviated=False))","metadata":{},"execution_count":null,"outputs":[]}]}