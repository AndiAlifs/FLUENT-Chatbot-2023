{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:03.273239Z","iopub.status.busy":"2024-07-02T20:17:03.272867Z","iopub.status.idle":"2024-07-02T20:17:27.680245Z","shell.execute_reply":"2024-07-02T20:17:27.679127Z","shell.execute_reply.started":"2024-07-02T20:17:03.273209Z"},"scrolled":true,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: neptune in ./.conda/envs/myenv/lib/python3.8/site-packages (1.10.4)\n","Requirement already satisfied: GitPython>=2.0.8 in ./.conda/envs/myenv/lib/python3.8/site-packages (from neptune) (3.1.43)\n","Requirement already satisfied: Pillow>=1.1.6 in ./.conda/envs/myenv/lib/python3.8/site-packages (from neptune) (9.4.0)\n","Requirement already satisfied: PyJWT in ./.conda/envs/myenv/lib/python3.8/site-packages (from neptune) (2.8.0)\n","Requirement already satisfied: boto3>=1.28.0 in ./.conda/envs/myenv/lib/python3.8/site-packages (from neptune) (1.34.136)\n","Requirement already satisfied: bravado<12.0.0,>=11.0.0 in ./.conda/envs/myenv/lib/python3.8/site-packages (from neptune) (11.0.3)\n","Requirement already satisfied: click>=7.0 in ./.conda/envs/myenv/lib/python3.8/site-packages (from neptune) (8.1.7)\n","Requirement already satisfied: future>=0.17.1 in ./.conda/envs/myenv/lib/python3.8/site-packages (from neptune) (1.0.0)\n","Requirement already satisfied: oauthlib>=2.1.0 in ./.conda/envs/myenv/lib/python3.8/site-packages (from neptune) (3.2.2)\n","Requirement already satisfied: packaging in ./.conda/envs/myenv/lib/python3.8/site-packages (from neptune) (23.0)\n","Requirement already satisfied: pandas in ./.conda/envs/myenv/lib/python3.8/site-packages (from neptune) (1.5.3)\n","Requirement already satisfied: psutil in ./.conda/envs/myenv/lib/python3.8/site-packages (from neptune) (5.9.0)\n","Requirement already satisfied: requests>=2.20.0 in ./.conda/envs/myenv/lib/python3.8/site-packages (from neptune) (2.32.3)\n","Requirement already satisfied: requests-oauthlib>=1.0.0 in ./.conda/envs/myenv/lib/python3.8/site-packages (from neptune) (2.0.0)\n","Requirement already satisfied: six>=1.12.0 in ./.conda/envs/myenv/lib/python3.8/site-packages (from neptune) (1.16.0)\n","Requirement already satisfied: swagger-spec-validator>=2.7.4 in ./.conda/envs/myenv/lib/python3.8/site-packages (from neptune) (3.0.4)\n","Requirement already satisfied: typing-extensions>=3.10.0 in ./.conda/envs/myenv/lib/python3.8/site-packages (from neptune) (4.6.3)\n","Requirement already satisfied: urllib3 in ./.conda/envs/myenv/lib/python3.8/site-packages (from neptune) (1.26.16)\n","Requirement already satisfied: websocket-client!=1.0.0,>=0.35.0 in ./.conda/envs/myenv/lib/python3.8/site-packages (from neptune) (1.8.0)\n","Requirement already satisfied: botocore<1.35.0,>=1.34.136 in ./.conda/envs/myenv/lib/python3.8/site-packages (from boto3>=1.28.0->neptune) (1.34.136)\n","Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in ./.conda/envs/myenv/lib/python3.8/site-packages (from boto3>=1.28.0->neptune) (1.0.1)\n","Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in ./.conda/envs/myenv/lib/python3.8/site-packages (from boto3>=1.28.0->neptune) (0.10.2)\n","Requirement already satisfied: bravado-core>=5.16.1 in ./.conda/envs/myenv/lib/python3.8/site-packages (from bravado<12.0.0,>=11.0.0->neptune) (6.1.1)\n","Requirement already satisfied: msgpack in ./.conda/envs/myenv/lib/python3.8/site-packages (from bravado<12.0.0,>=11.0.0->neptune) (1.0.8)\n","Requirement already satisfied: python-dateutil in ./.conda/envs/myenv/lib/python3.8/site-packages (from bravado<12.0.0,>=11.0.0->neptune) (2.8.2)\n","Requirement already satisfied: pyyaml in ./.conda/envs/myenv/lib/python3.8/site-packages (from bravado<12.0.0,>=11.0.0->neptune) (6.0.1)\n","Requirement already satisfied: simplejson in ./.conda/envs/myenv/lib/python3.8/site-packages (from bravado<12.0.0,>=11.0.0->neptune) (3.19.2)\n","Requirement already satisfied: monotonic in ./.conda/envs/myenv/lib/python3.8/site-packages (from bravado<12.0.0,>=11.0.0->neptune) (1.6)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in ./.conda/envs/myenv/lib/python3.8/site-packages (from GitPython>=2.0.8->neptune) (4.0.11)\n","Requirement already satisfied: charset-normalizer<4,>=2 in ./.conda/envs/myenv/lib/python3.8/site-packages (from requests>=2.20.0->neptune) (2.0.4)\n","Requirement already satisfied: idna<4,>=2.5 in ./.conda/envs/myenv/lib/python3.8/site-packages (from requests>=2.20.0->neptune) (3.4)\n","Requirement already satisfied: certifi>=2017.4.17 in ./.conda/envs/myenv/lib/python3.8/site-packages (from requests>=2.20.0->neptune) (2023.5.7)\n","Requirement already satisfied: jsonschema in ./.conda/envs/myenv/lib/python3.8/site-packages (from swagger-spec-validator>=2.7.4->neptune) (4.22.0)\n","Requirement already satisfied: importlib-resources>=1.3 in ./.conda/envs/myenv/lib/python3.8/site-packages (from swagger-spec-validator>=2.7.4->neptune) (5.12.0)\n","Requirement already satisfied: pytz>=2020.1 in ./.conda/envs/myenv/lib/python3.8/site-packages (from pandas->neptune) (2022.7)\n","Requirement already satisfied: numpy>=1.20.3 in ./.conda/envs/myenv/lib/python3.8/site-packages (from pandas->neptune) (1.24.3)\n","Requirement already satisfied: jsonref in ./.conda/envs/myenv/lib/python3.8/site-packages (from bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune) (1.1.0)\n","Requirement already satisfied: smmap<6,>=3.0.1 in ./.conda/envs/myenv/lib/python3.8/site-packages (from gitdb<5,>=4.0.1->GitPython>=2.0.8->neptune) (5.0.1)\n","Requirement already satisfied: zipp>=3.1.0 in ./.conda/envs/myenv/lib/python3.8/site-packages (from importlib-resources>=1.3->swagger-spec-validator>=2.7.4->neptune) (3.11.0)\n","Requirement already satisfied: attrs>=22.2.0 in ./.conda/envs/myenv/lib/python3.8/site-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune) (23.2.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in ./.conda/envs/myenv/lib/python3.8/site-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune) (2023.12.1)\n","Requirement already satisfied: pkgutil-resolve-name>=1.3.10 in ./.conda/envs/myenv/lib/python3.8/site-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune) (1.3.10)\n","Requirement already satisfied: referencing>=0.28.4 in ./.conda/envs/myenv/lib/python3.8/site-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune) (0.35.1)\n","Requirement already satisfied: rpds-py>=0.7.1 in ./.conda/envs/myenv/lib/python3.8/site-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune) (0.18.1)\n","Requirement already satisfied: fqdn in ./.conda/envs/myenv/lib/python3.8/site-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune) (1.5.1)\n","Requirement already satisfied: isoduration in ./.conda/envs/myenv/lib/python3.8/site-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune) (20.11.0)\n","Requirement already satisfied: jsonpointer>1.13 in ./.conda/envs/myenv/lib/python3.8/site-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune) (3.0.0)\n","Requirement already satisfied: rfc3339-validator in ./.conda/envs/myenv/lib/python3.8/site-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune) (0.1.4)\n","Requirement already satisfied: rfc3986-validator>0.1.0 in ./.conda/envs/myenv/lib/python3.8/site-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune) (0.1.1)\n","Requirement already satisfied: uri-template in ./.conda/envs/myenv/lib/python3.8/site-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune) (1.3.0)\n","Requirement already satisfied: webcolors>=1.11 in ./.conda/envs/myenv/lib/python3.8/site-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune) (24.6.0)\n","Requirement already satisfied: arrow>=0.15.0 in ./.conda/envs/myenv/lib/python3.8/site-packages (from isoduration->jsonschema->swagger-spec-validator>=2.7.4->neptune) (1.3.0)\n","Requirement already satisfied: types-python-dateutil>=2.8.10 in ./.conda/envs/myenv/lib/python3.8/site-packages (from arrow>=0.15.0->isoduration->jsonschema->swagger-spec-validator>=2.7.4->neptune) (2.9.0.20240316)\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["%pip install neptune"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:27.682419Z","iopub.status.busy":"2024-07-02T20:17:27.682115Z","iopub.status.idle":"2024-07-02T20:17:30.597078Z","shell.execute_reply":"2024-07-02T20:17:30.596113Z","shell.execute_reply.started":"2024-07-02T20:17:27.682391Z"},"trusted":true},"outputs":[],"source":["from collections import Counter\n","import json\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset\n","import torch.utils.data\n","import math\n","import torch.nn.functional as F\n","import neptune\n","import pandas as pd"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:30.598900Z","iopub.status.busy":"2024-07-02T20:17:30.598458Z","iopub.status.idle":"2024-07-02T20:17:32.694354Z","shell.execute_reply":"2024-07-02T20:17:32.693406Z","shell.execute_reply.started":"2024-07-02T20:17:30.598873Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Pertanyaan</th>\n","      <th>Jawaban</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>email Fitra A. Bachtiar</td>\n","      <td>fitra.bachtiar[at]ub.ac.id</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>NIK/NIP Fitra A. Bachtiar</td>\n","      <td>198406282019031006</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>nama lengkap Fitra A. Bachtiar</td>\n","      <td>Dr.Eng. Fitra A. Bachtiar</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Departemen Fitra A. Bachtiar</td>\n","      <td>Departemen Teknik Informatika</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Program Studi Fitra A. Bachtiar</td>\n","      <td>S2 Ilmu Komputer</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1229</th>\n","      <td>Apa Manfaat Konseling FILKOM ?</td>\n","      <td>1. Masalah ditangani oleh ahli yang kompeten d...</td>\n","    </tr>\n","    <tr>\n","      <th>1230</th>\n","      <td>Berikan informasi mengenai Layanan Konseling</td>\n","      <td>Informasi mengenai Layanan Konseling dapat dia...</td>\n","    </tr>\n","    <tr>\n","      <th>1231</th>\n","      <td>Siapa Konselor Bimbingan dan Konseling di FILK...</td>\n","      <td>Ada 2 konselor Bimbingan dan Konseling di FILK...</td>\n","    </tr>\n","    <tr>\n","      <th>1232</th>\n","      <td>Siapa Koordinator Konselor Sebaya ?</td>\n","      <td>Koordinator Konselor Sebaya adalah Muhammad Da...</td>\n","    </tr>\n","    <tr>\n","      <th>1233</th>\n","      <td>Berikan Rincian Layanan ULTKSP</td>\n","      <td>Rincian Layanan ULTKSP dapat diakses pada taut...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1198 rows × 2 columns</p>\n","</div>"],"text/plain":["                                             Pertanyaan  \\\n","0                               email Fitra A. Bachtiar   \n","1                             NIK/NIP Fitra A. Bachtiar   \n","2                        nama lengkap Fitra A. Bachtiar   \n","3                          Departemen Fitra A. Bachtiar   \n","4                       Program Studi Fitra A. Bachtiar   \n","...                                                 ...   \n","1229                     Apa Manfaat Konseling FILKOM ?   \n","1230       Berikan informasi mengenai Layanan Konseling   \n","1231  Siapa Konselor Bimbingan dan Konseling di FILK...   \n","1232                Siapa Koordinator Konselor Sebaya ?   \n","1233                     Berikan Rincian Layanan ULTKSP   \n","\n","                                                Jawaban  \n","0                            fitra.bachtiar[at]ub.ac.id  \n","1                                    198406282019031006  \n","2                             Dr.Eng. Fitra A. Bachtiar  \n","3                         Departemen Teknik Informatika  \n","4                                      S2 Ilmu Komputer  \n","...                                                 ...  \n","1229  1. Masalah ditangani oleh ahli yang kompeten d...  \n","1230  Informasi mengenai Layanan Konseling dapat dia...  \n","1231  Ada 2 konselor Bimbingan dan Konseling di FILK...  \n","1232  Koordinator Konselor Sebaya adalah Muhammad Da...  \n","1233  Rincian Layanan ULTKSP dapat diakses pada taut...  \n","\n","[1198 rows x 2 columns]"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["knowledgebase_url = 'https://github.com/AndiAlifs/FLUENT-Chatbot-2023/raw/main/KnowledgeBaseFilkom.xlsx'\n","knowledgebase = pd.read_excel(knowledgebase_url)\n","\n","qa_paired = knowledgebase.drop(columns=knowledgebase.columns.drop(['Pertanyaan', 'Jawaban']))\n","qa_paired.dropna(inplace=True)\n","qa_paired"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:32.697066Z","iopub.status.busy":"2024-07-02T20:17:32.696633Z","iopub.status.idle":"2024-07-02T20:17:32.702322Z","shell.execute_reply":"2024-07-02T20:17:32.701334Z","shell.execute_reply.started":"2024-07-02T20:17:32.697038Z"},"trusted":true},"outputs":[],"source":["def remove_punc(string):\n","    punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n","    no_punct = \"\"\n","    for char in string:\n","        if char not in punctuations:\n","            no_punct = no_punct + char  # space is also a character\n","    return no_punct.lower()"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:32.703938Z","iopub.status.busy":"2024-07-02T20:17:32.703658Z","iopub.status.idle":"2024-07-02T20:17:32.827025Z","shell.execute_reply":"2024-07-02T20:17:32.825935Z","shell.execute_reply.started":"2024-07-02T20:17:32.703915Z"},"trusted":true},"outputs":[],"source":["pairs = []\n","max_len = 90\n","\n","for line in qa_paired.iterrows():\n","    pertanyaan = line[1]['Pertanyaan']\n","    jawaban = line[1]['Jawaban']\n","    qa_pairs = []\n","    first = remove_punc(pertanyaan.strip())      \n","    second = remove_punc(jawaban.strip())\n","    qa_pairs.append(first.split()[:max_len])\n","    qa_pairs.append(second.split()[:max_len])\n","    pairs.append(qa_pairs)"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:32.828565Z","iopub.status.busy":"2024-07-02T20:17:32.828255Z","iopub.status.idle":"2024-07-02T20:17:32.838911Z","shell.execute_reply":"2024-07-02T20:17:32.837848Z","shell.execute_reply.started":"2024-07-02T20:17:32.828539Z"},"trusted":true},"outputs":[],"source":["word_freq = Counter()\n","for pair in pairs:\n","    word_freq.update(pair[0])\n","    word_freq.update(pair[1])"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:32.840576Z","iopub.status.busy":"2024-07-02T20:17:32.840244Z","iopub.status.idle":"2024-07-02T20:17:32.850265Z","shell.execute_reply":"2024-07-02T20:17:32.849352Z","shell.execute_reply.started":"2024-07-02T20:17:32.840548Z"},"trusted":true},"outputs":[],"source":["min_word_freq = 2\n","words = [w for w in word_freq.keys() if word_freq[w] > min_word_freq]\n","word_map = {k: v + 1 for v, k in enumerate(words)}\n","word_map['<unk>'] = len(word_map) + 1\n","word_map['<start>'] = len(word_map) + 1\n","word_map['<end>'] = len(word_map) + 1\n","word_map['<pad>'] = 0"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:32.851802Z","iopub.status.busy":"2024-07-02T20:17:32.851465Z","iopub.status.idle":"2024-07-02T20:17:32.860482Z","shell.execute_reply":"2024-07-02T20:17:32.859625Z","shell.execute_reply.started":"2024-07-02T20:17:32.851770Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Total words are: 1079\n"]}],"source":["print(\"Total words are: {}\".format(len(word_map)))"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:32.861919Z","iopub.status.busy":"2024-07-02T20:17:32.861579Z","iopub.status.idle":"2024-07-02T20:17:32.872138Z","shell.execute_reply":"2024-07-02T20:17:32.871399Z","shell.execute_reply.started":"2024-07-02T20:17:32.861895Z"},"trusted":true},"outputs":[],"source":["with open('WORDMAP_corpus_KBFILKOM.json', 'w') as j:\n","    json.dump(word_map, j)"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:32.877074Z","iopub.status.busy":"2024-07-02T20:17:32.876749Z","iopub.status.idle":"2024-07-02T20:17:32.884325Z","shell.execute_reply":"2024-07-02T20:17:32.883430Z","shell.execute_reply.started":"2024-07-02T20:17:32.877048Z"},"trusted":true},"outputs":[],"source":["def encode_question(words, word_map):\n","    enc_c = [word_map.get(word, word_map['<unk>']) for word in words] + [word_map['<pad>']] * (max_len - len(words))\n","    return enc_c\n","\n","def encode_reply(words, word_map):\n","    enc_c = [word_map['<start>']] + [word_map.get(word, word_map['<unk>']) for word in words] + \\\n","    [word_map['<end>']] + [word_map['<pad>']] * (max_len - len(words))\n","    return enc_c\n","\n","def encode_reply_with_maxlen(words, word_map):\n","    enc_c = [word_map['<start>']] + [word_map.get(word, word_map['<unk>']) for word in words] + [word_map['<end>']] + [word_map['<pad>']] * (max_len-2 - len(words))\n","    return enc_c"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:32.885632Z","iopub.status.busy":"2024-07-02T20:17:32.885341Z","iopub.status.idle":"2024-07-02T20:17:33.099456Z","shell.execute_reply":"2024-07-02T20:17:33.098486Z","shell.execute_reply.started":"2024-07-02T20:17:32.885608Z"},"trusted":true},"outputs":[],"source":["pairs_encoded = []\n","for pair in pairs:\n","    qus = encode_question(pair[0], word_map)\n","    ans = encode_reply(pair[1], word_map)\n","    pairs_encoded.append([qus, ans])\n","\n","with open('pairs_encoded_kbfilkom.json', 'w') as p:\n","    json.dump(pairs_encoded, p)"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:33.100894Z","iopub.status.busy":"2024-07-02T20:17:33.100609Z","iopub.status.idle":"2024-07-02T20:17:33.308867Z","shell.execute_reply":"2024-07-02T20:17:33.307678Z","shell.execute_reply.started":"2024-07-02T20:17:33.100868Z"},"trusted":true},"outputs":[],"source":["pairs_encoded_same_length = []\n","for pair in pairs:\n","    qus = encode_question(pair[0], word_map)\n","    ans = encode_reply_with_maxlen(pair[1], word_map)\n","    pairs_encoded_same_length.append([qus, ans])\n","\n","with open('pairs_encoded_kbfilkom_same_len.json', 'w') as p:\n","    json.dump(pairs_encoded_same_length, p)"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:33.310393Z","iopub.status.busy":"2024-07-02T20:17:33.310107Z","iopub.status.idle":"2024-07-02T20:17:33.317262Z","shell.execute_reply":"2024-07-02T20:17:33.316249Z","shell.execute_reply.started":"2024-07-02T20:17:33.310368Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'apa tujuan filkom <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>'"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["rev_word_map = {v: k for k, v in word_map.items()}\n","' '.join([rev_word_map[v] for v in pairs_encoded[15][0]])"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:33.318791Z","iopub.status.busy":"2024-07-02T20:17:33.318483Z","iopub.status.idle":"2024-07-02T20:17:33.325793Z","shell.execute_reply":"2024-07-02T20:17:33.325035Z","shell.execute_reply.started":"2024-07-02T20:17:33.318756Z"},"trusted":true},"outputs":[],"source":["class Dataset(Dataset):\n","\n","    def __init__(self):\n","\n","        self.pairs = json.load(open('pairs_encoded_kbfilkom.json'))\n","        self.dataset_size = len(self.pairs)\n","\n","    def __getitem__(self, i):\n","        \n","        question = torch.LongTensor(self.pairs[i][0])\n","        reply = torch.LongTensor(self.pairs[i][1])\n","            \n","        return question, reply\n","\n","    def __len__(self):\n","        return self.dataset_size"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:33.327276Z","iopub.status.busy":"2024-07-02T20:17:33.326939Z","iopub.status.idle":"2024-07-02T20:17:33.336359Z","shell.execute_reply":"2024-07-02T20:17:33.335543Z","shell.execute_reply.started":"2024-07-02T20:17:33.327245Z"},"trusted":true},"outputs":[],"source":["class Dataset_Same_Len(Dataset):\n","\n","    def __init__(self):\n","\n","        self.pairs = json.load(open('pairs_encoded_kbfilkom_same_len.json'))\n","        self.dataset_size = len(self.pairs)\n","\n","    def __getitem__(self, i):\n","        \n","        question = torch.LongTensor(self.pairs[i][0])\n","        reply = torch.LongTensor(self.pairs[i][1])\n","            \n","        return question, reply\n","\n","    def __len__(self):\n","        return self.dataset_size"]},{"cell_type":"markdown","metadata":{},"source":["## Train Loader"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:33.337842Z","iopub.status.busy":"2024-07-02T20:17:33.337481Z","iopub.status.idle":"2024-07-02T20:17:33.379910Z","shell.execute_reply":"2024-07-02T20:17:33.378865Z","shell.execute_reply.started":"2024-07-02T20:17:33.337813Z"},"trusted":true},"outputs":[],"source":["train_loader = torch.utils.data.DataLoader(Dataset(),\n","                                           batch_size = 50, \n","                                           shuffle=True, \n","                                           pin_memory=True)"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:33.381467Z","iopub.status.busy":"2024-07-02T20:17:33.381114Z","iopub.status.idle":"2024-07-02T20:17:33.388963Z","shell.execute_reply":"2024-07-02T20:17:33.388016Z","shell.execute_reply.started":"2024-07-02T20:17:33.381434Z"},"trusted":true},"outputs":[],"source":["def create_masks(question, reply_input, reply_target):\n","    \n","    def subsequent_mask(size):\n","        mask = torch.triu(torch.ones(size, size)).transpose(0, 1).type(dtype=torch.uint8)\n","        return mask.unsqueeze(0)\n","    \n","    question_mask = question!=0\n","    question_mask = question_mask.to(device)\n","    question_mask = question_mask.unsqueeze(1).unsqueeze(1)         # (batch_size, 1, 1, max_words)\n","     \n","    reply_input_mask = reply_input!=0\n","    reply_input_mask = reply_input_mask.unsqueeze(1)  # (batch_size, 1, max_words)\n","    reply_input_mask = reply_input_mask & subsequent_mask(reply_input.size(-1)).type_as(reply_input_mask.data) \n","    reply_input_mask = reply_input_mask.unsqueeze(1) # (batch_size, 1, max_words, max_words)\n","    reply_target_mask = reply_target!=0              # (batch_size, max_words)\n","    \n","    return question_mask, reply_input_mask, reply_target_mask"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:33.390611Z","iopub.status.busy":"2024-07-02T20:17:33.390255Z","iopub.status.idle":"2024-07-02T20:17:33.397673Z","shell.execute_reply":"2024-07-02T20:17:33.396839Z","shell.execute_reply.started":"2024-07-02T20:17:33.390578Z"},"trusted":true},"outputs":[],"source":["import os\n","\n","def create_directory(path):\n","    if not os.path.exists(path):\n","        os.makedirs(path)\n","        print(f\"Directory created at {path}\")\n","    else:\n","        print(f\"Directory already exists at {path}\")"]},{"cell_type":"markdown","metadata":{},"source":["# Architecture"]},{"cell_type":"markdown","metadata":{},"source":["## Embeddings"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:33.399270Z","iopub.status.busy":"2024-07-02T20:17:33.398914Z","iopub.status.idle":"2024-07-02T20:17:33.421634Z","shell.execute_reply":"2024-07-02T20:17:33.420718Z","shell.execute_reply.started":"2024-07-02T20:17:33.399239Z"},"trusted":true},"outputs":[],"source":["class Embeddings(nn.Module):\n","    \"\"\"\n","    Implements embeddings of the words and adds their positional encodings. \n","    \"\"\"\n","    def __init__(self, vocab_size, d_model, max_len = 50, num_layers = 6):\n","        super(Embeddings, self).__init__()\n","        self.d_model = d_model\n","        self.dropout = nn.Dropout(0.1)\n","        self.embed = nn.Embedding(vocab_size, d_model)\n","        self.pe = self.create_positinal_encoding(max_len, self.d_model)     # (1, max_len, d_model)\n","        self.te = self.create_positinal_encoding(num_layers, self.d_model)  # (1, num_layers, d_model)\n","        self.dropout = nn.Dropout(0.1)\n","        \n","    def create_positinal_encoding(self, max_len, d_model):\n","        pe = torch.zeros(max_len, d_model).to(device)\n","        for pos in range(max_len):   # for each position of the word\n","            for i in range(0, d_model, 2):   # for each dimension of the each position\n","                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/d_model)))\n","                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n","        pe = pe.unsqueeze(0)   # include the batch size\n","        return pe\n","        \n","    def forward(self, embedding, layer_idx):\n","        if layer_idx == 0:\n","            embedding = self.embed(embedding) * math.sqrt(self.d_model)\n","        embedding += self.pe[:, :embedding.size(1)]   # pe will automatically be expanded with the same batch size as encoded_words\n","        # embedding: (batch_size, max_len, d_model), te: (batch_size, 1, d_model)\n","        embedding += self.te[:, layer_idx, :].unsqueeze(1).repeat(1, embedding.size(1), 1)\n","        embedding = self.dropout(embedding)\n","        return embedding\n"]},{"cell_type":"markdown","metadata":{},"source":["## Multi-Head Attention"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:33.423711Z","iopub.status.busy":"2024-07-02T20:17:33.423056Z","iopub.status.idle":"2024-07-02T20:17:33.435766Z","shell.execute_reply":"2024-07-02T20:17:33.435028Z","shell.execute_reply.started":"2024-07-02T20:17:33.423675Z"},"trusted":true},"outputs":[],"source":["class MultiHeadAttention(nn.Module):\n","    \n","    def __init__(self, heads, d_model):\n","        super(MultiHeadAttention, self).__init__()\n","        assert d_model % heads == 0\n","        self.d_k = d_model // heads\n","        self.heads = heads\n","        self.dropout = nn.Dropout(0.1)\n","        self.query = nn.Linear(d_model, d_model)\n","        self.key = nn.Linear(d_model, d_model)\n","        self.value = nn.Linear(d_model, d_model)\n","        self.concat = nn.Linear(d_model, d_model)\n","        \n","    def forward(self, query, key, value, mask):\n","        \"\"\"\n","        query, key, value of shape: (batch_size, max_len, 512)\n","        mask of shape: (batch_size, 1, 1, max_words)\n","        \"\"\"\n","        # (batch_size, max_len, 512)\n","        query = self.query(query)\n","        key = self.key(key)        \n","        value = self.value(value)   \n","        \n","        # (batch_size, max_len, 512) --> (batch_size, max_len, h, d_k) --> (batch_size, h, max_len, d_k)\n","        query = query.view(query.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)   \n","        key = key.view(key.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)  \n","        value = value.view(value.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)  \n","        \n","        # (batch_size, h, max_len, d_k) matmul (batch_size, h, d_k, max_len) --> (batch_size, h, max_len, max_len)\n","        scores = torch.matmul(query, key.permute(0,1,3,2)) / math.sqrt(query.size(-1))\n","        scores = scores.masked_fill(mask == 0, -1e9)    # (batch_size, h, max_len, max_len)\n","        weights = F.softmax(scores, dim = -1)           # (batch_size, h, max_len, max_len)\n","        weights = self.dropout(weights)\n","        # (batch_size, h, max_len, max_len) matmul (batch_size, h, max_len, d_k) --> (batch_size, h, max_len, d_k)\n","        context = torch.matmul(weights, value)\n","        # (batch_size, h, max_len, d_k) --> (batch_size, max_len, h, d_k) --> (batch_size, max_len, h * d_k)\n","        context = context.permute(0,2,1,3).contiguous().view(context.shape[0], -1, self.heads * self.d_k)\n","        # (batch_size, max_len, h * d_k)\n","        interacted = self.concat(context)\n","        return interacted "]},{"cell_type":"markdown","metadata":{},"source":["## Feed Forward Neural Network"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:33.437331Z","iopub.status.busy":"2024-07-02T20:17:33.437039Z","iopub.status.idle":"2024-07-02T20:17:33.450588Z","shell.execute_reply":"2024-07-02T20:17:33.449684Z","shell.execute_reply.started":"2024-07-02T20:17:33.437306Z"},"trusted":true},"outputs":[],"source":["class FeedForward(nn.Module):\n","    def __init__(self, d_model, middle_dim = 2048):\n","        super(FeedForward, self).__init__()\n","        \n","        self.fc1 = nn.Linear(d_model, middle_dim)\n","        self.fc2 = nn.Linear(middle_dim, d_model)\n","        self.dropout = nn.Dropout(0.1)\n","\n","    def forward(self, x):\n","        out = F.relu(self.fc1(x))\n","        out = self.fc2(self.dropout(out))\n","        return out\n","\n","class FeedForwardLSTM(nn.Module):\n","    def __init__(self, d_model, middle_dim = 2048):\n","        super(FeedForwardLSTM, self).__init__()\n","        \n","        self.fc1 = nn.Linear(d_model, middle_dim)\n","        self.lstm = nn.LSTM(middle_dim, middle_dim)\n","        self.fc2 = nn.Linear(middle_dim, d_model)\n","        self.dropout = nn.Dropout(0.1)\n","\n","    def forward(self, x):\n","        out = F.relu(self.fc1(x))\n","        out, _ = self.lstm(out)\n","        out = self.fc2(self.dropout(out))\n","        return out\n","\n","class FeedForwardLayerNoReg(nn.Module):\n","    def __init__(self, d_model, middle_dim = 2048):\n","        super(FeedForward, self).__init__()\n","        \n","        self.fc1 = nn.Linear(d_model, middle_dim)\n","        self.fc2 = nn.Linear(middle_dim, d_model)\n","\n","    def forward(self, x):\n","        out = F.relu(self.fc1(x))\n","        out = self.fc2(out)\n","        return out"]},{"cell_type":"markdown","metadata":{},"source":["## Encoder"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:33.452046Z","iopub.status.busy":"2024-07-02T20:17:33.451709Z","iopub.status.idle":"2024-07-02T20:17:33.461741Z","shell.execute_reply":"2024-07-02T20:17:33.460847Z","shell.execute_reply.started":"2024-07-02T20:17:33.452019Z"},"trusted":true},"outputs":[],"source":["class EncoderLayer(nn.Module):\n","    def __init__(self, d_model, heads):\n","        super(EncoderLayer, self).__init__()\n","        self.layernorm = nn.LayerNorm(d_model)\n","        self.self_multihead = MultiHeadAttention(heads, d_model)\n","        self.feed_forward = FeedForward(d_model)\n","        self.dropout = nn.Dropout(0.1)\n","\n","    def forward(self, embeddings, mask):\n","        interacted = self.dropout(self.self_multihead(embeddings, embeddings, embeddings, mask))\n","        interacted = self.layernorm(interacted + embeddings)\n","        feed_forward_out = self.dropout(self.feed_forward(interacted))\n","        encoded = self.layernorm(feed_forward_out + interacted)\n","        return encoded\n","\n","class EncoderLayerNoReg(nn.Module):\n","    def __init__(self, d_model, heads):\n","        super(EncoderLayerNoReg, self).__init__()\n","        self.layernorm = nn.LayerNorm(d_model)\n","        self.self_multihead = MultiHeadAttention(heads, d_model)\n","        self.feed_forward = FeedForward(d_model)\n","\n","    def forward(self, embeddings, mask):\n","        interacted = self.self_multihead(embeddings, embeddings, embeddings, mask)\n","        interacted = self.layernorm(interacted + embeddings)\n","        feed_forward_out = self.feed_forward(interacted)\n","        encoded = self.layernorm(feed_forward_out + interacted)\n","        return encoded\n","        \n","        "]},{"cell_type":"markdown","metadata":{},"source":["## Decoder"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:33.463365Z","iopub.status.busy":"2024-07-02T20:17:33.463025Z","iopub.status.idle":"2024-07-02T20:17:33.475859Z","shell.execute_reply":"2024-07-02T20:17:33.475008Z","shell.execute_reply.started":"2024-07-02T20:17:33.463339Z"},"trusted":true},"outputs":[],"source":["class DecoderLayer(nn.Module):\n","    \n","    def __init__(self, d_model, heads):\n","        super(DecoderLayer, self).__init__()\n","        self.layernorm = nn.LayerNorm(d_model)\n","        self.self_multihead = MultiHeadAttention(heads, d_model)\n","        self.src_multihead = MultiHeadAttention(heads, d_model)\n","        self.feed_forward = FeedForward(d_model)\n","        self.dropout = nn.Dropout(0.1)\n","        \n","    def forward(self, embeddings, encoded, src_mask, target_mask):\n","        query = self.dropout(self.self_multihead(embeddings, embeddings, embeddings, target_mask))\n","        query = self.layernorm(query + embeddings)\n","        interacted = self.dropout(self.src_multihead(query, encoded, encoded, src_mask))\n","        interacted = self.layernorm(interacted + query)\n","        feed_forward_out = self.dropout(self.feed_forward(interacted))\n","        decoded = self.layernorm(feed_forward_out + interacted)\n","        return decoded\n","\n","class DecoderLayerNoReg(nn.Module):\n","    def __init__(self, d_model, heads):\n","        super(DecoderLayerNoReg, self).__init__()\n","        self.layernorm = nn.LayerNorm(d_model)\n","        self.self_multihead = MultiHeadAttention(heads, d_model)\n","        self.src_multihead = MultiHeadAttention(heads, d_model)\n","        self.feed_forward = FeedForward(d_model)\n","        \n","    def forward(self, embeddings, encoded, src_mask, target_mask):\n","        query = self.self_multihead(embeddings, embeddings, embeddings, target_mask)\n","        query = self.layernorm(query + embeddings)\n","        interacted = self.src_multihead(query, encoded, encoded, src_mask)\n","        interacted = self.layernorm(interacted + query)\n","        feed_forward_out = self.feed_forward(interacted)\n","        decoded = self.layernorm(feed_forward_out + interacted)\n","        return decoded"]},{"cell_type":"markdown","metadata":{},"source":["## Transformer Architecture"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:33.478079Z","iopub.status.busy":"2024-07-02T20:17:33.477782Z","iopub.status.idle":"2024-07-02T20:17:33.514932Z","shell.execute_reply":"2024-07-02T20:17:33.514023Z","shell.execute_reply.started":"2024-07-02T20:17:33.478055Z"},"trusted":true},"outputs":[],"source":["class Transformer(nn.Module):\n","    \n","    def __init__(self, d_model, heads, num_layers, word_map, max_len = 50):\n","        super(Transformer, self).__init__()\n","        \n","        self.d_model = d_model\n","        self.num_layers = num_layers\n","        self.vocab_size = len(word_map)\n","        self.embed = Embeddings(self.vocab_size, d_model, num_layers = num_layers, max_len = max_len)\n","        self.encoder = EncoderLayer(d_model, heads) \n","        self.decoder = DecoderLayer(d_model, heads)\n","        self.logit = nn.Linear(d_model, self.vocab_size)\n","        \n","    def encode(self, src_embeddings, src_mask):\n","        for i in range(self.num_layers):\n","            src_embeddings = self.embed(src_embeddings, i)\n","            src_embeddings = self.encoder(src_embeddings, src_mask)\n","        return src_embeddings\n","    \n","    def decode(self, tgt_embeddings, target_mask, src_embeddings, src_mask):\n","        for i in range(self.num_layers):\n","            tgt_embeddings = self.embed(tgt_embeddings, i)\n","            tgt_embeddings = self.decoder(tgt_embeddings, src_embeddings, src_mask, target_mask)\n","        return tgt_embeddings\n","        \n","    def forward(self, src_words, src_mask, target_words, target_mask):\n","        encoded = self.encode(src_words, src_mask)\n","        decoded = self.decode(target_words, target_mask, encoded, src_mask)\n","        out = F.log_softmax(self.logit(decoded), dim = 2)\n","        return out\n","\n","class TransformerNoReg(nn.Module):\n","    \n","    def __init__(self, d_model, heads, num_layers, word_map, max_len = 50):\n","        super(TransformerNoReg, self).__init__()\n","        \n","        self.d_model = d_model\n","        self.num_layers = num_layers\n","        self.vocab_size = len(word_map)\n","        self.embed = Embeddings(self.vocab_size, d_model, num_layers = num_layers, max_len = max_len)\n","        self.encoder = EncoderLayerNoReg(d_model, heads) \n","        self.decoder = DecoderLayerNoReg(d_model, heads)\n","        self.logit = nn.Linear(d_model, self.vocab_size)\n","        \n","    def encode(self, src_embeddings, src_mask):\n","        for i in range(self.num_layers):\n","            src_embeddings = self.embed(src_embeddings, i)\n","            src_embeddings = self.encoder(src_embeddings, src_mask)\n","        return src_embeddings\n","    \n","    def decode(self, tgt_embeddings, target_mask, src_embeddings, src_mask):\n","        for i in range(self.num_layers):\n","            tgt_embeddings = self.embed(tgt_embeddings, i)\n","            tgt_embeddings = self.decoder(tgt_embeddings, src_embeddings, src_mask, target_mask)\n","        return tgt_embeddings\n","        \n","    def forward(self, src_words, src_mask, target_words, target_mask):\n","        encoded = self.encode(src_words, src_mask)\n","        decoded = self.decode(target_words, target_mask, encoded, src_mask)\n","        out = F.log_softmax(self.logit(decoded), dim = 2)\n","        return out\n","\n","class TransformerLSTM(nn.Module):\n","    \n","    def __init__(self, d_model, heads, num_layers, word_map, max_len = 50):\n","        super(TransformerLSTM, self).__init__()\n","        \n","        self.d_model = d_model\n","        self.num_layers = num_layers\n","        self.vocab_size = len(word_map)\n","        self.embed = EmbeddingsLSTM(self.vocab_size, d_model, num_layers = num_layers)\n","        self.encoder = EncoderLayer(d_model, heads) \n","        self.decoder = DecoderLayer(d_model, heads)\n","        self.logit = nn.Linear(d_model, self.vocab_size)\n","        self.max_len = max_len\n","        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        \n","    def encode(self, src_embeddings, src_mask):\n","        encoder_hidden = torch.zeros([self.num_layers, self.max_len, self.d_model]).to(self.device) # 1 = number of LSTM layers\n","        cell_state = torch.zeros([self.num_layers, self.max_len, self.d_model]).to(self.device)\n","        for i in range(self.num_layers):\n","            src_embeddings, encoder_hidden, cell_state = self.embed(src_embeddings, i, encoder_hidden, cell_state)\n","            src_embeddings = self.encoder(src_embeddings, src_mask)\n","        return src_embeddings, encoder_hidden, cell_state\n","    \n","    def decode(self, tgt_embeddings, target_mask, src_embeddings, src_mask, encoder_hidden, cell_state):\n","        zeros = torch.zeros((self.num_layers, 1, self.d_model), device=encoder_hidden.device)\n","        encoder_hidden = torch.cat((encoder_hidden, zeros), dim=1)\n","        decoder_input = torch.Tensor([[0]]).long().to(self.device) # 0 = SOS_token\n","        decoder_hidden = encoder_hidden\n","        print(decoder_hidden.size())\n","        for i in range(self.num_layers):\n","            tgt_embeddings, decoder_hidden, cell_state = self.embed(tgt_embeddings, i, decoder_hidden, cell_state)\n","            tgt_embeddings = self.decoder(tgt_embeddings, src_embeddings, src_mask, target_mask)\n","        return tgt_embeddings\n","        \n","    def forward(self, src_words, src_mask, target_words, target_mask):\n","        encoded, encoder_hidden, cell_state = self.encode(src_words, src_mask)\n","        decoded = self.decode(target_words, target_mask, encoded, src_mask, encoder_hidden, cell_state)\n","        out = F.log_softmax(self.logit(decoded), dim = 2)\n","        return out\n","\n","class TransformerPreTrainedEmbedding(nn.Module):\n","    def __init__(self, d_model, heads, num_layers, word_map, max_len = 50):\n","        super(TransformerPreTrainedEmbedding, self).__init__()\n","        \n","        self.d_model = d_model\n","        self.num_layers = num_layers\n","        self.vocab_size = len(word_map)\n","        self.embed = PretrainedEmbedding(self.vocab_size, d_model, num_layers = num_layers, max_len = max_len)\n","        self.encoder = EncoderLayer(d_model, heads) \n","        self.decoder = DecoderLayer(d_model, heads)\n","        self.logit = nn.Linear(d_model, self.vocab_size)\n","        \n","    def encode(self, src_embeddings, src_mask):\n","        for i in range(self.num_layers):\n","            src_embeddings = self.embed(src_embeddings, i)\n","            src_embeddings = self.encoder(src_embeddings, src_mask)\n","        return src_embeddings\n","    \n","    def decode(self, tgt_embeddings, target_mask, src_embeddings, src_mask):\n","        for i in range(self.num_layers):\n","            tgt_embeddings = self.embed(tgt_embeddings, i)\n","            tgt_embeddings = self.decoder(tgt_embeddings, src_embeddings, src_mask, target_mask)\n","        return tgt_embeddings\n","        \n","    def forward(self, src_words, src_mask, target_words, target_mask):\n","        encoded = self.encode(src_words, src_mask)\n","        decoded = self.decode(target_words, target_mask, encoded, src_mask)\n","        out = F.log_softmax(self.logit(decoded), dim = 2)\n","        return out\n","\n","class TransformerDecoderOnly(nn.Module):    \n","    def __init__(self, d_model, heads, num_layers, word_map, max_len = 50):\n","        super(TransformerDecoderOnly, self).__init__()\n","        \n","        self.d_model = d_model\n","        self.num_layers = num_layers\n","        self.vocab_size = len(word_map)\n","        self.embed = Embeddings(self.vocab_size, d_model, num_layers = num_layers, max_len = max_len)\n","        self.encoder = EncoderLayer(d_model, heads) \n","        self.decoder = DecoderLayer(d_model, heads)\n","        self.logit = nn.Linear(d_model, self.vocab_size)\n","    \n","    def decode(self, tgt_embeddings, target_mask, src_embeddings, src_mask):\n","        for i in range(self.num_layers):\n","            tgt_embeddings = self.embed(tgt_embeddings, i)\n","            tgt_embeddings = self.decoder(tgt_embeddings, src_embeddings, src_mask, target_mask)\n","        return tgt_embeddings\n","        \n","    def forward(self, src_words, src_mask, target_words, target_mask):\n","        encoded = self.embed(src_words, 0)\n","        decoded = self.decode(target_words, target_mask, encoded, src_mask)\n","        out = F.log_softmax(self.logit(decoded), dim = 2)\n","        return out"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:33.516425Z","iopub.status.busy":"2024-07-02T20:17:33.516094Z","iopub.status.idle":"2024-07-02T20:17:33.527522Z","shell.execute_reply":"2024-07-02T20:17:33.526734Z","shell.execute_reply.started":"2024-07-02T20:17:33.516394Z"},"trusted":true},"outputs":[],"source":["class AdamWarmup:\n","    \n","    def __init__(self, model_size, warmup_steps, optimizer):\n","        \n","        self.model_size = model_size\n","        self.warmup_steps = warmup_steps\n","        self.optimizer = optimizer\n","        self.current_step = 0\n","        self.lr = 0\n","        \n","    def get_lr(self):\n","        return self.model_size ** (-0.5) * min(self.current_step ** (-0.5), self.current_step * self.warmup_steps ** (-1.5))\n","        \n","    def step(self):\n","        # Increment the number of steps each time we call the step function\n","        self.current_step += 1\n","        lr = self.get_lr()\n","        for param_group in self.optimizer.param_groups:\n","            param_group['lr'] = lr\n","        # update the learning rate\n","        self.lr = lr\n","        self.optimizer.step()       \n","        "]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:33.529363Z","iopub.status.busy":"2024-07-02T20:17:33.528622Z","iopub.status.idle":"2024-07-02T20:17:33.540886Z","shell.execute_reply":"2024-07-02T20:17:33.540153Z","shell.execute_reply.started":"2024-07-02T20:17:33.529331Z"},"trusted":true},"outputs":[],"source":["class LossWithLS(nn.Module):\n","\n","    def __init__(self, size, smooth):\n","        super(LossWithLS, self).__init__()\n","        self.criterion = nn.KLDivLoss(size_average=False, reduce=False)\n","        self.confidence = 1.0 - smooth\n","        self.smooth = smooth\n","        self.size = size\n","        \n","    def forward(self, prediction, target, mask):\n","        \"\"\"\n","        prediction of shape: (batch_size, max_words, vocab_size)\n","        target and mask of shape: (batch_size, max_words)\n","        \"\"\"\n","        prediction = prediction.view(-1, prediction.size(-1))   # (batch_size * max_words, vocab_size)\n","        target = target.contiguous().view(-1)   # (batch_size * max_words)\n","        mask = mask.float()\n","        mask = mask.view(-1)       # (batch_size * max_words)\n","        labels = prediction.data.clone()\n","        labels.fill_(self.smooth / (self.size - 1))\n","        labels.scatter_(1, target.data.unsqueeze(1), self.confidence)\n","        loss = self.criterion(prediction, labels)    # (batch_size * max_words, vocab_size)\n","        loss = (loss.sum(1) * mask).sum() / mask.sum()\n","        return loss\n","        "]},{"cell_type":"markdown","metadata":{},"source":["# Define Neptune Experiment"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:33.542496Z","iopub.status.busy":"2024-07-02T20:17:33.542228Z","iopub.status.idle":"2024-07-02T20:17:33.550662Z","shell.execute_reply":"2024-07-02T20:17:33.549840Z","shell.execute_reply.started":"2024-07-02T20:17:33.542473Z"},"trusted":true},"outputs":[],"source":["project = \"andialifs/fluent-tesis-24\"\n","api_token = \"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJjZTY2YWQ3My04OTBkLTQ2OWUtYTc1Ni1jYjk0MGZhMWFiNGEifQ==\"\n","\n","def neptune_init(name):\n","    run = neptune.init_run(\n","        project=project,\n","        api_token=api_token,\n","        name=name\n","    )\n","    return run"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:33.555852Z","iopub.status.busy":"2024-07-02T20:17:33.555608Z","iopub.status.idle":"2024-07-02T20:17:33.563546Z","shell.execute_reply":"2024-07-02T20:17:33.562660Z","shell.execute_reply.started":"2024-07-02T20:17:33.555831Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'max_split_size_mb:8000'"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["import os\n","\n","os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:8000\"\n","os.getenv(\"PYTORCH_CUDA_ALLOC_CONF\")"]},{"cell_type":"markdown","metadata":{},"source":["# Function"]},{"cell_type":"markdown","metadata":{},"source":["## Train"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:33.564996Z","iopub.status.busy":"2024-07-02T20:17:33.564656Z","iopub.status.idle":"2024-07-02T20:17:33.574590Z","shell.execute_reply":"2024-07-02T20:17:33.573653Z","shell.execute_reply.started":"2024-07-02T20:17:33.564947Z"},"trusted":true},"outputs":[],"source":["def train(train_loader, transformer, criterion, epoch):\n","    transformer.train()\n","    sum_loss = 0\n","    count = 0\n","\n","    for i, (question, reply) in enumerate(train_loader):\n","        \n","        samples = question.shape[0]\n","\n","        # Move to device\n","        question = question.to(device)\n","        reply = reply.to(device)\n","\n","        # Prepare Target Data\n","        reply_input = reply[:, :-1]\n","        reply_target = reply[:, 1:]\n","\n","        # Create mask and add dimensions\n","        question_mask, reply_input_mask, reply_target_mask = create_masks(question, reply_input, reply_target)\n","\n","        # Get the transformer outputs\n","        out = transformer(question, question_mask, reply_input, reply_input_mask)\n","\n","        # Compute the loss\n","        loss = criterion(out, reply_target, reply_target_mask)\n","        \n","        # Backprop\n","        transformer_optimizer.optimizer.zero_grad()\n","        loss.backward()\n","        transformer_optimizer.step()\n","        \n","        sum_loss += loss.item() * samples\n","        count += samples\n","        \n","        if i % 100 == 0:\n","            print(\"Epoch [{}][{}/{}]\\tLoss: {:.3f}\".format(epoch, i, len(train_loader), sum_loss/count))\n","    \n","    return sum_loss/count"]},{"cell_type":"markdown","metadata":{},"source":["## Evaluate"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:33.576127Z","iopub.status.busy":"2024-07-02T20:17:33.575799Z","iopub.status.idle":"2024-07-02T20:17:33.587390Z","shell.execute_reply":"2024-07-02T20:17:33.586545Z","shell.execute_reply.started":"2024-07-02T20:17:33.576103Z"},"trusted":true},"outputs":[],"source":["def evaluate(transformer, question, question_mask, max_len, word_map):\n","    \"\"\"\n","    Performs Greedy Decoding with a batch size of 1\n","    \"\"\"\n","    rev_word_map = {v: k for k, v in word_map.items()}\n","    transformer.eval()\n","    start_token = word_map['<start>']\n","    encoded = transformer.encode(question, question_mask)\n","    words = torch.LongTensor([[start_token]]).to(device)\n","    \n","    for step in range(max_len - 1):\n","        size = words.shape[1]\n","        target_mask = torch.triu(torch.ones(size, size)).transpose(0, 1).type(dtype=torch.uint8)\n","        target_mask = target_mask.to(device).unsqueeze(0).unsqueeze(0)\n","        decoded = transformer.decode(words, target_mask, encoded, question_mask)\n","        predictions = transformer.logit(decoded[:, -1])\n","        _, next_word = torch.max(predictions, dim = 1)\n","        next_word = next_word.item()\n","        if next_word == word_map['<end>']:\n","            break\n","        words = torch.cat([words, torch.LongTensor([[next_word]]).to(device)], dim = 1)   # (1,step+2)\n","        \n","    # Construct Sentence\n","    if words.dim() == 2:\n","        words = words.squeeze(0)\n","        words = words.tolist()\n","        \n","    sen_idx = [w for w in words if w not in {word_map['<start>']}]\n","    sentence = ' '.join([rev_word_map[sen_idx[k]] for k in range(len(sen_idx))])\n","    \n","    return sentence"]},{"cell_type":"markdown","metadata":{},"source":["# Experiment"]},{"cell_type":"markdown","metadata":{},"source":["## Transformers without reg"]},{"cell_type":"code","execution_count":40,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:33.588788Z","iopub.status.busy":"2024-07-02T20:17:33.588528Z","iopub.status.idle":"2024-07-02T20:17:47.395240Z","shell.execute_reply":"2024-07-02T20:17:47.393722Z","shell.execute_reply.started":"2024-07-02T20:17:33.588766Z"},"scrolled":true,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Skipping experiment 0 with d_model 512, heads8, num_layers5\n","\n","\n","Skipping experiment 1 with d_model 512, heads8, num_layers10\n","\n","\n","Skipping experiment 2 with d_model 512, heads16, num_layers5\n","\n","\n","Skipping experiment 3 with d_model 512, heads16, num_layers10\n","\n","\n","Skipping experiment 4 with d_model 512, heads32, num_layers5\n","\n","\n","Skipping experiment 5 with d_model 512, heads32, num_layers10\n","\n","\n","Skipping experiment 6 with d_model 1024, heads8, num_layers5\n","\n","\n","Skipping experiment 7 with d_model 1024, heads8, num_layers10\n","\n","\n","Skipping experiment 8 with d_model 1024, heads16, num_layers5\n","\n","\n","Skipping experiment 9 with d_model 1024, heads16, num_layers10\n","\n","\n","Skipping experiment 10 with d_model 1024, heads32, num_layers5\n","\n","\n","Skipping experiment 11 with d_model 1024, heads32, num_layers10\n","\n","\n","Skipping experiment 12 with d_model 2048, heads8, num_layers5\n","\n","\n","Skipping experiment 13 with d_model 2048, heads8, num_layers10\n","\n","\n","Skipping experiment 14 with d_model 2048, heads16, num_layers5\n","\n","\n","Running for experiment 15 with d_model 2048, heads16, num_layers10\n","\n","[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/andialifs/fluent-tesis-24/e/FLUENT24-79\n"]},{"name":"stderr","output_type":"stream","text":["/home/andyalyfsyah/.conda/envs/myenv/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [0][0/12]\tLoss: 5.156\n","Epoch [1][0/12]\tLoss: 5.049\n","Epoch [2][0/12]\tLoss: 4.608\n","Epoch [3][0/12]\tLoss: 4.526\n","Epoch [4][0/12]\tLoss: 4.342\n","Epoch [5][0/12]\tLoss: 4.071\n","Epoch [6][0/12]\tLoss: 4.036\n","Epoch [7][0/12]\tLoss: 4.092\n","Epoch [8][0/12]\tLoss: 4.076\n","Epoch [10][0/12]\tLoss: 4.040\n","Epoch [11][0/12]\tLoss: 3.927\n","Epoch [12][0/12]\tLoss: 4.045\n","Epoch [13][0/12]\tLoss: 3.989\n","Epoch [14][0/12]\tLoss: 3.873\n","Epoch [15][0/12]\tLoss: 3.832\n","Epoch [16][0/12]\tLoss: 4.009\n","Epoch [17][0/12]\tLoss: 3.784\n","Epoch [18][0/12]\tLoss: 3.777\n","Epoch [19][0/12]\tLoss: 3.729\n","Epoch [20][0/12]\tLoss: 3.806\n","Epoch [21][0/12]\tLoss: 3.710\n","Epoch [22][0/12]\tLoss: 3.549\n","Epoch [23][0/12]\tLoss: 3.696\n","Epoch [24][0/12]\tLoss: 3.566\n","Epoch [25][0/12]\tLoss: 3.625\n","Epoch [26][0/12]\tLoss: 3.291\n","Epoch [27][0/12]\tLoss: 3.621\n","Epoch [28][0/12]\tLoss: 3.374\n","Epoch [29][0/12]\tLoss: 3.372\n","Epoch [30][0/12]\tLoss: 3.402\n","Epoch [31][0/12]\tLoss: 3.352\n","Epoch [32][0/12]\tLoss: 3.220\n","Epoch [33][0/12]\tLoss: 3.341\n","Epoch [34][0/12]\tLoss: 3.450\n","Epoch [35][0/12]\tLoss: 3.313\n","Epoch [36][0/12]\tLoss: 3.215\n","Epoch [37][0/12]\tLoss: 3.150\n","Epoch [38][0/12]\tLoss: 3.073\n","Epoch [39][0/12]\tLoss: 3.236\n","Epoch [40][0/12]\tLoss: 3.079\n","Epoch [41][0/12]\tLoss: 3.015\n","Epoch [42][0/12]\tLoss: 3.096\n","Epoch [43][0/12]\tLoss: 2.833\n","Epoch [44][0/12]\tLoss: 2.826\n","Epoch [45][0/12]\tLoss: 2.926\n","Epoch [46][0/12]\tLoss: 2.852\n","Epoch [47][0/12]\tLoss: 2.992\n","Epoch [48][0/12]\tLoss: 2.755\n","Epoch [49][0/12]\tLoss: 2.800\n","Epoch [50][0/12]\tLoss: 2.844\n","Epoch [51][0/12]\tLoss: 2.690\n","Epoch [52][0/12]\tLoss: 2.430\n","Epoch [53][0/12]\tLoss: 2.623\n","Epoch [54][0/12]\tLoss: 2.358\n","Epoch [55][0/12]\tLoss: 2.793\n","Epoch [56][0/12]\tLoss: 2.614\n","Epoch [57][0/12]\tLoss: 2.677\n","Epoch [58][0/12]\tLoss: 2.433\n","Epoch [59][0/12]\tLoss: 2.587\n","Epoch [60][0/12]\tLoss: 2.452\n","Epoch [61][0/12]\tLoss: 2.386\n","Epoch [62][0/12]\tLoss: 2.514\n","Epoch [63][0/12]\tLoss: 2.421\n","Epoch [64][0/12]\tLoss: 2.406\n","Epoch [65][0/12]\tLoss: 2.303\n","Epoch [66][0/12]\tLoss: 2.357\n","Epoch [67][0/12]\tLoss: 2.380\n","Epoch [68][0/12]\tLoss: 2.248\n","Epoch [69][0/12]\tLoss: 2.223\n","Epoch [70][0/12]\tLoss: 2.252\n","Epoch [71][0/12]\tLoss: 2.187\n","Epoch [72][0/12]\tLoss: 2.083\n","Epoch [73][0/12]\tLoss: 2.149\n","Epoch [74][0/12]\tLoss: 2.138\n","Epoch [75][0/12]\tLoss: 2.081\n","Epoch [76][0/12]\tLoss: 2.023\n","Epoch [77][0/12]\tLoss: 1.927\n","Epoch [78][0/12]\tLoss: 1.831\n","Epoch [79][0/12]\tLoss: 1.895\n","Epoch [80][0/12]\tLoss: 1.996\n","Epoch [81][0/12]\tLoss: 1.795\n","Epoch [82][0/12]\tLoss: 1.748\n","Epoch [83][0/12]\tLoss: 1.919\n","Epoch [84][0/12]\tLoss: 1.687\n","Epoch [85][0/12]\tLoss: 1.766\n","Epoch [86][0/12]\tLoss: 1.706\n","Epoch [87][0/12]\tLoss: 1.754\n","Epoch [88][0/12]\tLoss: 1.608\n","Epoch [89][0/12]\tLoss: 1.318\n","Epoch [90][0/12]\tLoss: 1.326\n","Epoch [91][0/12]\tLoss: 1.410\n","Epoch [92][0/12]\tLoss: 1.348\n","Epoch [93][0/12]\tLoss: 1.350\n","Epoch [94][0/12]\tLoss: 1.651\n","Epoch [95][0/12]\tLoss: 1.399\n","Epoch [96][0/12]\tLoss: 1.349\n","Epoch [97][0/12]\tLoss: 1.313\n","Epoch [98][0/12]\tLoss: 1.172\n","Epoch [99][0/12]\tLoss: 1.056\n","[neptune] [info   ] Shutting down background jobs, please wait a moment...\n","[neptune] [info   ] Done!\n","[neptune] [info   ] Waiting for the remaining 1 operations to synchronize with Neptune. Do not kill this process.\n","[neptune] [info   ] All 1 operations synced, thanks for waiting!\n","[neptune] [info   ] Explore the metadata in the Neptune app: https://app.neptune.ai/andialifs/fluent-tesis-24/e/FLUENT24-79/metadata\n","\n","Running for experiment 16 with d_model 2048, heads32, num_layers5\n","\n","[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/andialifs/fluent-tesis-24/e/FLUENT24-80\n"]},{"name":"stderr","output_type":"stream","text":["/home/andyalyfsyah/.conda/envs/myenv/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [0][0/12]\tLoss: 5.202\n","Epoch [1][0/12]\tLoss: 5.062\n","Epoch [2][0/12]\tLoss: 4.734\n","Epoch [3][0/12]\tLoss: 4.371\n","Epoch [4][0/12]\tLoss: 4.236\n","Epoch [5][0/12]\tLoss: 4.167\n","Epoch [6][0/12]\tLoss: 4.178\n","Epoch [7][0/12]\tLoss: 3.889\n","Epoch [8][0/12]\tLoss: 4.102\n","Epoch [9][0/12]\tLoss: 4.067\n","Epoch [10][0/12]\tLoss: 4.009\n","Epoch [11][0/12]\tLoss: 3.869\n","Epoch [12][0/12]\tLoss: 3.987\n","Epoch [13][0/12]\tLoss: 3.984\n","Epoch [14][0/12]\tLoss: 3.879\n","Epoch [15][0/12]\tLoss: 3.601\n","Epoch [16][0/12]\tLoss: 3.539\n","Epoch [17][0/12]\tLoss: 3.507\n","Epoch [18][0/12]\tLoss: 3.415\n","Epoch [19][0/12]\tLoss: 3.446\n","Epoch [20][0/12]\tLoss: 3.041\n","Epoch [21][0/12]\tLoss: 3.090\n","Epoch [22][0/12]\tLoss: 3.004\n","Epoch [23][0/12]\tLoss: 2.906\n","Epoch [24][0/12]\tLoss: 3.072\n","Epoch [25][0/12]\tLoss: 2.878\n","Epoch [26][0/12]\tLoss: 2.795\n","Epoch [27][0/12]\tLoss: 2.622\n","Epoch [28][0/12]\tLoss: 2.341\n","Epoch [29][0/12]\tLoss: 2.367\n","Epoch [30][0/12]\tLoss: 2.489\n","Epoch [31][0/12]\tLoss: 2.395\n","Epoch [32][0/12]\tLoss: 2.238\n","Epoch [33][0/12]\tLoss: 2.099\n","Epoch [34][0/12]\tLoss: 1.988\n","Epoch [35][0/12]\tLoss: 1.993\n","Epoch [36][0/12]\tLoss: 1.909\n","Epoch [37][0/12]\tLoss: 2.032\n","Epoch [38][0/12]\tLoss: 1.927\n","Epoch [39][0/12]\tLoss: 1.988\n","Epoch [40][0/12]\tLoss: 1.965\n","Epoch [41][0/12]\tLoss: 1.662\n","Epoch [42][0/12]\tLoss: 1.531\n","Epoch [43][0/12]\tLoss: 1.570\n","Epoch [44][0/12]\tLoss: 1.602\n","Epoch [45][0/12]\tLoss: 1.381\n","Epoch [46][0/12]\tLoss: 1.510\n","Epoch [47][0/12]\tLoss: 1.471\n","Epoch [48][0/12]\tLoss: 1.307\n","Epoch [49][0/12]\tLoss: 1.339\n","Epoch [50][0/12]\tLoss: 1.392\n","Epoch [51][0/12]\tLoss: 1.306\n","Epoch [52][0/12]\tLoss: 1.121\n","Epoch [53][0/12]\tLoss: 1.202\n","Epoch [54][0/12]\tLoss: 1.350\n","Epoch [55][0/12]\tLoss: 1.175\n","Epoch [56][0/12]\tLoss: 1.198\n","Epoch [57][0/12]\tLoss: 1.011\n","Epoch [58][0/12]\tLoss: 1.118\n","Epoch [59][0/12]\tLoss: 1.034\n","Epoch [60][0/12]\tLoss: 0.926\n","Epoch [61][0/12]\tLoss: 1.111\n","Epoch [62][0/12]\tLoss: 0.784\n","Epoch [63][0/12]\tLoss: 0.815\n","Epoch [64][0/12]\tLoss: 0.840\n","Epoch [65][0/12]\tLoss: 0.768\n","Epoch [66][0/12]\tLoss: 0.771\n","Epoch [67][0/12]\tLoss: 0.658\n","Epoch [68][0/12]\tLoss: 0.812\n","Epoch [69][0/12]\tLoss: 0.702\n","Epoch [70][0/12]\tLoss: 0.730\n","Epoch [71][0/12]\tLoss: 0.724\n","Epoch [72][0/12]\tLoss: 0.807\n","Epoch [73][0/12]\tLoss: 0.674\n","Epoch [74][0/12]\tLoss: 0.601\n","Epoch [75][0/12]\tLoss: 0.589\n","Epoch [76][0/12]\tLoss: 0.576\n","Epoch [77][0/12]\tLoss: 0.529\n","Epoch [78][0/12]\tLoss: 0.443\n","Epoch [79][0/12]\tLoss: 0.514\n","Epoch [80][0/12]\tLoss: 0.542\n","Epoch [81][0/12]\tLoss: 0.477\n","Epoch [82][0/12]\tLoss: 0.458\n","Epoch [83][0/12]\tLoss: 0.475\n","Epoch [84][0/12]\tLoss: 0.467\n","Epoch [85][0/12]\tLoss: 0.461\n","Epoch [86][0/12]\tLoss: 0.364\n","Epoch [87][0/12]\tLoss: 0.450\n","Epoch [88][0/12]\tLoss: 0.365\n","Epoch [89][0/12]\tLoss: 0.408\n","Epoch [90][0/12]\tLoss: 0.331\n","Epoch [91][0/12]\tLoss: 0.373\n","Epoch [92][0/12]\tLoss: 0.382\n","Epoch [93][0/12]\tLoss: 0.348\n","Epoch [94][0/12]\tLoss: 0.379\n","Epoch [95][0/12]\tLoss: 0.328\n","Epoch [96][0/12]\tLoss: 0.324\n","Epoch [97][0/12]\tLoss: 0.351\n","Epoch [98][0/12]\tLoss: 0.310\n","Epoch [99][0/12]\tLoss: 0.287\n","[neptune] [info   ] Shutting down background jobs, please wait a moment...\n","[neptune] [info   ] Done!\n","[neptune] [info   ] Waiting for the remaining 1 operations to synchronize with Neptune. Do not kill this process.\n","[neptune] [info   ] All 1 operations synced, thanks for waiting!\n","[neptune] [info   ] Explore the metadata in the Neptune app: https://app.neptune.ai/andialifs/fluent-tesis-24/e/FLUENT24-80/metadata\n","\n","Running for experiment 17 with d_model 2048, heads32, num_layers10\n","\n","[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/andialifs/fluent-tesis-24/e/FLUENT24-81\n"]},{"name":"stderr","output_type":"stream","text":["/home/andyalyfsyah/.conda/envs/myenv/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [0][0/12]\tLoss: 5.305\n","Epoch [1][0/12]\tLoss: 5.135\n","Epoch [2][0/12]\tLoss: 4.741\n","Epoch [3][0/12]\tLoss: 4.543\n","Epoch [4][0/12]\tLoss: 4.287\n","Epoch [5][0/12]\tLoss: 4.217\n","Epoch [6][0/12]\tLoss: 4.077\n","Epoch [7][0/12]\tLoss: 4.044\n","Epoch [8][0/12]\tLoss: 4.000\n","Epoch [9][0/12]\tLoss: 4.013\n","Epoch [10][0/12]\tLoss: 4.043\n","Epoch [11][0/12]\tLoss: 4.042\n","Epoch [12][0/12]\tLoss: 3.944\n","Epoch [13][0/12]\tLoss: 3.967\n","Epoch [14][0/12]\tLoss: 3.917\n","Epoch [15][0/12]\tLoss: 3.865\n","Epoch [16][0/12]\tLoss: 3.986\n","Epoch [17][0/12]\tLoss: 3.857\n","Epoch [18][0/12]\tLoss: 3.783\n","Epoch [19][0/12]\tLoss: 3.795\n","Epoch [20][0/12]\tLoss: 3.743\n","Epoch [21][0/12]\tLoss: 3.673\n","Epoch [22][0/12]\tLoss: 3.752\n","Epoch [23][0/12]\tLoss: 3.691\n","Epoch [24][0/12]\tLoss: 3.647\n","Epoch [25][0/12]\tLoss: 3.610\n","Epoch [26][0/12]\tLoss: 3.487\n","Epoch [27][0/12]\tLoss: 3.622\n","Epoch [28][0/12]\tLoss: 3.374\n","Epoch [29][0/12]\tLoss: 3.396\n","Epoch [30][0/12]\tLoss: 3.317\n","Epoch [31][0/12]\tLoss: 3.426\n","Epoch [32][0/12]\tLoss: 3.413\n","Epoch [33][0/12]\tLoss: 3.393\n","Epoch [34][0/12]\tLoss: 3.228\n","Epoch [35][0/12]\tLoss: 3.093\n","Epoch [36][0/12]\tLoss: 3.052\n","Epoch [37][0/12]\tLoss: 3.220\n","Epoch [38][0/12]\tLoss: 3.218\n","Epoch [39][0/12]\tLoss: 3.080\n","Epoch [40][0/12]\tLoss: 2.977\n","Epoch [41][0/12]\tLoss: 2.940\n","Epoch [42][0/12]\tLoss: 2.896\n","Epoch [43][0/12]\tLoss: 3.079\n","Epoch [44][0/12]\tLoss: 2.881\n","Epoch [45][0/12]\tLoss: 3.001\n","Epoch [46][0/12]\tLoss: 2.811\n","Epoch [47][0/12]\tLoss: 2.855\n","Epoch [48][0/12]\tLoss: 2.644\n","Epoch [49][0/12]\tLoss: 2.708\n","Epoch [50][0/12]\tLoss: 2.783\n","Epoch [51][0/12]\tLoss: 2.823\n","Epoch [52][0/12]\tLoss: 2.476\n","Epoch [53][0/12]\tLoss: 2.595\n","Epoch [54][0/12]\tLoss: 2.694\n","Epoch [55][0/12]\tLoss: 2.631\n","Epoch [56][0/12]\tLoss: 2.455\n","Epoch [57][0/12]\tLoss: 2.441\n","Epoch [58][0/12]\tLoss: 2.464\n","Epoch [59][0/12]\tLoss: 2.498\n","Epoch [60][0/12]\tLoss: 2.597\n","Epoch [61][0/12]\tLoss: 2.546\n","Epoch [62][0/12]\tLoss: 2.499\n","Epoch [63][0/12]\tLoss: 2.282\n","Epoch [64][0/12]\tLoss: 2.218\n","Epoch [65][0/12]\tLoss: 2.016\n","Epoch [66][0/12]\tLoss: 2.438\n","Epoch [67][0/12]\tLoss: 2.139\n","Epoch [68][0/12]\tLoss: 2.094\n","Epoch [69][0/12]\tLoss: 2.236\n","Epoch [70][0/12]\tLoss: 1.997\n","Epoch [71][0/12]\tLoss: 2.103\n","Epoch [72][0/12]\tLoss: 1.934\n","Epoch [73][0/12]\tLoss: 2.166\n","Epoch [74][0/12]\tLoss: 2.049\n","Epoch [75][0/12]\tLoss: 2.011\n","Epoch [76][0/12]\tLoss: 1.968\n","Epoch [77][0/12]\tLoss: 1.810\n","Epoch [78][0/12]\tLoss: 2.016\n","Epoch [79][0/12]\tLoss: 1.960\n","Epoch [80][0/12]\tLoss: 1.731\n","Epoch [81][0/12]\tLoss: 1.669\n","Epoch [82][0/12]\tLoss: 1.734\n","Epoch [83][0/12]\tLoss: 1.591\n","Epoch [84][0/12]\tLoss: 1.711\n","Epoch [85][0/12]\tLoss: 1.498\n","Epoch [86][0/12]\tLoss: 1.657\n","Epoch [87][0/12]\tLoss: 1.552\n","Epoch [88][0/12]\tLoss: 1.631\n","Epoch [89][0/12]\tLoss: 1.503\n","Epoch [90][0/12]\tLoss: 1.258\n","Epoch [91][0/12]\tLoss: 1.475\n","Epoch [92][0/12]\tLoss: 1.373\n","Epoch [93][0/12]\tLoss: 1.336\n","Epoch [94][0/12]\tLoss: 1.105\n","Epoch [95][0/12]\tLoss: 1.380\n","Epoch [96][0/12]\tLoss: 1.190\n","Epoch [97][0/12]\tLoss: 1.081\n","Epoch [98][0/12]\tLoss: 1.020\n","Epoch [99][0/12]\tLoss: 1.027\n","[neptune] [info   ] Shutting down background jobs, please wait a moment...\n","[neptune] [info   ] Done!\n","[neptune] [info   ] Waiting for the remaining 1 operations to synchronize with Neptune. Do not kill this process.\n","[neptune] [info   ] All 1 operations synced, thanks for waiting!\n","[neptune] [info   ] Explore the metadata in the Neptune app: https://app.neptune.ai/andialifs/fluent-tesis-24/e/FLUENT24-81/metadata\n","\n","Running for experiment 18 with d_model 4096, heads8, num_layers5\n","\n","[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/andialifs/fluent-tesis-24/e/FLUENT24-82\n"]},{"name":"stderr","output_type":"stream","text":["/home/andyalyfsyah/.conda/envs/myenv/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [0][0/12]\tLoss: 5.245\n","Epoch [1][0/12]\tLoss: 4.997\n","Epoch [2][0/12]\tLoss: 4.554\n","Epoch [3][0/12]\tLoss: 4.162\n","Epoch [4][0/12]\tLoss: 4.080\n","Epoch [5][0/12]\tLoss: 4.062\n","Epoch [6][0/12]\tLoss: 4.088\n","Epoch [7][0/12]\tLoss: 4.035\n","Epoch [8][0/12]\tLoss: 3.864\n","Epoch [9][0/12]\tLoss: 4.037\n","Epoch [10][0/12]\tLoss: 3.855\n","Epoch [11][0/12]\tLoss: 3.762\n","Epoch [12][0/12]\tLoss: 3.771\n","Epoch [13][0/12]\tLoss: 3.803\n","Epoch [14][0/12]\tLoss: 3.614\n","Epoch [15][0/12]\tLoss: 3.478\n","Epoch [16][0/12]\tLoss: 3.437\n","Epoch [17][0/12]\tLoss: 3.238\n","Epoch [18][0/12]\tLoss: 3.247\n","Epoch [19][0/12]\tLoss: 3.001\n","Epoch [20][0/12]\tLoss: 3.000\n","Epoch [21][0/12]\tLoss: 2.792\n","Epoch [22][0/12]\tLoss: 2.444\n","Epoch [23][0/12]\tLoss: 2.828\n","Epoch [24][0/12]\tLoss: 2.557\n","Epoch [25][0/12]\tLoss: 2.459\n","Epoch [26][0/12]\tLoss: 2.480\n","Epoch [27][0/12]\tLoss: 2.088\n","Epoch [28][0/12]\tLoss: 2.233\n","Epoch [29][0/12]\tLoss: 2.347\n","Epoch [30][0/12]\tLoss: 2.040\n","Epoch [31][0/12]\tLoss: 2.020\n","Epoch [32][0/12]\tLoss: 1.896\n","Epoch [33][0/12]\tLoss: 1.625\n","Epoch [34][0/12]\tLoss: 1.695\n","Epoch [35][0/12]\tLoss: 1.752\n","Epoch [36][0/12]\tLoss: 1.688\n","Epoch [37][0/12]\tLoss: 1.393\n","Epoch [38][0/12]\tLoss: 1.523\n","Epoch [39][0/12]\tLoss: 1.231\n","Epoch [40][0/12]\tLoss: 1.479\n","Epoch [41][0/12]\tLoss: 1.490\n","Epoch [42][0/12]\tLoss: 1.258\n","Epoch [43][0/12]\tLoss: 1.278\n","Epoch [44][0/12]\tLoss: 1.304\n","Epoch [45][0/12]\tLoss: 1.064\n","Epoch [46][0/12]\tLoss: 0.996\n","Epoch [47][0/12]\tLoss: 0.996\n","Epoch [48][0/12]\tLoss: 1.079\n","Epoch [49][0/12]\tLoss: 1.046\n","Epoch [50][0/12]\tLoss: 0.769\n","Epoch [51][0/12]\tLoss: 0.808\n","Epoch [52][0/12]\tLoss: 0.777\n","Epoch [53][0/12]\tLoss: 0.814\n","Epoch [54][0/12]\tLoss: 0.874\n","Epoch [55][0/12]\tLoss: 0.815\n","Epoch [56][0/12]\tLoss: 0.647\n","Epoch [57][0/12]\tLoss: 0.693\n","Epoch [58][0/12]\tLoss: 0.626\n","Epoch [59][0/12]\tLoss: 0.626\n","Epoch [60][0/12]\tLoss: 0.680\n","Epoch [61][0/12]\tLoss: 0.695\n","Epoch [62][0/12]\tLoss: 0.594\n","Epoch [63][0/12]\tLoss: 0.511\n","Epoch [64][0/12]\tLoss: 0.570\n","Epoch [65][0/12]\tLoss: 0.477\n","Epoch [66][0/12]\tLoss: 0.540\n","Epoch [67][0/12]\tLoss: 0.542\n","Epoch [68][0/12]\tLoss: 0.456\n","Epoch [69][0/12]\tLoss: 0.485\n","Epoch [70][0/12]\tLoss: 0.427\n","Epoch [71][0/12]\tLoss: 0.431\n","Epoch [72][0/12]\tLoss: 0.426\n","Epoch [73][0/12]\tLoss: 0.354\n","Epoch [74][0/12]\tLoss: 0.373\n","Epoch [75][0/12]\tLoss: 0.413\n","Epoch [76][0/12]\tLoss: 0.341\n","Epoch [77][0/12]\tLoss: 0.408\n","Epoch [78][0/12]\tLoss: 0.346\n","Epoch [79][0/12]\tLoss: 0.352\n","Epoch [80][0/12]\tLoss: 0.337\n","Epoch [81][0/12]\tLoss: 0.351\n","Epoch [82][0/12]\tLoss: 0.352\n","Epoch [83][0/12]\tLoss: 0.305\n","Epoch [84][0/12]\tLoss: 0.275\n","Epoch [85][0/12]\tLoss: 0.304\n","Epoch [86][0/12]\tLoss: 0.282\n","Epoch [87][0/12]\tLoss: 0.278\n","Epoch [88][0/12]\tLoss: 0.264\n","Epoch [89][0/12]\tLoss: 0.275\n","Epoch [90][0/12]\tLoss: 0.299\n","Epoch [91][0/12]\tLoss: 0.298\n","Epoch [92][0/12]\tLoss: 0.266\n","Epoch [93][0/12]\tLoss: 0.248\n","Epoch [94][0/12]\tLoss: 0.258\n","Epoch [95][0/12]\tLoss: 0.251\n","Epoch [96][0/12]\tLoss: 0.250\n","Epoch [97][0/12]\tLoss: 0.205\n","Epoch [98][0/12]\tLoss: 0.266\n","Epoch [99][0/12]\tLoss: 0.235\n","[neptune] [info   ] Shutting down background jobs, please wait a moment...\n","[neptune] [info   ] Done!\n","[neptune] [info   ] Waiting for the remaining 1 operations to synchronize with Neptune. Do not kill this process.\n","[neptune] [info   ] All 1 operations synced, thanks for waiting!\n","[neptune] [info   ] Explore the metadata in the Neptune app: https://app.neptune.ai/andialifs/fluent-tesis-24/e/FLUENT24-82/metadata\n","\n","Running for experiment 19 with d_model 4096, heads8, num_layers10\n","\n","[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/andialifs/fluent-tesis-24/e/FLUENT24-83\n"]},{"name":"stderr","output_type":"stream","text":["/home/andyalyfsyah/.conda/envs/myenv/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [0][0/12]\tLoss: 5.212\n","Epoch [1][0/12]\tLoss: 5.006\n","Epoch [2][0/12]\tLoss: 4.476\n","Epoch [3][0/12]\tLoss: 4.137\n","Epoch [4][0/12]\tLoss: 4.158\n","Epoch [5][0/12]\tLoss: 4.043\n","Epoch [6][0/12]\tLoss: 4.092\n","Epoch [7][0/12]\tLoss: 4.070\n","Epoch [8][0/12]\tLoss: 3.994\n","Epoch [9][0/12]\tLoss: 3.931\n","Epoch [10][0/12]\tLoss: 4.079\n","Epoch [11][0/12]\tLoss: 4.009\n","Epoch [12][0/12]\tLoss: 3.951\n","Epoch [13][0/12]\tLoss: 3.858\n","Epoch [14][0/12]\tLoss: 3.801\n","Epoch [15][0/12]\tLoss: 3.749\n","Epoch [16][0/12]\tLoss: 3.854\n","Epoch [17][0/12]\tLoss: 3.660\n","Epoch [18][0/12]\tLoss: 3.801\n","Epoch [19][0/12]\tLoss: 3.621\n","Epoch [20][0/12]\tLoss: 3.695\n","Epoch [21][0/12]\tLoss: 3.556\n","Epoch [22][0/12]\tLoss: 3.574\n","Epoch [23][0/12]\tLoss: 3.519\n","Epoch [24][0/12]\tLoss: 3.421\n","Epoch [25][0/12]\tLoss: 3.348\n","Epoch [26][0/12]\tLoss: 3.515\n","Epoch [27][0/12]\tLoss: 3.439\n","Epoch [28][0/12]\tLoss: 3.519\n","Epoch [29][0/12]\tLoss: 3.381\n","Epoch [30][0/12]\tLoss: 3.212\n","Epoch [31][0/12]\tLoss: 3.406\n","Epoch [32][0/12]\tLoss: 3.111\n","Epoch [33][0/12]\tLoss: 3.181\n","Epoch [34][0/12]\tLoss: 3.225\n","Epoch [35][0/12]\tLoss: 2.917\n","Epoch [36][0/12]\tLoss: 3.080\n","Epoch [37][0/12]\tLoss: 3.067\n","Epoch [38][0/12]\tLoss: 3.156\n","Epoch [39][0/12]\tLoss: 2.822\n","Epoch [40][0/12]\tLoss: 2.986\n"]}],"source":["d_model = [512, 1024, 2048, 4096]\n","heads = [8, 16, 32]\n","num_layers = [5, 10]\n","epochs = 100\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","with open('WORDMAP_corpus_KBFILKOM.json', 'r') as j:\n","    word_map = json.load(j)\n","\n","transformer_experiment = pd.DataFrame(columns = ['experiment_id', 'd_model', 'heads', 'num_layers', 'train_loss'])\n","loss_history = {}\n","\n","experiment_id = -1\n","\n","for d_m in d_model:\n","    for h in heads:\n","        for n_l in num_layers: \n","            parameters = {\n","                'd_model': d_m,\n","                'heads': h,\n","                'num_layers': n_l\n","            }\n","            \n","            experiment_id += 1\n","            \n","            if experiment_id <= 14:\n","                print('\\nSkipping experiment {} with d_model {}, heads{}, num_layers{}\\n'.format(experiment_id, d_m, h, n_l))\n","                continue\n","            \n","            print('\\nRunning for experiment {} with d_model {}, heads{}, num_layers{}\\n'.format(experiment_id, d_m, h, n_l))\n","            name = \"experiment_2724_noreg_\" + str(experiment_id)\n","\n","            run = neptune_init(name)\n","            run['parameters'] = parameters\n","            run['tags'] = \"transformers-vanilla-no-reg\"\n","            \n","            transformer_experiment.loc[experiment_id, 'experiment_id'] = 'experiment_{}'.format(str(experiment_id))\n","            transformer_experiment.loc[experiment_id, 'd_model'] = d_m\n","            transformer_experiment.loc[experiment_id, 'heads'] = h\n","            transformer_experiment.loc[experiment_id, 'num_layers'] = n_l\n","\n","            transformer = Transformer(d_model = d_m, heads = h, num_layers = n_l, word_map = word_map, max_len=95)\n","            transformer = transformer.to(device)\n","            adam_optimizer = torch.optim.Adam(transformer.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-9)\n","            transformer_optimizer = AdamWarmup(model_size = d_m, warmup_steps = 4000, optimizer = adam_optimizer)\n","            criterion = LossWithLS(len(word_map), 0.2)\n","\n","            loss_list_experiment = []\n","            for epoch in range(epochs):\n","                loss_train = train(train_loader, transformer, criterion, epoch)\n","\n","                state = {'epoch': epoch, 'transformer': transformer, 'transformer_optimizer': transformer_optimizer}\n","                # torch.save(state, 'checkpoint_experiment_' + str(epoch) + 'id_'+ str(experiment_id) +'.pth.tar')\n","\n","                loss_list_experiment.append(loss_train)\n","\n","                run['train/loss'].append(loss_train)\n","\n","            transformer_experiment.loc[experiment_id, 'train_loss'] = loss_train\n","            loss_history['experiment_{}'.format(str(experiment_id))] = loss_list_experiment\n","            \n","            # torch.cuda.empty_cache() \n","            run.stop()\n"]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:16:32.298394Z","iopub.status.busy":"2024-07-02T20:16:32.297969Z","iopub.status.idle":"2024-07-02T20:16:32.305442Z","shell.execute_reply":"2024-07-02T20:16:32.304012Z","shell.execute_reply.started":"2024-07-02T20:16:32.298351Z"},"trusted":true},"outputs":[],"source":["torch.cuda.empty_cache() "]},{"cell_type":"markdown","metadata":{},"source":["## Transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"trusted":true},"outputs":[],"source":["d_model = [512, 1024, 2048, 4096]\n","heads = [8, 16, 32]\n","num_layers = [5, 10]\n","epochs = 100\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","with open('WORDMAP_corpus_KBFILKOM.json', 'r') as j:\n","    word_map = json.load(j)\n","\n","transformer_experiment = pd.DataFrame(columns = ['experiment_id', 'd_model', 'heads', 'num_layers', 'train_loss'])\n","loss_history = {}\n","\n","experiment_id = -1\n","\n","for d_m in d_model:\n","    for h in heads:\n","        for n_l in num_layers: \n","            experiment_id += 1\n","            print('\\nRunning for experiment {} with d_model {}, heads{}, num_layers{}\\n'.format(experiment_id, d_m, h, n_l))\n","\n","            run = neptune.init_run(\n","                project=project,\n","                api_token=api_token,\n","                name=\"experiment_1724_\" + str(experiment_id)\n","            ) \n","            run['parameters'] = {\n","                'd_model': d_m,\n","                'heads': h,\n","                'num_layers': n_l\n","            }\n","\n","            transformer_experiment.loc[experiment_id, 'experiment_id'] = 'experiment_{}'.format(str(experiment_id))\n","            transformer_experiment.loc[experiment_id, 'd_model'] = d_m\n","            transformer_experiment.loc[experiment_id, 'heads'] = h\n","            transformer_experiment.loc[experiment_id, 'num_layers'] = n_l\n","\n","            transformer = Transformer(d_model = d_m, heads = h, num_layers = n_l, word_map = word_map, max_len=95)\n","            transformer = transformer.to(device)\n","            adam_optimizer = torch.optim.Adam(transformer.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-9)\n","            transformer_optimizer = AdamWarmup(model_size = d_m, warmup_steps = 4000, optimizer = adam_optimizer)\n","            criterion = LossWithLS(len(word_map), 0.2)\n","\n","            loss_list_experiment = []\n","            for epoch in range(epochs):\n","                loss_train = train(train_loader, transformer, criterion, epoch)\n","\n","                state = {'epoch': epoch, 'transformer': transformer, 'transformer_optimizer': transformer_optimizer}\n","                torch.save(state, 'checkpoint_experiment_' + str(epoch) + 'id_'+ str(experiment_id) +'.pth.tar')\n","\n","                loss_list_experiment.append(loss_train)\n","\n","                run['train/loss'].append(loss_train)\n","\n","            transformer_experiment.loc[experiment_id, 'train_loss'] = loss_train\n","            loss_history['experiment_{}'.format(str(experiment_id))] = loss_list_experiment\n","            \n","            run.stop()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import yaml\n","\n","with open('loss_history_transformer_experiment_KBFILKOM.yaml', 'w') as file:\n","    documents = yaml.dump(loss_history, file)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["transformer_experiment.dropna(inplace=True)\n","transformer_experiment"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import yaml\n","\n","with open('history_rnn_150524.yaml', 'r') as file:\n","    history_rnn = yaml.load(file, Loader=yaml.FullLoader)"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true},"outputs":[],"source":["import matplotlib \n","import matplotlib.pyplot as plt\n","\n","loss_history_key = list(loss_history.keys())\n","\n","plt.figure(figsize=(15,10))\n","plt.title(\"Training loss vs. Number of Epochs\")\n","plt.xlabel(\"Number of Epochs\")\n","plt.ylabel(\"Training Loss\")\n","z\n","\n","for key in loss_history_key:\n","    loss_list = loss_history[key]\n","    labels = f'd_model: {transformer_experiment[transformer_experiment[\"experiment_id\"] == key][\"d_model\"].values[0]}, heads: {transformer_experiment[transformer_experiment[\"experiment_id\"] == key][\"heads\"].values[0]}, num_layers: {transformer_experiment[transformer_experiment[\"experiment_id\"] == key][\"num_layers\"].values[0]}'\n","    plt.plot(loss_list, label = labels)\n","\n","    \n","plt.plot(history_rnn['loss'], \n","                label = 'LSTM (Baseline FLUENT 2023)', \n","                linestyle='dashed', \n","                color='black', \n","                linewidth=2.5, \n","                alpha=0.7, \n","                marker='o', \n","                markerfacecolor='black', \n","                markersize=5\n","        )\n","\n","plt.legend()\n","torch.cuda.is_available()\n","plt.grid()"]},{"cell_type":"markdown","metadata":{},"source":["# Training"]},{"cell_type":"markdown","metadata":{},"source":["## Pretrained"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["directory = 'experiment_pretrained_1'\n","\n","d_model = 2048\n","heads = 16\n","num_layers = 5\n","epochs = 100\n","\n","loss_history_pratrained_embed_transformer = []\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","with open('WORDMAP_corpus_KBFILKOM.json', 'r') as j:\n","    word_map = json.load(j)\n","    \n","transformer = TransformerPreTrainedEmbedding(d_model = d_model, heads = heads, num_layers = num_layers, word_map = word_map, max_len=95)\n","transformer = transformer.to(device)\n","adam_optimizer = torch.optim.Adam(transformer.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-9)\n","transformer_optimizer = AdamWarmup(model_size = d_model, warmup_steps = 4000, optimizer = adam_optimizer)\n","criterion = LossWithLS(len(word_map), 0.2)\n","\n","for epoch in range(epochs):\n","    loss_train = train(train_loader, transformer, criterion, epoch)\n","\n","    state = {'epoch': epoch, 'transformer': transformer, 'transformer_optimizer': transformer_optimizer}\n","    torch.save(state, directory + '/checkpoint_' + str(epoch) +'.pth.tar')\n","\n","    loss_history_pratrained_embed_transformer.append(loss_train)"]},{"cell_type":"markdown","metadata":{},"source":["## Vanilla"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[],"source":["torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Directory already exists at experiment_vanilla_050724\n"]},{"name":"stderr","output_type":"stream","text":["/home/andyalyfsyah/.conda/envs/myenv/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [0][0/24]\tLoss: 5.261\n","Epoch [1][0/24]\tLoss: 4.532\n","Epoch [2][0/24]\tLoss: 3.925\n","Epoch [3][0/24]\tLoss: 4.109\n","Epoch [4][0/24]\tLoss: 4.089\n","Epoch [5][0/24]\tLoss: 3.980\n","Epoch [6][0/24]\tLoss: 4.042\n","Epoch [7][0/24]\tLoss: 3.827\n","Epoch [8][0/24]\tLoss: 3.780\n","Epoch [9][0/24]\tLoss: 3.802\n","Epoch [10][0/24]\tLoss: 3.671\n","Epoch [11][0/24]\tLoss: 3.694\n","Epoch [12][0/24]\tLoss: 3.865\n","Epoch [13][0/24]\tLoss: 3.871\n","Epoch [14][0/24]\tLoss: 3.647\n","Epoch [15][0/24]\tLoss: 3.346\n","Epoch [16][0/24]\tLoss: 3.645\n","Epoch [17][0/24]\tLoss: 3.275\n","Epoch [18][0/24]\tLoss: 3.354\n","Epoch [19][0/24]\tLoss: 3.461\n","Epoch [20][0/24]\tLoss: 3.447\n","Epoch [21][0/24]\tLoss: 3.221\n","Epoch [22][0/24]\tLoss: 3.325\n","Epoch [23][0/24]\tLoss: 3.106\n","Epoch [24][0/24]\tLoss: 3.184\n","Epoch [25][0/24]\tLoss: 3.191\n","Epoch [26][0/24]\tLoss: 3.222\n","Epoch [27][0/24]\tLoss: 3.029\n","Epoch [28][0/24]\tLoss: 3.199\n","Epoch [29][0/24]\tLoss: 3.019\n","Epoch [30][0/24]\tLoss: 2.921\n","Epoch [31][0/24]\tLoss: 3.036\n","Epoch [32][0/24]\tLoss: 2.987\n","Epoch [33][0/24]\tLoss: 2.823\n","Epoch [34][0/24]\tLoss: 2.645\n","Epoch [35][0/24]\tLoss: 2.952\n","Epoch [36][0/24]\tLoss: 2.677\n","Epoch [37][0/24]\tLoss: 2.725\n","Epoch [38][0/24]\tLoss: 2.962\n","Epoch [39][0/24]\tLoss: 2.974\n","Epoch [40][0/24]\tLoss: 2.882\n","Epoch [41][0/24]\tLoss: 2.813\n","Epoch [42][0/24]\tLoss: 2.846\n","Epoch [43][0/24]\tLoss: 2.535\n","Epoch [44][0/24]\tLoss: 2.954\n","Epoch [45][0/24]\tLoss: 2.855\n","Epoch [46][0/24]\tLoss: 2.798\n","Epoch [47][0/24]\tLoss: 2.660\n","Epoch [48][0/24]\tLoss: 2.640\n","Epoch [49][0/24]\tLoss: 2.618\n","Epoch [50][0/24]\tLoss: 2.697\n","Epoch [51][0/24]\tLoss: 2.571\n","Epoch [52][0/24]\tLoss: 2.660\n","Epoch [53][0/24]\tLoss: 2.523\n","Epoch [54][0/24]\tLoss: 2.676\n","Epoch [55][0/24]\tLoss: 2.407\n","Epoch [56][0/24]\tLoss: 2.382\n","Epoch [57][0/24]\tLoss: 2.420\n","Epoch [58][0/24]\tLoss: 2.608\n","Epoch [59][0/24]\tLoss: 2.577\n","Epoch [60][0/24]\tLoss: 2.336\n","Epoch [61][0/24]\tLoss: 2.429\n","Epoch [62][0/24]\tLoss: 2.349\n","Epoch [63][0/24]\tLoss: 2.460\n","Epoch [64][0/24]\tLoss: 2.059\n","Epoch [65][0/24]\tLoss: 2.514\n","Epoch [66][0/24]\tLoss: 2.481\n","Epoch [67][0/24]\tLoss: 2.368\n","Epoch [68][0/24]\tLoss: 2.381\n","Epoch [69][0/24]\tLoss: 2.212\n","Epoch [70][0/24]\tLoss: 2.838\n","Epoch [71][0/24]\tLoss: 2.336\n","Epoch [72][0/24]\tLoss: 2.215\n","Epoch [81][0/24]\tLoss: 2.350\n","Epoch [82][0/24]\tLoss: 2.127\n","Epoch [83][0/24]\tLoss: 2.053\n","Epoch [84][0/24]\tLoss: 2.112\n","Epoch [85][0/24]\tLoss: 2.311\n","Epoch [86][0/24]\tLoss: 1.909\n","Epoch [87][0/24]\tLoss: 2.213\n","Epoch [88][0/24]\tLoss: 2.318\n","Epoch [89][0/24]\tLoss: 2.091\n","Epoch [90][0/24]\tLoss: 2.208\n","Epoch [91][0/24]\tLoss: 1.837\n","Epoch [92][0/24]\tLoss: 2.122\n","Epoch [93][0/24]\tLoss: 1.949\n","Epoch [94][0/24]\tLoss: 2.209\n","Epoch [95][0/24]\tLoss: 1.979\n","Epoch [96][0/24]\tLoss: 2.001\n","Epoch [97][0/24]\tLoss: 2.013\n","Epoch [98][0/24]\tLoss: 1.933\n","Epoch [99][0/24]\tLoss: 1.879\n"]}],"source":["directory = 'experiment_vanilla_050724'\n","create_directory(directory)\n","\n","d_model = 4096\n","heads = 16\n","num_layers = 10\n","epochs = 100\n","\n","loss_history_vanilla_transformer = []\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","with open('WORDMAP_corpus_KBFILKOM.json', 'r') as j:\n","    word_map = json.load(j)\n","    \n","transformer = Transformer(d_model = d_model, heads = heads, num_layers = num_layers, word_map = word_map, max_len=95)\n","transformer = transformer.to(device)\n","adam_optimizer = torch.optim.Adam(transformer.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-9)\n","transformer_optimizer = AdamWarmup(model_size = d_model, warmup_steps = 4000, optimizer = adam_optimizer)\n","criterion = LossWithLS(len(word_map), 0.2)\n","\n","for epoch in range(epochs):\n","    loss_train = train(train_loader, transformer, criterion, epoch)\n","\n","    state = {'epoch': epoch, 'transformer': transformer, 'transformer_optimizer': transformer_optimizer}\n","    # torch.save(state, directory + '/checkpoint_' + str(epoch) +'.pth.tar')\n","\n","    loss_history_vanilla_transformer.append(loss_train)\n","\n","import yaml \n","\n","with open(directory + '/loss_history_vanilla_transformer.yaml', 'w') as file:\n","    yaml.dump(loss_history_vanilla_transformer, file)"]},{"cell_type":"markdown","metadata":{},"source":["## Vanilla without Regularization"]},{"cell_type":"code","execution_count":97,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Directory already exists at transformers_vanillanoreg_01724\n"]},{"name":"stderr","output_type":"stream","text":["[neptune] [warning] NeptuneWarning: The following monitoring options are disabled by default in interactive sessions: 'capture_stdout', 'capture_stderr', 'capture_traceback', and 'capture_hardware_metrics'. To enable them, set each parameter to 'True' when initializing the run. The monitoring will continue until you call run.stop() or the kernel stops. Also note: Your source files can only be tracked if you pass the path(s) to the 'source_code' argument. For help, see the Neptune docs: https://docs.neptune.ai/logging/source_code/\n"]},{"name":"stdout","output_type":"stream","text":["[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/andialifs/fluent-tesis-24/e/FLUENT24-88\n"]},{"name":"stderr","output_type":"stream","text":["/home/andyalyfsyah/.conda/envs/myenv/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [0][0/24]\tLoss: 5.195\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[97], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m criterion \u001b[38;5;241m=\u001b[39m LossWithLS(\u001b[38;5;28mlen\u001b[39m(word_map), \u001b[38;5;241m0.2\u001b[39m)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(parameters[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[0;32m---> 29\u001b[0m     loss_train \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     state \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m: epoch, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer\u001b[39m\u001b[38;5;124m'\u001b[39m: transformer, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer_optimizer\u001b[39m\u001b[38;5;124m'\u001b[39m: transformer_optimizer}\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;66;03m# torch.save(state, directory + '/checkpoint_' + str(epoch) +'.pth.tar')\u001b[39;00m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m# run['model_checkpoint'].upload(directory + '/checkpoint_' + str(epoch) +'.pth.tar')\u001b[39;00m\n","Cell \u001b[0;32mIn[95], line 29\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(train_loader, transformer, criterion, epoch)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Backprop\u001b[39;00m\n\u001b[1;32m     28\u001b[0m transformer_optimizer\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 29\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m transformer_optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     32\u001b[0m sum_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m samples\n","File \u001b[0;32m~/.conda/envs/myenv/lib/python3.8/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/.conda/envs/myenv/lib/python3.8/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["directory = 'transformers_vanillanoreg_01724'\n","create_directory(directory)\n","\n","run = neptune_init(directory)\n","\n","parameters = {\n","    'd_model': 2048,\n","    'heads': 16,\n","    'num_layers': 15,\n","    'epochs': 100,\n","}\n","run['parameters'] = parameters\n","\n","loss_history_vanillanoreg_transformer = []\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n","# device = \"cpu\"\n","\n","with open('WORDMAP_corpus_KBFILKOM.json', 'r') as j:\n","    word_map = json.load(j)\n","    \n","transformer = TransformerNoReg(d_model = parameters['d_model'], heads = parameters['heads'], num_layers = parameters['num_layers'], word_map = word_map, max_len=95)\n","transformer = transformer.to(device)\n","adam_optimizer = torch.optim.Adam(transformer.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-9)\n","transformer_optimizer = AdamWarmup(model_size = parameters['d_model'], warmup_steps = 4000, optimizer = adam_optimizer)\n","criterion = LossWithLS(len(word_map), 0.2)\n","\n","for epoch in range(parameters['epochs']):\n","    loss_train = train(train_loader, transformer, criterion, epoch)\n","\n","    state = {'epoch': epoch, 'transformer': transformer, 'transformer_optimizer': transformer_optimizer}\n","    # torch.save(state, directory + '/checkpoint_' + str(epoch) +'.pth.tar')\n","    # run['model_checkpoint'].upload(directory + '/checkpoint_' + str(epoch) +'.pth.tar')\n","\n","    loss_history_vanillanoreg_transformer.append(loss_train)\n","    run['train/loss'].append(loss_train)\n","\n","with open(directory + '/loss_history_vanillanoreg_transformer.yaml', 'w') as file:\n","    yaml.dump(loss_history_vanilla_transformer, file)\n","\n","run.stop()"]},{"cell_type":"markdown","metadata":{},"source":["## Decoder Only"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["directory = 'experiment_deconly_17624'\n","create_directory(directory)\n","\n","d_model = 2048\n","heads = 16\n","num_layers = 5\n","epochs = 100\n","\n","loss_history_decoder_transformer = []\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","with open('WORDMAP_corpus_KBFILKOM.json', 'r') as j:\n","    word_map = json.load(j)\n","    \n","transformer = TransformerDecoderOnly(d_model = d_model, heads = heads, num_layers = num_layers, word_map = word_map, max_len=95)\n","transformer = transformer.to(device)\n","adam_optimizer = torch.optim.Adam(transformer.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-9)\n","transformer_optimizer = AdamWarmup(model_size = d_model, warmup_steps = 4000, optimizer = adam_optimizer)\n","criterion = LossWithLS(len(word_map), 0.2)\n","\n","for epoch in range(epochs):\n","    loss_train = train(train_loader, transformer, criterion, epoch)\n","\n","    state = {'epoch': epoch, 'transformer': transformer, 'transformer_optimizer': transformer_optimizer}\n","    torch.save(state, directory + '/checkpoint_' + str(epoch) +'.pth.tar')\n","\n","    loss_history_decoder_transformer.append(loss_train)"]},{"cell_type":"markdown","metadata":{},"source":["## LSTM_Transformer"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["directory = 'experiment_2_lstm'\n","\n","d_model = 1024\n","heads = 32\n","num_layers = 10\n","epochs = 100\n","\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","loss_history_lstm_transformer = []\n","\n","with open('WORDMAP_corpus_KBFILKOM.json', 'r') as j:\n","    word_map = json.load(j)\n","    \n","transformer = TransformerLSTM(d_model = d_model, heads = heads, num_layers = num_layers, word_map = word_map, max_len=90)\n","transformer = transformer.to(device)\n","adam_optimizer = torch.optim.Adam(transformer.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-9)\n","transformer_optimizer = AdamWarmup(model_size = d_model, warmup_steps = 4000, optimizer = adam_optimizer)\n","criterion = LossWithLS(len(word_map), 0.2)\n","\n","\n","for epoch in range(epochs):\n","    loss_train = train(train_loader, transformer, criterion, epoch)\n","\n","    state = {'epoch': epoch, 'transformer': transformer, 'transformer_optimizer': transformer_optimizer}\n","    torch.save(state, directory + '/checkpoint_' + str(epoch) +'.pth.tar')\n","\n","    loss_history_lstm_transformer.append(loss_train)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import yaml\n","\n","with open(directory + '/loss_history_lstm_transformer.yaml', 'w') as file:\n","    yaml.dump(loss_history_lstm_transformer, file)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["directory = 'experiment_1'\n","checkpoint = torch.load(directory + '/checkpoint_99.pth.tar')\n","transformer = checkpoint['transformer']\n","\n","question = \"Visi FILKOM\" \n","max_len = 50\n","enc_qus = [word_map.get(word, word_map['<unk>']) for word in question.split()]\n","question = torch.LongTensor(enc_qus).to(device).unsqueeze(0)\n","question_mask = (question!=0).to(device).unsqueeze(1).unsqueeze(1)  \n","sentence = evaluate(transformer, question, question_mask, int(max_len), word_map)\n","print(sentence)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["checkpoint"]},{"cell_type":"markdown","metadata":{},"source":["# Reset Cache"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","\n","torch.cuda.empty_cache()\n","\n","print(torch.cuda.memory_summary(device=None, abbreviated=False))"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30733,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"My Environment","language":"python","name":"myenv"}},"nbformat":4,"nbformat_minor":4}
