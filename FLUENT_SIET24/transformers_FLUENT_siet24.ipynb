{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T20:17:27.682419Z",
     "iopub.status.busy": "2024-07-02T20:17:27.682115Z",
     "iopub.status.idle": "2024-07-02T20:17:30.597078Z",
     "shell.execute_reply": "2024-07-02T20:17:30.596113Z",
     "shell.execute_reply.started": "2024-07-02T20:17:27.682391Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "import neptune\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import yaml\n",
    "from collections import Counter\n",
    "from torchinfo import summary\n",
    "from sklearn.decomposition import PCA\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T20:17:30.598900Z",
     "iopub.status.busy": "2024-07-02T20:17:30.598458Z",
     "iopub.status.idle": "2024-07-02T20:17:32.694354Z",
     "shell.execute_reply": "2024-07-02T20:17:32.693406Z",
     "shell.execute_reply.started": "2024-07-02T20:17:30.598873Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pertanyaan</th>\n",
       "      <th>Jawaban</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>visi filkom</td>\n",
       "      <td>menjadi fakultas yang berdaya saing internasio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>misi filkom</td>\n",
       "      <td>menyelenggarakan pendidikan di bidang teknolog...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>apa tujuan filkom?</td>\n",
       "      <td>menghasilkan lulusan yang kompeten , profesion...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sasaran pendidikan filkom</td>\n",
       "      <td>meningkatkan kompetensi dan kualifikasi pendid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>email fitra a. bachtiar</td>\n",
       "      <td>fitra.bachtiar[at]ub.ac.id</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>bidang penelitian fitra a. bachtiar</td>\n",
       "      <td>affective computing, affective engineering, in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>tanggal dibentuk ptiik</td>\n",
       "      <td>27 oktober 2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sasaran pengabdian filkom</td>\n",
       "      <td>1. meningkatkan kualitas dan kuantitas pengabd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sasaran kerjasama filkom</td>\n",
       "      <td>1. mengadakan kerjasama pendidikan, penlitian ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>dekan fakultas ilmu komputer filkom</td>\n",
       "      <td>prof. ir. wayan firdaus mahmudy, s.si., mt., p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>wakil dekan bidang akademik / wakil dekan 1</td>\n",
       "      <td>dr. eng. ir. herman tolle, st., mt.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>wakil dekan bidang umum, keuangan, dan sumber ...</td>\n",
       "      <td>agus wahyu widodo, st., m.cs.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>wakil dekan bidang kemahasiswaan, alumni, dan ...</td>\n",
       "      <td>drs. muh. arif rahman, m.kom.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ketua departemen teknik informatika</td>\n",
       "      <td>achmad basuki, s.t., m.mg., ph.d.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>sekretaris departemen teknik informatika</td>\n",
       "      <td>ir. primantara hari trisnawan, m.sc.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ketua program studi magister ilmu komputer</td>\n",
       "      <td>sabriansyah rizqika akbar, s.t., m.eng., ph.d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ketua program studi sarjana teknik informatika</td>\n",
       "      <td>adhitya bhawiyuga, s.kom., m.sc.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ketua program studi sarjana teknik komputer</td>\n",
       "      <td>barlian henryranu prasetio, s.t., m.t., ph.d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ketua departemen sistem informasi</td>\n",
       "      <td>issa arwani, s.kom., m.sc.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>seketaris departemen sistem informasi</td>\n",
       "      <td>satrio agung wicaksono, s.kom., m.kom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>ketua program studi sarjana sistem informasi</td>\n",
       "      <td>yusi tyroni mursityo, s.kom., m.s.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>ketua program studi sarjana pendidikan teknolo...</td>\n",
       "      <td>ir. admaja dwi herlambang, s.pd., m.pd.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>ketua program studi sarjana teknologi informasi</td>\n",
       "      <td>ir. widhy hayuhardhika nugraha putra, s.kom., ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>berikan saya informasi alumni</td>\n",
       "      <td>informasi alumni dapat diakses pada link berik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>apa saja layanan kemahasiswaan filkom ub ?</td>\n",
       "      <td>1. pengajuan proposal dan lpj kegiatan kemahas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>bagaimana pengajuan proposal dan lpj kegiatan ...</td>\n",
       "      <td>pengajuan proposal kegiatan kemahasiswaan\\npen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>berikan informasi dokumen kemahasiswaan ?</td>\n",
       "      <td>informasi dokumen kemahasiswaan dapat dilihat ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>bagaimana pengajuan surat tugas dosen pembimbi...</td>\n",
       "      <td>informasi pengajuan surat tugas dosen pembimbi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>bagaimana permohonan validasi data skm ?</td>\n",
       "      <td>permohonan validasi data skm dapat dilihat pad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>berikan informasi mengenai validasi syarat wisuda</td>\n",
       "      <td>1. unggah dokumen di siam\\n2. mengisi gform pe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>bagaimana mengakses tracer study fakultas ?</td>\n",
       "      <td>tracer study fakultas dapat diakses pada tauta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>bagaimana cara pendaftaran wisuda ulang ?</td>\n",
       "      <td>pendaftaran wisuda ulang dapat diakses pada ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>berikan informasi mengenai layanan bimbingan d...</td>\n",
       "      <td>layanan bimbingan dan konseling dapat diaksesp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>berikan informasi mengenai layanan ultksp (uni...</td>\n",
       "      <td>layanan ultksp dapat diaksespada tautan beriku...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>berikan informasi mengenai tracking layanan ke...</td>\n",
       "      <td>tracking layanan kemahasiswaan dapat diaksespa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>berikan informasi mengenai bimbingan dan konse...</td>\n",
       "      <td>dalam perjalanannya menuntut ilmu, mahasiswa t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>apa tujuan unit konseling ?</td>\n",
       "      <td>1. mewujudkan potensi dirinya secara optimal, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>apa fungsi bimbingan dan konseling serta penas...</td>\n",
       "      <td>1. penyaluran: bimbingan berfungsi dalam memba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>apa saja program layanan unit konseling ?</td>\n",
       "      <td>1. pelayanan bantuan pemecahan masalah, baik y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>apa manfaat konseling filkom ?</td>\n",
       "      <td>1. masalah ditangani oleh ahli yang kompeten d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>berikan informasi mengenai layanan konseling</td>\n",
       "      <td>informasi mengenai layanan konseling dapat dia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>siapa konselor bimbingan dan konseling di filk...</td>\n",
       "      <td>ada 2 konselor bimbingan dan konseling di filk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>siapa koordinator konselor sebaya ?</td>\n",
       "      <td>koordinator konselor sebaya adalah muhammad da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>berikan rincian layanan ultksp</td>\n",
       "      <td>rincian layanan ultksp dapat diakses pada taut...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Pertanyaan  \\\n",
       "0                                         visi filkom   \n",
       "1                                         misi filkom   \n",
       "2                                  apa tujuan filkom?   \n",
       "3                           sasaran pendidikan filkom   \n",
       "4                             email fitra a. bachtiar   \n",
       "5                 bidang penelitian fitra a. bachtiar   \n",
       "6                              tanggal dibentuk ptiik   \n",
       "7                           sasaran pengabdian filkom   \n",
       "8                            sasaran kerjasama filkom   \n",
       "9                 dekan fakultas ilmu komputer filkom   \n",
       "10        wakil dekan bidang akademik / wakil dekan 1   \n",
       "11  wakil dekan bidang umum, keuangan, dan sumber ...   \n",
       "12  wakil dekan bidang kemahasiswaan, alumni, dan ...   \n",
       "13                ketua departemen teknik informatika   \n",
       "14           sekretaris departemen teknik informatika   \n",
       "15         ketua program studi magister ilmu komputer   \n",
       "16     ketua program studi sarjana teknik informatika   \n",
       "17        ketua program studi sarjana teknik komputer   \n",
       "18                  ketua departemen sistem informasi   \n",
       "19              seketaris departemen sistem informasi   \n",
       "20       ketua program studi sarjana sistem informasi   \n",
       "21  ketua program studi sarjana pendidikan teknolo...   \n",
       "22    ketua program studi sarjana teknologi informasi   \n",
       "23                      berikan saya informasi alumni   \n",
       "24         apa saja layanan kemahasiswaan filkom ub ?   \n",
       "25  bagaimana pengajuan proposal dan lpj kegiatan ...   \n",
       "26          berikan informasi dokumen kemahasiswaan ?   \n",
       "27  bagaimana pengajuan surat tugas dosen pembimbi...   \n",
       "28           bagaimana permohonan validasi data skm ?   \n",
       "29  berikan informasi mengenai validasi syarat wisuda   \n",
       "30        bagaimana mengakses tracer study fakultas ?   \n",
       "31          bagaimana cara pendaftaran wisuda ulang ?   \n",
       "32  berikan informasi mengenai layanan bimbingan d...   \n",
       "33  berikan informasi mengenai layanan ultksp (uni...   \n",
       "34  berikan informasi mengenai tracking layanan ke...   \n",
       "35  berikan informasi mengenai bimbingan dan konse...   \n",
       "36                        apa tujuan unit konseling ?   \n",
       "37  apa fungsi bimbingan dan konseling serta penas...   \n",
       "38          apa saja program layanan unit konseling ?   \n",
       "39                     apa manfaat konseling filkom ?   \n",
       "40       berikan informasi mengenai layanan konseling   \n",
       "41  siapa konselor bimbingan dan konseling di filk...   \n",
       "42                siapa koordinator konselor sebaya ?   \n",
       "43                     berikan rincian layanan ultksp   \n",
       "\n",
       "                                              Jawaban  \n",
       "0   menjadi fakultas yang berdaya saing internasio...  \n",
       "1   menyelenggarakan pendidikan di bidang teknolog...  \n",
       "2   menghasilkan lulusan yang kompeten , profesion...  \n",
       "3   meningkatkan kompetensi dan kualifikasi pendid...  \n",
       "4                          fitra.bachtiar[at]ub.ac.id  \n",
       "5   affective computing, affective engineering, in...  \n",
       "6                                     27 oktober 2011  \n",
       "7   1. meningkatkan kualitas dan kuantitas pengabd...  \n",
       "8   1. mengadakan kerjasama pendidikan, penlitian ...  \n",
       "9   prof. ir. wayan firdaus mahmudy, s.si., mt., p...  \n",
       "10                dr. eng. ir. herman tolle, st., mt.  \n",
       "11                      agus wahyu widodo, st., m.cs.  \n",
       "12                      drs. muh. arif rahman, m.kom.  \n",
       "13                  achmad basuki, s.t., m.mg., ph.d.  \n",
       "14               ir. primantara hari trisnawan, m.sc.  \n",
       "15      sabriansyah rizqika akbar, s.t., m.eng., ph.d  \n",
       "16                   adhitya bhawiyuga, s.kom., m.sc.  \n",
       "17       barlian henryranu prasetio, s.t., m.t., ph.d  \n",
       "18                         issa arwani, s.kom., m.sc.  \n",
       "19              satrio agung wicaksono, s.kom., m.kom  \n",
       "20                 yusi tyroni mursityo, s.kom., m.s.  \n",
       "21            ir. admaja dwi herlambang, s.pd., m.pd.  \n",
       "22  ir. widhy hayuhardhika nugraha putra, s.kom., ...  \n",
       "23  informasi alumni dapat diakses pada link berik...  \n",
       "24  1. pengajuan proposal dan lpj kegiatan kemahas...  \n",
       "25  pengajuan proposal kegiatan kemahasiswaan\\npen...  \n",
       "26  informasi dokumen kemahasiswaan dapat dilihat ...  \n",
       "27  informasi pengajuan surat tugas dosen pembimbi...  \n",
       "28  permohonan validasi data skm dapat dilihat pad...  \n",
       "29  1. unggah dokumen di siam\\n2. mengisi gform pe...  \n",
       "30  tracer study fakultas dapat diakses pada tauta...  \n",
       "31  pendaftaran wisuda ulang dapat diakses pada ta...  \n",
       "32  layanan bimbingan dan konseling dapat diaksesp...  \n",
       "33  layanan ultksp dapat diaksespada tautan beriku...  \n",
       "34  tracking layanan kemahasiswaan dapat diaksespa...  \n",
       "35  dalam perjalanannya menuntut ilmu, mahasiswa t...  \n",
       "36  1. mewujudkan potensi dirinya secara optimal, ...  \n",
       "37  1. penyaluran: bimbingan berfungsi dalam memba...  \n",
       "38  1. pelayanan bantuan pemecahan masalah, baik y...  \n",
       "39  1. masalah ditangani oleh ahli yang kompeten d...  \n",
       "40  informasi mengenai layanan konseling dapat dia...  \n",
       "41  ada 2 konselor bimbingan dan konseling di filk...  \n",
       "42  koordinator konselor sebaya adalah muhammad da...  \n",
       "43  rincian layanan ultksp dapat diakses pada taut...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# knowledgebase_url = 'https://github.com/AndiAlifs/FLUENT-Chatbot-2023/raw/main/KnowledgeBaseFilkom.xlsx'\n",
    "knowledgebase_url = 'https://github.com/AndiAlifs/FLUENT-Chatbot-2023/raw/main/KnowledgeBaseFilkom_simple.xlsx'\n",
    "knowledgebase = pd.read_excel(knowledgebase_url)\n",
    "\n",
    "qa_paired = knowledgebase.drop(columns=knowledgebase.columns.drop(['Pertanyaan', 'Jawaban']))\n",
    "qa_paired.dropna(inplace=True)\n",
    "qa_paired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T20:17:32.697066Z",
     "iopub.status.busy": "2024-07-02T20:17:32.696633Z",
     "iopub.status.idle": "2024-07-02T20:17:32.702322Z",
     "shell.execute_reply": "2024-07-02T20:17:32.701334Z",
     "shell.execute_reply.started": "2024-07-02T20:17:32.697038Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def remove_punc(string):\n",
    "    punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~1234567890'''\n",
    "    no_punct = \"\"\n",
    "    for char in string:\n",
    "        if char not in punctuations:\n",
    "            no_punct = no_punct + char  # space is also a character\n",
    "    return no_punct.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T20:17:32.703938Z",
     "iopub.status.busy": "2024-07-02T20:17:32.703658Z",
     "iopub.status.idle": "2024-07-02T20:17:32.827025Z",
     "shell.execute_reply": "2024-07-02T20:17:32.825935Z",
     "shell.execute_reply.started": "2024-07-02T20:17:32.703915Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pairs = []\n",
    "max_len = 90\n",
    "\n",
    "for line in qa_paired.iterrows():\n",
    "    pertanyaan = line[1]['Pertanyaan']\n",
    "    jawaban = line[1]['Jawaban']\n",
    "    qa_pairs = []\n",
    "    first = remove_punc(pertanyaan.strip())      \n",
    "    second = remove_punc(jawaban.strip())\n",
    "\n",
    "    if len(first) == 0 or len(second) == 0:\n",
    "        continue\n",
    "\n",
    "    \n",
    "    qa_pairs.append(first.split()[:max_len])\n",
    "    qa_pairs.append(second.split()[:max_len])\n",
    "    pairs.append(qa_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[visi, filkom]</td>\n",
       "      <td>[menjadi, fakultas, yang, berdaya, saing, inte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[misi, filkom]</td>\n",
       "      <td>[menyelenggarakan, pendidikan, di, bidang, tek...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[apa, tujuan, filkom]</td>\n",
       "      <td>[menghasilkan, lulusan, yang, kompeten, profes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[sasaran, pendidikan, filkom]</td>\n",
       "      <td>[meningkatkan, kompetensi, dan, kualifikasi, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[email, fitra, a, bachtiar]</td>\n",
       "      <td>[fitrabachtiaratubacid]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[bidang, penelitian, fitra, a, bachtiar]</td>\n",
       "      <td>[affective, computing, affective, engineering,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[tanggal, dibentuk, ptiik]</td>\n",
       "      <td>[oktober]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[sasaran, pengabdian, filkom]</td>\n",
       "      <td>[meningkatkan, kualitas, dan, kuantitas, penga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[sasaran, kerjasama, filkom]</td>\n",
       "      <td>[mengadakan, kerjasama, pendidikan, penlitian,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[dekan, fakultas, ilmu, komputer, filkom]</td>\n",
       "      <td>[prof, ir, wayan, firdaus, mahmudy, ssi, mt, phd]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[wakil, dekan, bidang, akademik, wakil, dekan]</td>\n",
       "      <td>[dr, eng, ir, herman, tolle, st, mt]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[wakil, dekan, bidang, umum, keuangan, dan, su...</td>\n",
       "      <td>[agus, wahyu, widodo, st, mcs]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[wakil, dekan, bidang, kemahasiswaan, alumni, ...</td>\n",
       "      <td>[drs, muh, arif, rahman, mkom]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[ketua, departemen, teknik, informatika]</td>\n",
       "      <td>[achmad, basuki, st, mmg, phd]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[sekretaris, departemen, teknik, informatika]</td>\n",
       "      <td>[ir, primantara, hari, trisnawan, msc]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[ketua, program, studi, magister, ilmu, komputer]</td>\n",
       "      <td>[sabriansyah, rizqika, akbar, st, meng, phd]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>[ketua, program, studi, sarjana, teknik, infor...</td>\n",
       "      <td>[adhitya, bhawiyuga, skom, msc]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>[ketua, program, studi, sarjana, teknik, kompu...</td>\n",
       "      <td>[barlian, henryranu, prasetio, st, mt, phd]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>[ketua, departemen, sistem, informasi]</td>\n",
       "      <td>[issa, arwani, skom, msc]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>[seketaris, departemen, sistem, informasi]</td>\n",
       "      <td>[satrio, agung, wicaksono, skom, mkom]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>[ketua, program, studi, sarjana, sistem, infor...</td>\n",
       "      <td>[yusi, tyroni, mursityo, skom, ms]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>[ketua, program, studi, sarjana, pendidikan, t...</td>\n",
       "      <td>[ir, admaja, dwi, herlambang, spd, mpd]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[ketua, program, studi, sarjana, teknologi, in...</td>\n",
       "      <td>[ir, widhy, hayuhardhika, nugraha, putra, skom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>[berikan, saya, informasi, alumni]</td>\n",
       "      <td>[informasi, alumni, dapat, diakses, pada, link...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>[apa, saja, layanan, kemahasiswaan, filkom, ub]</td>\n",
       "      <td>[pengajuan, proposal, dan, lpj, kegiatan, kema...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>[bagaimana, pengajuan, proposal, dan, lpj, keg...</td>\n",
       "      <td>[pengajuan, proposal, kegiatan, kemahasiswaan,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>[berikan, informasi, dokumen, kemahasiswaan]</td>\n",
       "      <td>[informasi, dokumen, kemahasiswaan, dapat, dil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>[bagaimana, pengajuan, surat, tugas, dosen, pe...</td>\n",
       "      <td>[informasi, pengajuan, surat, tugas, dosen, pe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>[bagaimana, permohonan, validasi, data, skm]</td>\n",
       "      <td>[permohonan, validasi, data, skm, dapat, dilih...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>[berikan, informasi, mengenai, validasi, syara...</td>\n",
       "      <td>[unggah, dokumen, di, siam, mengisi, gform, pe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>[bagaimana, mengakses, tracer, study, fakultas]</td>\n",
       "      <td>[tracer, study, fakultas, dapat, diakses, pada...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>[bagaimana, cara, pendaftaran, wisuda, ulang]</td>\n",
       "      <td>[pendaftaran, wisuda, ulang, dapat, diakses, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>[berikan, informasi, mengenai, layanan, bimbin...</td>\n",
       "      <td>[layanan, bimbingan, dan, konseling, dapat, di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>[berikan, informasi, mengenai, layanan, ultksp...</td>\n",
       "      <td>[layanan, ultksp, dapat, diaksespada, tautan, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>[berikan, informasi, mengenai, tracking, layan...</td>\n",
       "      <td>[tracking, layanan, kemahasiswaan, dapat, diak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>[berikan, informasi, mengenai, bimbingan, dan,...</td>\n",
       "      <td>[dalam, perjalanannya, menuntut, ilmu, mahasis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>[apa, tujuan, unit, konseling]</td>\n",
       "      <td>[mewujudkan, potensi, dirinya, secara, optimal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>[apa, fungsi, bimbingan, dan, konseling, serta...</td>\n",
       "      <td>[penyaluran, bimbingan, berfungsi, dalam, memb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>[apa, saja, program, layanan, unit, konseling]</td>\n",
       "      <td>[pelayanan, bantuan, pemecahan, masalah, baik,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>[apa, manfaat, konseling, filkom]</td>\n",
       "      <td>[masalah, ditangani, oleh, ahli, yang, kompete...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>[berikan, informasi, mengenai, layanan, konsel...</td>\n",
       "      <td>[informasi, mengenai, layanan, konseling, dapa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>[siapa, konselor, bimbingan, dan, konseling, d...</td>\n",
       "      <td>[ada, konselor, bimbingan, dan, konseling, di,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>[siapa, koordinator, konselor, sebaya]</td>\n",
       "      <td>[koordinator, konselor, sebaya, adalah, muhamm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>[berikan, rincian, layanan, ultksp]</td>\n",
       "      <td>[rincian, layanan, ultksp, dapat, diakses, pad...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             question  \\\n",
       "0                                      [visi, filkom]   \n",
       "1                                      [misi, filkom]   \n",
       "2                               [apa, tujuan, filkom]   \n",
       "3                       [sasaran, pendidikan, filkom]   \n",
       "4                         [email, fitra, a, bachtiar]   \n",
       "5            [bidang, penelitian, fitra, a, bachtiar]   \n",
       "6                          [tanggal, dibentuk, ptiik]   \n",
       "7                       [sasaran, pengabdian, filkom]   \n",
       "8                        [sasaran, kerjasama, filkom]   \n",
       "9           [dekan, fakultas, ilmu, komputer, filkom]   \n",
       "10     [wakil, dekan, bidang, akademik, wakil, dekan]   \n",
       "11  [wakil, dekan, bidang, umum, keuangan, dan, su...   \n",
       "12  [wakil, dekan, bidang, kemahasiswaan, alumni, ...   \n",
       "13           [ketua, departemen, teknik, informatika]   \n",
       "14      [sekretaris, departemen, teknik, informatika]   \n",
       "15  [ketua, program, studi, magister, ilmu, komputer]   \n",
       "16  [ketua, program, studi, sarjana, teknik, infor...   \n",
       "17  [ketua, program, studi, sarjana, teknik, kompu...   \n",
       "18             [ketua, departemen, sistem, informasi]   \n",
       "19         [seketaris, departemen, sistem, informasi]   \n",
       "20  [ketua, program, studi, sarjana, sistem, infor...   \n",
       "21  [ketua, program, studi, sarjana, pendidikan, t...   \n",
       "22  [ketua, program, studi, sarjana, teknologi, in...   \n",
       "23                 [berikan, saya, informasi, alumni]   \n",
       "24    [apa, saja, layanan, kemahasiswaan, filkom, ub]   \n",
       "25  [bagaimana, pengajuan, proposal, dan, lpj, keg...   \n",
       "26       [berikan, informasi, dokumen, kemahasiswaan]   \n",
       "27  [bagaimana, pengajuan, surat, tugas, dosen, pe...   \n",
       "28       [bagaimana, permohonan, validasi, data, skm]   \n",
       "29  [berikan, informasi, mengenai, validasi, syara...   \n",
       "30    [bagaimana, mengakses, tracer, study, fakultas]   \n",
       "31      [bagaimana, cara, pendaftaran, wisuda, ulang]   \n",
       "32  [berikan, informasi, mengenai, layanan, bimbin...   \n",
       "33  [berikan, informasi, mengenai, layanan, ultksp...   \n",
       "34  [berikan, informasi, mengenai, tracking, layan...   \n",
       "35  [berikan, informasi, mengenai, bimbingan, dan,...   \n",
       "36                     [apa, tujuan, unit, konseling]   \n",
       "37  [apa, fungsi, bimbingan, dan, konseling, serta...   \n",
       "38     [apa, saja, program, layanan, unit, konseling]   \n",
       "39                  [apa, manfaat, konseling, filkom]   \n",
       "40  [berikan, informasi, mengenai, layanan, konsel...   \n",
       "41  [siapa, konselor, bimbingan, dan, konseling, d...   \n",
       "42             [siapa, koordinator, konselor, sebaya]   \n",
       "43                [berikan, rincian, layanan, ultksp]   \n",
       "\n",
       "                                               answer  \n",
       "0   [menjadi, fakultas, yang, berdaya, saing, inte...  \n",
       "1   [menyelenggarakan, pendidikan, di, bidang, tek...  \n",
       "2   [menghasilkan, lulusan, yang, kompeten, profes...  \n",
       "3   [meningkatkan, kompetensi, dan, kualifikasi, p...  \n",
       "4                             [fitrabachtiaratubacid]  \n",
       "5   [affective, computing, affective, engineering,...  \n",
       "6                                           [oktober]  \n",
       "7   [meningkatkan, kualitas, dan, kuantitas, penga...  \n",
       "8   [mengadakan, kerjasama, pendidikan, penlitian,...  \n",
       "9   [prof, ir, wayan, firdaus, mahmudy, ssi, mt, phd]  \n",
       "10               [dr, eng, ir, herman, tolle, st, mt]  \n",
       "11                     [agus, wahyu, widodo, st, mcs]  \n",
       "12                     [drs, muh, arif, rahman, mkom]  \n",
       "13                     [achmad, basuki, st, mmg, phd]  \n",
       "14             [ir, primantara, hari, trisnawan, msc]  \n",
       "15       [sabriansyah, rizqika, akbar, st, meng, phd]  \n",
       "16                    [adhitya, bhawiyuga, skom, msc]  \n",
       "17        [barlian, henryranu, prasetio, st, mt, phd]  \n",
       "18                          [issa, arwani, skom, msc]  \n",
       "19             [satrio, agung, wicaksono, skom, mkom]  \n",
       "20                 [yusi, tyroni, mursityo, skom, ms]  \n",
       "21            [ir, admaja, dwi, herlambang, spd, mpd]  \n",
       "22  [ir, widhy, hayuhardhika, nugraha, putra, skom...  \n",
       "23  [informasi, alumni, dapat, diakses, pada, link...  \n",
       "24  [pengajuan, proposal, dan, lpj, kegiatan, kema...  \n",
       "25  [pengajuan, proposal, kegiatan, kemahasiswaan,...  \n",
       "26  [informasi, dokumen, kemahasiswaan, dapat, dil...  \n",
       "27  [informasi, pengajuan, surat, tugas, dosen, pe...  \n",
       "28  [permohonan, validasi, data, skm, dapat, dilih...  \n",
       "29  [unggah, dokumen, di, siam, mengisi, gform, pe...  \n",
       "30  [tracer, study, fakultas, dapat, diakses, pada...  \n",
       "31  [pendaftaran, wisuda, ulang, dapat, diakses, p...  \n",
       "32  [layanan, bimbingan, dan, konseling, dapat, di...  \n",
       "33  [layanan, ultksp, dapat, diaksespada, tautan, ...  \n",
       "34  [tracking, layanan, kemahasiswaan, dapat, diak...  \n",
       "35  [dalam, perjalanannya, menuntut, ilmu, mahasis...  \n",
       "36  [mewujudkan, potensi, dirinya, secara, optimal...  \n",
       "37  [penyaluran, bimbingan, berfungsi, dalam, memb...  \n",
       "38  [pelayanan, bantuan, pemecahan, masalah, baik,...  \n",
       "39  [masalah, ditangani, oleh, ahli, yang, kompete...  \n",
       "40  [informasi, mengenai, layanan, konseling, dapa...  \n",
       "41  [ada, konselor, bimbingan, dan, konseling, di,...  \n",
       "42  [koordinator, konselor, sebaya, adalah, muhamm...  \n",
       "43  [rincian, layanan, ultksp, dapat, diakses, pad...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs_df = pd.DataFrame(pairs, columns=['question', 'answer'])\n",
    "pairs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T20:17:32.828565Z",
     "iopub.status.busy": "2024-07-02T20:17:32.828255Z",
     "iopub.status.idle": "2024-07-02T20:17:32.838911Z",
     "shell.execute_reply": "2024-07-02T20:17:32.837848Z",
     "shell.execute_reply.started": "2024-07-02T20:17:32.828539Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "word_freq = Counter()\n",
    "for pair in pairs:\n",
    "    word_freq.update(pair[0])\n",
    "    word_freq.update(pair[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T20:17:32.840576Z",
     "iopub.status.busy": "2024-07-02T20:17:32.840244Z",
     "iopub.status.idle": "2024-07-02T20:17:32.850265Z",
     "shell.execute_reply": "2024-07-02T20:17:32.849352Z",
     "shell.execute_reply.started": "2024-07-02T20:17:32.840548Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "min_word_freq = 0\n",
    "words = [w for w in word_freq.keys() if word_freq[w] > min_word_freq]\n",
    "word_map = {k: v + 1 for v, k in enumerate(words)}\n",
    "word_map['<unk>'] = len(word_map) + 1\n",
    "word_map['<start>'] = len(word_map) + 1\n",
    "word_map['<end>'] = len(word_map) + 1\n",
    "word_map['<pad>'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T20:17:32.851802Z",
     "iopub.status.busy": "2024-07-02T20:17:32.851465Z",
     "iopub.status.idle": "2024-07-02T20:17:32.860482Z",
     "shell.execute_reply": "2024-07-02T20:17:32.859625Z",
     "shell.execute_reply.started": "2024-07-02T20:17:32.851770Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words are: 440\n"
     ]
    }
   ],
   "source": [
    "print(\"Total words are: {}\".format(len(word_map)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T20:17:32.861919Z",
     "iopub.status.busy": "2024-07-02T20:17:32.861579Z",
     "iopub.status.idle": "2024-07-02T20:17:32.872138Z",
     "shell.execute_reply": "2024-07-02T20:17:32.871399Z",
     "shell.execute_reply.started": "2024-07-02T20:17:32.861895Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "with open('WORDMAP_corpus_KBFILKOM.json', 'w') as j:\n",
    "    json.dump(word_map, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T20:17:32.877074Z",
     "iopub.status.busy": "2024-07-02T20:17:32.876749Z",
     "iopub.status.idle": "2024-07-02T20:17:32.884325Z",
     "shell.execute_reply": "2024-07-02T20:17:32.883430Z",
     "shell.execute_reply.started": "2024-07-02T20:17:32.877048Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def encode_question(words, word_map):\n",
    "    enc_c = [word_map.get(word, word_map['<unk>']) for word in words] + [word_map['<pad>']] * (max_len - len(words))\n",
    "    return enc_c\n",
    "\n",
    "\n",
    "def encode_question_left(words, word_map):\n",
    "    enc_c = [word_map['<pad>']] * (max_len - len(words)) + [word_map.get(word, word_map['<unk>']) for word in words]\n",
    "    return enc_c\n",
    "\n",
    "def encode_reply(words, word_map):\n",
    "    enc_c = [word_map['<start>']] + [word_map.get(word, word_map['<unk>']) for word in words] + \\\n",
    "    [word_map['<end>']] + [word_map['<pad>']] * (max_len - len(words))\n",
    "    return enc_c\n",
    "\n",
    "def encode_reply_with_maxlen(words, word_map):\n",
    "    enc_c = [word_map['<start>']] + [word_map.get(word, word_map['<unk>']) for word in words] + [word_map['<end>']] + [word_map['<pad>']] * (max_len-2 - len(words))\n",
    "    return enc_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T20:17:32.885632Z",
     "iopub.status.busy": "2024-07-02T20:17:32.885341Z",
     "iopub.status.idle": "2024-07-02T20:17:33.099456Z",
     "shell.execute_reply": "2024-07-02T20:17:33.098486Z",
     "shell.execute_reply.started": "2024-07-02T20:17:32.885608Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pairs_encoded = []\n",
    "for pair in pairs:\n",
    "    qus = encode_question(pair[0], word_map)\n",
    "    # qus = encode_question_left(pair[0], word_map)\n",
    "    ans = encode_reply(pair[1], word_map)\n",
    "    pairs_encoded.append([qus, ans])\n",
    "\n",
    "with open('pairs_encoded_kbfilkom.json', 'w') as p:\n",
    "    json.dump(pairs_encoded, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T20:17:33.318791Z",
     "iopub.status.busy": "2024-07-02T20:17:33.318483Z",
     "iopub.status.idle": "2024-07-02T20:17:33.325793Z",
     "shell.execute_reply": "2024-07-02T20:17:33.325035Z",
     "shell.execute_reply.started": "2024-07-02T20:17:33.318756Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.pairs = json.load(open('pairs_encoded_kbfilkom.json'))\n",
    "        self.dataset_size = len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        \n",
    "        question = torch.LongTensor(self.pairs[i][0])\n",
    "        reply = torch.LongTensor(self.pairs[i][1])\n",
    "            \n",
    "        return question, reply\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T20:17:33.337842Z",
     "iopub.status.busy": "2024-07-02T20:17:33.337481Z",
     "iopub.status.idle": "2024-07-02T20:17:33.379910Z",
     "shell.execute_reply": "2024-07-02T20:17:33.378865Z",
     "shell.execute_reply.started": "2024-07-02T20:17:33.337813Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(Dataset(),\n",
    "                                           batch_size = 100, \n",
    "                                           shuffle=True, \n",
    "                                           pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T20:17:33.381467Z",
     "iopub.status.busy": "2024-07-02T20:17:33.381114Z",
     "iopub.status.idle": "2024-07-02T20:17:33.388963Z",
     "shell.execute_reply": "2024-07-02T20:17:33.388016Z",
     "shell.execute_reply.started": "2024-07-02T20:17:33.381434Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def create_masks(question, reply_input, reply_target):\n",
    "    \n",
    "    def subsequent_mask(size):\n",
    "        mask = torch.triu(torch.ones(size, size)).transpose(0, 1).type(dtype=torch.uint8)\n",
    "        return mask.unsqueeze(0)\n",
    "    \n",
    "    question_mask = question!=0\n",
    "    question_mask = question_mask.to(device)\n",
    "    question_mask = question_mask.unsqueeze(1).unsqueeze(1)         # (batch_size, 1, 1, max_words)\n",
    "     \n",
    "    reply_input_mask = reply_input!=0\n",
    "    reply_input_mask = reply_input_mask.unsqueeze(1)  # (batch_size, 1, max_words)\n",
    "    reply_input_mask = reply_input_mask & subsequent_mask(reply_input.size(-1)).type_as(reply_input_mask.data) \n",
    "    reply_input_mask = reply_input_mask.unsqueeze(1) # (batch_size, 1, max_words, max_words)\n",
    "    reply_target_mask = reply_target!=0              # (batch_size, max_words)\n",
    "    \n",
    "    return question_mask, reply_input_mask, reply_target_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T20:17:33.390611Z",
     "iopub.status.busy": "2024-07-02T20:17:33.390255Z",
     "iopub.status.idle": "2024-07-02T20:17:33.397673Z",
     "shell.execute_reply": "2024-07-02T20:17:33.396839Z",
     "shell.execute_reply.started": "2024-07-02T20:17:33.390578Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def create_directory(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "        print(f\"Directory created at {path}\")\n",
    "    else:\n",
    "        print(f\"Directory already exists at {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T20:17:33.399270Z",
     "iopub.status.busy": "2024-07-02T20:17:33.398914Z",
     "iopub.status.idle": "2024-07-02T20:17:33.421634Z",
     "shell.execute_reply": "2024-07-02T20:17:33.420718Z",
     "shell.execute_reply.started": "2024-07-02T20:17:33.399239Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements embeddings of the words and adds their positional encodings. \n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, d_model, max_len = 50, num_layers = 6):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.pe = self.create_positinal_encoding(max_len, self.d_model)     # (1, max_len, d_model)\n",
    "        self.te = self.create_positinal_encoding(num_layers, self.d_model)  # (1, num_layers, d_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def create_positinal_encoding(self, max_len, d_model):\n",
    "        pe = torch.zeros(max_len, d_model).to(device)\n",
    "        for pos in range(max_len):   # for each position of the word\n",
    "            for i in range(0, d_model, 2):   # for each dimension of the each position\n",
    "                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
    "                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n",
    "        pe = pe.unsqueeze(0)   # include the batch size\n",
    "        return pe\n",
    "        \n",
    "    def forward(self, embedding, layer_idx):\n",
    "        if layer_idx == 0:\n",
    "            embedding = self.embed(embedding) * math.sqrt(self.d_model)\n",
    "        embedding += self.pe[:, :embedding.size(1)]   # pe will automatically be expanded with the same batch size as encoded_words\n",
    "        # embedding: (batch_size, max_len, d_model), te: (batch_size, 1, d_model)\n",
    "        embedding += self.te[:, layer_idx, :].unsqueeze(1).repeat(1, embedding.size(1), 1)\n",
    "        embedding = self.dropout(embedding)\n",
    "        return embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T20:17:33.423711Z",
     "iopub.status.busy": "2024-07-02T20:17:33.423056Z",
     "iopub.status.idle": "2024-07-02T20:17:33.435766Z",
     "shell.execute_reply": "2024-07-02T20:17:33.435028Z",
     "shell.execute_reply.started": "2024-07-02T20:17:33.423675Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, heads, d_model):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % heads == 0\n",
    "        self.d_k = d_model // heads\n",
    "        self.heads = heads\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.query = nn.Linear(d_model, d_model)\n",
    "        self.key = nn.Linear(d_model, d_model)\n",
    "        self.value = nn.Linear(d_model, d_model)\n",
    "        self.concat = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, query, key, value, mask):\n",
    "        \"\"\"\n",
    "        query, key, value of shape: (batch_size, max_len, 512)\n",
    "        mask of shape: (batch_size, 1, 1, max_words)\n",
    "        \"\"\"\n",
    "        # (batch_size, max_len, 512)\n",
    "        query = self.query(query)\n",
    "        key = self.key(key)        \n",
    "        value = self.value(value)   \n",
    "        \n",
    "        # (batch_size, max_len, 512) --> (batch_size, max_len, h, d_k) --> (batch_size, h, max_len, d_k)\n",
    "        query = query.view(query.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)   \n",
    "        key = key.view(key.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)  \n",
    "        value = value.view(value.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)  \n",
    "        \n",
    "        # (batch_size, h, max_len, d_k) matmul (batch_size, h, d_k, max_len) --> (batch_size, h, max_len, max_len)\n",
    "        scores = torch.matmul(query, key.permute(0,1,3,2)) / math.sqrt(query.size(-1))\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)    # (batch_size, h, max_len, max_len)\n",
    "        weights = F.softmax(scores, dim = -1)           # (batch_size, h, max_len, max_len)\n",
    "        weights = self.dropout(weights)\n",
    "        # (batch_size, h, max_len, max_len) matmul (batch_size, h, max_len, d_k) --> (batch_size, h, max_len, d_k)\n",
    "        context = torch.matmul(weights, value)\n",
    "        # (batch_size, h, max_len, d_k) --> (batch_size, max_len, h, d_k) --> (batch_size, max_len, h * d_k)\n",
    "        context = context.permute(0,2,1,3).contiguous().view(context.shape[0], -1, self.heads * self.d_k)\n",
    "        # (batch_size, max_len, h * d_k)\n",
    "        interacted = self.concat(context)\n",
    "        return interacted "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed Forward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T20:17:33.437331Z",
     "iopub.status.busy": "2024-07-02T20:17:33.437039Z",
     "iopub.status.idle": "2024-07-02T20:17:33.450588Z",
     "shell.execute_reply": "2024-07-02T20:17:33.449684Z",
     "shell.execute_reply.started": "2024-07-02T20:17:33.437306Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, middle_dim = 2048):\n",
    "        super(FeedForward, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(d_model, middle_dim)\n",
    "        self.fc2 = nn.Linear(middle_dim, d_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.fc1(x))\n",
    "        out = self.fc2(self.dropout(out))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T20:17:33.452046Z",
     "iopub.status.busy": "2024-07-02T20:17:33.451709Z",
     "iopub.status.idle": "2024-07-02T20:17:33.461741Z",
     "shell.execute_reply": "2024-07-02T20:17:33.460847Z",
     "shell.execute_reply.started": "2024-07-02T20:17:33.452019Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, heads):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.layernorm = nn.LayerNorm(d_model)\n",
    "        self.self_multihead = MultiHeadAttention(heads, d_model)\n",
    "        self.feed_forward = FeedForward(d_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, embeddings, mask):\n",
    "        interacted = self.dropout(self.self_multihead(embeddings, embeddings, embeddings, mask))\n",
    "        interacted = self.layernorm(interacted + embeddings)\n",
    "        feed_forward_out = self.dropout(self.feed_forward(interacted))\n",
    "        encoded = self.layernorm(feed_forward_out + interacted)\n",
    "        return encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T20:17:33.463365Z",
     "iopub.status.busy": "2024-07-02T20:17:33.463025Z",
     "iopub.status.idle": "2024-07-02T20:17:33.475859Z",
     "shell.execute_reply": "2024-07-02T20:17:33.475008Z",
     "shell.execute_reply.started": "2024-07-02T20:17:33.463339Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, heads):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.layernorm = nn.LayerNorm(d_model)\n",
    "        self.self_multihead = MultiHeadAttention(heads, d_model)\n",
    "        self.src_multihead = MultiHeadAttention(heads, d_model)\n",
    "        self.feed_forward = FeedForward(d_model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, embeddings, encoded, src_mask, target_mask):\n",
    "        query = self.dropout(self.self_multihead(embeddings, embeddings, embeddings, target_mask))\n",
    "        query = self.layernorm(query + embeddings)\n",
    "        interacted = self.dropout(self.src_multihead(query, encoded, encoded, src_mask))\n",
    "        interacted = self.layernorm(interacted + query)\n",
    "        feed_forward_out = self.dropout(self.feed_forward(interacted))\n",
    "        decoded = self.layernorm(feed_forward_out + interacted)\n",
    "        return decoded\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T20:17:33.478079Z",
     "iopub.status.busy": "2024-07-02T20:17:33.477782Z",
     "iopub.status.idle": "2024-07-02T20:17:33.514932Z",
     "shell.execute_reply": "2024-07-02T20:17:33.514023Z",
     "shell.execute_reply.started": "2024-07-02T20:17:33.478055Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, heads, num_layers_enc, num_layers_dec, word_map, max_len = 50):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_layers_enc = num_layers_enc\n",
    "        self.num_layers_dec = num_layers_dec\n",
    "        self.vocab_size = len(word_map)\n",
    "        self.embed_enc = Embeddings(self.vocab_size, d_model, num_layers = num_layers_enc, max_len = max_len)\n",
    "        self.embed_dec = Embeddings(self.vocab_size, d_model, num_layers = num_layers_dec, max_len = max_len)\n",
    "        self.encoder = EncoderLayer(d_model, heads) \n",
    "        self.decoder = DecoderLayer(d_model, heads)\n",
    "        self.logit = nn.Linear(d_model, self.vocab_size)\n",
    "        \n",
    "    def encode(self, src_embeddings, src_mask):\n",
    "        for i in range(self.num_layers_enc):\n",
    "            src_embeddings = self.embed_enc(src_embeddings, i)\n",
    "            src_embeddings = self.encoder(src_embeddings, src_mask)\n",
    "        return src_embeddings\n",
    "    \n",
    "    def decode(self, tgt_embeddings, target_mask, src_embeddings, src_mask):\n",
    "        for i in range(self.num_layers_enc):\n",
    "            tgt_embeddings = self.embed_enc(tgt_embeddings, i)\n",
    "            tgt_embeddings = self.decoder(tgt_embeddings, src_embeddings, src_mask, target_mask)\n",
    "        return tgt_embeddings\n",
    "        \n",
    "    def forward(self, src_words, src_mask, target_words, target_mask):\n",
    "        encoded = self.encode(src_words, src_mask)\n",
    "        print(\"size of encoded\", encoded.size())\n",
    "        decoded = self.decode(target_words, target_mask, encoded, src_mask)\n",
    "        out = F.log_softmax(self.logit(decoded), dim = 2)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T20:17:33.516425Z",
     "iopub.status.busy": "2024-07-02T20:17:33.516094Z",
     "iopub.status.idle": "2024-07-02T20:17:33.527522Z",
     "shell.execute_reply": "2024-07-02T20:17:33.526734Z",
     "shell.execute_reply.started": "2024-07-02T20:17:33.516394Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class AdamWarmup:\n",
    "    \n",
    "    def __init__(self, model_size, warmup_steps, optimizer):\n",
    "        \n",
    "        self.model_size = model_size\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.optimizer = optimizer\n",
    "        self.current_step = 0\n",
    "        self.lr = 0\n",
    "        \n",
    "    def get_lr(self):\n",
    "        return self.model_size ** (-0.5) * min(self.current_step ** (-0.5), self.current_step * self.warmup_steps ** (-1.5))\n",
    "        \n",
    "    def step(self):\n",
    "        # Increment the number of steps each time we call the step function\n",
    "        self.current_step += 1\n",
    "        lr = self.get_lr()\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        # update the learning rate\n",
    "        self.lr = lr\n",
    "        self.optimizer.step()       \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T20:17:33.529363Z",
     "iopub.status.busy": "2024-07-02T20:17:33.528622Z",
     "iopub.status.idle": "2024-07-02T20:17:33.540886Z",
     "shell.execute_reply": "2024-07-02T20:17:33.540153Z",
     "shell.execute_reply.started": "2024-07-02T20:17:33.529331Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class LossWithLS(nn.Module):\n",
    "\n",
    "    def __init__(self, size, smooth):\n",
    "        super(LossWithLS, self).__init__()\n",
    "        self.criterion = nn.KLDivLoss(size_average=False, reduce=False)\n",
    "        self.confidence = 1.0 - smooth\n",
    "        self.smooth = smooth\n",
    "        self.size = size\n",
    "        \n",
    "    def forward(self, prediction, target, mask):\n",
    "        \"\"\"\n",
    "        prediction of shape: (batch_size, max_words, vocab_size)\n",
    "        target and mask of shape: (batch_size, max_words)\n",
    "        \"\"\"\n",
    "        prediction = prediction.view(-1, prediction.size(-1))   # (batch_size * max_words, vocab_size)\n",
    "        target = target.contiguous().view(-1)   # (batch_size * max_words)\n",
    "        mask = mask.float()\n",
    "        mask = mask.view(-1)       # (batch_size * max_words)\n",
    "        labels = prediction.data.clone()\n",
    "        labels.fill_(self.smooth / (self.size - 1))\n",
    "        labels.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        loss = self.criterion(prediction, labels)    # (batch_size * max_words, vocab_size)\n",
    "        loss = (loss.sum(1) * mask).sum() / mask.sum()\n",
    "        return loss\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Neptune Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T20:17:33.542496Z",
     "iopub.status.busy": "2024-07-02T20:17:33.542228Z",
     "iopub.status.idle": "2024-07-02T20:17:33.550662Z",
     "shell.execute_reply": "2024-07-02T20:17:33.549840Z",
     "shell.execute_reply.started": "2024-07-02T20:17:33.542473Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "project = \"andialifs/fluent-tesis-playground-24\"\n",
    "api_token = \"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJjZTY2YWQ3My04OTBkLTQ2OWUtYTc1Ni1jYjk0MGZhMWFiNGEifQ==\"\n",
    "\n",
    "def neptune_init(name=\"cobain\"):\n",
    "    run = neptune.init_run(\n",
    "        project=project,\n",
    "        api_token=api_token,\n",
    "        name=name\n",
    "    )\n",
    "    return run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "def calculate_bleu(preds, questions, answers):\n",
    "    bleu_score_1 = 0\n",
    "    bleu_score_2 = 0\n",
    "    bleu_score_3 = 0\n",
    "    bleu_score_4 = 0\n",
    "    bleu_score_all = 0\n",
    "\n",
    "    num_of_rows_calculated = 0\n",
    "\n",
    "    for i, (question, real_answer) in enumerate(zip(questions, answers)):\n",
    "        # print(f\"Question: {question}\")\n",
    "        # print(f\"Real Answer: {real_answer}\")\n",
    "        # print(f\"Predicted Answer: {preds[i]}\")\n",
    "        try:\n",
    "            refs = [real_answer.split(' ')]\n",
    "            hyp = preds[i].split(' ')\n",
    "\n",
    "            bleu_score_1 += sentence_bleu(refs, hyp, weights=(1,0,0,0))\n",
    "            bleu_score_2 += sentence_bleu(refs, hyp, weights=(0,1,0,0))\n",
    "            bleu_score_3 += sentence_bleu(refs, hyp, weights=(0,0,1,0))\n",
    "            bleu_score_4 += sentence_bleu(refs, hyp, weights=(0,0,0,1))\n",
    "            bleu_score_all += sentence_bleu(refs, hyp, weights=(.25,.25,.25,.25))\n",
    "\n",
    "            num_of_rows_calculated+=1\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    results = {\"1-gram\": (bleu_score_1/num_of_rows_calculated),\n",
    "                \"2-gram\": (bleu_score_2/num_of_rows_calculated),\n",
    "                \"3-gram\": (bleu_score_3/num_of_rows_calculated),\n",
    "                \"4-gram\": (bleu_score_all/num_of_rows_calculated)}\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T20:17:33.564996Z",
     "iopub.status.busy": "2024-07-02T20:17:33.564656Z",
     "iopub.status.idle": "2024-07-02T20:17:33.574590Z",
     "shell.execute_reply": "2024-07-02T20:17:33.573653Z",
     "shell.execute_reply.started": "2024-07-02T20:17:33.564947Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train(train_loader, transformer, criterion, epoch):\n",
    "    transformer.train()\n",
    "    sum_loss = 0\n",
    "    count = 0\n",
    "\n",
    "    for i, (question, reply) in enumerate(train_loader):\n",
    "        \n",
    "        samples = question.shape[0]\n",
    "\n",
    "        # Move to device\n",
    "        question = question.to(device)\n",
    "        reply = reply.to(device)\n",
    "\n",
    "        # Prepare Target Data\n",
    "        reply_input = reply[:, :-1]\n",
    "        reply_target = reply[:, 1:]\n",
    "\n",
    "        # Create mask and add dimensions\n",
    "        question_mask, reply_input_mask, reply_target_mask = create_masks(question, reply_input, reply_target)\n",
    "\n",
    "        # Get the transformer outputs\n",
    "        out = transformer(question, question_mask, reply_input, reply_input_mask)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(out, reply_target, reply_target_mask)\n",
    "        \n",
    "        # Backprop\n",
    "        transformer_optimizer.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        transformer_optimizer.step()\n",
    "        \n",
    "        sum_loss += loss.item() * samples\n",
    "        count += samples\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print(\"Epoch [{}][{}/{}]\\tLoss: {:.3f}\".format(epoch, i, len(train_loader), sum_loss/count))\n",
    "    \n",
    "    return sum_loss/count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T20:17:33.576127Z",
     "iopub.status.busy": "2024-07-02T20:17:33.575799Z",
     "iopub.status.idle": "2024-07-02T20:17:33.587390Z",
     "shell.execute_reply": "2024-07-02T20:17:33.586545Z",
     "shell.execute_reply.started": "2024-07-02T20:17:33.576103Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate(transformer, question, question_mask, max_len, word_map):\n",
    "    \"\"\"\n",
    "    Performs Greedy Decoding with a batch size of 1\n",
    "    \"\"\"\n",
    "    rev_word_map = {v: k for k, v in word_map.items()}\n",
    "    transformer.eval()\n",
    "    start_token = word_map['<start>']\n",
    "    encoded = transformer.encode(question, question_mask)\n",
    "    words = torch.LongTensor([[start_token]]).to(device)\n",
    "    \n",
    "    for step in range(max_len - 1):\n",
    "        size = words.shape[1]\n",
    "        target_mask = torch.triu(torch.ones(size, size)).transpose(0, 1).type(dtype=torch.uint8)\n",
    "        target_mask = target_mask.to(device).unsqueeze(0).unsqueeze(0)\n",
    "        decoded = transformer.decode(words, target_mask, encoded, question_mask)\n",
    "        predictions = transformer.logit(decoded[:, -1])\n",
    "        _, next_word = torch.max(predictions, dim = 1)\n",
    "        next_word = next_word.item()\n",
    "        if next_word == word_map['<end>']:\n",
    "            break\n",
    "        words = torch.cat([words, torch.LongTensor([[next_word]]).to(device)], dim = 1)   # (1,step+2)\n",
    "        \n",
    "    # Construct Sentence\n",
    "    if words.dim() == 2:\n",
    "        words = words.squeeze(0)\n",
    "        words = words.tolist()\n",
    "        \n",
    "    sen_idx = [w for w in words if w not in {word_map['<start>']}]\n",
    "    sentence = ' '.join([rev_word_map[sen_idx[k]] for k in range(len(sen_idx))])\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers without reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T20:17:33.588788Z",
     "iopub.status.busy": "2024-07-02T20:17:33.588528Z",
     "iopub.status.idle": "2024-07-02T20:17:47.395240Z",
     "shell.execute_reply": "2024-07-02T20:17:47.393722Z",
     "shell.execute_reply.started": "2024-07-02T20:17:33.588766Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory already exists at experiment_siet24_288\n",
      "Skipping experiment 0\n",
      "Skipping experiment 1\n",
      "Skipping experiment 2\n",
      "Skipping experiment 3\n",
      "Skipping experiment 4\n",
      "Skipping experiment 5\n",
      "Skipping experiment 6\n",
      "Skipping experiment 7\n",
      "Skipping experiment 8\n",
      "Skipping experiment 9\n",
      "Skipping experiment 10\n",
      "Skipping experiment 11\n",
      "Skipping experiment 12\n",
      "Skipping experiment 13\n",
      "Skipping experiment 14\n",
      "Skipping experiment 15\n",
      "Skipping experiment 16\n",
      "Skipping experiment 17\n",
      "Skipping experiment 18\n",
      "Skipping experiment 19\n",
      "Skipping experiment 20\n",
      "Skipping experiment 21\n",
      "Skipping experiment 22\n",
      "Skipping experiment 23\n",
      "Skipping experiment 24\n",
      "Skipping experiment 25\n",
      "Skipping experiment 26\n",
      "Skipping experiment 27\n",
      "Skipping experiment 28\n",
      "Skipping experiment 29\n",
      "Skipping experiment 30\n",
      "Skipping experiment 31\n",
      "Skipping experiment 32\n",
      "Skipping experiment 33\n",
      "Skipping experiment 34\n",
      "Skipping experiment 35\n",
      "Skipping experiment 36\n",
      "Skipping experiment 37\n",
      "Skipping experiment 38\n",
      "Skipping experiment 39\n",
      "Skipping experiment 40\n",
      "Skipping experiment 41\n",
      "Skipping experiment 42\n",
      "Skipping experiment 43\n",
      "Skipping experiment 44\n",
      "Skipping experiment 45\n",
      "Skipping experiment 46\n",
      "Skipping experiment 47\n",
      "Skipping experiment 48\n",
      "Skipping experiment 49\n",
      "Skipping experiment 50\n",
      "Skipping experiment 51\n",
      "Skipping experiment 52\n",
      "Skipping experiment 53\n",
      "Skipping experiment 54\n",
      "Skipping experiment 55\n",
      "Skipping experiment 56\n",
      "Skipping experiment 57\n",
      "Skipping experiment 58\n",
      "Skipping experiment 59\n",
      "Skipping experiment 60\n",
      "Skipping experiment 61\n",
      "Skipping experiment 62\n",
      "Skipping experiment 63\n",
      "Skipping experiment 64\n",
      "Skipping experiment 65\n",
      "Skipping experiment 66\n",
      "Skipping experiment 67\n",
      "Skipping experiment 68\n",
      "Skipping experiment 69\n",
      "Skipping experiment 70\n",
      "Skipping experiment 71\n",
      "Skipping experiment 72\n",
      "Skipping experiment 73\n",
      "Skipping experiment 74\n",
      "Skipping experiment 75\n",
      "Skipping experiment 76\n",
      "Skipping experiment 77\n",
      "Skipping experiment 78\n",
      "\n",
      "Running for experiment 79 with d_model 2048, heads4, num_layers_enc 5, num_layers_dec 3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[neptune] [warning] NeptuneWarning: The following monitoring options are disabled by default in interactive sessions: 'capture_stdout', 'capture_stderr', 'capture_traceback', and 'capture_hardware_metrics'. To enable them, set each parameter to 'True' when initializing the run. The monitoring will continue until you call run.stop() or the kernel stops. Also note: Your source files can only be tracked if you pass the path(s) to the 'source_code' argument. For help, see the Neptune docs: https://docs.neptune.ai/logging/source_code/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/andialifs/siet-24/e/SIET-104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][0/12]\tLoss: 5.935\n",
      "Epoch [1][0/12]\tLoss: 5.828\n",
      "Epoch [2][0/12]\tLoss: 5.520\n",
      "Epoch [3][0/12]\tLoss: 5.331\n",
      "Epoch [4][0/12]\tLoss: 5.165\n",
      "Epoch [5][0/12]\tLoss: 4.859\n",
      "Epoch [6][0/12]\tLoss: 4.788\n",
      "Epoch [7][0/12]\tLoss: 4.743\n",
      "Epoch [8][0/12]\tLoss: 4.612\n",
      "Epoch [9][0/12]\tLoss: 4.655\n",
      "Epoch [10][0/12]\tLoss: 4.605\n",
      "Epoch [11][0/12]\tLoss: 4.580\n",
      "Epoch [12][0/12]\tLoss: 4.466\n",
      "Epoch [13][0/12]\tLoss: 4.463\n",
      "Epoch [14][0/12]\tLoss: 4.618\n",
      "Epoch [15][0/12]\tLoss: 4.486\n",
      "Epoch [16][0/12]\tLoss: 4.140\n",
      "Epoch [17][0/12]\tLoss: 4.358\n",
      "Epoch [18][0/12]\tLoss: 4.109\n",
      "Epoch [19][0/12]\tLoss: 4.053\n",
      "Epoch [20][0/12]\tLoss: 3.954\n",
      "Epoch [21][0/12]\tLoss: 3.694\n",
      "Epoch [22][0/12]\tLoss: 3.606\n",
      "Epoch [23][0/12]\tLoss: 3.390\n",
      "Epoch [24][0/12]\tLoss: 3.612\n",
      "Epoch [25][0/12]\tLoss: 3.277\n",
      "Epoch [26][0/12]\tLoss: 2.925\n",
      "Epoch [27][0/12]\tLoss: 3.166\n",
      "Epoch [28][0/12]\tLoss: 3.443\n",
      "Epoch [29][0/12]\tLoss: 3.311\n",
      "Epoch [30][0/12]\tLoss: 2.850\n",
      "Epoch [31][0/12]\tLoss: 2.854\n",
      "Epoch [32][0/12]\tLoss: 2.833\n",
      "Epoch [33][0/12]\tLoss: 2.691\n",
      "Epoch [34][0/12]\tLoss: 2.476\n",
      "Epoch [35][0/12]\tLoss: 2.379\n",
      "Epoch [36][0/12]\tLoss: 2.148\n",
      "Epoch [37][0/12]\tLoss: 2.343\n",
      "Epoch [38][0/12]\tLoss: 2.055\n",
      "Epoch [39][0/12]\tLoss: 2.224\n",
      "Epoch [40][0/12]\tLoss: 2.327\n",
      "Epoch [41][0/12]\tLoss: 2.352\n",
      "Epoch [42][0/12]\tLoss: 1.984\n",
      "Epoch [43][0/12]\tLoss: 1.879\n",
      "Epoch [44][0/12]\tLoss: 2.001\n",
      "Epoch [45][0/12]\tLoss: 1.726\n",
      "Epoch [46][0/12]\tLoss: 1.687\n",
      "Epoch [47][0/12]\tLoss: 1.787\n",
      "Epoch [48][0/12]\tLoss: 1.688\n",
      "Epoch [49][0/12]\tLoss: 1.552\n",
      "Epoch [50][0/12]\tLoss: 1.756\n",
      "Epoch [51][0/12]\tLoss: 1.323\n",
      "Epoch [52][0/12]\tLoss: 1.480\n",
      "Epoch [53][0/12]\tLoss: 1.408\n",
      "Epoch [54][0/12]\tLoss: 1.314\n",
      "Epoch [55][0/12]\tLoss: 1.191\n",
      "Epoch [56][0/12]\tLoss: 1.331\n",
      "Epoch [57][0/12]\tLoss: 1.117\n",
      "Epoch [58][0/12]\tLoss: 0.984\n",
      "Epoch [59][0/12]\tLoss: 1.048\n",
      "Epoch [60][0/12]\tLoss: 1.114\n",
      "Epoch [61][0/12]\tLoss: 0.908\n",
      "Epoch [62][0/12]\tLoss: 0.962\n",
      "Epoch [63][0/12]\tLoss: 0.983\n",
      "Epoch [64][0/12]\tLoss: 0.913\n",
      "Epoch [65][0/12]\tLoss: 0.819\n",
      "Epoch [66][0/12]\tLoss: 0.714\n",
      "Epoch [67][0/12]\tLoss: 0.808\n",
      "Epoch [68][0/12]\tLoss: 0.687\n",
      "Epoch [69][0/12]\tLoss: 0.737\n",
      "Epoch [70][0/12]\tLoss: 0.598\n",
      "Epoch [71][0/12]\tLoss: 0.639\n",
      "Epoch [72][0/12]\tLoss: 0.620\n",
      "Epoch [73][0/12]\tLoss: 0.649\n",
      "Epoch [74][0/12]\tLoss: 0.561\n",
      "Epoch [75][0/12]\tLoss: 0.539\n",
      "Epoch [76][0/12]\tLoss: 0.527\n",
      "Epoch [77][0/12]\tLoss: 0.560\n",
      "Epoch [78][0/12]\tLoss: 0.489\n",
      "Epoch [79][0/12]\tLoss: 0.492\n",
      "Epoch [80][0/12]\tLoss: 0.469\n",
      "Epoch [81][0/12]\tLoss: 0.434\n",
      "Epoch [82][0/12]\tLoss: 0.407\n",
      "Epoch [83][0/12]\tLoss: 0.443\n",
      "Epoch [84][0/12]\tLoss: 0.440\n",
      "Epoch [85][0/12]\tLoss: 0.397\n",
      "Epoch [86][0/12]\tLoss: 0.424\n",
      "Epoch [87][0/12]\tLoss: 0.367\n",
      "Epoch [88][0/12]\tLoss: 0.311\n",
      "Epoch [89][0/12]\tLoss: 0.390\n",
      "Epoch [90][0/12]\tLoss: 0.384\n",
      "Epoch [91][0/12]\tLoss: 0.348\n",
      "Epoch [92][0/12]\tLoss: 0.347\n",
      "Epoch [93][0/12]\tLoss: 0.371\n",
      "Epoch [94][0/12]\tLoss: 0.331\n",
      "Epoch [95][0/12]\tLoss: 0.341\n",
      "Epoch [96][0/12]\tLoss: 0.310\n",
      "Epoch [97][0/12]\tLoss: 0.317\n",
      "Epoch [98][0/12]\tLoss: 0.291\n",
      "Epoch [99][0/12]\tLoss: 0.300\n",
      "Epoch [100][0/12]\tLoss: 0.318\n",
      "Epoch [101][0/12]\tLoss: 0.267\n",
      "Epoch [102][0/12]\tLoss: 0.279\n",
      "Epoch [103][0/12]\tLoss: 0.305\n",
      "Epoch [104][0/12]\tLoss: 0.255\n",
      "Epoch [105][0/12]\tLoss: 0.261\n",
      "Epoch [106][0/12]\tLoss: 0.286\n",
      "Epoch [107][0/12]\tLoss: 0.275\n",
      "Epoch [108][0/12]\tLoss: 0.245\n",
      "Epoch [109][0/12]\tLoss: 0.287\n",
      "Epoch [110][0/12]\tLoss: 0.260\n",
      "Epoch [111][0/12]\tLoss: 0.228\n",
      "Epoch [112][0/12]\tLoss: 0.236\n",
      "Epoch [113][0/12]\tLoss: 0.240\n",
      "Epoch [114][0/12]\tLoss: 0.239\n",
      "Epoch [115][0/12]\tLoss: 0.225\n",
      "Epoch [116][0/12]\tLoss: 0.247\n",
      "Epoch [117][0/12]\tLoss: 0.248\n",
      "Epoch [118][0/12]\tLoss: 0.209\n",
      "Epoch [119][0/12]\tLoss: 0.212\n",
      "Epoch [120][0/12]\tLoss: 0.223\n",
      "Epoch [121][0/12]\tLoss: 0.223\n",
      "Epoch [122][0/12]\tLoss: 0.237\n",
      "Epoch [123][0/12]\tLoss: 0.225\n",
      "Epoch [124][0/12]\tLoss: 0.241\n",
      "Epoch [125][0/12]\tLoss: 0.204\n",
      "Epoch [126][0/12]\tLoss: 0.224\n",
      "Epoch [127][0/12]\tLoss: 0.216\n",
      "Epoch [128][0/12]\tLoss: 0.193\n",
      "Epoch [129][0/12]\tLoss: 0.193\n",
      "Epoch [130][0/12]\tLoss: 0.211\n",
      "Epoch [131][0/12]\tLoss: 0.239\n",
      "Epoch [132][0/12]\tLoss: 0.202\n",
      "Epoch [133][0/12]\tLoss: 0.220\n",
      "Epoch [134][0/12]\tLoss: 0.223\n",
      "Epoch [135][0/12]\tLoss: 0.203\n",
      "Epoch [136][0/12]\tLoss: 0.193\n",
      "Epoch [137][0/12]\tLoss: 0.196\n",
      "Epoch [138][0/12]\tLoss: 0.191\n",
      "Epoch [139][0/12]\tLoss: 0.206\n",
      "Epoch [140][0/12]\tLoss: 0.167\n",
      "Epoch [141][0/12]\tLoss: 0.195\n",
      "Epoch [142][0/12]\tLoss: 0.189\n",
      "Epoch [143][0/12]\tLoss: 0.177\n",
      "Epoch [144][0/12]\tLoss: 0.196\n",
      "Epoch [145][0/12]\tLoss: 0.201\n",
      "Epoch [146][0/12]\tLoss: 0.170\n",
      "Epoch [147][0/12]\tLoss: 0.171\n",
      "Epoch [148][0/12]\tLoss: 0.176\n",
      "Epoch [149][0/12]\tLoss: 0.181\n",
      "[neptune] [info   ] Shutting down background jobs, please wait a moment...\n",
      "[neptune] [info   ] Done!\n",
      "[neptune] [info   ] Waiting for the remaining 7 operations to synchronize with Neptune. Do not kill this process.\n",
      "[neptune] [info   ] All 7 operations synced, thanks for waiting!\n",
      "[neptune] [info   ] Explore the metadata in the Neptune app: https://app.neptune.ai/andialifs/siet-24/e/SIET-104/metadata\n",
      "\n",
      "Running for experiment 80 with d_model 2048, heads4, num_layers_enc 5, num_layers_dec 5\n",
      "\n",
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/andialifs/siet-24/e/SIET-105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][0/12]\tLoss: 5.981\n",
      "Epoch [1][0/12]\tLoss: 5.901\n",
      "Epoch [2][0/12]\tLoss: 5.609\n",
      "Epoch [3][0/12]\tLoss: 5.316\n",
      "Epoch [4][0/12]\tLoss: 5.158\n",
      "Epoch [5][0/12]\tLoss: 4.980\n",
      "Epoch [6][0/12]\tLoss: 4.832\n",
      "Epoch [7][0/12]\tLoss: 4.777\n",
      "Epoch [8][0/12]\tLoss: 4.831\n",
      "Epoch [9][0/12]\tLoss: 4.623\n",
      "Epoch [10][0/12]\tLoss: 4.529\n",
      "Epoch [11][0/12]\tLoss: 4.489\n",
      "Epoch [12][0/12]\tLoss: 4.630\n",
      "Epoch [13][0/12]\tLoss: 4.643\n",
      "Epoch [14][0/12]\tLoss: 4.506\n",
      "Epoch [15][0/12]\tLoss: 4.431\n",
      "Epoch [16][0/12]\tLoss: 4.330\n",
      "Epoch [17][0/12]\tLoss: 4.104\n",
      "Epoch [18][0/12]\tLoss: 4.021\n",
      "Epoch [19][0/12]\tLoss: 4.249\n",
      "Epoch [20][0/12]\tLoss: 3.793\n",
      "Epoch [21][0/12]\tLoss: 3.870\n",
      "Epoch [22][0/12]\tLoss: 3.588\n",
      "Epoch [23][0/12]\tLoss: 3.436\n",
      "Epoch [24][0/12]\tLoss: 3.272\n",
      "Epoch [25][0/12]\tLoss: 3.153\n",
      "Epoch [26][0/12]\tLoss: 3.393\n",
      "Epoch [27][0/12]\tLoss: 3.538\n",
      "Epoch [28][0/12]\tLoss: 2.975\n",
      "Epoch [29][0/12]\tLoss: 3.287\n",
      "Epoch [30][0/12]\tLoss: 2.785\n",
      "Epoch [31][0/12]\tLoss: 2.481\n",
      "Epoch [32][0/12]\tLoss: 2.491\n",
      "Epoch [33][0/12]\tLoss: 2.399\n",
      "Epoch [34][0/12]\tLoss: 2.522\n",
      "Epoch [35][0/12]\tLoss: 2.473\n",
      "Epoch [36][0/12]\tLoss: 2.529\n",
      "Epoch [37][0/12]\tLoss: 2.040\n",
      "Epoch [38][0/12]\tLoss: 2.308\n",
      "Epoch [39][0/12]\tLoss: 2.010\n",
      "Epoch [40][0/12]\tLoss: 1.958\n",
      "Epoch [41][0/12]\tLoss: 2.188\n",
      "Epoch [42][0/12]\tLoss: 2.092\n",
      "Epoch [43][0/12]\tLoss: 1.964\n",
      "Epoch [44][0/12]\tLoss: 1.832\n",
      "Epoch [45][0/12]\tLoss: 1.860\n",
      "Epoch [46][0/12]\tLoss: 1.736\n",
      "Epoch [47][0/12]\tLoss: 1.851\n",
      "Epoch [48][0/12]\tLoss: 1.465\n",
      "Epoch [49][0/12]\tLoss: 1.706\n",
      "Epoch [50][0/12]\tLoss: 1.287\n",
      "Epoch [51][0/12]\tLoss: 1.535\n",
      "Epoch [52][0/12]\tLoss: 1.498\n",
      "Epoch [53][0/12]\tLoss: 1.265\n",
      "Epoch [54][0/12]\tLoss: 1.218\n",
      "Epoch [55][0/12]\tLoss: 1.253\n",
      "Epoch [56][0/12]\tLoss: 1.166\n",
      "Epoch [57][0/12]\tLoss: 1.126\n",
      "Epoch [58][0/12]\tLoss: 1.272\n",
      "Epoch [59][0/12]\tLoss: 1.044\n",
      "Epoch [60][0/12]\tLoss: 0.903\n",
      "Epoch [61][0/12]\tLoss: 1.202\n",
      "Epoch [62][0/12]\tLoss: 1.195\n",
      "Epoch [63][0/12]\tLoss: 0.846\n",
      "Epoch [64][0/12]\tLoss: 0.788\n",
      "Epoch [65][0/12]\tLoss: 0.822\n",
      "Epoch [66][0/12]\tLoss: 0.708\n",
      "Epoch [67][0/12]\tLoss: 0.841\n",
      "Epoch [68][0/12]\tLoss: 0.812\n",
      "Epoch [69][0/12]\tLoss: 0.686\n",
      "Epoch [70][0/12]\tLoss: 0.749\n",
      "Epoch [71][0/12]\tLoss: 0.684\n",
      "Epoch [72][0/12]\tLoss: 0.603\n",
      "Epoch [73][0/12]\tLoss: 0.573\n",
      "Epoch [74][0/12]\tLoss: 0.579\n",
      "Epoch [75][0/12]\tLoss: 0.602\n",
      "Epoch [76][0/12]\tLoss: 0.494\n",
      "Epoch [77][0/12]\tLoss: 0.554\n",
      "Epoch [78][0/12]\tLoss: 0.486\n",
      "Epoch [79][0/12]\tLoss: 0.568\n",
      "Epoch [80][0/12]\tLoss: 0.462\n",
      "Epoch [81][0/12]\tLoss: 0.468\n",
      "Epoch [82][0/12]\tLoss: 0.463\n",
      "Epoch [83][0/12]\tLoss: 0.472\n",
      "Epoch [84][0/12]\tLoss: 0.425\n",
      "Epoch [85][0/12]\tLoss: 0.413\n",
      "Epoch [86][0/12]\tLoss: 0.405\n",
      "Epoch [87][0/12]\tLoss: 0.400\n",
      "Epoch [88][0/12]\tLoss: 0.361\n",
      "Epoch [89][0/12]\tLoss: 0.389\n",
      "Epoch [90][0/12]\tLoss: 0.386\n",
      "Epoch [91][0/12]\tLoss: 0.349\n",
      "Epoch [92][0/12]\tLoss: 0.316\n",
      "Epoch [93][0/12]\tLoss: 0.321\n",
      "Epoch [94][0/12]\tLoss: 0.365\n",
      "Epoch [95][0/12]\tLoss: 0.298\n",
      "Epoch [96][0/12]\tLoss: 0.286\n",
      "Epoch [97][0/12]\tLoss: 0.284\n",
      "Epoch [98][0/12]\tLoss: 0.294\n",
      "Epoch [99][0/12]\tLoss: 0.289\n",
      "Epoch [100][0/12]\tLoss: 0.293\n",
      "Epoch [101][0/12]\tLoss: 0.275\n",
      "Epoch [102][0/12]\tLoss: 0.283\n",
      "Epoch [103][0/12]\tLoss: 0.267\n",
      "Epoch [104][0/12]\tLoss: 0.259\n",
      "Epoch [105][0/12]\tLoss: 0.272\n",
      "Epoch [106][0/12]\tLoss: 0.255\n",
      "Epoch [107][0/12]\tLoss: 0.272\n",
      "Epoch [108][0/12]\tLoss: 0.261\n",
      "Epoch [109][0/12]\tLoss: 0.251\n",
      "Epoch [110][0/12]\tLoss: 0.235\n",
      "Epoch [111][0/12]\tLoss: 0.268\n",
      "Epoch [112][0/12]\tLoss: 0.240\n",
      "Epoch [113][0/12]\tLoss: 0.278\n",
      "Epoch [114][0/12]\tLoss: 0.227\n",
      "Epoch [115][0/12]\tLoss: 0.230\n",
      "Epoch [116][0/12]\tLoss: 0.223\n",
      "Epoch [117][0/12]\tLoss: 0.246\n",
      "Epoch [118][0/12]\tLoss: 0.216\n",
      "Epoch [119][0/12]\tLoss: 0.229\n",
      "Epoch [120][0/12]\tLoss: 0.278\n",
      "Epoch [121][0/12]\tLoss: 0.229\n",
      "Epoch [122][0/12]\tLoss: 0.226\n",
      "Epoch [123][0/12]\tLoss: 0.226\n",
      "Epoch [124][0/12]\tLoss: 0.214\n",
      "Epoch [125][0/12]\tLoss: 0.224\n",
      "Epoch [126][0/12]\tLoss: 0.223\n",
      "Epoch [127][0/12]\tLoss: 0.196\n",
      "Epoch [128][0/12]\tLoss: 0.211\n",
      "Epoch [129][0/12]\tLoss: 0.236\n",
      "Epoch [130][0/12]\tLoss: 0.195\n",
      "Epoch [131][0/12]\tLoss: 0.214\n",
      "Epoch [132][0/12]\tLoss: 0.214\n",
      "Epoch [133][0/12]\tLoss: 0.200\n",
      "Epoch [134][0/12]\tLoss: 0.201\n",
      "Epoch [135][0/12]\tLoss: 0.214\n",
      "Epoch [136][0/12]\tLoss: 0.193\n",
      "Epoch [137][0/12]\tLoss: 0.225\n",
      "Epoch [138][0/12]\tLoss: 0.194\n",
      "Epoch [139][0/12]\tLoss: 0.186\n",
      "Epoch [140][0/12]\tLoss: 0.199\n",
      "Epoch [141][0/12]\tLoss: 0.202\n",
      "Epoch [142][0/12]\tLoss: 0.179\n",
      "Epoch [143][0/12]\tLoss: 0.170\n",
      "Epoch [144][0/12]\tLoss: 0.211\n",
      "Epoch [145][0/12]\tLoss: 0.186\n",
      "Epoch [146][0/12]\tLoss: 0.213\n",
      "Epoch [147][0/12]\tLoss: 0.182\n",
      "Epoch [148][0/12]\tLoss: 0.204\n",
      "Epoch [149][0/12]\tLoss: 0.181\n",
      "[neptune] [info   ] Shutting down background jobs, please wait a moment...\n",
      "[neptune] [info   ] Done!\n",
      "[neptune] [info   ] All 0 operations synced, thanks for waiting!\n",
      "[neptune] [info   ] Explore the metadata in the Neptune app: https://app.neptune.ai/andialifs/siet-24/e/SIET-105/metadata\n",
      "\n",
      "Running for experiment 81 with d_model 2048, heads8, num_layers_enc 1, num_layers_dec 1\n",
      "\n",
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/andialifs/siet-24/e/SIET-106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][0/12]\tLoss: 5.930\n",
      "Epoch [1][0/12]\tLoss: 5.912\n",
      "Epoch [2][0/12]\tLoss: 5.851\n",
      "Epoch [3][0/12]\tLoss: 5.776\n",
      "Epoch [4][0/12]\tLoss: 5.699\n",
      "Epoch [5][0/12]\tLoss: 5.589\n",
      "Epoch [6][0/12]\tLoss: 5.384\n",
      "Epoch [7][0/12]\tLoss: 5.157\n",
      "Epoch [8][0/12]\tLoss: 4.878\n",
      "Epoch [9][0/12]\tLoss: 4.804\n",
      "Epoch [10][0/12]\tLoss: 4.543\n",
      "Epoch [11][0/12]\tLoss: 4.391\n",
      "Epoch [12][0/12]\tLoss: 4.259\n",
      "Epoch [13][0/12]\tLoss: 4.052\n",
      "Epoch [14][0/12]\tLoss: 3.934\n",
      "Epoch [15][0/12]\tLoss: 3.883\n",
      "Epoch [16][0/12]\tLoss: 3.535\n",
      "Epoch [17][0/12]\tLoss: 3.448\n",
      "Epoch [18][0/12]\tLoss: 3.599\n",
      "Epoch [19][0/12]\tLoss: 3.413\n",
      "Epoch [20][0/12]\tLoss: 3.485\n",
      "Epoch [21][0/12]\tLoss: 2.887\n",
      "Epoch [22][0/12]\tLoss: 2.624\n",
      "Epoch [23][0/12]\tLoss: 2.727\n",
      "Epoch [24][0/12]\tLoss: 2.731\n",
      "Epoch [25][0/12]\tLoss: 2.922\n",
      "Epoch [26][0/12]\tLoss: 2.424\n",
      "Epoch [27][0/12]\tLoss: 2.439\n",
      "Epoch [28][0/12]\tLoss: 2.193\n",
      "Epoch [29][0/12]\tLoss: 2.336\n",
      "Epoch [30][0/12]\tLoss: 1.804\n",
      "Epoch [31][0/12]\tLoss: 1.854\n",
      "Epoch [32][0/12]\tLoss: 1.888\n",
      "Epoch [33][0/12]\tLoss: 2.110\n",
      "Epoch [34][0/12]\tLoss: 1.528\n",
      "Epoch [35][0/12]\tLoss: 1.598\n",
      "Epoch [36][0/12]\tLoss: 1.508\n",
      "Epoch [37][0/12]\tLoss: 1.475\n",
      "Epoch [38][0/12]\tLoss: 1.351\n",
      "Epoch [39][0/12]\tLoss: 1.431\n",
      "Epoch [40][0/12]\tLoss: 1.259\n",
      "Epoch [41][0/12]\tLoss: 1.352\n",
      "Epoch [42][0/12]\tLoss: 1.123\n",
      "Epoch [43][0/12]\tLoss: 0.941\n",
      "Epoch [44][0/12]\tLoss: 0.895\n",
      "Epoch [45][0/12]\tLoss: 0.927\n",
      "Epoch [46][0/12]\tLoss: 0.878\n",
      "Epoch [47][0/12]\tLoss: 0.771\n",
      "Epoch [48][0/12]\tLoss: 0.728\n",
      "Epoch [49][0/12]\tLoss: 0.698\n",
      "Epoch [50][0/12]\tLoss: 0.584\n",
      "Epoch [51][0/12]\tLoss: 0.675\n",
      "Epoch [52][0/12]\tLoss: 0.547\n",
      "Epoch [53][0/12]\tLoss: 0.491\n",
      "Epoch [54][0/12]\tLoss: 0.501\n",
      "Epoch [55][0/12]\tLoss: 0.472\n",
      "Epoch [56][0/12]\tLoss: 0.503\n",
      "Epoch [57][0/12]\tLoss: 0.415\n",
      "Epoch [58][0/12]\tLoss: 0.403\n",
      "Epoch [59][0/12]\tLoss: 0.399\n",
      "Epoch [60][0/12]\tLoss: 0.429\n",
      "Epoch [61][0/12]\tLoss: 0.385\n",
      "Epoch [62][0/12]\tLoss: 0.375\n",
      "Epoch [63][0/12]\tLoss: 0.355\n",
      "Epoch [64][0/12]\tLoss: 0.367\n",
      "Epoch [65][0/12]\tLoss: 0.345\n",
      "Epoch [66][0/12]\tLoss: 0.353\n",
      "Epoch [67][0/12]\tLoss: 0.327\n",
      "Epoch [68][0/12]\tLoss: 0.331\n",
      "Epoch [69][0/12]\tLoss: 0.321\n",
      "Epoch [70][0/12]\tLoss: 0.325\n",
      "Epoch [71][0/12]\tLoss: 0.287\n",
      "Epoch [72][0/12]\tLoss: 0.285\n",
      "Epoch [73][0/12]\tLoss: 0.297\n",
      "Epoch [74][0/12]\tLoss: 0.269\n",
      "Epoch [75][0/12]\tLoss: 0.313\n",
      "Epoch [76][0/12]\tLoss: 0.304\n",
      "Epoch [77][0/12]\tLoss: 0.297\n",
      "Epoch [78][0/12]\tLoss: 0.307\n",
      "Epoch [79][0/12]\tLoss: 0.315\n",
      "Epoch [80][0/12]\tLoss: 0.261\n",
      "Epoch [81][0/12]\tLoss: 0.260\n",
      "Epoch [82][0/12]\tLoss: 0.291\n",
      "Epoch [83][0/12]\tLoss: 0.243\n",
      "Epoch [84][0/12]\tLoss: 0.282\n",
      "Epoch [85][0/12]\tLoss: 0.276\n",
      "Epoch [86][0/12]\tLoss: 0.255\n",
      "Epoch [87][0/12]\tLoss: 0.271\n",
      "Epoch [88][0/12]\tLoss: 0.270\n",
      "Epoch [89][0/12]\tLoss: 0.250\n",
      "Epoch [90][0/12]\tLoss: 0.257\n",
      "Epoch [91][0/12]\tLoss: 0.273\n",
      "Epoch [92][0/12]\tLoss: 0.275\n",
      "Epoch [93][0/12]\tLoss: 0.243\n",
      "Epoch [94][0/12]\tLoss: 0.255\n",
      "Epoch [95][0/12]\tLoss: 0.236\n",
      "Epoch [96][0/12]\tLoss: 0.279\n",
      "Epoch [97][0/12]\tLoss: 0.255\n",
      "Epoch [98][0/12]\tLoss: 0.242\n",
      "Epoch [99][0/12]\tLoss: 0.251\n",
      "Epoch [100][0/12]\tLoss: 0.244\n",
      "Epoch [101][0/12]\tLoss: 0.257\n",
      "Epoch [102][0/12]\tLoss: 0.254\n",
      "Epoch [103][0/12]\tLoss: 0.231\n",
      "Epoch [104][0/12]\tLoss: 0.244\n",
      "Epoch [105][0/12]\tLoss: 0.284\n",
      "Epoch [106][0/12]\tLoss: 0.235\n",
      "Epoch [107][0/12]\tLoss: 0.289\n",
      "Epoch [108][0/12]\tLoss: 0.252\n",
      "Epoch [109][0/12]\tLoss: 0.216\n",
      "Epoch [110][0/12]\tLoss: 0.211\n",
      "Epoch [111][0/12]\tLoss: 0.233\n",
      "Epoch [112][0/12]\tLoss: 0.273\n",
      "Epoch [113][0/12]\tLoss: 0.224\n",
      "Epoch [114][0/12]\tLoss: 0.269\n",
      "Epoch [115][0/12]\tLoss: 0.264\n",
      "Epoch [116][0/12]\tLoss: 0.226\n",
      "Epoch [117][0/12]\tLoss: 0.257\n",
      "Epoch [118][0/12]\tLoss: 0.255\n",
      "Epoch [119][0/12]\tLoss: 0.222\n",
      "Epoch [120][0/12]\tLoss: 0.214\n",
      "Epoch [121][0/12]\tLoss: 0.249\n",
      "Epoch [122][0/12]\tLoss: 0.234\n",
      "Epoch [123][0/12]\tLoss: 0.241\n",
      "Epoch [124][0/12]\tLoss: 0.202\n",
      "Epoch [125][0/12]\tLoss: 0.271\n",
      "Epoch [126][0/12]\tLoss: 0.231\n",
      "Epoch [127][0/12]\tLoss: 0.245\n",
      "Epoch [128][0/12]\tLoss: 0.265\n",
      "Epoch [129][0/12]\tLoss: 0.204\n",
      "Epoch [130][0/12]\tLoss: 0.224\n",
      "Epoch [131][0/12]\tLoss: 0.216\n",
      "Epoch [132][0/12]\tLoss: 0.241\n",
      "Epoch [133][0/12]\tLoss: 0.226\n",
      "Epoch [134][0/12]\tLoss: 0.226\n",
      "Epoch [135][0/12]\tLoss: 0.254\n",
      "Epoch [136][0/12]\tLoss: 0.241\n",
      "Epoch [137][0/12]\tLoss: 0.214\n",
      "Epoch [138][0/12]\tLoss: 0.236\n",
      "Epoch [139][0/12]\tLoss: 0.221\n",
      "Epoch [140][0/12]\tLoss: 0.249\n",
      "Epoch [141][0/12]\tLoss: 0.233\n",
      "Epoch [142][0/12]\tLoss: 0.230\n",
      "Epoch [143][0/12]\tLoss: 0.255\n",
      "Epoch [144][0/12]\tLoss: 0.217\n",
      "Epoch [145][0/12]\tLoss: 0.252\n",
      "Epoch [146][0/12]\tLoss: 0.239\n",
      "Epoch [147][0/12]\tLoss: 0.236\n",
      "Epoch [148][0/12]\tLoss: 0.218\n",
      "Epoch [149][0/12]\tLoss: 0.198\n",
      "[neptune] [info   ] Shutting down background jobs, please wait a moment...\n",
      "[neptune] [info   ] Done!\n",
      "[neptune] [info   ] Waiting for the remaining 7 operations to synchronize with Neptune. Do not kill this process.\n",
      "[neptune] [info   ] All 7 operations synced, thanks for waiting!\n",
      "[neptune] [info   ] Explore the metadata in the Neptune app: https://app.neptune.ai/andialifs/siet-24/e/SIET-106/metadata\n",
      "\n",
      "Running for experiment 82 with d_model 2048, heads8, num_layers_enc 1, num_layers_dec 3\n",
      "\n",
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/andialifs/siet-24/e/SIET-107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][0/12]\tLoss: 5.956\n",
      "Epoch [1][0/12]\tLoss: 5.926\n",
      "Epoch [2][0/12]\tLoss: 5.879\n",
      "Epoch [3][0/12]\tLoss: 5.807\n",
      "Epoch [4][0/12]\tLoss: 5.690\n",
      "Epoch [5][0/12]\tLoss: 5.538\n",
      "Epoch [6][0/12]\tLoss: 5.396\n",
      "Epoch [7][0/12]\tLoss: 5.174\n",
      "Epoch [8][0/12]\tLoss: 4.920\n",
      "Epoch [9][0/12]\tLoss: 4.879\n",
      "Epoch [10][0/12]\tLoss: 4.489\n",
      "Epoch [11][0/12]\tLoss: 4.113\n",
      "Epoch [12][0/12]\tLoss: 4.440\n",
      "Epoch [13][0/12]\tLoss: 4.119\n",
      "Epoch [14][0/12]\tLoss: 3.918\n",
      "Epoch [15][0/12]\tLoss: 3.953\n",
      "Epoch [16][0/12]\tLoss: 3.678\n",
      "Epoch [17][0/12]\tLoss: 3.585\n",
      "Epoch [18][0/12]\tLoss: 3.398\n",
      "Epoch [19][0/12]\tLoss: 3.302\n",
      "Epoch [20][0/12]\tLoss: 3.104\n",
      "Epoch [21][0/12]\tLoss: 2.903\n",
      "Epoch [22][0/12]\tLoss: 3.027\n",
      "Epoch [23][0/12]\tLoss: 2.695\n",
      "Epoch [24][0/12]\tLoss: 2.569\n",
      "Epoch [25][0/12]\tLoss: 2.808\n",
      "Epoch [26][0/12]\tLoss: 2.395\n",
      "Epoch [27][0/12]\tLoss: 2.295\n",
      "Epoch [28][0/12]\tLoss: 2.506\n",
      "Epoch [29][0/12]\tLoss: 2.367\n",
      "Epoch [30][0/12]\tLoss: 2.131\n",
      "Epoch [31][0/12]\tLoss: 2.037\n",
      "Epoch [32][0/12]\tLoss: 1.927\n",
      "Epoch [33][0/12]\tLoss: 1.602\n",
      "Epoch [34][0/12]\tLoss: 1.643\n",
      "Epoch [35][0/12]\tLoss: 1.626\n",
      "Epoch [36][0/12]\tLoss: 1.452\n",
      "Epoch [37][0/12]\tLoss: 1.452\n",
      "Epoch [38][0/12]\tLoss: 1.462\n",
      "Epoch [39][0/12]\tLoss: 1.126\n",
      "Epoch [40][0/12]\tLoss: 1.305\n",
      "Epoch [41][0/12]\tLoss: 1.031\n",
      "Epoch [42][0/12]\tLoss: 1.148\n",
      "Epoch [43][0/12]\tLoss: 0.845\n",
      "Epoch [44][0/12]\tLoss: 1.008\n",
      "Epoch [45][0/12]\tLoss: 0.963\n",
      "Epoch [46][0/12]\tLoss: 0.721\n",
      "Epoch [47][0/12]\tLoss: 0.785\n",
      "Epoch [48][0/12]\tLoss: 0.741\n",
      "Epoch [49][0/12]\tLoss: 0.710\n",
      "Epoch [50][0/12]\tLoss: 0.630\n",
      "Epoch [51][0/12]\tLoss: 0.623\n",
      "Epoch [52][0/12]\tLoss: 0.550\n",
      "Epoch [53][0/12]\tLoss: 0.556\n",
      "Epoch [54][0/12]\tLoss: 0.478\n",
      "Epoch [55][0/12]\tLoss: 0.411\n",
      "Epoch [56][0/12]\tLoss: 0.444\n",
      "Epoch [57][0/12]\tLoss: 0.417\n",
      "Epoch [58][0/12]\tLoss: 0.394\n",
      "Epoch [59][0/12]\tLoss: 0.410\n",
      "Epoch [60][0/12]\tLoss: 0.387\n",
      "Epoch [61][0/12]\tLoss: 0.351\n",
      "Epoch [62][0/12]\tLoss: 0.354\n",
      "Epoch [63][0/12]\tLoss: 0.367\n",
      "Epoch [64][0/12]\tLoss: 0.321\n",
      "Epoch [65][0/12]\tLoss: 0.346\n",
      "Epoch [66][0/12]\tLoss: 0.398\n",
      "Epoch [67][0/12]\tLoss: 0.351\n",
      "Epoch [68][0/12]\tLoss: 0.343\n",
      "Epoch [69][0/12]\tLoss: 0.340\n",
      "Epoch [70][0/12]\tLoss: 0.366\n",
      "Epoch [71][0/12]\tLoss: 0.314\n",
      "Epoch [72][0/12]\tLoss: 0.304\n",
      "Epoch [73][0/12]\tLoss: 0.299\n",
      "Epoch [74][0/12]\tLoss: 0.319\n",
      "Epoch [75][0/12]\tLoss: 0.276\n",
      "Epoch [76][0/12]\tLoss: 0.306\n",
      "Epoch [77][0/12]\tLoss: 0.307\n",
      "Epoch [78][0/12]\tLoss: 0.294\n",
      "Epoch [79][0/12]\tLoss: 0.267\n",
      "Epoch [80][0/12]\tLoss: 0.279\n",
      "Epoch [81][0/12]\tLoss: 0.253\n",
      "Epoch [82][0/12]\tLoss: 0.301\n",
      "Epoch [83][0/12]\tLoss: 0.266\n",
      "Epoch [84][0/12]\tLoss: 0.279\n",
      "Epoch [85][0/12]\tLoss: 0.266\n",
      "Epoch [86][0/12]\tLoss: 0.252\n",
      "Epoch [87][0/12]\tLoss: 0.262\n",
      "Epoch [88][0/12]\tLoss: 0.285\n",
      "Epoch [89][0/12]\tLoss: 0.261\n",
      "Epoch [90][0/12]\tLoss: 0.272\n",
      "Epoch [91][0/12]\tLoss: 0.252\n",
      "Epoch [92][0/12]\tLoss: 0.252\n",
      "Epoch [93][0/12]\tLoss: 0.298\n",
      "Epoch [94][0/12]\tLoss: 0.254\n",
      "Epoch [95][0/12]\tLoss: 0.275\n",
      "Epoch [96][0/12]\tLoss: 0.271\n",
      "Epoch [97][0/12]\tLoss: 0.262\n",
      "Epoch [98][0/12]\tLoss: 0.254\n",
      "Epoch [99][0/12]\tLoss: 0.286\n",
      "Epoch [100][0/12]\tLoss: 0.209\n",
      "Epoch [101][0/12]\tLoss: 0.247\n",
      "Epoch [102][0/12]\tLoss: 0.227\n",
      "Epoch [103][0/12]\tLoss: 0.223\n",
      "Epoch [104][0/12]\tLoss: 0.237\n",
      "Epoch [105][0/12]\tLoss: 0.265\n",
      "Epoch [106][0/12]\tLoss: 0.244\n",
      "Epoch [107][0/12]\tLoss: 0.220\n",
      "Epoch [108][0/12]\tLoss: 0.221\n",
      "Epoch [109][0/12]\tLoss: 0.233\n",
      "Epoch [110][0/12]\tLoss: 0.256\n",
      "Epoch [111][0/12]\tLoss: 0.210\n",
      "Epoch [112][0/12]\tLoss: 0.246\n",
      "Epoch [113][0/12]\tLoss: 0.247\n",
      "Epoch [114][0/12]\tLoss: 0.248\n",
      "Epoch [115][0/12]\tLoss: 0.228\n",
      "Epoch [116][0/12]\tLoss: 0.211\n",
      "Epoch [117][0/12]\tLoss: 0.230\n",
      "Epoch [118][0/12]\tLoss: 0.219\n",
      "Epoch [119][0/12]\tLoss: 0.232\n",
      "Epoch [120][0/12]\tLoss: 0.226\n",
      "Epoch [121][0/12]\tLoss: 0.251\n",
      "Epoch [122][0/12]\tLoss: 0.249\n",
      "Epoch [123][0/12]\tLoss: 0.226\n",
      "Epoch [124][0/12]\tLoss: 0.236\n",
      "Epoch [125][0/12]\tLoss: 0.230\n",
      "Epoch [126][0/12]\tLoss: 0.254\n",
      "Epoch [127][0/12]\tLoss: 0.227\n",
      "Epoch [128][0/12]\tLoss: 0.235\n",
      "Epoch [129][0/12]\tLoss: 0.204\n",
      "Epoch [130][0/12]\tLoss: 0.201\n",
      "Epoch [131][0/12]\tLoss: 0.229\n",
      "Epoch [132][0/12]\tLoss: 0.230\n",
      "Epoch [133][0/12]\tLoss: 0.210\n",
      "Epoch [134][0/12]\tLoss: 0.261\n",
      "Epoch [135][0/12]\tLoss: 0.237\n",
      "Epoch [136][0/12]\tLoss: 0.226\n",
      "Epoch [137][0/12]\tLoss: 0.226\n",
      "Epoch [138][0/12]\tLoss: 0.228\n",
      "Epoch [139][0/12]\tLoss: 0.206\n",
      "Epoch [140][0/12]\tLoss: 0.232\n",
      "Epoch [141][0/12]\tLoss: 0.197\n",
      "Epoch [142][0/12]\tLoss: 0.224\n",
      "Epoch [143][0/12]\tLoss: 0.241\n",
      "Epoch [144][0/12]\tLoss: 0.245\n",
      "Epoch [145][0/12]\tLoss: 0.211\n",
      "Epoch [146][0/12]\tLoss: 0.243\n",
      "Epoch [147][0/12]\tLoss: 0.199\n",
      "Epoch [148][0/12]\tLoss: 0.184\n",
      "Epoch [149][0/12]\tLoss: 0.216\n",
      "[neptune] [info   ] Shutting down background jobs, please wait a moment...\n",
      "[neptune] [info   ] Done!\n",
      "[neptune] [info   ] All 0 operations synced, thanks for waiting!\n",
      "[neptune] [info   ] Explore the metadata in the Neptune app: https://app.neptune.ai/andialifs/siet-24/e/SIET-107/metadata\n",
      "\n",
      "Running for experiment 83 with d_model 2048, heads8, num_layers_enc 1, num_layers_dec 5\n",
      "\n",
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/andialifs/siet-24/e/SIET-108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][0/12]\tLoss: 5.957\n",
      "Epoch [1][0/12]\tLoss: 5.948\n",
      "Epoch [2][0/12]\tLoss: 5.862\n",
      "Epoch [3][0/12]\tLoss: 5.762\n",
      "Epoch [4][0/12]\tLoss: 5.666\n",
      "Epoch [5][0/12]\tLoss: 5.585\n",
      "Epoch [6][0/12]\tLoss: 5.305\n",
      "Epoch [7][0/12]\tLoss: 5.082\n",
      "Epoch [8][0/12]\tLoss: 4.899\n",
      "Epoch [9][0/12]\tLoss: 4.671\n",
      "Epoch [10][0/12]\tLoss: 4.521\n",
      "Epoch [11][0/12]\tLoss: 4.368\n",
      "Epoch [12][0/12]\tLoss: 4.229\n",
      "Epoch [13][0/12]\tLoss: 4.286\n",
      "Epoch [14][0/12]\tLoss: 3.968\n",
      "Epoch [15][0/12]\tLoss: 3.961\n",
      "Epoch [16][0/12]\tLoss: 3.751\n",
      "Epoch [17][0/12]\tLoss: 3.806\n",
      "Epoch [18][0/12]\tLoss: 3.642\n",
      "Epoch [19][0/12]\tLoss: 3.341\n",
      "Epoch [20][0/12]\tLoss: 3.256\n",
      "Epoch [21][0/12]\tLoss: 3.229\n",
      "Epoch [22][0/12]\tLoss: 3.114\n",
      "Epoch [23][0/12]\tLoss: 2.875\n",
      "Epoch [24][0/12]\tLoss: 2.444\n",
      "Epoch [25][0/12]\tLoss: 2.759\n",
      "Epoch [26][0/12]\tLoss: 2.437\n",
      "Epoch [27][0/12]\tLoss: 2.124\n",
      "Epoch [28][0/12]\tLoss: 2.210\n",
      "Epoch [29][0/12]\tLoss: 2.290\n",
      "Epoch [30][0/12]\tLoss: 2.275\n",
      "Epoch [31][0/12]\tLoss: 2.107\n",
      "Epoch [32][0/12]\tLoss: 1.878\n",
      "Epoch [33][0/12]\tLoss: 1.831\n",
      "Epoch [34][0/12]\tLoss: 1.527\n",
      "Epoch [35][0/12]\tLoss: 1.350\n",
      "Epoch [36][0/12]\tLoss: 1.596\n",
      "Epoch [37][0/12]\tLoss: 1.568\n",
      "Epoch [38][0/12]\tLoss: 1.258\n",
      "Epoch [39][0/12]\tLoss: 1.073\n",
      "Epoch [40][0/12]\tLoss: 1.145\n",
      "Epoch [41][0/12]\tLoss: 1.311\n",
      "Epoch [42][0/12]\tLoss: 1.025\n",
      "Epoch [43][0/12]\tLoss: 1.032\n",
      "Epoch [44][0/12]\tLoss: 0.915\n",
      "Epoch [45][0/12]\tLoss: 0.848\n",
      "Epoch [46][0/12]\tLoss: 0.761\n",
      "Epoch [47][0/12]\tLoss: 0.698\n",
      "Epoch [48][0/12]\tLoss: 0.793\n",
      "Epoch [49][0/12]\tLoss: 0.584\n",
      "Epoch [50][0/12]\tLoss: 0.722\n",
      "Epoch [51][0/12]\tLoss: 0.592\n",
      "Epoch [52][0/12]\tLoss: 0.600\n",
      "Epoch [53][0/12]\tLoss: 0.478\n",
      "Epoch [54][0/12]\tLoss: 0.501\n",
      "Epoch [55][0/12]\tLoss: 0.467\n",
      "Epoch [56][0/12]\tLoss: 0.428\n",
      "Epoch [57][0/12]\tLoss: 0.478\n",
      "Epoch [58][0/12]\tLoss: 0.390\n",
      "Epoch [59][0/12]\tLoss: 0.413\n",
      "Epoch [60][0/12]\tLoss: 0.425\n",
      "Epoch [61][0/12]\tLoss: 0.356\n",
      "Epoch [62][0/12]\tLoss: 0.331\n",
      "Epoch [63][0/12]\tLoss: 0.353\n",
      "Epoch [64][0/12]\tLoss: 0.347\n",
      "Epoch [65][0/12]\tLoss: 0.313\n",
      "Epoch [66][0/12]\tLoss: 0.339\n",
      "Epoch [67][0/12]\tLoss: 0.333\n",
      "Epoch [68][0/12]\tLoss: 0.293\n",
      "Epoch [69][0/12]\tLoss: 0.340\n",
      "Epoch [70][0/12]\tLoss: 0.326\n",
      "Epoch [71][0/12]\tLoss: 0.299\n",
      "Epoch [72][0/12]\tLoss: 0.316\n",
      "Epoch [73][0/12]\tLoss: 0.284\n",
      "Epoch [74][0/12]\tLoss: 0.301\n",
      "Epoch [75][0/12]\tLoss: 0.265\n",
      "Epoch [76][0/12]\tLoss: 0.275\n",
      "Epoch [77][0/12]\tLoss: 0.263\n",
      "Epoch [78][0/12]\tLoss: 0.298\n",
      "Epoch [79][0/12]\tLoss: 0.272\n",
      "Epoch [80][0/12]\tLoss: 0.268\n",
      "Epoch [81][0/12]\tLoss: 0.269\n",
      "Epoch [82][0/12]\tLoss: 0.274\n",
      "Epoch [83][0/12]\tLoss: 0.289\n",
      "Epoch [84][0/12]\tLoss: 0.292\n",
      "Epoch [85][0/12]\tLoss: 0.246\n",
      "Epoch [86][0/12]\tLoss: 0.245\n",
      "Epoch [87][0/12]\tLoss: 0.247\n",
      "Epoch [88][0/12]\tLoss: 0.250\n",
      "Epoch [89][0/12]\tLoss: 0.256\n",
      "Epoch [90][0/12]\tLoss: 0.290\n",
      "Epoch [91][0/12]\tLoss: 0.266\n",
      "Epoch [92][0/12]\tLoss: 0.275\n",
      "Epoch [93][0/12]\tLoss: 0.250\n",
      "Epoch [94][0/12]\tLoss: 0.253\n",
      "Epoch [95][0/12]\tLoss: 0.248\n",
      "Epoch [96][0/12]\tLoss: 0.242\n",
      "Epoch [97][0/12]\tLoss: 0.264\n",
      "Epoch [98][0/12]\tLoss: 0.258\n",
      "Epoch [99][0/12]\tLoss: 0.258\n",
      "Epoch [100][0/12]\tLoss: 0.258\n",
      "Epoch [101][0/12]\tLoss: 0.241\n",
      "Epoch [102][0/12]\tLoss: 0.236\n",
      "Epoch [103][0/12]\tLoss: 0.256\n",
      "Epoch [104][0/12]\tLoss: 0.220\n",
      "Epoch [105][0/12]\tLoss: 0.247\n",
      "Epoch [106][0/12]\tLoss: 0.209\n",
      "Epoch [107][0/12]\tLoss: 0.243\n",
      "Epoch [108][0/12]\tLoss: 0.203\n",
      "Epoch [109][0/12]\tLoss: 0.244\n",
      "Epoch [110][0/12]\tLoss: 0.232\n",
      "Epoch [111][0/12]\tLoss: 0.213\n",
      "Epoch [112][0/12]\tLoss: 0.201\n",
      "Epoch [113][0/12]\tLoss: 0.224\n",
      "Epoch [114][0/12]\tLoss: 0.245\n",
      "Epoch [115][0/12]\tLoss: 0.260\n",
      "Epoch [116][0/12]\tLoss: 0.228\n",
      "Epoch [117][0/12]\tLoss: 0.256\n",
      "Epoch [118][0/12]\tLoss: 0.230\n",
      "Epoch [124][0/12]\tLoss: 0.230\n",
      "Epoch [125][0/12]\tLoss: 0.203\n",
      "Epoch [126][0/12]\tLoss: 0.221\n",
      "Epoch [127][0/12]\tLoss: 0.208\n",
      "Epoch [128][0/12]\tLoss: 0.193\n",
      "Epoch [129][0/12]\tLoss: 0.227\n",
      "Epoch [130][0/12]\tLoss: 0.228\n",
      "Epoch [131][0/12]\tLoss: 0.242\n",
      "Epoch [132][0/12]\tLoss: 0.228\n",
      "Epoch [133][0/12]\tLoss: 0.251\n",
      "Epoch [134][0/12]\tLoss: 0.229\n",
      "Epoch [135][0/12]\tLoss: 0.254\n",
      "Epoch [136][0/12]\tLoss: 0.219\n",
      "Epoch [137][0/12]\tLoss: 0.210\n",
      "Epoch [138][0/12]\tLoss: 0.213\n",
      "Epoch [139][0/12]\tLoss: 0.231\n",
      "Epoch [140][0/12]\tLoss: 0.225\n",
      "Epoch [141][0/12]\tLoss: 0.220\n",
      "Epoch [142][0/12]\tLoss: 0.194\n",
      "Epoch [143][0/12]\tLoss: 0.216\n",
      "Epoch [144][0/12]\tLoss: 0.223\n",
      "Epoch [145][0/12]\tLoss: 0.220\n",
      "Epoch [146][0/12]\tLoss: 0.235\n",
      "Epoch [147][0/12]\tLoss: 0.213\n",
      "Epoch [148][0/12]\tLoss: 0.249\n",
      "Epoch [149][0/12]\tLoss: 0.240\n",
      "[neptune] [info   ] Shutting down background jobs, please wait a moment...\n",
      "[neptune] [info   ] Done!\n",
      "[neptune] [info   ] All 0 operations synced, thanks for waiting!\n",
      "[neptune] [info   ] Explore the metadata in the Neptune app: https://app.neptune.ai/andialifs/siet-24/e/SIET-108/metadata\n",
      "\n",
      "Running for experiment 84 with d_model 2048, heads8, num_layers_enc 3, num_layers_dec 1\n",
      "\n",
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/andialifs/siet-24/e/SIET-109\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][0/12]\tLoss: 5.980\n",
      "Epoch [1][0/12]\tLoss: 5.901\n",
      "Epoch [2][0/12]\tLoss: 5.634\n",
      "Epoch [3][0/12]\tLoss: 5.360\n",
      "Epoch [4][0/12]\tLoss: 5.059\n",
      "Epoch [5][0/12]\tLoss: 4.989\n",
      "Epoch [6][0/12]\tLoss: 4.790\n",
      "Epoch [7][0/12]\tLoss: 4.752\n",
      "Epoch [8][0/12]\tLoss: 4.706\n",
      "Epoch [9][0/12]\tLoss: 4.656\n",
      "Epoch [10][0/12]\tLoss: 4.654\n",
      "Epoch [11][0/12]\tLoss: 4.443\n",
      "Epoch [12][0/12]\tLoss: 4.519\n",
      "Epoch [13][0/12]\tLoss: 4.343\n",
      "Epoch [14][0/12]\tLoss: 4.225\n",
      "Epoch [15][0/12]\tLoss: 3.929\n",
      "Epoch [16][0/12]\tLoss: 3.858\n",
      "Epoch [17][0/12]\tLoss: 3.688\n",
      "Epoch [18][0/12]\tLoss: 3.597\n",
      "Epoch [19][0/12]\tLoss: 3.607\n",
      "Epoch [20][0/12]\tLoss: 3.553\n",
      "Epoch [21][0/12]\tLoss: 3.099\n",
      "Epoch [22][0/12]\tLoss: 3.325\n",
      "Epoch [23][0/12]\tLoss: 3.170\n",
      "Epoch [24][0/12]\tLoss: 3.027\n",
      "Epoch [25][0/12]\tLoss: 2.985\n",
      "Epoch [26][0/12]\tLoss: 2.753\n",
      "Epoch [27][0/12]\tLoss: 2.831\n",
      "Epoch [28][0/12]\tLoss: 2.602\n",
      "Epoch [29][0/12]\tLoss: 2.350\n",
      "Epoch [30][0/12]\tLoss: 2.064\n",
      "Epoch [31][0/12]\tLoss: 2.233\n",
      "Epoch [32][0/12]\tLoss: 2.180\n",
      "Epoch [33][0/12]\tLoss: 2.077\n",
      "Epoch [34][0/12]\tLoss: 2.183\n",
      "Epoch [35][0/12]\tLoss: 1.782\n",
      "Epoch [36][0/12]\tLoss: 1.823\n",
      "Epoch [37][0/12]\tLoss: 1.898\n",
      "Epoch [38][0/12]\tLoss: 1.656\n",
      "Epoch [39][0/12]\tLoss: 1.557\n",
      "Epoch [40][0/12]\tLoss: 1.521\n",
      "Epoch [41][0/12]\tLoss: 1.194\n",
      "Epoch [42][0/12]\tLoss: 1.033\n",
      "Epoch [43][0/12]\tLoss: 1.197\n",
      "Epoch [44][0/12]\tLoss: 1.357\n",
      "Epoch [45][0/12]\tLoss: 1.167\n",
      "Epoch [46][0/12]\tLoss: 1.070\n",
      "Epoch [47][0/12]\tLoss: 1.125\n",
      "Epoch [48][0/12]\tLoss: 0.970\n",
      "Epoch [49][0/12]\tLoss: 1.065\n",
      "Epoch [50][0/12]\tLoss: 0.910\n",
      "Epoch [51][0/12]\tLoss: 0.845\n",
      "Epoch [52][0/12]\tLoss: 0.904\n",
      "Epoch [53][0/12]\tLoss: 0.669\n",
      "Epoch [54][0/12]\tLoss: 0.817\n",
      "Epoch [55][0/12]\tLoss: 0.613\n",
      "Epoch [56][0/12]\tLoss: 0.792\n",
      "Epoch [57][0/12]\tLoss: 0.596\n",
      "Epoch [58][0/12]\tLoss: 0.592\n",
      "Epoch [59][0/12]\tLoss: 0.617\n",
      "Epoch [60][0/12]\tLoss: 0.556\n",
      "Epoch [61][0/12]\tLoss: 0.429\n",
      "Epoch [62][0/12]\tLoss: 0.529\n",
      "Epoch [63][0/12]\tLoss: 0.421\n",
      "Epoch [64][0/12]\tLoss: 0.468\n",
      "Epoch [65][0/12]\tLoss: 0.473\n",
      "Epoch [66][0/12]\tLoss: 0.378\n",
      "Epoch [67][0/12]\tLoss: 0.386\n",
      "Epoch [68][0/12]\tLoss: 0.359\n",
      "Epoch [69][0/12]\tLoss: 0.377\n",
      "Epoch [70][0/12]\tLoss: 0.324\n",
      "Epoch [71][0/12]\tLoss: 0.352\n",
      "Epoch [72][0/12]\tLoss: 0.352\n",
      "Epoch [73][0/12]\tLoss: 0.303\n",
      "Epoch [74][0/12]\tLoss: 0.296\n",
      "Epoch [75][0/12]\tLoss: 0.290\n",
      "Epoch [76][0/12]\tLoss: 0.253\n",
      "Epoch [77][0/12]\tLoss: 0.281\n",
      "Epoch [78][0/12]\tLoss: 0.295\n",
      "Epoch [79][0/12]\tLoss: 0.276\n",
      "Epoch [80][0/12]\tLoss: 0.251\n",
      "Epoch [81][0/12]\tLoss: 0.249\n",
      "Epoch [82][0/12]\tLoss: 0.284\n",
      "Epoch [83][0/12]\tLoss: 0.241\n",
      "Epoch [84][0/12]\tLoss: 0.247\n",
      "Epoch [85][0/12]\tLoss: 0.233\n",
      "Epoch [86][0/12]\tLoss: 0.221\n",
      "Epoch [87][0/12]\tLoss: 0.252\n",
      "Epoch [88][0/12]\tLoss: 0.224\n",
      "Epoch [89][0/12]\tLoss: 0.227\n",
      "Epoch [90][0/12]\tLoss: 0.216\n",
      "Epoch [91][0/12]\tLoss: 0.196\n",
      "Epoch [92][0/12]\tLoss: 0.230\n",
      "Epoch [93][0/12]\tLoss: 0.215\n",
      "Epoch [94][0/12]\tLoss: 0.217\n",
      "Epoch [95][0/12]\tLoss: 0.196\n",
      "Epoch [96][0/12]\tLoss: 0.195\n",
      "Epoch [97][0/12]\tLoss: 0.192\n",
      "Epoch [98][0/12]\tLoss: 0.206\n",
      "Epoch [99][0/12]\tLoss: 0.203\n",
      "Epoch [100][0/12]\tLoss: 0.185\n",
      "Epoch [101][0/12]\tLoss: 0.211\n",
      "Epoch [102][0/12]\tLoss: 0.201\n",
      "Epoch [103][0/12]\tLoss: 0.186\n",
      "Epoch [104][0/12]\tLoss: 0.197\n",
      "Epoch [105][0/12]\tLoss: 0.199\n",
      "Epoch [106][0/12]\tLoss: 0.165\n",
      "Epoch [107][0/12]\tLoss: 0.180\n",
      "Epoch [108][0/12]\tLoss: 0.172\n",
      "Epoch [109][0/12]\tLoss: 0.183\n",
      "Epoch [110][0/12]\tLoss: 0.175\n",
      "Epoch [111][0/12]\tLoss: 0.146\n",
      "Epoch [112][0/12]\tLoss: 0.154\n",
      "Epoch [113][0/12]\tLoss: 0.189\n",
      "Epoch [114][0/12]\tLoss: 0.197\n",
      "Epoch [115][0/12]\tLoss: 0.164\n",
      "Epoch [116][0/12]\tLoss: 0.171\n",
      "Epoch [117][0/12]\tLoss: 0.160\n",
      "Epoch [118][0/12]\tLoss: 0.171\n",
      "Epoch [119][0/12]\tLoss: 0.177\n",
      "Epoch [120][0/12]\tLoss: 0.185\n",
      "Epoch [121][0/12]\tLoss: 0.149\n",
      "Epoch [122][0/12]\tLoss: 0.149\n",
      "Epoch [123][0/12]\tLoss: 0.170\n",
      "Epoch [124][0/12]\tLoss: 0.169\n",
      "Epoch [125][0/12]\tLoss: 0.164\n",
      "Epoch [126][0/12]\tLoss: 0.162\n",
      "Epoch [127][0/12]\tLoss: 0.154\n",
      "Epoch [128][0/12]\tLoss: 0.162\n",
      "Epoch [129][0/12]\tLoss: 0.139\n",
      "Epoch [130][0/12]\tLoss: 0.148\n",
      "Epoch [131][0/12]\tLoss: 0.139\n",
      "Epoch [132][0/12]\tLoss: 0.172\n",
      "Epoch [133][0/12]\tLoss: 0.148\n",
      "Epoch [134][0/12]\tLoss: 0.158\n",
      "Epoch [135][0/12]\tLoss: 0.142\n",
      "Epoch [136][0/12]\tLoss: 0.136\n",
      "Epoch [137][0/12]\tLoss: 0.145\n",
      "Epoch [138][0/12]\tLoss: 0.162\n",
      "Epoch [139][0/12]\tLoss: 0.146\n",
      "Epoch [140][0/12]\tLoss: 0.151\n",
      "Epoch [141][0/12]\tLoss: 0.154\n",
      "Epoch [142][0/12]\tLoss: 0.132\n",
      "Epoch [143][0/12]\tLoss: 0.159\n",
      "Epoch [144][0/12]\tLoss: 0.144\n",
      "Epoch [145][0/12]\tLoss: 0.158\n",
      "Epoch [146][0/12]\tLoss: 0.126\n",
      "Epoch [147][0/12]\tLoss: 0.138\n",
      "Epoch [148][0/12]\tLoss: 0.133\n",
      "Epoch [149][0/12]\tLoss: 0.118\n",
      "[neptune] [info   ] Shutting down background jobs, please wait a moment...\n",
      "[neptune] [info   ] Done!\n",
      "[neptune] [info   ] All 0 operations synced, thanks for waiting!\n",
      "[neptune] [info   ] Explore the metadata in the Neptune app: https://app.neptune.ai/andialifs/siet-24/e/SIET-109/metadata\n",
      "\n",
      "Running for experiment 85 with d_model 2048, heads8, num_layers_enc 3, num_layers_dec 3\n",
      "\n",
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/andialifs/siet-24/e/SIET-110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][0/12]\tLoss: 6.006\n",
      "Epoch [1][0/12]\tLoss: 5.867\n",
      "Epoch [2][0/12]\tLoss: 5.672\n",
      "Epoch [3][0/12]\tLoss: 5.380\n",
      "Epoch [4][0/12]\tLoss: 5.154\n",
      "Epoch [5][0/12]\tLoss: 4.991\n",
      "Epoch [6][0/12]\tLoss: 4.841\n",
      "Epoch [7][0/12]\tLoss: 4.725\n",
      "Epoch [8][0/12]\tLoss: 4.556\n",
      "Epoch [9][0/12]\tLoss: 4.720\n",
      "Epoch [10][0/12]\tLoss: 4.515\n",
      "Epoch [11][0/12]\tLoss: 4.423\n",
      "Epoch [12][0/12]\tLoss: 4.257\n",
      "Epoch [13][0/12]\tLoss: 4.292\n",
      "Epoch [14][0/12]\tLoss: 3.987\n",
      "Epoch [15][0/12]\tLoss: 3.969\n",
      "Epoch [16][0/12]\tLoss: 4.090\n",
      "Epoch [17][0/12]\tLoss: 3.887\n",
      "Epoch [18][0/12]\tLoss: 3.555\n",
      "Epoch [19][0/12]\tLoss: 3.691\n",
      "Epoch [20][0/12]\tLoss: 3.385\n",
      "Epoch [21][0/12]\tLoss: 3.533\n",
      "Epoch [22][0/12]\tLoss: 3.178\n",
      "Epoch [23][0/12]\tLoss: 2.939\n",
      "Epoch [24][0/12]\tLoss: 2.981\n",
      "Epoch [25][0/12]\tLoss: 2.847\n",
      "Epoch [26][0/12]\tLoss: 2.829\n",
      "Epoch [27][0/12]\tLoss: 2.330\n",
      "Epoch [28][0/12]\tLoss: 2.638\n",
      "Epoch [29][0/12]\tLoss: 2.362\n",
      "Epoch [30][0/12]\tLoss: 2.586\n",
      "Epoch [31][0/12]\tLoss: 2.250\n",
      "Epoch [32][0/12]\tLoss: 2.378\n",
      "Epoch [33][0/12]\tLoss: 2.061\n",
      "Epoch [34][0/12]\tLoss: 1.588\n",
      "Epoch [35][0/12]\tLoss: 1.800\n",
      "Epoch [36][0/12]\tLoss: 2.062\n",
      "Epoch [37][0/12]\tLoss: 1.772\n",
      "Epoch [38][0/12]\tLoss: 1.814\n",
      "Epoch [39][0/12]\tLoss: 1.598\n",
      "Epoch [40][0/12]\tLoss: 1.577\n",
      "Epoch [41][0/12]\tLoss: 1.387\n",
      "Epoch [42][0/12]\tLoss: 1.352\n",
      "Epoch [43][0/12]\tLoss: 1.304\n",
      "Epoch [44][0/12]\tLoss: 1.309\n",
      "Epoch [45][0/12]\tLoss: 1.407\n",
      "Epoch [46][0/12]\tLoss: 1.172\n",
      "Epoch [47][0/12]\tLoss: 1.365\n",
      "Epoch [48][0/12]\tLoss: 1.160\n",
      "Epoch [49][0/12]\tLoss: 0.967\n",
      "Epoch [50][0/12]\tLoss: 0.795\n",
      "Epoch [51][0/12]\tLoss: 0.714\n",
      "Epoch [52][0/12]\tLoss: 0.807\n",
      "Epoch [53][0/12]\tLoss: 0.917\n",
      "Epoch [54][0/12]\tLoss: 0.800\n",
      "Epoch [55][0/12]\tLoss: 0.789\n",
      "Epoch [56][0/12]\tLoss: 0.648\n",
      "Epoch [57][0/12]\tLoss: 0.574\n",
      "Epoch [58][0/12]\tLoss: 0.541\n",
      "Epoch [59][0/12]\tLoss: 0.664\n",
      "Epoch [60][0/12]\tLoss: 0.504\n",
      "Epoch [61][0/12]\tLoss: 0.521\n",
      "Epoch [62][0/12]\tLoss: 0.482\n",
      "Epoch [63][0/12]\tLoss: 0.547\n",
      "Epoch [64][0/12]\tLoss: 0.431\n",
      "Epoch [65][0/12]\tLoss: 0.440\n",
      "Epoch [66][0/12]\tLoss: 0.353\n",
      "Epoch [67][0/12]\tLoss: 0.410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][0/12]\tLoss: 5.987\n",
      "Epoch [1][0/12]\tLoss: 5.885\n",
      "Epoch [2][0/12]\tLoss: 5.655\n",
      "Epoch [3][0/12]\tLoss: 5.350\n",
      "Epoch [4][0/12]\tLoss: 5.223\n",
      "Epoch [5][0/12]\tLoss: 5.026\n",
      "Epoch [6][0/12]\tLoss: 4.845\n",
      "Epoch [7][0/12]\tLoss: 4.747\n",
      "Epoch [8][0/12]\tLoss: 4.801\n",
      "Epoch [9][0/12]\tLoss: 4.493\n",
      "Epoch [10][0/12]\tLoss: 4.520\n",
      "Epoch [11][0/12]\tLoss: 4.387\n",
      "Epoch [12][0/12]\tLoss: 4.459\n",
      "Epoch [13][0/12]\tLoss: 4.127\n",
      "Epoch [14][0/12]\tLoss: 4.229\n",
      "Epoch [15][0/12]\tLoss: 4.017\n",
      "Epoch [16][0/12]\tLoss: 3.757\n",
      "Epoch [17][0/12]\tLoss: 3.837\n",
      "Epoch [18][0/12]\tLoss: 3.620\n",
      "Epoch [19][0/12]\tLoss: 3.081\n",
      "Epoch [20][0/12]\tLoss: 3.390\n",
      "Epoch [21][0/12]\tLoss: 3.202\n",
      "Epoch [22][0/12]\tLoss: 3.254\n",
      "Epoch [23][0/12]\tLoss: 3.179\n",
      "Epoch [24][0/12]\tLoss: 2.803\n",
      "Epoch [25][0/12]\tLoss: 3.112\n",
      "Epoch [26][0/12]\tLoss: 2.873\n",
      "Epoch [27][0/12]\tLoss: 2.748\n",
      "Epoch [28][0/12]\tLoss: 2.149\n",
      "Epoch [29][0/12]\tLoss: 2.450\n",
      "Epoch [30][0/12]\tLoss: 2.403\n",
      "Epoch [31][0/12]\tLoss: 2.381\n",
      "Epoch [32][0/12]\tLoss: 2.235\n",
      "Epoch [33][0/12]\tLoss: 2.018\n",
      "Epoch [34][0/12]\tLoss: 1.955\n",
      "Epoch [35][0/12]\tLoss: 2.123\n",
      "Epoch [36][0/12]\tLoss: 1.713\n",
      "Epoch [37][0/12]\tLoss: 1.766\n",
      "Epoch [38][0/12]\tLoss: 1.663\n",
      "Epoch [39][0/12]\tLoss: 1.376\n",
      "Epoch [40][0/12]\tLoss: 1.453\n",
      "Epoch [41][0/12]\tLoss: 1.468\n",
      "Epoch [42][0/12]\tLoss: 1.518\n",
      "Epoch [43][0/12]\tLoss: 1.276\n",
      "Epoch [44][0/12]\tLoss: 1.188\n",
      "Epoch [45][0/12]\tLoss: 1.254\n",
      "Epoch [46][0/12]\tLoss: 1.204\n",
      "Epoch [47][0/12]\tLoss: 1.013\n",
      "Epoch [48][0/12]\tLoss: 0.930\n",
      "Epoch [49][0/12]\tLoss: 0.882\n",
      "Epoch [50][0/12]\tLoss: 0.934\n",
      "Epoch [51][0/12]\tLoss: 0.935\n",
      "Epoch [52][0/12]\tLoss: 0.807\n",
      "Epoch [53][0/12]\tLoss: 0.791\n",
      "Epoch [54][0/12]\tLoss: 0.754\n",
      "Epoch [55][0/12]\tLoss: 0.720\n",
      "Epoch [56][0/12]\tLoss: 0.620\n",
      "Epoch [57][0/12]\tLoss: 0.573\n",
      "Epoch [58][0/12]\tLoss: 0.668\n",
      "Epoch [59][0/12]\tLoss: 0.547\n",
      "Epoch [60][0/12]\tLoss: 0.514\n",
      "Epoch [61][0/12]\tLoss: 0.601\n",
      "Epoch [62][0/12]\tLoss: 0.506\n",
      "Epoch [63][0/12]\tLoss: 0.572\n",
      "Epoch [64][0/12]\tLoss: 0.513\n",
      "Epoch [65][0/12]\tLoss: 0.411\n",
      "Epoch [66][0/12]\tLoss: 0.435\n",
      "Epoch [67][0/12]\tLoss: 0.378\n",
      "Epoch [68][0/12]\tLoss: 0.323\n",
      "Epoch [69][0/12]\tLoss: 0.407\n",
      "Epoch [70][0/12]\tLoss: 0.379\n",
      "Epoch [71][0/12]\tLoss: 0.324\n",
      "Epoch [72][0/12]\tLoss: 0.320\n",
      "Epoch [73][0/12]\tLoss: 0.311\n",
      "Epoch [74][0/12]\tLoss: 0.282\n",
      "Epoch [75][0/12]\tLoss: 0.302\n",
      "Epoch [76][0/12]\tLoss: 0.256\n",
      "Epoch [77][0/12]\tLoss: 0.280\n",
      "Epoch [78][0/12]\tLoss: 0.283\n",
      "Epoch [79][0/12]\tLoss: 0.297\n",
      "Epoch [80][0/12]\tLoss: 0.265\n",
      "Epoch [81][0/12]\tLoss: 0.264\n",
      "Epoch [82][0/12]\tLoss: 0.274\n",
      "Epoch [83][0/12]\tLoss: 0.244\n",
      "Epoch [84][0/12]\tLoss: 0.238\n",
      "Epoch [85][0/12]\tLoss: 0.228\n",
      "Epoch [86][0/12]\tLoss: 0.231\n",
      "Epoch [87][0/12]\tLoss: 0.195\n",
      "Epoch [88][0/12]\tLoss: 0.249\n",
      "Epoch [89][0/12]\tLoss: 0.258\n",
      "Epoch [90][0/12]\tLoss: 0.223\n",
      "Epoch [91][0/12]\tLoss: 0.211\n",
      "Epoch [92][0/12]\tLoss: 0.202\n",
      "Epoch [93][0/12]\tLoss: 0.224\n",
      "Epoch [94][0/12]\tLoss: 0.203\n",
      "Epoch [95][0/12]\tLoss: 0.193\n",
      "Epoch [96][0/12]\tLoss: 0.201\n",
      "Epoch [97][0/12]\tLoss: 0.201\n",
      "Epoch [98][0/12]\tLoss: 0.185\n",
      "Epoch [99][0/12]\tLoss: 0.188\n",
      "Epoch [100][0/12]\tLoss: 0.213\n",
      "Epoch [101][0/12]\tLoss: 0.198\n",
      "Epoch [102][0/12]\tLoss: 0.202\n",
      "Epoch [103][0/12]\tLoss: 0.177\n",
      "Epoch [104][0/12]\tLoss: 0.204\n",
      "Epoch [105][0/12]\tLoss: 0.166\n",
      "Epoch [106][0/12]\tLoss: 0.176\n",
      "Epoch [107][0/12]\tLoss: 0.183\n",
      "Epoch [108][0/12]\tLoss: 0.168\n",
      "Epoch [109][0/12]\tLoss: 0.166\n",
      "Epoch [110][0/12]\tLoss: 0.182\n",
      "Epoch [111][0/12]\tLoss: 0.175\n",
      "Epoch [112][0/12]\tLoss: 0.174\n",
      "Epoch [113][0/12]\tLoss: 0.170\n",
      "Epoch [114][0/12]\tLoss: 0.172\n",
      "Epoch [115][0/12]\tLoss: 0.153\n",
      "Epoch [116][0/12]\tLoss: 0.162\n",
      "Epoch [117][0/12]\tLoss: 0.162\n",
      "Epoch [118][0/12]\tLoss: 0.167\n",
      "Epoch [119][0/12]\tLoss: 0.145\n",
      "Epoch [120][0/12]\tLoss: 0.157\n",
      "Epoch [121][0/12]\tLoss: 0.159\n",
      "Epoch [122][0/12]\tLoss: 0.180\n",
      "Epoch [123][0/12]\tLoss: 0.155\n",
      "Epoch [124][0/12]\tLoss: 0.172\n",
      "Epoch [125][0/12]\tLoss: 0.151\n",
      "Epoch [126][0/12]\tLoss: 0.155\n",
      "Epoch [127][0/12]\tLoss: 0.158\n",
      "Epoch [128][0/12]\tLoss: 0.146\n",
      "Epoch [129][0/12]\tLoss: 0.166\n",
      "Epoch [130][0/12]\tLoss: 0.151\n",
      "Epoch [131][0/12]\tLoss: 0.153\n",
      "Epoch [132][0/12]\tLoss: 0.137\n",
      "Epoch [133][0/12]\tLoss: 0.176\n",
      "Epoch [134][0/12]\tLoss: 0.163\n",
      "Epoch [135][0/12]\tLoss: 0.155\n",
      "Epoch [136][0/12]\tLoss: 0.152\n",
      "Epoch [137][0/12]\tLoss: 0.147\n",
      "Epoch [138][0/12]\tLoss: 0.138\n",
      "Epoch [139][0/12]\tLoss: 0.155\n",
      "Epoch [140][0/12]\tLoss: 0.121\n",
      "Epoch [141][0/12]\tLoss: 0.134\n",
      "Epoch [142][0/12]\tLoss: 0.125\n",
      "Epoch [143][0/12]\tLoss: 0.145\n",
      "Epoch [144][0/12]\tLoss: 0.132\n",
      "Epoch [145][0/12]\tLoss: 0.142\n",
      "Epoch [146][0/12]\tLoss: 0.147\n",
      "Epoch [147][0/12]\tLoss: 0.130\n",
      "Epoch [148][0/12]\tLoss: 0.128\n",
      "Epoch [149][0/12]\tLoss: 0.129\n",
      "[neptune] [info   ] Shutting down background jobs, please wait a moment...\n",
      "[neptune] [info   ] Done!\n",
      "[neptune] [info   ] All 0 operations synced, thanks for waiting!\n",
      "[neptune] [info   ] Explore the metadata in the Neptune app: https://app.neptune.ai/andialifs/siet-24/e/SIET-111/metadata\n",
      "\n",
      "Running for experiment 87 with d_model 2048, heads8, num_layers_enc 5, num_layers_dec 1\n",
      "\n",
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/andialifs/siet-24/e/SIET-112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][0/12]\tLoss: 5.960\n",
      "Epoch [1][0/12]\tLoss: 5.839\n",
      "Epoch [2][0/12]\tLoss: 5.568\n",
      "Epoch [3][0/12]\tLoss: 5.265\n",
      "Epoch [4][0/12]\tLoss: 5.195\n",
      "Epoch [5][0/12]\tLoss: 4.866\n",
      "Epoch [6][0/12]\tLoss: 4.842\n",
      "Epoch [7][0/12]\tLoss: 4.800\n",
      "Epoch [8][0/12]\tLoss: 4.725\n",
      "Epoch [9][0/12]\tLoss: 4.641\n",
      "Epoch [10][0/12]\tLoss: 4.764\n",
      "Epoch [11][0/12]\tLoss: 4.752\n",
      "Epoch [12][0/12]\tLoss: 4.461\n",
      "Epoch [13][0/12]\tLoss: 4.695\n",
      "Epoch [14][0/12]\tLoss: 4.458\n",
      "Epoch [15][0/12]\tLoss: 4.470\n",
      "Epoch [16][0/12]\tLoss: 4.240\n",
      "Epoch [17][0/12]\tLoss: 4.149\n",
      "Epoch [18][0/12]\tLoss: 4.108\n",
      "Epoch [19][0/12]\tLoss: 3.873\n",
      "Epoch [20][0/12]\tLoss: 3.821\n",
      "Epoch [21][0/12]\tLoss: 3.916\n",
      "Epoch [22][0/12]\tLoss: 3.755\n",
      "Epoch [23][0/12]\tLoss: 3.522\n",
      "Epoch [24][0/12]\tLoss: 3.384\n",
      "Epoch [25][0/12]\tLoss: 3.390\n",
      "Epoch [26][0/12]\tLoss: 3.518\n",
      "Epoch [27][0/12]\tLoss: 3.242\n",
      "Epoch [28][0/12]\tLoss: 3.217\n",
      "Epoch [29][0/12]\tLoss: 3.272\n",
      "Epoch [30][0/12]\tLoss: 2.765\n",
      "Epoch [31][0/12]\tLoss: 2.596\n",
      "Epoch [32][0/12]\tLoss: 2.646\n",
      "Epoch [33][0/12]\tLoss: 2.503\n",
      "Epoch [34][0/12]\tLoss: 2.536\n",
      "Epoch [35][0/12]\tLoss: 2.368\n",
      "Epoch [36][0/12]\tLoss: 2.315\n",
      "Epoch [37][0/12]\tLoss: 2.243\n",
      "Epoch [38][0/12]\tLoss: 2.174\n",
      "Epoch [39][0/12]\tLoss: 2.465\n",
      "Epoch [40][0/12]\tLoss: 2.075\n",
      "Epoch [41][0/12]\tLoss: 1.809\n",
      "Epoch [42][0/12]\tLoss: 1.494\n",
      "Epoch [43][0/12]\tLoss: 1.779\n",
      "Epoch [44][0/12]\tLoss: 2.018\n",
      "Epoch [45][0/12]\tLoss: 1.816\n",
      "Epoch [46][0/12]\tLoss: 1.809\n",
      "Epoch [47][0/12]\tLoss: 1.742\n",
      "Epoch [48][0/12]\tLoss: 1.488\n",
      "Epoch [49][0/12]\tLoss: 1.605\n",
      "Epoch [50][0/12]\tLoss: 1.453\n",
      "Epoch [51][0/12]\tLoss: 1.482\n",
      "Epoch [52][0/12]\tLoss: 1.539\n",
      "Epoch [53][0/12]\tLoss: 1.410\n",
      "Epoch [54][0/12]\tLoss: 0.959\n",
      "Epoch [55][0/12]\tLoss: 1.099\n",
      "Epoch [56][0/12]\tLoss: 1.255\n",
      "Epoch [57][0/12]\tLoss: 1.204\n",
      "Epoch [58][0/12]\tLoss: 1.185\n",
      "Epoch [59][0/12]\tLoss: 1.219\n",
      "Epoch [60][0/12]\tLoss: 0.909\n",
      "Epoch [61][0/12]\tLoss: 1.036\n",
      "Epoch [62][0/12]\tLoss: 1.000\n",
      "Epoch [63][0/12]\tLoss: 0.853\n",
      "Epoch [64][0/12]\tLoss: 0.785\n",
      "Epoch [65][0/12]\tLoss: 0.756\n",
      "Epoch [66][0/12]\tLoss: 0.791\n",
      "Epoch [67][0/12]\tLoss: 0.931\n",
      "Epoch [68][0/12]\tLoss: 0.690\n",
      "Epoch [69][0/12]\tLoss: 0.636\n",
      "Epoch [70][0/12]\tLoss: 0.812\n",
      "Epoch [71][0/12]\tLoss: 0.690\n",
      "Epoch [72][0/12]\tLoss: 0.686\n",
      "Epoch [73][0/12]\tLoss: 0.600\n",
      "Epoch [74][0/12]\tLoss: 0.670\n",
      "Epoch [75][0/12]\tLoss: 0.516\n",
      "Epoch [76][0/12]\tLoss: 0.484\n",
      "Epoch [77][0/12]\tLoss: 0.538\n",
      "Epoch [78][0/12]\tLoss: 0.462\n",
      "Epoch [79][0/12]\tLoss: 0.434\n",
      "Epoch [80][0/12]\tLoss: 0.429\n",
      "Epoch [81][0/12]\tLoss: 0.419\n",
      "Epoch [82][0/12]\tLoss: 0.473\n",
      "Epoch [83][0/12]\tLoss: 0.428\n",
      "Epoch [84][0/12]\tLoss: 0.420\n",
      "Epoch [85][0/12]\tLoss: 0.364\n",
      "Epoch [86][0/12]\tLoss: 0.405\n",
      "Epoch [87][0/12]\tLoss: 0.399\n",
      "Epoch [88][0/12]\tLoss: 0.353\n",
      "Epoch [89][0/12]\tLoss: 0.364\n",
      "Epoch [90][0/12]\tLoss: 0.349\n",
      "Epoch [91][0/12]\tLoss: 0.344\n",
      "Epoch [92][0/12]\tLoss: 0.308\n",
      "Epoch [93][0/12]\tLoss: 0.322\n",
      "Epoch [94][0/12]\tLoss: 0.329\n",
      "Epoch [95][0/12]\tLoss: 0.298\n",
      "Epoch [96][0/12]\tLoss: 0.290\n",
      "Epoch [97][0/12]\tLoss: 0.324\n",
      "Epoch [98][0/12]\tLoss: 0.293\n",
      "Epoch [99][0/12]\tLoss: 0.295\n",
      "Epoch [100][0/12]\tLoss: 0.295\n",
      "Epoch [101][0/12]\tLoss: 0.309\n",
      "Epoch [102][0/12]\tLoss: 0.276\n",
      "Epoch [103][0/12]\tLoss: 0.263\n",
      "Epoch [104][0/12]\tLoss: 0.260\n",
      "Epoch [105][0/12]\tLoss: 0.252\n",
      "Epoch [106][0/12]\tLoss: 0.242\n",
      "Epoch [107][0/12]\tLoss: 0.275\n",
      "Epoch [108][0/12]\tLoss: 0.265\n",
      "Epoch [109][0/12]\tLoss: 0.257\n",
      "Epoch [110][0/12]\tLoss: 0.259\n",
      "Epoch [111][0/12]\tLoss: 0.237\n",
      "Epoch [112][0/12]\tLoss: 0.238\n",
      "Epoch [113][0/12]\tLoss: 0.245\n",
      "Epoch [114][0/12]\tLoss: 0.246\n",
      "Epoch [115][0/12]\tLoss: 0.229\n",
      "Epoch [116][0/12]\tLoss: 0.230\n",
      "Epoch [117][0/12]\tLoss: 0.226\n",
      "Epoch [118][0/12]\tLoss: 0.223\n",
      "Epoch [119][0/12]\tLoss: 0.219\n",
      "Epoch [120][0/12]\tLoss: 0.248\n",
      "Epoch [121][0/12]\tLoss: 0.230\n",
      "Epoch [122][0/12]\tLoss: 0.230\n",
      "Epoch [123][0/12]\tLoss: 0.211\n",
      "Epoch [124][0/12]\tLoss: 0.232\n",
      "Epoch [125][0/12]\tLoss: 0.210\n",
      "Epoch [126][0/12]\tLoss: 0.188\n",
      "Epoch [127][0/12]\tLoss: 0.217\n",
      "Epoch [128][0/12]\tLoss: 0.237\n",
      "Epoch [129][0/12]\tLoss: 0.176\n",
      "Epoch [130][0/12]\tLoss: 0.199\n",
      "Epoch [131][0/12]\tLoss: 0.185\n",
      "Epoch [132][0/12]\tLoss: 0.170\n",
      "Epoch [133][0/12]\tLoss: 0.200\n",
      "Epoch [134][0/12]\tLoss: 0.190\n",
      "Epoch [135][0/12]\tLoss: 0.194\n",
      "Epoch [136][0/12]\tLoss: 0.185\n",
      "Epoch [137][0/12]\tLoss: 0.176\n",
      "Epoch [138][0/12]\tLoss: 0.169\n",
      "Epoch [139][0/12]\tLoss: 0.190\n",
      "Epoch [140][0/12]\tLoss: 0.173\n",
      "Epoch [141][0/12]\tLoss: 0.195\n",
      "Epoch [142][0/12]\tLoss: 0.186\n",
      "Epoch [143][0/12]\tLoss: 0.189\n",
      "Epoch [144][0/12]\tLoss: 0.185\n",
      "Epoch [145][0/12]\tLoss: 0.162\n",
      "Epoch [146][0/12]\tLoss: 0.181\n",
      "Epoch [147][0/12]\tLoss: 0.165\n",
      "Epoch [148][0/12]\tLoss: 0.169\n",
      "Epoch [149][0/12]\tLoss: 0.190\n",
      "[neptune] [info   ] Shutting down background jobs, please wait a moment...\n",
      "[neptune] [info   ] Done!\n",
      "[neptune] [info   ] All 0 operations synced, thanks for waiting!\n",
      "[neptune] [info   ] Explore the metadata in the Neptune app: https://app.neptune.ai/andialifs/siet-24/e/SIET-112/metadata\n",
      "\n",
      "Running for experiment 88 with d_model 2048, heads8, num_layers_enc 5, num_layers_dec 3\n",
      "\n",
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/andialifs/siet-24/e/SIET-113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][0/12]\tLoss: 5.919\n",
      "Epoch [1][0/12]\tLoss: 5.793\n",
      "Epoch [2][0/12]\tLoss: 5.534\n",
      "Epoch [3][0/12]\tLoss: 5.313\n",
      "Epoch [4][0/12]\tLoss: 5.108\n",
      "Epoch [5][0/12]\tLoss: 5.010\n",
      "Epoch [6][0/12]\tLoss: 4.878\n",
      "Epoch [7][0/12]\tLoss: 4.856\n",
      "Epoch [8][0/12]\tLoss: 4.802\n",
      "Epoch [9][0/12]\tLoss: 4.673\n",
      "Epoch [10][0/12]\tLoss: 4.659\n",
      "Epoch [11][0/12]\tLoss: 4.666\n",
      "Epoch [12][0/12]\tLoss: 4.618\n",
      "Epoch [13][0/12]\tLoss: 4.562\n",
      "Epoch [14][0/12]\tLoss: 4.616\n",
      "Epoch [15][0/12]\tLoss: 4.299\n",
      "Epoch [16][0/12]\tLoss: 4.428\n",
      "Epoch [17][0/12]\tLoss: 4.101\n",
      "Epoch [18][0/12]\tLoss: 3.760\n",
      "Epoch [19][0/12]\tLoss: 3.864\n",
      "Epoch [20][0/12]\tLoss: 3.968\n",
      "Epoch [21][0/12]\tLoss: 3.801\n",
      "Epoch [22][0/12]\tLoss: 3.699\n",
      "Epoch [23][0/12]\tLoss: 3.414\n",
      "Epoch [24][0/12]\tLoss: 3.424\n",
      "Epoch [25][0/12]\tLoss: 3.547\n",
      "Epoch [26][0/12]\tLoss: 3.196\n",
      "Epoch [27][0/12]\tLoss: 3.343\n",
      "Epoch [28][0/12]\tLoss: 3.059\n",
      "Epoch [29][0/12]\tLoss: 3.063\n",
      "Epoch [30][0/12]\tLoss: 2.771\n",
      "Epoch [31][0/12]\tLoss: 2.601\n",
      "Epoch [32][0/12]\tLoss: 2.793\n",
      "Epoch [33][0/12]\tLoss: 2.378\n",
      "Epoch [34][0/12]\tLoss: 2.560\n",
      "Epoch [35][0/12]\tLoss: 2.495\n",
      "Epoch [36][0/12]\tLoss: 2.469\n",
      "Epoch [37][0/12]\tLoss: 2.357\n",
      "Epoch [38][0/12]\tLoss: 2.181\n",
      "Epoch [39][0/12]\tLoss: 2.389\n",
      "Epoch [40][0/12]\tLoss: 2.038\n",
      "Epoch [41][0/12]\tLoss: 2.024\n",
      "Epoch [42][0/12]\tLoss: 1.946\n",
      "Epoch [43][0/12]\tLoss: 2.082\n",
      "Epoch [44][0/12]\tLoss: 1.995\n",
      "Epoch [45][0/12]\tLoss: 2.111\n",
      "Epoch [46][0/12]\tLoss: 1.755\n",
      "Epoch [47][0/12]\tLoss: 1.883\n",
      "Epoch [48][0/12]\tLoss: 1.544\n",
      "Epoch [49][0/12]\tLoss: 1.387\n",
      "Epoch [50][0/12]\tLoss: 1.430\n",
      "Epoch [51][0/12]\tLoss: 1.303\n",
      "Epoch [52][0/12]\tLoss: 1.542\n",
      "Epoch [53][0/12]\tLoss: 1.021\n",
      "Epoch [54][0/12]\tLoss: 1.357\n",
      "Epoch [55][0/12]\tLoss: 1.251\n",
      "Epoch [56][0/12]\tLoss: 1.191\n",
      "Epoch [57][0/12]\tLoss: 1.093\n",
      "Epoch [58][0/12]\tLoss: 1.166\n",
      "Epoch [59][0/12]\tLoss: 0.974\n",
      "Epoch [60][0/12]\tLoss: 0.978\n",
      "Epoch [61][0/12]\tLoss: 0.939\n",
      "Epoch [62][0/12]\tLoss: 0.873\n",
      "Epoch [63][0/12]\tLoss: 1.006\n",
      "Epoch [64][0/12]\tLoss: 0.837\n",
      "Epoch [65][0/12]\tLoss: 0.703\n",
      "Epoch [66][0/12]\tLoss: 0.801\n",
      "Epoch [67][0/12]\tLoss: 0.822\n",
      "Epoch [68][0/12]\tLoss: 0.787\n",
      "Epoch [69][0/12]\tLoss: 0.615\n",
      "Epoch [70][0/12]\tLoss: 0.612\n",
      "Epoch [71][0/12]\tLoss: 0.618\n",
      "Epoch [72][0/12]\tLoss: 0.606\n",
      "Epoch [73][0/12]\tLoss: 0.569\n",
      "Epoch [74][0/12]\tLoss: 0.525\n",
      "Epoch [75][0/12]\tLoss: 0.597\n",
      "Epoch [76][0/12]\tLoss: 0.500\n",
      "Epoch [77][0/12]\tLoss: 0.533\n",
      "Epoch [78][0/12]\tLoss: 0.558\n",
      "Epoch [79][0/12]\tLoss: 0.473\n",
      "Epoch [80][0/12]\tLoss: 0.469\n",
      "Epoch [81][0/12]\tLoss: 0.460\n",
      "Epoch [82][0/12]\tLoss: 0.437\n",
      "Epoch [83][0/12]\tLoss: 0.437\n",
      "Epoch [84][0/12]\tLoss: 0.447\n",
      "Epoch [85][0/12]\tLoss: 0.365\n",
      "Epoch [86][0/12]\tLoss: 0.389\n",
      "Epoch [87][0/12]\tLoss: 0.359\n",
      "Epoch [88][0/12]\tLoss: 0.345\n",
      "Epoch [89][0/12]\tLoss: 0.371\n",
      "Epoch [90][0/12]\tLoss: 0.326\n",
      "Epoch [91][0/12]\tLoss: 0.342\n",
      "Epoch [92][0/12]\tLoss: 0.359\n",
      "Epoch [93][0/12]\tLoss: 0.298\n",
      "Epoch [94][0/12]\tLoss: 0.353\n",
      "Epoch [95][0/12]\tLoss: 0.319\n",
      "Epoch [96][0/12]\tLoss: 0.269\n",
      "Epoch [97][0/12]\tLoss: 0.296\n",
      "Epoch [98][0/12]\tLoss: 0.287\n",
      "Epoch [99][0/12]\tLoss: 0.280\n",
      "Epoch [100][0/12]\tLoss: 0.287\n",
      "Epoch [101][0/12]\tLoss: 0.268\n",
      "Epoch [102][0/12]\tLoss: 0.263\n",
      "Epoch [103][0/12]\tLoss: 0.252\n",
      "Epoch [104][0/12]\tLoss: 0.272\n",
      "Epoch [105][0/12]\tLoss: 0.255\n",
      "Epoch [106][0/12]\tLoss: 0.256\n",
      "Epoch [107][0/12]\tLoss: 0.265\n",
      "Epoch [108][0/12]\tLoss: 0.234\n",
      "Epoch [109][0/12]\tLoss: 0.239\n",
      "Epoch [110][0/12]\tLoss: 0.240\n",
      "Epoch [111][0/12]\tLoss: 0.252\n",
      "Epoch [112][0/12]\tLoss: 0.245\n",
      "Epoch [113][0/12]\tLoss: 0.242\n",
      "Epoch [114][0/12]\tLoss: 0.219\n",
      "Epoch [115][0/12]\tLoss: 0.226\n",
      "Epoch [116][0/12]\tLoss: 0.220\n",
      "Epoch [117][0/12]\tLoss: 0.215\n",
      "Epoch [118][0/12]\tLoss: 0.232\n",
      "Epoch [119][0/12]\tLoss: 0.236\n",
      "Epoch [120][0/12]\tLoss: 0.232\n",
      "Epoch [121][0/12]\tLoss: 0.199\n",
      "Epoch [122][0/12]\tLoss: 0.199\n",
      "Epoch [123][0/12]\tLoss: 0.200\n",
      "Epoch [124][0/12]\tLoss: 0.193\n",
      "Epoch [125][0/12]\tLoss: 0.207\n",
      "Epoch [126][0/12]\tLoss: 0.211\n",
      "Epoch [127][0/12]\tLoss: 0.217\n",
      "Epoch [128][0/12]\tLoss: 0.208\n",
      "Epoch [129][0/12]\tLoss: 0.178\n",
      "Epoch [130][0/12]\tLoss: 0.208\n",
      "Epoch [131][0/12]\tLoss: 0.204\n",
      "Epoch [132][0/12]\tLoss: 0.189\n",
      "Epoch [133][0/12]\tLoss: 0.209\n",
      "Epoch [134][0/12]\tLoss: 0.197\n",
      "Epoch [135][0/12]\tLoss: 0.195\n",
      "Epoch [136][0/12]\tLoss: 0.178\n",
      "Epoch [137][0/12]\tLoss: 0.206\n",
      "Epoch [138][0/12]\tLoss: 0.186\n",
      "Epoch [139][0/12]\tLoss: 0.217\n",
      "Epoch [140][0/12]\tLoss: 0.180\n",
      "Epoch [141][0/12]\tLoss: 0.172\n",
      "Epoch [142][0/12]\tLoss: 0.161\n",
      "Epoch [143][0/12]\tLoss: 0.162\n",
      "Epoch [144][0/12]\tLoss: 0.181\n",
      "Epoch [145][0/12]\tLoss: 0.162\n",
      "Epoch [146][0/12]\tLoss: 0.167\n",
      "Epoch [147][0/12]\tLoss: 0.165\n",
      "Epoch [148][0/12]\tLoss: 0.164\n",
      "Epoch [149][0/12]\tLoss: 0.192\n",
      "[neptune] [info   ] Shutting down background jobs, please wait a moment...\n",
      "[neptune] [info   ] Done!\n",
      "[neptune] [info   ] All 0 operations synced, thanks for waiting!\n",
      "[neptune] [info   ] Explore the metadata in the Neptune app: https://app.neptune.ai/andialifs/siet-24/e/SIET-113/metadata\n",
      "\n",
      "Running for experiment 89 with d_model 2048, heads8, num_layers_enc 5, num_layers_dec 5\n",
      "\n",
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/andialifs/siet-24/e/SIET-114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][0/12]\tLoss: 5.869\n",
      "Epoch [1][0/12]\tLoss: 5.795\n",
      "Epoch [2][0/12]\tLoss: 5.518\n",
      "Epoch [3][0/12]\tLoss: 5.308\n",
      "Epoch [4][0/12]\tLoss: 5.135\n",
      "Epoch [5][0/12]\tLoss: 4.899\n",
      "Epoch [6][0/12]\tLoss: 4.799\n",
      "Epoch [7][0/12]\tLoss: 4.878\n",
      "Epoch [8][0/12]\tLoss: 4.700\n",
      "Epoch [9][0/12]\tLoss: 4.679\n",
      "Epoch [10][0/12]\tLoss: 4.535\n",
      "Epoch [11][0/12]\tLoss: 4.711\n",
      "Epoch [12][0/12]\tLoss: 4.548\n",
      "Epoch [13][0/12]\tLoss: 4.602\n",
      "Epoch [14][0/12]\tLoss: 4.388\n",
      "Epoch [15][0/12]\tLoss: 4.616\n",
      "Epoch [16][0/12]\tLoss: 4.394\n",
      "Epoch [17][0/12]\tLoss: 4.199\n",
      "Epoch [18][0/12]\tLoss: 3.922\n",
      "Epoch [19][0/12]\tLoss: 4.063\n",
      "Epoch [20][0/12]\tLoss: 3.915\n",
      "Epoch [21][0/12]\tLoss: 3.864\n",
      "Epoch [22][0/12]\tLoss: 3.798\n",
      "Epoch [23][0/12]\tLoss: 3.532\n",
      "Epoch [24][0/12]\tLoss: 3.649\n",
      "Epoch [25][0/12]\tLoss: 3.390\n",
      "Epoch [26][0/12]\tLoss: 3.180\n",
      "Epoch [27][0/12]\tLoss: 3.196\n",
      "Epoch [28][0/12]\tLoss: 3.100\n",
      "Epoch [29][0/12]\tLoss: 2.907\n",
      "Epoch [30][0/12]\tLoss: 2.677\n",
      "Epoch [31][0/12]\tLoss: 2.941\n",
      "Epoch [32][0/12]\tLoss: 2.892\n",
      "Epoch [33][0/12]\tLoss: 2.715\n",
      "Epoch [34][0/12]\tLoss: 2.451\n",
      "Epoch [35][0/12]\tLoss: 2.272\n",
      "Epoch [36][0/12]\tLoss: 2.722\n",
      "Epoch [37][0/12]\tLoss: 2.632\n",
      "Epoch [38][0/12]\tLoss: 2.229\n",
      "Epoch [39][0/12]\tLoss: 1.879\n",
      "Epoch [40][0/12]\tLoss: 2.011\n",
      "Epoch [41][0/12]\tLoss: 1.817\n",
      "Epoch [42][0/12]\tLoss: 2.082\n",
      "Epoch [43][0/12]\tLoss: 1.874\n",
      "Epoch [44][0/12]\tLoss: 1.958\n",
      "Epoch [45][0/12]\tLoss: 1.840\n",
      "Epoch [46][0/12]\tLoss: 1.697\n",
      "Epoch [47][0/12]\tLoss: 1.787\n",
      "Epoch [48][0/12]\tLoss: 1.751\n",
      "Epoch [49][0/12]\tLoss: 1.584\n",
      "Epoch [50][0/12]\tLoss: 1.403\n",
      "Epoch [51][0/12]\tLoss: 1.624\n",
      "Epoch [52][0/12]\tLoss: 1.257\n",
      "Epoch [53][0/12]\tLoss: 1.287\n",
      "Epoch [54][0/12]\tLoss: 1.193\n",
      "Epoch [55][0/12]\tLoss: 1.215\n",
      "Epoch [56][0/12]\tLoss: 1.140\n",
      "Epoch [57][0/12]\tLoss: 1.115\n",
      "Epoch [58][0/12]\tLoss: 1.185\n",
      "Epoch [59][0/12]\tLoss: 1.124\n",
      "Epoch [60][0/12]\tLoss: 1.215\n",
      "Epoch [61][0/12]\tLoss: 1.150\n",
      "Epoch [62][0/12]\tLoss: 0.965\n",
      "Epoch [63][0/12]\tLoss: 0.858\n",
      "Epoch [64][0/12]\tLoss: 0.855\n",
      "Epoch [65][0/12]\tLoss: 0.822\n",
      "Epoch [66][0/12]\tLoss: 0.999\n",
      "Epoch [67][0/12]\tLoss: 0.658\n",
      "Epoch [68][0/12]\tLoss: 0.757\n",
      "Epoch [69][0/12]\tLoss: 0.722\n",
      "Epoch [70][0/12]\tLoss: 0.621\n",
      "Epoch [71][0/12]\tLoss: 0.646\n",
      "Epoch [72][0/12]\tLoss: 0.621\n",
      "Epoch [73][0/12]\tLoss: 0.575\n",
      "Epoch [74][0/12]\tLoss: 0.575\n",
      "Epoch [75][0/12]\tLoss: 0.600\n",
      "Epoch [76][0/12]\tLoss: 0.550\n",
      "Epoch [77][0/12]\tLoss: 0.587\n",
      "Epoch [78][0/12]\tLoss: 0.559\n",
      "Epoch [79][0/12]\tLoss: 0.411\n",
      "Epoch [80][0/12]\tLoss: 0.450\n",
      "Epoch [81][0/12]\tLoss: 0.418\n",
      "Epoch [82][0/12]\tLoss: 0.417\n",
      "Epoch [83][0/12]\tLoss: 0.378\n",
      "Epoch [84][0/12]\tLoss: 0.386\n",
      "Epoch [85][0/12]\tLoss: 0.398\n",
      "Epoch [86][0/12]\tLoss: 0.367\n",
      "Epoch [87][0/12]\tLoss: 0.398\n",
      "Epoch [88][0/12]\tLoss: 0.366\n",
      "Epoch [89][0/12]\tLoss: 0.376\n",
      "Epoch [90][0/12]\tLoss: 0.337\n",
      "Epoch [91][0/12]\tLoss: 0.303\n",
      "Epoch [92][0/12]\tLoss: 0.304\n",
      "Epoch [93][0/12]\tLoss: 0.310\n",
      "Epoch [94][0/12]\tLoss: 0.311\n",
      "Epoch [95][0/12]\tLoss: 0.315\n",
      "Epoch [96][0/12]\tLoss: 0.314\n",
      "Epoch [97][0/12]\tLoss: 0.332\n",
      "Epoch [98][0/12]\tLoss: 0.297\n",
      "Epoch [99][0/12]\tLoss: 0.271\n",
      "Epoch [100][0/12]\tLoss: 0.295\n",
      "Epoch [101][0/12]\tLoss: 0.267\n",
      "Epoch [102][0/12]\tLoss: 0.280\n",
      "Epoch [103][0/12]\tLoss: 0.270\n",
      "Epoch [104][0/12]\tLoss: 0.261\n",
      "Epoch [105][0/12]\tLoss: 0.239\n",
      "Epoch [106][0/12]\tLoss: 0.275\n",
      "Epoch [107][0/12]\tLoss: 0.261\n",
      "Epoch [108][0/12]\tLoss: 0.269\n",
      "Epoch [109][0/12]\tLoss: 0.262\n",
      "Epoch [110][0/12]\tLoss: 0.260\n",
      "Epoch [111][0/12]\tLoss: 0.246\n",
      "Epoch [112][0/12]\tLoss: 0.227\n",
      "Epoch [113][0/12]\tLoss: 0.226\n",
      "Epoch [114][0/12]\tLoss: 0.226\n",
      "Epoch [115][0/12]\tLoss: 0.220\n",
      "Epoch [116][0/12]\tLoss: 0.218\n",
      "Epoch [117][0/12]\tLoss: 0.208\n",
      "Epoch [118][0/12]\tLoss: 0.239\n",
      "Epoch [119][0/12]\tLoss: 0.229\n",
      "Epoch [120][0/12]\tLoss: 0.224\n",
      "Epoch [121][0/12]\tLoss: 0.211\n",
      "Epoch [122][0/12]\tLoss: 0.217\n",
      "Epoch [123][0/12]\tLoss: 0.233\n",
      "Epoch [124][0/12]\tLoss: 0.209\n",
      "Epoch [125][0/12]\tLoss: 0.185\n",
      "Epoch [126][0/12]\tLoss: 0.214\n",
      "Epoch [127][0/12]\tLoss: 0.202\n",
      "Epoch [128][0/12]\tLoss: 0.206\n",
      "Epoch [129][0/12]\tLoss: 0.194\n",
      "Epoch [130][0/12]\tLoss: 0.192\n",
      "Epoch [131][0/12]\tLoss: 0.186\n",
      "Epoch [132][0/12]\tLoss: 0.204\n",
      "Epoch [133][0/12]\tLoss: 0.209\n",
      "Epoch [134][0/12]\tLoss: 0.176\n",
      "Epoch [135][0/12]\tLoss: 0.191\n",
      "Epoch [136][0/12]\tLoss: 0.198\n",
      "Epoch [137][0/12]\tLoss: 0.178\n",
      "Epoch [138][0/12]\tLoss: 0.174\n",
      "Epoch [139][0/12]\tLoss: 0.202\n",
      "Epoch [140][0/12]\tLoss: 0.174\n",
      "Epoch [141][0/12]\tLoss: 0.170\n",
      "Epoch [142][0/12]\tLoss: 0.166\n",
      "Epoch [143][0/12]\tLoss: 0.184\n",
      "Epoch [144][0/12]\tLoss: 0.173\n",
      "Epoch [145][0/12]\tLoss: 0.188\n",
      "Epoch [146][0/12]\tLoss: 0.181\n",
      "Epoch [147][0/12]\tLoss: 0.176\n",
      "Epoch [148][0/12]\tLoss: 0.177\n",
      "Epoch [149][0/12]\tLoss: 0.176\n",
      "[neptune] [info   ] Shutting down background jobs, please wait a moment...\n",
      "[neptune] [info   ] Done!\n",
      "[neptune] [info   ] All 0 operations synced, thanks for waiting!\n",
      "[neptune] [info   ] Explore the metadata in the Neptune app: https://app.neptune.ai/andialifs/siet-24/e/SIET-114/metadata\n",
      "\n",
      "Running for experiment 90 with d_model 2048, heads16, num_layers_enc 1, num_layers_dec 1\n",
      "\n",
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/andialifs/siet-24/e/SIET-115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][0/12]\tLoss: 5.932\n",
      "Epoch [1][0/12]\tLoss: 5.921\n",
      "Epoch [2][0/12]\tLoss: 5.856\n",
      "Epoch [3][0/12]\tLoss: 5.792\n",
      "Epoch [4][0/12]\tLoss: 5.663\n",
      "Epoch [5][0/12]\tLoss: 5.586\n",
      "Epoch [6][0/12]\tLoss: 5.369\n",
      "Epoch [7][0/12]\tLoss: 5.138\n",
      "Epoch [8][0/12]\tLoss: 4.889\n",
      "Epoch [9][0/12]\tLoss: 4.732\n",
      "Epoch [10][0/12]\tLoss: 4.518\n",
      "Epoch [11][0/12]\tLoss: 4.317\n",
      "Epoch [12][0/12]\tLoss: 4.374\n",
      "Epoch [13][0/12]\tLoss: 4.038\n",
      "Epoch [14][0/12]\tLoss: 3.964\n",
      "Epoch [15][0/12]\tLoss: 3.850\n",
      "Epoch [16][0/12]\tLoss: 4.032\n",
      "Epoch [17][0/12]\tLoss: 3.741\n",
      "Epoch [18][0/12]\tLoss: 3.476\n",
      "Epoch [19][0/12]\tLoss: 3.374\n",
      "Epoch [20][0/12]\tLoss: 3.024\n",
      "Epoch [21][0/12]\tLoss: 3.113\n",
      "Epoch [22][0/12]\tLoss: 2.701\n",
      "Epoch [23][0/12]\tLoss: 2.974\n",
      "Epoch [24][0/12]\tLoss: 2.955\n",
      "Epoch [25][0/12]\tLoss: 2.805\n",
      "Epoch [26][0/12]\tLoss: 2.450\n",
      "Epoch [27][0/12]\tLoss: 2.411\n",
      "Epoch [28][0/12]\tLoss: 2.060\n",
      "Epoch [29][0/12]\tLoss: 2.155\n",
      "Epoch [30][0/12]\tLoss: 2.253\n",
      "Epoch [31][0/12]\tLoss: 2.007\n",
      "Epoch [32][0/12]\tLoss: 2.104\n",
      "Epoch [33][0/12]\tLoss: 1.823\n",
      "Epoch [34][0/12]\tLoss: 1.494\n",
      "Epoch [35][0/12]\tLoss: 1.533\n",
      "Epoch [36][0/12]\tLoss: 1.381\n",
      "Epoch [37][0/12]\tLoss: 1.495\n",
      "Epoch [38][0/12]\tLoss: 1.207\n",
      "Epoch [39][0/12]\tLoss: 1.009\n",
      "Epoch [40][0/12]\tLoss: 1.161\n",
      "Epoch [41][0/12]\tLoss: 1.131\n",
      "Epoch [42][0/12]\tLoss: 1.253\n",
      "Epoch [43][0/12]\tLoss: 0.934\n",
      "Epoch [44][0/12]\tLoss: 0.734\n",
      "Epoch [45][0/12]\tLoss: 0.773\n",
      "Epoch [46][0/12]\tLoss: 0.665\n",
      "Epoch [47][0/12]\tLoss: 0.628\n",
      "Epoch [48][0/12]\tLoss: 0.624\n",
      "Epoch [49][0/12]\tLoss: 0.610\n",
      "Epoch [50][0/12]\tLoss: 0.562\n",
      "Epoch [51][0/12]\tLoss: 0.544\n",
      "Epoch [52][0/12]\tLoss: 0.506\n",
      "Epoch [53][0/12]\tLoss: 0.500\n",
      "Epoch [54][0/12]\tLoss: 0.435\n",
      "Epoch [55][0/12]\tLoss: 0.436\n",
      "Epoch [56][0/12]\tLoss: 0.434\n",
      "Epoch [57][0/12]\tLoss: 0.387\n",
      "Epoch [58][0/12]\tLoss: 0.342\n",
      "Epoch [59][0/12]\tLoss: 0.352\n",
      "Epoch [60][0/12]\tLoss: 0.374\n",
      "Epoch [61][0/12]\tLoss: 0.311\n",
      "Epoch [62][0/12]\tLoss: 0.301\n",
      "Epoch [63][0/12]\tLoss: 0.308\n",
      "Epoch [64][0/12]\tLoss: 0.296\n",
      "Epoch [65][0/12]\tLoss: 0.300\n",
      "Epoch [66][0/12]\tLoss: 0.302\n",
      "Epoch [67][0/12]\tLoss: 0.272\n",
      "Epoch [68][0/12]\tLoss: 0.245\n",
      "Epoch [69][0/12]\tLoss: 0.314\n",
      "Epoch [70][0/12]\tLoss: 0.269\n",
      "Epoch [71][0/12]\tLoss: 0.293\n",
      "Epoch [72][0/12]\tLoss: 0.254\n",
      "Epoch [73][0/12]\tLoss: 0.304\n",
      "Epoch [74][0/12]\tLoss: 0.261\n",
      "Epoch [75][0/12]\tLoss: 0.257\n",
      "Epoch [76][0/12]\tLoss: 0.242\n",
      "Epoch [77][0/12]\tLoss: 0.243\n",
      "Epoch [78][0/12]\tLoss: 0.262\n",
      "Epoch [79][0/12]\tLoss: 0.230\n",
      "Epoch [80][0/12]\tLoss: 0.239\n",
      "Epoch [81][0/12]\tLoss: 0.242\n",
      "Epoch [82][0/12]\tLoss: 0.222\n",
      "Epoch [83][0/12]\tLoss: 0.228\n",
      "Epoch [84][0/12]\tLoss: 0.239\n",
      "Epoch [85][0/12]\tLoss: 0.197\n",
      "Epoch [86][0/12]\tLoss: 0.229\n",
      "Epoch [87][0/12]\tLoss: 0.223\n",
      "Epoch [88][0/12]\tLoss: 0.237\n",
      "Epoch [89][0/12]\tLoss: 0.225\n",
      "Epoch [90][0/12]\tLoss: 0.238\n",
      "Epoch [91][0/12]\tLoss: 0.234\n",
      "Epoch [92][0/12]\tLoss: 0.191\n",
      "Epoch [93][0/12]\tLoss: 0.215\n",
      "Epoch [94][0/12]\tLoss: 0.234\n",
      "Epoch [95][0/12]\tLoss: 0.279\n",
      "Epoch [96][0/12]\tLoss: 0.241\n",
      "Epoch [97][0/12]\tLoss: 0.236\n",
      "Epoch [98][0/12]\tLoss: 0.235\n",
      "Epoch [99][0/12]\tLoss: 0.227\n",
      "Epoch [100][0/12]\tLoss: 0.206\n",
      "Epoch [101][0/12]\tLoss: 0.221\n",
      "Epoch [102][0/12]\tLoss: 0.192\n",
      "Epoch [103][0/12]\tLoss: 0.212\n",
      "Epoch [104][0/12]\tLoss: 0.200\n",
      "Epoch [105][0/12]\tLoss: 0.214\n",
      "Epoch [106][0/12]\tLoss: 0.194\n",
      "Epoch [107][0/12]\tLoss: 0.170\n",
      "Epoch [108][0/12]\tLoss: 0.210\n",
      "Epoch [109][0/12]\tLoss: 0.220\n",
      "Epoch [110][0/12]\tLoss: 0.212\n",
      "Epoch [111][0/12]\tLoss: 0.198\n",
      "Epoch [112][0/12]\tLoss: 0.192\n",
      "Epoch [113][0/12]\tLoss: 0.201\n",
      "Epoch [114][0/12]\tLoss: 0.185\n",
      "Epoch [115][0/12]\tLoss: 0.193\n",
      "Epoch [116][0/12]\tLoss: 0.188\n",
      "Epoch [117][0/12]\tLoss: 0.201\n",
      "Epoch [118][0/12]\tLoss: 0.181\n",
      "Epoch [119][0/12]\tLoss: 0.196\n",
      "Epoch [120][0/12]\tLoss: 0.201\n",
      "Epoch [121][0/12]\tLoss: 0.222\n",
      "Epoch [122][0/12]\tLoss: 0.227\n",
      "Epoch [123][0/12]\tLoss: 0.193\n",
      "Epoch [124][0/12]\tLoss: 0.202\n",
      "Epoch [125][0/12]\tLoss: 0.204\n",
      "Epoch [126][0/12]\tLoss: 0.178\n",
      "Epoch [127][0/12]\tLoss: 0.192\n",
      "Epoch [128][0/12]\tLoss: 0.214\n",
      "Epoch [129][0/12]\tLoss: 0.207\n",
      "Epoch [130][0/12]\tLoss: 0.187\n",
      "Epoch [131][0/12]\tLoss: 0.200\n",
      "Epoch [132][0/12]\tLoss: 0.193\n",
      "Epoch [133][0/12]\tLoss: 0.194\n",
      "Epoch [134][0/12]\tLoss: 0.184\n",
      "Epoch [135][0/12]\tLoss: 0.178\n",
      "Epoch [136][0/12]\tLoss: 0.187\n",
      "Epoch [137][0/12]\tLoss: 0.176\n",
      "Epoch [138][0/12]\tLoss: 0.187\n",
      "Epoch [139][0/12]\tLoss: 0.204\n",
      "Epoch [140][0/12]\tLoss: 0.184\n",
      "Epoch [141][0/12]\tLoss: 0.190\n",
      "Epoch [142][0/12]\tLoss: 0.197\n",
      "Epoch [143][0/12]\tLoss: 0.171\n",
      "Epoch [144][0/12]\tLoss: 0.204\n",
      "Epoch [145][0/12]\tLoss: 0.173\n",
      "Epoch [146][0/12]\tLoss: 0.189\n",
      "Epoch [147][0/12]\tLoss: 0.194\n",
      "Epoch [148][0/12]\tLoss: 0.165\n",
      "Epoch [149][0/12]\tLoss: 0.193\n",
      "[neptune] [info   ] Shutting down background jobs, please wait a moment...\n",
      "[neptune] [info   ] Done!\n",
      "[neptune] [info   ] All 0 operations synced, thanks for waiting!\n",
      "[neptune] [info   ] Explore the metadata in the Neptune app: https://app.neptune.ai/andialifs/siet-24/e/SIET-115/metadata\n",
      "\n",
      "Running for experiment 91 with d_model 2048, heads16, num_layers_enc 1, num_layers_dec 3\n",
      "\n",
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/andialifs/siet-24/e/SIET-116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][0/12]\tLoss: 5.973\n",
      "Epoch [1][0/12]\tLoss: 5.927\n",
      "Epoch [2][0/12]\tLoss: 5.898\n",
      "Epoch [3][0/12]\tLoss: 5.807\n",
      "Epoch [4][0/12]\tLoss: 5.693\n",
      "Epoch [5][0/12]\tLoss: 5.490\n",
      "Epoch [6][0/12]\tLoss: 5.387\n",
      "Epoch [7][0/12]\tLoss: 5.182\n",
      "Epoch [8][0/12]\tLoss: 4.746\n",
      "Epoch [9][0/12]\tLoss: 4.808\n",
      "Epoch [10][0/12]\tLoss: 4.441\n",
      "Epoch [11][0/12]\tLoss: 4.359\n",
      "Epoch [12][0/12]\tLoss: 4.303\n",
      "Epoch [13][0/12]\tLoss: 4.157\n",
      "Epoch [14][0/12]\tLoss: 4.009\n",
      "Epoch [15][0/12]\tLoss: 3.742\n",
      "Epoch [16][0/12]\tLoss: 3.563\n",
      "Epoch [17][0/12]\tLoss: 3.497\n",
      "Epoch [18][0/12]\tLoss: 3.581\n",
      "Epoch [19][0/12]\tLoss: 3.514\n",
      "Epoch [20][0/12]\tLoss: 3.171\n",
      "Epoch [21][0/12]\tLoss: 2.907\n",
      "Epoch [22][0/12]\tLoss: 2.990\n",
      "Epoch [23][0/12]\tLoss: 2.861\n",
      "Epoch [24][0/12]\tLoss: 2.531\n",
      "Epoch [25][0/12]\tLoss: 2.673\n",
      "Epoch [26][0/12]\tLoss: 2.718\n",
      "Epoch [27][0/12]\tLoss: 1.861\n",
      "Epoch [28][0/12]\tLoss: 2.346\n",
      "Epoch [29][0/12]\tLoss: 2.139\n",
      "Epoch [30][0/12]\tLoss: 1.893\n",
      "Epoch [31][0/12]\tLoss: 2.026\n",
      "Epoch [32][0/12]\tLoss: 2.061\n",
      "Epoch [33][0/12]\tLoss: 1.772\n",
      "Epoch [34][0/12]\tLoss: 1.865\n",
      "Epoch [35][0/12]\tLoss: 1.650\n",
      "Epoch [36][0/12]\tLoss: 1.486\n",
      "Epoch [37][0/12]\tLoss: 1.232\n",
      "Epoch [38][0/12]\tLoss: 1.369\n",
      "Epoch [39][0/12]\tLoss: 1.128\n",
      "Epoch [40][0/12]\tLoss: 1.214\n",
      "Epoch [41][0/12]\tLoss: 1.087\n",
      "Epoch [42][0/12]\tLoss: 1.024\n",
      "Epoch [43][0/12]\tLoss: 0.945\n",
      "Epoch [44][0/12]\tLoss: 0.885\n",
      "Epoch [45][0/12]\tLoss: 0.821\n",
      "Epoch [46][0/12]\tLoss: 0.902\n",
      "Epoch [47][0/12]\tLoss: 0.774\n",
      "Epoch [48][0/12]\tLoss: 0.617\n",
      "Epoch [49][0/12]\tLoss: 0.636\n",
      "Epoch [50][0/12]\tLoss: 0.536\n",
      "Epoch [51][0/12]\tLoss: 0.506\n",
      "Epoch [52][0/12]\tLoss: 0.589\n",
      "Epoch [53][0/12]\tLoss: 0.462\n",
      "Epoch [54][0/12]\tLoss: 0.471\n",
      "Epoch [55][0/12]\tLoss: 0.441\n",
      "Epoch [56][0/12]\tLoss: 0.355\n",
      "Epoch [57][0/12]\tLoss: 0.389\n",
      "Epoch [58][0/12]\tLoss: 0.376\n",
      "Epoch [59][0/12]\tLoss: 0.380\n",
      "Epoch [60][0/12]\tLoss: 0.391\n",
      "Epoch [61][0/12]\tLoss: 0.365\n",
      "Epoch [62][0/12]\tLoss: 0.341\n",
      "Epoch [63][0/12]\tLoss: 0.368\n",
      "Epoch [64][0/12]\tLoss: 0.279\n",
      "Epoch [65][0/12]\tLoss: 0.280\n",
      "Epoch [66][0/12]\tLoss: 0.277\n",
      "Epoch [67][0/12]\tLoss: 0.314\n",
      "Epoch [68][0/12]\tLoss: 0.277\n",
      "Epoch [69][0/12]\tLoss: 0.298\n",
      "Epoch [70][0/12]\tLoss: 0.286\n",
      "Epoch [71][0/12]\tLoss: 0.260\n",
      "Epoch [72][0/12]\tLoss: 0.303\n",
      "Epoch [73][0/12]\tLoss: 0.288\n",
      "Epoch [74][0/12]\tLoss: 0.277\n",
      "Epoch [75][0/12]\tLoss: 0.260\n",
      "Epoch [76][0/12]\tLoss: 0.264\n",
      "Epoch [77][0/12]\tLoss: 0.229\n",
      "Epoch [78][0/12]\tLoss: 0.255\n",
      "Epoch [79][0/12]\tLoss: 0.237\n",
      "Epoch [80][0/12]\tLoss: 0.245\n",
      "Epoch [81][0/12]\tLoss: 0.278\n",
      "Epoch [82][0/12]\tLoss: 0.232\n",
      "Epoch [83][0/12]\tLoss: 0.247\n",
      "Epoch [84][0/12]\tLoss: 0.230\n",
      "Epoch [85][0/12]\tLoss: 0.241\n",
      "Epoch [86][0/12]\tLoss: 0.276\n",
      "Epoch [87][0/12]\tLoss: 0.253\n",
      "Epoch [88][0/12]\tLoss: 0.237\n",
      "Epoch [89][0/12]\tLoss: 0.233\n",
      "Epoch [90][0/12]\tLoss: 0.218\n",
      "Epoch [91][0/12]\tLoss: 0.226\n",
      "Epoch [92][0/12]\tLoss: 0.214\n",
      "Epoch [93][0/12]\tLoss: 0.237\n",
      "Epoch [94][0/12]\tLoss: 0.235\n",
      "Epoch [95][0/12]\tLoss: 0.275\n",
      "Epoch [96][0/12]\tLoss: 0.233\n",
      "Epoch [97][0/12]\tLoss: 0.219\n",
      "Epoch [98][0/12]\tLoss: 0.208\n",
      "Epoch [99][0/12]\tLoss: 0.231\n",
      "Epoch [100][0/12]\tLoss: 0.225\n",
      "Epoch [101][0/12]\tLoss: 0.209\n",
      "Epoch [102][0/12]\tLoss: 0.224\n",
      "Epoch [103][0/12]\tLoss: 0.201\n",
      "Epoch [104][0/12]\tLoss: 0.185\n",
      "Epoch [105][0/12]\tLoss: 0.200\n",
      "Epoch [106][0/12]\tLoss: 0.190\n",
      "Epoch [107][0/12]\tLoss: 0.189\n",
      "Epoch [108][0/12]\tLoss: 0.211\n",
      "Epoch [109][0/12]\tLoss: 0.210\n",
      "Epoch [110][0/12]\tLoss: 0.213\n",
      "Epoch [111][0/12]\tLoss: 0.208\n",
      "Epoch [112][0/12]\tLoss: 0.224\n",
      "Epoch [113][0/12]\tLoss: 0.210\n",
      "Epoch [114][0/12]\tLoss: 0.196\n",
      "Epoch [115][0/12]\tLoss: 0.201\n",
      "Epoch [116][0/12]\tLoss: 0.214\n",
      "Epoch [117][0/12]\tLoss: 0.194\n",
      "Epoch [118][0/12]\tLoss: 0.184\n",
      "Epoch [119][0/12]\tLoss: 0.224\n",
      "Epoch [120][0/12]\tLoss: 0.211\n",
      "Epoch [121][0/12]\tLoss: 0.208\n",
      "Epoch [122][0/12]\tLoss: 0.182\n",
      "Epoch [123][0/12]\tLoss: 0.207\n",
      "Epoch [124][0/12]\tLoss: 0.201\n",
      "Epoch [125][0/12]\tLoss: 0.202\n",
      "Epoch [126][0/12]\tLoss: 0.169\n",
      "Epoch [127][0/12]\tLoss: 0.191\n",
      "Epoch [128][0/12]\tLoss: 0.217\n",
      "Epoch [129][0/12]\tLoss: 0.209\n",
      "Epoch [130][0/12]\tLoss: 0.207\n",
      "Epoch [131][0/12]\tLoss: 0.187\n",
      "Epoch [132][0/12]\tLoss: 0.193\n",
      "Epoch [133][0/12]\tLoss: 0.171\n",
      "Epoch [134][0/12]\tLoss: 0.198\n",
      "Epoch [135][0/12]\tLoss: 0.168\n",
      "Epoch [136][0/12]\tLoss: 0.195\n",
      "Epoch [137][0/12]\tLoss: 0.195\n",
      "Epoch [138][0/12]\tLoss: 0.179\n",
      "Epoch [139][0/12]\tLoss: 0.188\n",
      "Epoch [140][0/12]\tLoss: 0.195\n",
      "Epoch [141][0/12]\tLoss: 0.200\n",
      "Epoch [142][0/12]\tLoss: 0.199\n",
      "Epoch [143][0/12]\tLoss: 0.192\n",
      "Epoch [144][0/12]\tLoss: 0.208\n",
      "Epoch [145][0/12]\tLoss: 0.186\n",
      "Epoch [146][0/12]\tLoss: 0.194\n",
      "Epoch [147][0/12]\tLoss: 0.198\n",
      "Epoch [148][0/12]\tLoss: 0.183\n",
      "Epoch [149][0/12]\tLoss: 0.172\n",
      "[neptune] [info   ] Shutting down background jobs, please wait a moment...\n",
      "[neptune] [info   ] Done!\n",
      "[neptune] [info   ] All 0 operations synced, thanks for waiting!\n",
      "[neptune] [info   ] Explore the metadata in the Neptune app: https://app.neptune.ai/andialifs/siet-24/e/SIET-116/metadata\n",
      "\n",
      "Running for experiment 92 with d_model 2048, heads16, num_layers_enc 1, num_layers_dec 5\n",
      "\n",
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/andialifs/siet-24/e/SIET-117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][0/12]\tLoss: 5.929\n",
      "Epoch [1][0/12]\tLoss: 5.895\n",
      "Epoch [2][0/12]\tLoss: 5.856\n",
      "Epoch [3][0/12]\tLoss: 5.786\n",
      "Epoch [4][0/12]\tLoss: 5.604\n",
      "Epoch [5][0/12]\tLoss: 5.521\n",
      "Epoch [6][0/12]\tLoss: 5.304\n",
      "Epoch [7][0/12]\tLoss: 5.142\n",
      "Epoch [8][0/12]\tLoss: 4.818\n",
      "Epoch [9][0/12]\tLoss: 4.753\n",
      "Epoch [10][0/12]\tLoss: 4.546\n",
      "Epoch [11][0/12]\tLoss: 4.529\n",
      "Epoch [12][0/12]\tLoss: 4.057\n",
      "Epoch [13][0/12]\tLoss: 4.110\n",
      "Epoch [14][0/12]\tLoss: 3.879\n",
      "Epoch [15][0/12]\tLoss: 3.489\n",
      "Epoch [16][0/12]\tLoss: 3.699\n",
      "Epoch [17][0/12]\tLoss: 3.334\n",
      "Epoch [18][0/12]\tLoss: 3.619\n",
      "Epoch [19][0/12]\tLoss: 3.356\n",
      "Epoch [20][0/12]\tLoss: 3.046\n",
      "Epoch [21][0/12]\tLoss: 2.934\n",
      "Epoch [22][0/12]\tLoss: 2.674\n",
      "Epoch [23][0/12]\tLoss: 2.902\n",
      "Epoch [24][0/12]\tLoss: 2.904\n",
      "Epoch [25][0/12]\tLoss: 2.130\n",
      "Epoch [26][0/12]\tLoss: 2.585\n",
      "Epoch [27][0/12]\tLoss: 2.448\n",
      "Epoch [28][0/12]\tLoss: 2.410\n",
      "Epoch [29][0/12]\tLoss: 2.069\n",
      "Epoch [30][0/12]\tLoss: 1.996\n",
      "Epoch [31][0/12]\tLoss: 1.821\n",
      "Epoch [32][0/12]\tLoss: 1.672\n",
      "Epoch [33][0/12]\tLoss: 1.682\n",
      "Epoch [34][0/12]\tLoss: 2.093\n",
      "Epoch [35][0/12]\tLoss: 1.522\n",
      "Epoch [36][0/12]\tLoss: 1.430\n",
      "Epoch [37][0/12]\tLoss: 1.196\n",
      "Epoch [38][0/12]\tLoss: 1.219\n",
      "Epoch [39][0/12]\tLoss: 0.951\n",
      "Epoch [40][0/12]\tLoss: 1.299\n",
      "Epoch [41][0/12]\tLoss: 1.069\n",
      "Epoch [42][0/12]\tLoss: 1.130\n",
      "Epoch [43][0/12]\tLoss: 1.103\n",
      "Epoch [44][0/12]\tLoss: 0.977\n",
      "Epoch [45][0/12]\tLoss: 0.880\n",
      "Epoch [46][0/12]\tLoss: 0.779\n",
      "Epoch [47][0/12]\tLoss: 0.661\n",
      "Epoch [48][0/12]\tLoss: 0.681\n",
      "Epoch [49][0/12]\tLoss: 0.672\n",
      "Epoch [50][0/12]\tLoss: 0.532\n",
      "Epoch [51][0/12]\tLoss: 0.525\n",
      "Epoch [52][0/12]\tLoss: 0.578\n",
      "Epoch [53][0/12]\tLoss: 0.489\n",
      "Epoch [54][0/12]\tLoss: 0.411\n",
      "Epoch [55][0/12]\tLoss: 0.456\n",
      "Epoch [56][0/12]\tLoss: 0.432\n",
      "Epoch [57][0/12]\tLoss: 0.394\n",
      "Epoch [58][0/12]\tLoss: 0.375\n",
      "Epoch [59][0/12]\tLoss: 0.358\n",
      "Epoch [60][0/12]\tLoss: 0.379\n",
      "Epoch [61][0/12]\tLoss: 0.361\n",
      "Epoch [62][0/12]\tLoss: 0.305\n",
      "Epoch [63][0/12]\tLoss: 0.300\n",
      "Epoch [64][0/12]\tLoss: 0.324\n",
      "Epoch [65][0/12]\tLoss: 0.291\n",
      "Epoch [66][0/12]\tLoss: 0.337\n",
      "Epoch [67][0/12]\tLoss: 0.300\n",
      "Epoch [68][0/12]\tLoss: 0.286\n",
      "Epoch [69][0/12]\tLoss: 0.258\n",
      "Epoch [70][0/12]\tLoss: 0.286\n",
      "Epoch [71][0/12]\tLoss: 0.292\n",
      "Epoch [72][0/12]\tLoss: 0.259\n",
      "Epoch [73][0/12]\tLoss: 0.288\n",
      "Epoch [74][0/12]\tLoss: 0.288\n",
      "Epoch [75][0/12]\tLoss: 0.276\n",
      "Epoch [76][0/12]\tLoss: 0.244\n",
      "Epoch [77][0/12]\tLoss: 0.221\n",
      "Epoch [78][0/12]\tLoss: 0.236\n",
      "Epoch [79][0/12]\tLoss: 0.242\n",
      "Epoch [80][0/12]\tLoss: 0.239\n",
      "Epoch [81][0/12]\tLoss: 0.215\n",
      "Epoch [82][0/12]\tLoss: 0.212\n",
      "Epoch [83][0/12]\tLoss: 0.213\n",
      "Epoch [84][0/12]\tLoss: 0.213\n",
      "Epoch [85][0/12]\tLoss: 0.203\n",
      "Epoch [86][0/12]\tLoss: 0.220\n",
      "Epoch [87][0/12]\tLoss: 0.215\n",
      "Epoch [88][0/12]\tLoss: 0.253\n",
      "Epoch [89][0/12]\tLoss: 0.251\n",
      "Epoch [90][0/12]\tLoss: 0.194\n",
      "Epoch [91][0/12]\tLoss: 0.246\n",
      "Epoch [92][0/12]\tLoss: 0.242\n",
      "Epoch [93][0/12]\tLoss: 0.224\n",
      "Epoch [94][0/12]\tLoss: 0.215\n",
      "Epoch [95][0/12]\tLoss: 0.244\n",
      "Epoch [96][0/12]\tLoss: 0.212\n",
      "Epoch [97][0/12]\tLoss: 0.211\n",
      "Epoch [98][0/12]\tLoss: 0.222\n",
      "Epoch [99][0/12]\tLoss: 0.216\n",
      "Epoch [100][0/12]\tLoss: 0.244\n",
      "Epoch [101][0/12]\tLoss: 0.218\n",
      "Epoch [102][0/12]\tLoss: 0.185\n",
      "Epoch [103][0/12]\tLoss: 0.205\n",
      "Epoch [104][0/12]\tLoss: 0.228\n",
      "Epoch [105][0/12]\tLoss: 0.193\n",
      "Epoch [106][0/12]\tLoss: 0.224\n",
      "Epoch [107][0/12]\tLoss: 0.189\n",
      "Epoch [108][0/12]\tLoss: 0.221\n",
      "Epoch [109][0/12]\tLoss: 0.203\n",
      "Epoch [110][0/12]\tLoss: 0.203\n",
      "Epoch [111][0/12]\tLoss: 0.222\n",
      "Epoch [112][0/12]\tLoss: 0.208\n",
      "Epoch [113][0/12]\tLoss: 0.213\n",
      "Epoch [114][0/12]\tLoss: 0.240\n",
      "Epoch [115][0/12]\tLoss: 0.212\n",
      "Epoch [116][0/12]\tLoss: 0.185\n",
      "Epoch [117][0/12]\tLoss: 0.206\n",
      "Epoch [118][0/12]\tLoss: 0.192\n",
      "Epoch [119][0/12]\tLoss: 0.195\n",
      "Epoch [120][0/12]\tLoss: 0.197\n",
      "Epoch [121][0/12]\tLoss: 0.210\n",
      "Epoch [122][0/12]\tLoss: 0.206\n",
      "Epoch [123][0/12]\tLoss: 0.194\n",
      "Epoch [124][0/12]\tLoss: 0.193\n",
      "Epoch [125][0/12]\tLoss: 0.209\n",
      "Epoch [126][0/12]\tLoss: 0.200\n",
      "Epoch [127][0/12]\tLoss: 0.233\n",
      "Epoch [128][0/12]\tLoss: 0.192\n",
      "Epoch [129][0/12]\tLoss: 0.184\n",
      "Epoch [130][0/12]\tLoss: 0.177\n",
      "Epoch [131][0/12]\tLoss: 0.189\n",
      "Epoch [132][0/12]\tLoss: 0.207\n",
      "Epoch [133][0/12]\tLoss: 0.176\n",
      "Epoch [134][0/12]\tLoss: 0.194\n",
      "Epoch [135][0/12]\tLoss: 0.176\n",
      "Epoch [136][0/12]\tLoss: 0.170\n",
      "Epoch [137][0/12]\tLoss: 0.200\n",
      "Epoch [138][0/12]\tLoss: 0.180\n",
      "Epoch [139][0/12]\tLoss: 0.166\n",
      "Epoch [140][0/12]\tLoss: 0.217\n",
      "Epoch [141][0/12]\tLoss: 0.230\n",
      "Epoch [142][0/12]\tLoss: 0.173\n",
      "Epoch [143][0/12]\tLoss: 0.206\n",
      "Epoch [144][0/12]\tLoss: 0.215\n",
      "Epoch [145][0/12]\tLoss: 0.190\n",
      "Epoch [146][0/12]\tLoss: 0.170\n",
      "Epoch [147][0/12]\tLoss: 0.167\n",
      "Epoch [148][0/12]\tLoss: 0.197\n",
      "Epoch [149][0/12]\tLoss: 0.183\n",
      "[neptune] [info   ] Shutting down background jobs, please wait a moment...\n",
      "[neptune] [info   ] Done!\n",
      "[neptune] [info   ] All 0 operations synced, thanks for waiting!\n",
      "[neptune] [info   ] Explore the metadata in the Neptune app: https://app.neptune.ai/andialifs/siet-24/e/SIET-117/metadata\n",
      "\n",
      "Running for experiment 93 with d_model 2048, heads16, num_layers_enc 3, num_layers_dec 1\n",
      "\n",
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/andialifs/siet-24/e/SIET-118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][0/12]\tLoss: 5.924\n",
      "Epoch [1][0/12]\tLoss: 5.829\n",
      "Epoch [2][0/12]\tLoss: 5.624\n",
      "Epoch [3][0/12]\tLoss: 5.298\n",
      "Epoch [4][0/12]\tLoss: 5.081\n",
      "Epoch [5][0/12]\tLoss: 4.939\n",
      "Epoch [6][0/12]\tLoss: 4.882\n",
      "Epoch [7][0/12]\tLoss: 4.754\n",
      "Epoch [8][0/12]\tLoss: 4.712\n",
      "Epoch [9][0/12]\tLoss: 4.747\n",
      "Epoch [10][0/12]\tLoss: 4.451\n",
      "Epoch [11][0/12]\tLoss: 4.473\n",
      "Epoch [12][0/12]\tLoss: 4.547\n",
      "Epoch [13][0/12]\tLoss: 4.368\n",
      "Epoch [14][0/12]\tLoss: 4.237\n",
      "Epoch [15][0/12]\tLoss: 4.039\n",
      "Epoch [16][0/12]\tLoss: 3.891\n",
      "Epoch [17][0/12]\tLoss: 3.684\n",
      "Epoch [18][0/12]\tLoss: 3.711\n",
      "Epoch [19][0/12]\tLoss: 3.276\n",
      "Epoch [20][0/12]\tLoss: 3.199\n",
      "Epoch [21][0/12]\tLoss: 3.265\n",
      "Epoch [22][0/12]\tLoss: 3.068\n",
      "Epoch [23][0/12]\tLoss: 2.712\n",
      "Epoch [24][0/12]\tLoss: 2.935\n",
      "Epoch [25][0/12]\tLoss: 2.788\n",
      "Epoch [26][0/12]\tLoss: 2.604\n",
      "Epoch [27][0/12]\tLoss: 2.710\n",
      "Epoch [28][0/12]\tLoss: 2.301\n",
      "Epoch [29][0/12]\tLoss: 2.355\n",
      "Epoch [30][0/12]\tLoss: 2.640\n",
      "Epoch [31][0/12]\tLoss: 2.082\n",
      "Epoch [32][0/12]\tLoss: 2.012\n",
      "Epoch [33][0/12]\tLoss: 2.541\n",
      "Epoch [34][0/12]\tLoss: 2.054\n",
      "Epoch [35][0/12]\tLoss: 2.098\n",
      "Epoch [36][0/12]\tLoss: 1.779\n",
      "Epoch [37][0/12]\tLoss: 1.799\n",
      "Epoch [38][0/12]\tLoss: 1.588\n",
      "Epoch [39][0/12]\tLoss: 1.884\n",
      "Epoch [40][0/12]\tLoss: 1.573\n",
      "Epoch [41][0/12]\tLoss: 1.699\n",
      "Epoch [42][0/12]\tLoss: 1.474\n",
      "Epoch [43][0/12]\tLoss: 1.236\n",
      "Epoch [44][0/12]\tLoss: 1.415\n",
      "Epoch [45][0/12]\tLoss: 1.099\n",
      "Epoch [46][0/12]\tLoss: 1.186\n",
      "Epoch [47][0/12]\tLoss: 1.037\n",
      "Epoch [48][0/12]\tLoss: 1.097\n",
      "Epoch [49][0/12]\tLoss: 1.041\n",
      "Epoch [50][0/12]\tLoss: 0.924\n",
      "Epoch [51][0/12]\tLoss: 0.973\n",
      "Epoch [52][0/12]\tLoss: 0.861\n",
      "Epoch [53][0/12]\tLoss: 0.856\n",
      "Epoch [54][0/12]\tLoss: 0.728\n",
      "Epoch [55][0/12]\tLoss: 0.697\n",
      "Epoch [56][0/12]\tLoss: 0.667\n",
      "Epoch [57][0/12]\tLoss: 0.563\n",
      "Epoch [58][0/12]\tLoss: 0.585\n",
      "Epoch [59][0/12]\tLoss: 0.480\n",
      "Epoch [60][0/12]\tLoss: 0.476\n",
      "Epoch [61][0/12]\tLoss: 0.557\n",
      "Epoch [62][0/12]\tLoss: 0.451\n",
      "Epoch [63][0/12]\tLoss: 0.444\n",
      "Epoch [64][0/12]\tLoss: 0.467\n",
      "Epoch [65][0/12]\tLoss: 0.405\n",
      "Epoch [66][0/12]\tLoss: 0.371\n",
      "Epoch [67][0/12]\tLoss: 0.349\n",
      "Epoch [68][0/12]\tLoss: 0.402\n",
      "Epoch [69][0/12]\tLoss: 0.366\n",
      "Epoch [70][0/12]\tLoss: 0.334\n",
      "Epoch [71][0/12]\tLoss: 0.329\n",
      "Epoch [72][0/12]\tLoss: 0.323\n",
      "Epoch [73][0/12]\tLoss: 0.343\n",
      "Epoch [74][0/12]\tLoss: 0.284\n",
      "Epoch [75][0/12]\tLoss: 0.315\n",
      "Epoch [76][0/12]\tLoss: 0.281\n",
      "Epoch [77][0/12]\tLoss: 0.265\n",
      "Epoch [78][0/12]\tLoss: 0.245\n",
      "Epoch [79][0/12]\tLoss: 0.270\n",
      "Epoch [80][0/12]\tLoss: 0.256\n",
      "Epoch [81][0/12]\tLoss: 0.259\n",
      "Epoch [82][0/12]\tLoss: 0.222\n",
      "Epoch [83][0/12]\tLoss: 0.280\n",
      "Epoch [84][0/12]\tLoss: 0.265\n",
      "Epoch [85][0/12]\tLoss: 0.251\n",
      "Epoch [86][0/12]\tLoss: 0.234\n",
      "Epoch [87][0/12]\tLoss: 0.214\n",
      "Epoch [88][0/12]\tLoss: 0.218\n",
      "Epoch [89][0/12]\tLoss: 0.227\n",
      "Epoch [90][0/12]\tLoss: 0.180\n",
      "Epoch [91][0/12]\tLoss: 0.213\n",
      "Epoch [92][0/12]\tLoss: 0.193\n",
      "Epoch [93][0/12]\tLoss: 0.204\n",
      "Epoch [94][0/12]\tLoss: 0.185\n",
      "Epoch [95][0/12]\tLoss: 0.195\n",
      "Epoch [96][0/12]\tLoss: 0.197\n",
      "Epoch [97][0/12]\tLoss: 0.200\n",
      "Epoch [98][0/12]\tLoss: 0.195\n",
      "Epoch [99][0/12]\tLoss: 0.192\n",
      "Epoch [100][0/12]\tLoss: 0.183\n",
      "Epoch [101][0/12]\tLoss: 0.191\n",
      "Epoch [102][0/12]\tLoss: 0.205\n",
      "Epoch [103][0/12]\tLoss: 0.194\n",
      "Epoch [104][0/12]\tLoss: 0.180\n",
      "Epoch [105][0/12]\tLoss: 0.175\n",
      "Epoch [106][0/12]\tLoss: 0.178\n",
      "Epoch [107][0/12]\tLoss: 0.182\n",
      "Epoch [108][0/12]\tLoss: 0.174\n",
      "Epoch [109][0/12]\tLoss: 0.174\n",
      "Epoch [110][0/12]\tLoss: 0.184\n",
      "Epoch [111][0/12]\tLoss: 0.184\n",
      "Epoch [112][0/12]\tLoss: 0.163\n",
      "Epoch [113][0/12]\tLoss: 0.160\n",
      "Epoch [114][0/12]\tLoss: 0.164\n",
      "Epoch [115][0/12]\tLoss: 0.160\n",
      "Epoch [116][0/12]\tLoss: 0.163\n",
      "Epoch [117][0/12]\tLoss: 0.154\n",
      "Epoch [118][0/12]\tLoss: 0.166\n",
      "Epoch [119][0/12]\tLoss: 0.156\n",
      "Epoch [120][0/12]\tLoss: 0.150\n",
      "Epoch [121][0/12]\tLoss: 0.161\n",
      "Epoch [122][0/12]\tLoss: 0.147\n",
      "Epoch [123][0/12]\tLoss: 0.171\n",
      "Epoch [124][0/12]\tLoss: 0.154\n",
      "Epoch [125][0/12]\tLoss: 0.139\n",
      "Epoch [126][0/12]\tLoss: 0.149\n",
      "Epoch [127][0/12]\tLoss: 0.136\n",
      "Epoch [128][0/12]\tLoss: 0.150\n",
      "Epoch [129][0/12]\tLoss: 0.147\n",
      "Epoch [130][0/12]\tLoss: 0.137\n",
      "Epoch [131][0/12]\tLoss: 0.140\n",
      "Epoch [132][0/12]\tLoss: 0.137\n",
      "Epoch [133][0/12]\tLoss: 0.133\n",
      "Epoch [134][0/12]\tLoss: 0.137\n",
      "Epoch [135][0/12]\tLoss: 0.136\n",
      "Epoch [136][0/12]\tLoss: 0.139\n",
      "Epoch [137][0/12]\tLoss: 0.132\n",
      "Epoch [138][0/12]\tLoss: 0.134\n",
      "Epoch [139][0/12]\tLoss: 0.134\n",
      "Epoch [140][0/12]\tLoss: 0.133\n",
      "Epoch [141][0/12]\tLoss: 0.166\n",
      "Epoch [142][0/12]\tLoss: 0.175\n",
      "Epoch [143][0/12]\tLoss: 0.123\n",
      "Epoch [144][0/12]\tLoss: 0.143\n",
      "Epoch [145][0/12]\tLoss: 0.132\n",
      "Epoch [146][0/12]\tLoss: 0.135\n",
      "Epoch [147][0/12]\tLoss: 0.126\n",
      "Epoch [148][0/12]\tLoss: 0.118\n",
      "Epoch [149][0/12]\tLoss: 0.135\n",
      "[neptune] [info   ] Shutting down background jobs, please wait a moment...\n",
      "[neptune] [info   ] Done!\n",
      "[neptune] [info   ] All 0 operations synced, thanks for waiting!\n",
      "[neptune] [info   ] Explore the metadata in the Neptune app: https://app.neptune.ai/andialifs/siet-24/e/SIET-118/metadata\n",
      "\n",
      "Running for experiment 94 with d_model 2048, heads16, num_layers_enc 3, num_layers_dec 3\n",
      "\n",
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/andialifs/siet-24/e/SIET-119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][0/12]\tLoss: 5.960\n",
      "Epoch [1][0/12]\tLoss: 5.873\n",
      "Epoch [2][0/12]\tLoss: 5.616\n",
      "Epoch [3][0/12]\tLoss: 5.323\n",
      "Epoch [4][0/12]\tLoss: 5.140\n",
      "Epoch [5][0/12]\tLoss: 4.991\n",
      "Epoch [6][0/12]\tLoss: 4.959\n",
      "Epoch [7][0/12]\tLoss: 4.698\n",
      "Epoch [8][0/12]\tLoss: 4.722\n",
      "Epoch [9][0/12]\tLoss: 4.642\n",
      "Epoch [10][0/12]\tLoss: 4.623\n",
      "Epoch [11][0/12]\tLoss: 4.538\n",
      "Epoch [12][0/12]\tLoss: 4.377\n",
      "Epoch [13][0/12]\tLoss: 4.341\n",
      "Epoch [14][0/12]\tLoss: 3.943\n",
      "Epoch [15][0/12]\tLoss: 3.989\n",
      "Epoch [16][0/12]\tLoss: 3.911\n",
      "Epoch [17][0/12]\tLoss: 3.848\n",
      "Epoch [18][0/12]\tLoss: 3.550\n",
      "Epoch [19][0/12]\tLoss: 3.580\n",
      "Epoch [20][0/12]\tLoss: 3.421\n",
      "Epoch [21][0/12]\tLoss: 3.075\n",
      "Epoch [22][0/12]\tLoss: 3.425\n",
      "Epoch [23][0/12]\tLoss: 3.156\n",
      "Epoch [24][0/12]\tLoss: 2.933\n",
      "Epoch [25][0/12]\tLoss: 2.765\n",
      "Epoch [26][0/12]\tLoss: 2.824\n",
      "Epoch [27][0/12]\tLoss: 2.557\n",
      "Epoch [28][0/12]\tLoss: 2.412\n",
      "Epoch [29][0/12]\tLoss: 1.833\n",
      "Epoch [30][0/12]\tLoss: 2.112\n",
      "Epoch [31][0/12]\tLoss: 2.309\n",
      "Epoch [32][0/12]\tLoss: 2.014\n",
      "Epoch [33][0/12]\tLoss: 2.412\n",
      "Epoch [34][0/12]\tLoss: 1.478\n",
      "Epoch [35][0/12]\tLoss: 1.983\n",
      "Epoch [36][0/12]\tLoss: 1.816\n",
      "Epoch [37][0/12]\tLoss: 1.564\n",
      "Epoch [38][0/12]\tLoss: 1.617\n",
      "Epoch [39][0/12]\tLoss: 1.644\n",
      "Epoch [40][0/12]\tLoss: 1.598\n",
      "Epoch [41][0/12]\tLoss: 1.256\n",
      "Epoch [42][0/12]\tLoss: 1.623\n",
      "Epoch [43][0/12]\tLoss: 1.203\n",
      "Epoch [44][0/12]\tLoss: 1.277\n",
      "Epoch [45][0/12]\tLoss: 1.213\n",
      "Epoch [46][0/12]\tLoss: 1.211\n",
      "Epoch [47][0/12]\tLoss: 0.919\n",
      "Epoch [48][0/12]\tLoss: 1.025\n",
      "Epoch [49][0/12]\tLoss: 1.191\n",
      "Epoch [50][0/12]\tLoss: 0.933\n",
      "Epoch [51][0/12]\tLoss: 0.907\n",
      "Epoch [52][0/12]\tLoss: 0.793\n",
      "Epoch [53][0/12]\tLoss: 0.803\n",
      "Epoch [54][0/12]\tLoss: 0.753\n",
      "Epoch [55][0/12]\tLoss: 0.679\n",
      "Epoch [56][0/12]\tLoss: 0.770\n",
      "Epoch [57][0/12]\tLoss: 0.696\n",
      "Epoch [58][0/12]\tLoss: 0.458\n",
      "Epoch [59][0/12]\tLoss: 0.574\n",
      "Epoch [60][0/12]\tLoss: 0.477\n",
      "Epoch [61][0/12]\tLoss: 0.470\n",
      "Epoch [62][0/12]\tLoss: 0.444\n",
      "Epoch [63][0/12]\tLoss: 0.437\n",
      "Epoch [64][0/12]\tLoss: 0.364\n",
      "Epoch [65][0/12]\tLoss: 0.409\n",
      "Epoch [66][0/12]\tLoss: 0.390\n",
      "Epoch [67][0/12]\tLoss: 0.403\n",
      "Epoch [68][0/12]\tLoss: 0.361\n",
      "Epoch [69][0/12]\tLoss: 0.376\n",
      "Epoch [70][0/12]\tLoss: 0.342\n",
      "Epoch [71][0/12]\tLoss: 0.362\n",
      "Epoch [72][0/12]\tLoss: 0.301\n",
      "Epoch [73][0/12]\tLoss: 0.298\n",
      "Epoch [74][0/12]\tLoss: 0.263\n",
      "Epoch [75][0/12]\tLoss: 0.285\n",
      "Epoch [76][0/12]\tLoss: 0.266\n",
      "Epoch [77][0/12]\tLoss: 0.279\n",
      "Epoch [78][0/12]\tLoss: 0.263\n",
      "Epoch [79][0/12]\tLoss: 0.254\n",
      "Epoch [80][0/12]\tLoss: 0.267\n",
      "Epoch [81][0/12]\tLoss: 0.237\n",
      "Epoch [82][0/12]\tLoss: 0.228\n",
      "Epoch [83][0/12]\tLoss: 0.238\n",
      "Epoch [84][0/12]\tLoss: 0.241\n",
      "Epoch [85][0/12]\tLoss: 0.228\n",
      "Epoch [86][0/12]\tLoss: 0.234\n",
      "Epoch [87][0/12]\tLoss: 0.241\n",
      "Epoch [88][0/12]\tLoss: 0.201\n",
      "Epoch [89][0/12]\tLoss: 0.199\n",
      "Epoch [90][0/12]\tLoss: 0.203\n",
      "Epoch [91][0/12]\tLoss: 0.219\n",
      "Epoch [92][0/12]\tLoss: 0.205\n",
      "Epoch [93][0/12]\tLoss: 0.213\n",
      "Epoch [94][0/12]\tLoss: 0.202\n",
      "Epoch [95][0/12]\tLoss: 0.199\n",
      "Epoch [96][0/12]\tLoss: 0.197\n",
      "Epoch [97][0/12]\tLoss: 0.205\n",
      "Epoch [98][0/12]\tLoss: 0.192\n",
      "Epoch [99][0/12]\tLoss: 0.200\n",
      "Epoch [100][0/12]\tLoss: 0.207\n",
      "Epoch [101][0/12]\tLoss: 0.178\n",
      "Epoch [102][0/12]\tLoss: 0.218\n",
      "Epoch [103][0/12]\tLoss: 0.175\n",
      "Epoch [104][0/12]\tLoss: 0.179\n",
      "Epoch [105][0/12]\tLoss: 0.181\n",
      "Epoch [106][0/12]\tLoss: 0.177\n",
      "Epoch [107][0/12]\tLoss: 0.185\n",
      "Epoch [108][0/12]\tLoss: 0.176\n",
      "Epoch [109][0/12]\tLoss: 0.175\n",
      "Epoch [110][0/12]\tLoss: 0.162\n",
      "Epoch [111][0/12]\tLoss: 0.168\n",
      "Epoch [112][0/12]\tLoss: 0.154\n",
      "Epoch [113][0/12]\tLoss: 0.142\n",
      "Epoch [114][0/12]\tLoss: 0.146\n",
      "Epoch [115][0/12]\tLoss: 0.208\n",
      "Epoch [116][0/12]\tLoss: 0.159\n",
      "Epoch [117][0/12]\tLoss: 0.172\n",
      "Epoch [118][0/12]\tLoss: 0.168\n",
      "Epoch [119][0/12]\tLoss: 0.170\n",
      "Epoch [120][0/12]\tLoss: 0.158\n",
      "Epoch [121][0/12]\tLoss: 0.154\n",
      "Epoch [122][0/12]\tLoss: 0.156\n",
      "Epoch [123][0/12]\tLoss: 0.149\n",
      "Epoch [124][0/12]\tLoss: 0.144\n",
      "Epoch [125][0/12]\tLoss: 0.152\n",
      "Epoch [126][0/12]\tLoss: 0.165\n",
      "Epoch [127][0/12]\tLoss: 0.147\n",
      "Epoch [128][0/12]\tLoss: 0.151\n",
      "Epoch [129][0/12]\tLoss: 0.146\n",
      "Epoch [130][0/12]\tLoss: 0.154\n",
      "Epoch [131][0/12]\tLoss: 0.161\n",
      "Epoch [132][0/12]\tLoss: 0.152\n",
      "Epoch [133][0/12]\tLoss: 0.150\n",
      "Epoch [134][0/12]\tLoss: 0.134\n",
      "Epoch [135][0/12]\tLoss: 0.137\n",
      "Epoch [136][0/12]\tLoss: 0.138\n",
      "Epoch [137][0/12]\tLoss: 0.158\n",
      "Epoch [138][0/12]\tLoss: 0.143\n",
      "Epoch [139][0/12]\tLoss: 0.141\n",
      "Epoch [140][0/12]\tLoss: 0.135\n",
      "Epoch [141][0/12]\tLoss: 0.154\n",
      "Epoch [142][0/12]\tLoss: 0.144\n",
      "Epoch [143][0/12]\tLoss: 0.129\n",
      "Epoch [144][0/12]\tLoss: 0.147\n",
      "Epoch [145][0/12]\tLoss: 0.143\n",
      "Epoch [146][0/12]\tLoss: 0.142\n",
      "Epoch [147][0/12]\tLoss: 0.133\n",
      "Epoch [148][0/12]\tLoss: 0.135\n",
      "Epoch [149][0/12]\tLoss: 0.132\n",
      "[neptune] [info   ] Shutting down background jobs, please wait a moment...\n",
      "[neptune] [info   ] Done!\n",
      "[neptune] [info   ] All 0 operations synced, thanks for waiting!\n",
      "[neptune] [info   ] Explore the metadata in the Neptune app: https://app.neptune.ai/andialifs/siet-24/e/SIET-119/metadata\n",
      "\n",
      "Running for experiment 95 with d_model 2048, heads16, num_layers_enc 3, num_layers_dec 5\n",
      "\n",
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/andialifs/siet-24/e/SIET-121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][0/12]\tLoss: 5.954\n",
      "Epoch [1][0/12]\tLoss: 5.852\n",
      "Epoch [2][0/12]\tLoss: 5.665\n",
      "Epoch [3][0/12]\tLoss: 5.392\n",
      "Epoch [4][0/12]\tLoss: 5.200\n",
      "Epoch [5][0/12]\tLoss: 5.015\n",
      "Epoch [6][0/12]\tLoss: 5.002\n",
      "Epoch [7][0/12]\tLoss: 4.768\n",
      "Epoch [8][0/12]\tLoss: 4.783\n",
      "Epoch [9][0/12]\tLoss: 4.737\n",
      "Epoch [10][0/12]\tLoss: 4.504\n",
      "Epoch [11][0/12]\tLoss: 4.516\n",
      "Epoch [12][0/12]\tLoss: 4.385\n",
      "Epoch [13][0/12]\tLoss: 4.342\n",
      "Epoch [14][0/12]\tLoss: 4.058\n",
      "Epoch [15][0/12]\tLoss: 3.946\n",
      "Epoch [16][0/12]\tLoss: 3.739\n",
      "Epoch [17][0/12]\tLoss: 3.462\n",
      "Epoch [18][0/12]\tLoss: 3.524\n",
      "Epoch [19][0/12]\tLoss: 3.388\n",
      "Epoch [20][0/12]\tLoss: 3.318\n",
      "Epoch [21][0/12]\tLoss: 3.114\n",
      "Epoch [22][0/12]\tLoss: 3.136\n",
      "Epoch [23][0/12]\tLoss: 2.783\n",
      "Epoch [24][0/12]\tLoss: 2.831\n",
      "Epoch [25][0/12]\tLoss: 2.900\n",
      "Epoch [26][0/12]\tLoss: 2.599\n",
      "Epoch [27][0/12]\tLoss: 2.707\n",
      "Epoch [28][0/12]\tLoss: 2.650\n",
      "Epoch [29][0/12]\tLoss: 2.068\n",
      "Epoch [30][0/12]\tLoss: 2.166\n",
      "Epoch [31][0/12]\tLoss: 1.978\n",
      "Epoch [32][0/12]\tLoss: 2.130\n",
      "Epoch [33][0/12]\tLoss: 2.002\n",
      "Epoch [34][0/12]\tLoss: 1.856\n",
      "Epoch [35][0/12]\tLoss: 2.000\n",
      "Epoch [36][0/12]\tLoss: 1.796\n",
      "Epoch [37][0/12]\tLoss: 1.554\n",
      "Epoch [38][0/12]\tLoss: 1.705\n",
      "Epoch [39][0/12]\tLoss: 1.827\n",
      "Epoch [40][0/12]\tLoss: 1.489\n",
      "Epoch [41][0/12]\tLoss: 1.282\n",
      "Epoch [42][0/12]\tLoss: 1.253\n",
      "Epoch [43][0/12]\tLoss: 1.495\n",
      "Epoch [44][0/12]\tLoss: 1.327\n",
      "Epoch [45][0/12]\tLoss: 1.226\n",
      "Epoch [46][0/12]\tLoss: 1.213\n",
      "Epoch [47][0/12]\tLoss: 1.124\n",
      "Epoch [48][0/12]\tLoss: 1.128\n",
      "Epoch [49][0/12]\tLoss: 0.866\n",
      "Epoch [50][0/12]\tLoss: 0.948\n",
      "Epoch [51][0/12]\tLoss: 0.762\n",
      "Epoch [52][0/12]\tLoss: 0.941\n",
      "Epoch [53][0/12]\tLoss: 0.702\n",
      "Epoch [54][0/12]\tLoss: 0.794\n",
      "Epoch [55][0/12]\tLoss: 0.632\n",
      "Epoch [56][0/12]\tLoss: 0.770\n",
      "Epoch [57][0/12]\tLoss: 0.682\n",
      "Epoch [58][0/12]\tLoss: 0.683\n",
      "Epoch [59][0/12]\tLoss: 0.612\n",
      "Epoch [60][0/12]\tLoss: 0.562\n",
      "Epoch [61][0/12]\tLoss: 0.481\n",
      "Epoch [62][0/12]\tLoss: 0.532\n",
      "Epoch [63][0/12]\tLoss: 0.416\n",
      "Epoch [64][0/12]\tLoss: 0.456\n",
      "Epoch [65][0/12]\tLoss: 0.460\n",
      "Epoch [66][0/12]\tLoss: 0.364\n",
      "Epoch [67][0/12]\tLoss: 0.385\n",
      "Epoch [68][0/12]\tLoss: 0.327\n",
      "Epoch [69][0/12]\tLoss: 0.361\n",
      "Epoch [70][0/12]\tLoss: 0.379\n",
      "Epoch [71][0/12]\tLoss: 0.331\n",
      "Epoch [72][0/12]\tLoss: 0.337\n",
      "Epoch [73][0/12]\tLoss: 0.334\n",
      "Epoch [74][0/12]\tLoss: 0.290\n",
      "Epoch [75][0/12]\tLoss: 0.287\n",
      "Epoch [76][0/12]\tLoss: 0.332\n",
      "Epoch [77][0/12]\tLoss: 0.282\n",
      "Epoch [78][0/12]\tLoss: 0.281\n",
      "Epoch [79][0/12]\tLoss: 0.265\n",
      "Epoch [80][0/12]\tLoss: 0.244\n",
      "Epoch [81][0/12]\tLoss: 0.236\n",
      "Epoch [82][0/12]\tLoss: 0.252\n",
      "Epoch [83][0/12]\tLoss: 0.206\n",
      "Epoch [84][0/12]\tLoss: 0.254\n",
      "Epoch [85][0/12]\tLoss: 0.220\n",
      "Epoch [86][0/12]\tLoss: 0.241\n",
      "Epoch [87][0/12]\tLoss: 0.267\n",
      "Epoch [88][0/12]\tLoss: 0.227\n",
      "Epoch [89][0/12]\tLoss: 0.200\n",
      "Epoch [90][0/12]\tLoss: 0.194\n",
      "Epoch [91][0/12]\tLoss: 0.224\n",
      "Epoch [92][0/12]\tLoss: 0.213\n",
      "Epoch [93][0/12]\tLoss: 0.190\n",
      "Epoch [94][0/12]\tLoss: 0.231\n",
      "Epoch [95][0/12]\tLoss: 0.194\n",
      "Epoch [96][0/12]\tLoss: 0.192\n",
      "Epoch [97][0/12]\tLoss: 0.197\n",
      "Epoch [98][0/12]\tLoss: 0.205\n",
      "Epoch [99][0/12]\tLoss: 0.191\n",
      "Epoch [100][0/12]\tLoss: 0.193\n",
      "Epoch [101][0/12]\tLoss: 0.168\n",
      "Epoch [102][0/12]\tLoss: 0.200\n",
      "Epoch [103][0/12]\tLoss: 0.163\n",
      "Epoch [104][0/12]\tLoss: 0.167\n",
      "Epoch [105][0/12]\tLoss: 0.200\n",
      "Epoch [106][0/12]\tLoss: 0.172\n",
      "Epoch [107][0/12]\tLoss: 0.254\n",
      "Epoch [108][0/12]\tLoss: 0.184\n",
      "Epoch [109][0/12]\tLoss: 0.190\n",
      "Epoch [110][0/12]\tLoss: 0.199\n",
      "Epoch [111][0/12]\tLoss: 0.167\n",
      "Epoch [112][0/12]\tLoss: 0.187\n",
      "Epoch [113][0/12]\tLoss: 0.166\n",
      "Epoch [114][0/12]\tLoss: 0.180\n",
      "Epoch [115][0/12]\tLoss: 0.155\n",
      "Epoch [116][0/12]\tLoss: 0.193\n",
      "Epoch [117][0/12]\tLoss: 0.179\n",
      "Epoch [118][0/12]\tLoss: 0.143\n",
      "Epoch [119][0/12]\tLoss: 0.144\n",
      "Epoch [120][0/12]\tLoss: 0.141\n",
      "Epoch [121][0/12]\tLoss: 0.152\n",
      "Epoch [122][0/12]\tLoss: 0.167\n",
      "Epoch [123][0/12]\tLoss: 0.156\n",
      "Epoch [124][0/12]\tLoss: 0.150\n",
      "Epoch [125][0/12]\tLoss: 0.164\n",
      "Epoch [126][0/12]\tLoss: 0.155\n",
      "Epoch [127][0/12]\tLoss: 0.140\n",
      "Epoch [128][0/12]\tLoss: 0.135\n",
      "Epoch [129][0/12]\tLoss: 0.153\n",
      "Epoch [130][0/12]\tLoss: 0.141\n",
      "Epoch [131][0/12]\tLoss: 0.157\n",
      "Epoch [132][0/12]\tLoss: 0.143\n",
      "Epoch [133][0/12]\tLoss: 0.129\n",
      "Epoch [134][0/12]\tLoss: 0.153\n",
      "Epoch [135][0/12]\tLoss: 0.135\n",
      "Epoch [136][0/12]\tLoss: 0.122\n",
      "Epoch [137][0/12]\tLoss: 0.154\n",
      "Epoch [138][0/12]\tLoss: 0.133\n",
      "Epoch [139][0/12]\tLoss: 0.134\n",
      "Epoch [140][0/12]\tLoss: 0.126\n",
      "Epoch [141][0/12]\tLoss: 0.143\n",
      "Epoch [142][0/12]\tLoss: 0.131\n",
      "Epoch [143][0/12]\tLoss: 0.148\n",
      "Epoch [144][0/12]\tLoss: 0.121\n",
      "Epoch [145][0/12]\tLoss: 0.119\n",
      "Epoch [146][0/12]\tLoss: 0.136\n",
      "Epoch [147][0/12]\tLoss: 0.159\n",
      "Epoch [148][0/12]\tLoss: 0.138\n",
      "Epoch [149][0/12]\tLoss: 0.130\n",
      "[neptune] [info   ] Shutting down background jobs, please wait a moment...\n",
      "[neptune] [info   ] Done!\n",
      "[neptune] [info   ] All 0 operations synced, thanks for waiting!\n",
      "[neptune] [info   ] Explore the metadata in the Neptune app: https://app.neptune.ai/andialifs/siet-24/e/SIET-121/metadata\n",
      "\n",
      "Running for experiment 96 with d_model 2048, heads16, num_layers_enc 5, num_layers_dec 1\n",
      "\n",
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/andialifs/siet-24/e/SIET-123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][0/12]\tLoss: 6.017\n",
      "Epoch [1][0/12]\tLoss: 5.917\n",
      "Epoch [2][0/12]\tLoss: 5.667\n",
      "Epoch [3][0/12]\tLoss: 5.351\n",
      "Epoch [4][0/12]\tLoss: 5.179\n",
      "Epoch [5][0/12]\tLoss: 5.056\n",
      "Epoch [6][0/12]\tLoss: 4.820\n",
      "Epoch [7][0/12]\tLoss: 4.844\n",
      "Epoch [8][0/12]\tLoss: 4.731\n",
      "Epoch [9][0/12]\tLoss: 4.726\n",
      "Epoch [10][0/12]\tLoss: 4.648\n",
      "Epoch [11][0/12]\tLoss: 4.659\n",
      "Epoch [12][0/12]\tLoss: 4.676\n",
      "Epoch [13][0/12]\tLoss: 4.591\n",
      "Epoch [14][0/12]\tLoss: 4.567\n",
      "Epoch [15][0/12]\tLoss: 4.407\n",
      "Epoch [16][0/12]\tLoss: 4.398\n",
      "Epoch [17][0/12]\tLoss: 4.314\n",
      "Epoch [18][0/12]\tLoss: 3.936\n",
      "Epoch [19][0/12]\tLoss: 4.100\n",
      "Epoch [20][0/12]\tLoss: 3.804\n",
      "Epoch [21][0/12]\tLoss: 3.801\n",
      "Epoch [22][0/12]\tLoss: 3.627\n",
      "Epoch [23][0/12]\tLoss: 3.798\n",
      "Epoch [24][0/12]\tLoss: 3.534\n",
      "Epoch [25][0/12]\tLoss: 3.289\n",
      "Epoch [26][0/12]\tLoss: 3.370\n",
      "Epoch [27][0/12]\tLoss: 3.477\n",
      "Epoch [28][0/12]\tLoss: 3.235\n",
      "Epoch [29][0/12]\tLoss: 3.029\n",
      "Epoch [30][0/12]\tLoss: 3.110\n",
      "Epoch [31][0/12]\tLoss: 2.961\n",
      "Epoch [32][0/12]\tLoss: 2.928\n",
      "Epoch [33][0/12]\tLoss: 2.438\n",
      "Epoch [34][0/12]\tLoss: 2.599\n",
      "Epoch [35][0/12]\tLoss: 2.737\n",
      "Epoch [36][0/12]\tLoss: 2.738\n",
      "Epoch [37][0/12]\tLoss: 2.353\n",
      "Epoch [38][0/12]\tLoss: 2.460\n",
      "Epoch [39][0/12]\tLoss: 1.863\n",
      "Epoch [40][0/12]\tLoss: 2.201\n",
      "Epoch [41][0/12]\tLoss: 1.990\n",
      "Epoch [42][0/12]\tLoss: 1.841\n",
      "Epoch [43][0/12]\tLoss: 1.968\n",
      "Epoch [44][0/12]\tLoss: 1.984\n",
      "Epoch [45][0/12]\tLoss: 1.715\n",
      "Epoch [46][0/12]\tLoss: 1.680\n",
      "Epoch [47][0/12]\tLoss: 1.854\n",
      "Epoch [48][0/12]\tLoss: 1.672\n",
      "Epoch [49][0/12]\tLoss: 1.581\n",
      "Epoch [50][0/12]\tLoss: 1.593\n",
      "Epoch [51][0/12]\tLoss: 1.471\n",
      "Epoch [52][0/12]\tLoss: 1.338\n",
      "Epoch [53][0/12]\tLoss: 1.352\n",
      "Epoch [54][0/12]\tLoss: 1.215\n",
      "Epoch [55][0/12]\tLoss: 1.257\n",
      "Epoch [56][0/12]\tLoss: 1.207\n",
      "Epoch [57][0/12]\tLoss: 0.940\n",
      "Epoch [58][0/12]\tLoss: 1.181\n",
      "Epoch [59][0/12]\tLoss: 1.177\n",
      "Epoch [60][0/12]\tLoss: 1.037\n",
      "Epoch [61][0/12]\tLoss: 1.115\n",
      "Epoch [62][0/12]\tLoss: 0.899\n",
      "Epoch [63][0/12]\tLoss: 0.862\n",
      "Epoch [64][0/12]\tLoss: 0.964\n",
      "Epoch [65][0/12]\tLoss: 0.980\n",
      "Epoch [66][0/12]\tLoss: 0.795\n",
      "Epoch [67][0/12]\tLoss: 0.763\n",
      "Epoch [68][0/12]\tLoss: 0.793\n",
      "Epoch [69][0/12]\tLoss: 0.668\n",
      "Epoch [70][0/12]\tLoss: 0.690\n",
      "Epoch [71][0/12]\tLoss: 0.685\n",
      "Epoch [72][0/12]\tLoss: 0.660\n",
      "Epoch [73][0/12]\tLoss: 0.557\n",
      "Epoch [74][0/12]\tLoss: 0.539\n",
      "Epoch [75][0/12]\tLoss: 0.556\n",
      "Epoch [76][0/12]\tLoss: 0.501\n",
      "Epoch [77][0/12]\tLoss: 0.421\n",
      "Epoch [78][0/12]\tLoss: 0.449\n",
      "Epoch [79][0/12]\tLoss: 0.462\n",
      "Epoch [80][0/12]\tLoss: 0.484\n",
      "Epoch [81][0/12]\tLoss: 0.440\n",
      "Epoch [82][0/12]\tLoss: 0.408\n",
      "Epoch [83][0/12]\tLoss: 0.418\n",
      "Epoch [84][0/12]\tLoss: 0.429\n",
      "Epoch [85][0/12]\tLoss: 0.331\n",
      "Epoch [86][0/12]\tLoss: 0.367\n",
      "Epoch [87][0/12]\tLoss: 0.333\n",
      "Epoch [88][0/12]\tLoss: 0.362\n",
      "Epoch [89][0/12]\tLoss: 0.378\n",
      "Epoch [90][0/12]\tLoss: 0.355\n",
      "Epoch [91][0/12]\tLoss: 0.352\n",
      "Epoch [92][0/12]\tLoss: 0.305\n",
      "Epoch [93][0/12]\tLoss: 0.305\n",
      "Epoch [94][0/12]\tLoss: 0.304\n",
      "Epoch [95][0/12]\tLoss: 0.276\n",
      "Epoch [96][0/12]\tLoss: 0.341\n",
      "Epoch [97][0/12]\tLoss: 0.313\n",
      "Epoch [98][0/12]\tLoss: 0.271\n",
      "Epoch [99][0/12]\tLoss: 0.273\n",
      "Epoch [100][0/12]\tLoss: 0.279\n",
      "Epoch [101][0/12]\tLoss: 0.304\n",
      "Epoch [102][0/12]\tLoss: 0.277\n",
      "Epoch [103][0/12]\tLoss: 0.240\n",
      "Epoch [104][0/12]\tLoss: 0.252\n",
      "Epoch [105][0/12]\tLoss: 0.258\n",
      "Epoch [106][0/12]\tLoss: 0.266\n",
      "Epoch [107][0/12]\tLoss: 0.244\n",
      "Epoch [108][0/12]\tLoss: 0.249\n",
      "Epoch [109][0/12]\tLoss: 0.243\n",
      "Epoch [110][0/12]\tLoss: 0.249\n",
      "Epoch [111][0/12]\tLoss: 0.244\n",
      "Epoch [112][0/12]\tLoss: 0.220\n",
      "Epoch [113][0/12]\tLoss: 0.224\n",
      "Epoch [114][0/12]\tLoss: 0.210\n",
      "Epoch [115][0/12]\tLoss: 0.233\n",
      "Epoch [116][0/12]\tLoss: 0.214\n",
      "Epoch [117][0/12]\tLoss: 0.194\n",
      "Epoch [118][0/12]\tLoss: 0.183\n",
      "Epoch [119][0/12]\tLoss: 0.201\n",
      "Epoch [120][0/12]\tLoss: 0.195\n",
      "Epoch [121][0/12]\tLoss: 0.212\n",
      "Epoch [122][0/12]\tLoss: 0.223\n",
      "Epoch [123][0/12]\tLoss: 0.214\n",
      "Epoch [124][0/12]\tLoss: 0.196\n",
      "Epoch [125][0/12]\tLoss: 0.221\n",
      "Epoch [126][0/12]\tLoss: 0.195\n",
      "Epoch [127][0/12]\tLoss: 0.201\n",
      "Epoch [128][0/12]\tLoss: 0.186\n",
      "Epoch [129][0/12]\tLoss: 0.186\n",
      "Epoch [130][0/12]\tLoss: 0.177\n",
      "Epoch [131][0/12]\tLoss: 0.203\n",
      "Epoch [132][0/12]\tLoss: 0.191\n",
      "Epoch [133][0/12]\tLoss: 0.189\n",
      "Epoch [134][0/12]\tLoss: 0.183\n",
      "Epoch [135][0/12]\tLoss: 0.183\n",
      "Epoch [136][0/12]\tLoss: 0.171\n",
      "Epoch [137][0/12]\tLoss: 0.218\n",
      "Epoch [138][0/12]\tLoss: 0.166\n",
      "Epoch [139][0/12]\tLoss: 0.167\n",
      "Epoch [140][0/12]\tLoss: 0.202\n",
      "Epoch [141][0/12]\tLoss: 0.182\n",
      "Epoch [142][0/12]\tLoss: 0.167\n",
      "Epoch [143][0/12]\tLoss: 0.183\n",
      "Epoch [144][0/12]\tLoss: 0.176\n",
      "Epoch [145][0/12]\tLoss: 0.184\n",
      "Epoch [146][0/12]\tLoss: 0.168\n",
      "Epoch [147][0/12]\tLoss: 0.178\n",
      "Epoch [148][0/12]\tLoss: 0.176\n",
      "Epoch [149][0/12]\tLoss: 0.152\n",
      "[neptune] [info   ] Shutting down background jobs, please wait a moment...\n",
      "[neptune] [info   ] Done!\n",
      "[neptune] [info   ] All 0 operations synced, thanks for waiting!\n",
      "[neptune] [info   ] Explore the metadata in the Neptune app: https://app.neptune.ai/andialifs/siet-24/e/SIET-123/metadata\n",
      "\n",
      "Running for experiment 97 with d_model 2048, heads16, num_layers_enc 5, num_layers_dec 3\n",
      "\n",
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/andialifs/siet-24/e/SIET-124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][0/12]\tLoss: 6.039\n",
      "Epoch [1][0/12]\tLoss: 5.924\n",
      "Epoch [2][0/12]\tLoss: 5.689\n",
      "Epoch [3][0/12]\tLoss: 5.429\n",
      "Epoch [4][0/12]\tLoss: 5.162\n",
      "Epoch [5][0/12]\tLoss: 4.963\n",
      "Epoch [6][0/12]\tLoss: 4.932\n",
      "Epoch [7][0/12]\tLoss: 4.743\n",
      "Epoch [8][0/12]\tLoss: 4.740\n",
      "Epoch [9][0/12]\tLoss: 4.670\n",
      "Epoch [10][0/12]\tLoss: 4.686\n",
      "Epoch [11][0/12]\tLoss: 4.690\n",
      "Epoch [12][0/12]\tLoss: 4.661\n",
      "Epoch [13][0/12]\tLoss: 4.498\n",
      "Epoch [14][0/12]\tLoss: 4.470\n",
      "Epoch [15][0/12]\tLoss: 4.546\n",
      "Epoch [16][0/12]\tLoss: 4.191\n",
      "Epoch [17][0/12]\tLoss: 4.171\n",
      "Epoch [18][0/12]\tLoss: 4.006\n",
      "Epoch [19][0/12]\tLoss: 3.968\n",
      "Epoch [20][0/12]\tLoss: 4.002\n",
      "Epoch [21][0/12]\tLoss: 3.683\n",
      "Epoch [22][0/12]\tLoss: 3.886\n",
      "Epoch [23][0/12]\tLoss: 3.633\n",
      "Epoch [24][0/12]\tLoss: 3.737\n",
      "Epoch [25][0/12]\tLoss: 3.317\n",
      "Epoch [26][0/12]\tLoss: 3.346\n",
      "Epoch [27][0/12]\tLoss: 3.302\n",
      "Epoch [28][0/12]\tLoss: 3.053\n",
      "Epoch [29][0/12]\tLoss: 2.824\n",
      "Epoch [30][0/12]\tLoss: 3.201\n",
      "Epoch [31][0/12]\tLoss: 2.770\n",
      "Epoch [32][0/12]\tLoss: 2.749\n",
      "Epoch [33][0/12]\tLoss: 2.845\n",
      "Epoch [34][0/12]\tLoss: 2.611\n",
      "Epoch [35][0/12]\tLoss: 2.609\n",
      "Epoch [36][0/12]\tLoss: 2.166\n",
      "Epoch [37][0/12]\tLoss: 2.314\n",
      "Epoch [38][0/12]\tLoss: 2.166\n",
      "Epoch [39][0/12]\tLoss: 1.942\n",
      "Epoch [40][0/12]\tLoss: 2.279\n",
      "Epoch [41][0/12]\tLoss: 2.101\n",
      "Epoch [42][0/12]\tLoss: 1.876\n",
      "Epoch [43][0/12]\tLoss: 1.837\n",
      "Epoch [44][0/12]\tLoss: 1.715\n",
      "Epoch [45][0/12]\tLoss: 1.821\n",
      "Epoch [46][0/12]\tLoss: 1.623\n",
      "Epoch [47][0/12]\tLoss: 1.739\n",
      "Epoch [48][0/12]\tLoss: 1.740\n",
      "Epoch [49][0/12]\tLoss: 1.714\n",
      "Epoch [50][0/12]\tLoss: 1.570\n",
      "Epoch [51][0/12]\tLoss: 1.593\n",
      "Epoch [52][0/12]\tLoss: 1.253\n",
      "Epoch [53][0/12]\tLoss: 1.345\n",
      "Epoch [54][0/12]\tLoss: 1.243\n",
      "Epoch [55][0/12]\tLoss: 1.155\n",
      "Epoch [56][0/12]\tLoss: 1.154\n",
      "Epoch [57][0/12]\tLoss: 1.051\n",
      "Epoch [58][0/12]\tLoss: 1.139\n",
      "Epoch [59][0/12]\tLoss: 1.211\n",
      "Epoch [60][0/12]\tLoss: 0.856\n",
      "Epoch [61][0/12]\tLoss: 0.848\n",
      "Epoch [62][0/12]\tLoss: 0.894\n",
      "Epoch [63][0/12]\tLoss: 0.829\n",
      "Epoch [64][0/12]\tLoss: 0.936\n",
      "Epoch [65][0/12]\tLoss: 0.901\n",
      "Epoch [66][0/12]\tLoss: 0.960\n",
      "Epoch [67][0/12]\tLoss: 0.781\n",
      "Epoch [68][0/12]\tLoss: 0.759\n",
      "Epoch [69][0/12]\tLoss: 0.702\n",
      "Epoch [70][0/12]\tLoss: 0.580\n",
      "Epoch [71][0/12]\tLoss: 0.640\n",
      "Epoch [72][0/12]\tLoss: 0.592\n",
      "Epoch [73][0/12]\tLoss: 0.703\n",
      "Epoch [74][0/12]\tLoss: 0.656\n",
      "Epoch [75][0/12]\tLoss: 0.532\n",
      "Epoch [76][0/12]\tLoss: 0.540\n",
      "Epoch [77][0/12]\tLoss: 0.449\n",
      "Epoch [78][0/12]\tLoss: 0.466\n",
      "Epoch [79][0/12]\tLoss: 0.448\n",
      "Epoch [80][0/12]\tLoss: 0.438\n",
      "Epoch [81][0/12]\tLoss: 0.436\n",
      "Epoch [82][0/12]\tLoss: 0.424\n",
      "Epoch [83][0/12]\tLoss: 0.402\n",
      "Epoch [84][0/12]\tLoss: 0.366\n",
      "Epoch [85][0/12]\tLoss: 0.396\n",
      "Epoch [86][0/12]\tLoss: 0.361\n",
      "Epoch [87][0/12]\tLoss: 0.364\n",
      "Epoch [88][0/12]\tLoss: 0.363\n",
      "Epoch [89][0/12]\tLoss: 0.333\n",
      "Epoch [90][0/12]\tLoss: 0.331\n",
      "Epoch [91][0/12]\tLoss: 0.312\n",
      "Epoch [92][0/12]\tLoss: 0.349\n",
      "Epoch [93][0/12]\tLoss: 0.305\n",
      "Epoch [94][0/12]\tLoss: 0.324\n",
      "Epoch [95][0/12]\tLoss: 0.304\n",
      "Epoch [96][0/12]\tLoss: 0.310\n",
      "Epoch [97][0/12]\tLoss: 0.306\n",
      "Epoch [98][0/12]\tLoss: 0.307\n",
      "Epoch [99][0/12]\tLoss: 0.273\n",
      "Epoch [100][0/12]\tLoss: 0.270\n",
      "Epoch [101][0/12]\tLoss: 0.257\n",
      "Epoch [102][0/12]\tLoss: 0.262\n",
      "Epoch [103][0/12]\tLoss: 0.262\n",
      "Epoch [104][0/12]\tLoss: 0.267\n",
      "Epoch [105][0/12]\tLoss: 0.268\n",
      "Epoch [106][0/12]\tLoss: 0.265\n",
      "Epoch [107][0/12]\tLoss: 0.255\n",
      "Epoch [108][0/12]\tLoss: 0.245\n",
      "Epoch [109][0/12]\tLoss: 0.245\n",
      "Epoch [110][0/12]\tLoss: 0.238\n",
      "Epoch [111][0/12]\tLoss: 0.237\n",
      "Epoch [112][0/12]\tLoss: 0.228\n",
      "Epoch [113][0/12]\tLoss: 0.220\n",
      "Epoch [114][0/12]\tLoss: 0.226\n",
      "Epoch [115][0/12]\tLoss: 0.219\n",
      "Epoch [116][0/12]\tLoss: 0.232\n",
      "Epoch [117][0/12]\tLoss: 0.208\n",
      "Epoch [118][0/12]\tLoss: 0.236\n",
      "Epoch [119][0/12]\tLoss: 0.215\n",
      "Epoch [120][0/12]\tLoss: 0.209\n",
      "Epoch [121][0/12]\tLoss: 0.216\n",
      "Epoch [122][0/12]\tLoss: 0.205\n",
      "Epoch [123][0/12]\tLoss: 0.185\n",
      "Epoch [124][0/12]\tLoss: 0.204\n",
      "Epoch [125][0/12]\tLoss: 0.192\n",
      "Epoch [126][0/12]\tLoss: 0.195\n",
      "Epoch [127][0/12]\tLoss: 0.224\n",
      "Epoch [128][0/12]\tLoss: 0.185\n",
      "Epoch [129][0/12]\tLoss: 0.202\n",
      "Epoch [130][0/12]\tLoss: 0.204\n",
      "Epoch [131][0/12]\tLoss: 0.204\n",
      "Epoch [132][0/12]\tLoss: 0.204\n",
      "Epoch [133][0/12]\tLoss: 0.215\n",
      "Epoch [134][0/12]\tLoss: 0.179\n",
      "Epoch [135][0/12]\tLoss: 0.179\n",
      "Epoch [136][0/12]\tLoss: 0.176\n",
      "Epoch [137][0/12]\tLoss: 0.182\n",
      "Epoch [138][0/12]\tLoss: 0.173\n",
      "Epoch [139][0/12]\tLoss: 0.204\n",
      "Epoch [140][0/12]\tLoss: 0.199\n",
      "Epoch [141][0/12]\tLoss: 0.171\n",
      "Epoch [142][0/12]\tLoss: 0.165\n",
      "Epoch [143][0/12]\tLoss: 0.166\n",
      "Epoch [144][0/12]\tLoss: 0.180\n",
      "Epoch [145][0/12]\tLoss: 0.159\n",
      "Epoch [146][0/12]\tLoss: 0.179\n",
      "Epoch [147][0/12]\tLoss: 0.178\n",
      "Epoch [148][0/12]\tLoss: 0.165\n",
      "Epoch [149][0/12]\tLoss: 0.156\n",
      "[neptune] [info   ] Shutting down background jobs, please wait a moment...\n",
      "[neptune] [info   ] Done!\n",
      "[neptune] [info   ] All 0 operations synced, thanks for waiting!\n",
      "[neptune] [info   ] Explore the metadata in the Neptune app: https://app.neptune.ai/andialifs/siet-24/e/SIET-124/metadata\n",
      "\n",
      "Running for experiment 98 with d_model 2048, heads16, num_layers_enc 5, num_layers_dec 5\n",
      "\n",
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/andialifs/siet-24/e/SIET-125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][0/12]\tLoss: 5.965\n",
      "Epoch [1][0/12]\tLoss: 5.853\n",
      "Epoch [2][0/12]\tLoss: 5.496\n",
      "Epoch [3][0/12]\tLoss: 5.334\n",
      "Epoch [4][0/12]\tLoss: 5.114\n",
      "Epoch [5][0/12]\tLoss: 4.987\n",
      "Epoch [6][0/12]\tLoss: 4.792\n",
      "Epoch [7][0/12]\tLoss: 4.864\n",
      "Epoch [8][0/12]\tLoss: 4.813\n",
      "Epoch [9][0/12]\tLoss: 4.623\n",
      "Epoch [10][0/12]\tLoss: 4.581\n",
      "Epoch [11][0/12]\tLoss: 4.639\n",
      "Epoch [12][0/12]\tLoss: 4.548\n",
      "Epoch [13][0/12]\tLoss: 4.584\n",
      "Epoch [14][0/12]\tLoss: 4.535\n",
      "Epoch [15][0/12]\tLoss: 4.466\n",
      "Epoch [16][0/12]\tLoss: 4.172\n",
      "Epoch [17][0/12]\tLoss: 4.206\n",
      "Epoch [18][0/12]\tLoss: 4.152\n",
      "Epoch [19][0/12]\tLoss: 4.123\n",
      "Epoch [20][0/12]\tLoss: 3.777\n",
      "Epoch [21][0/12]\tLoss: 3.862\n",
      "Epoch [22][0/12]\tLoss: 3.811\n",
      "Epoch [23][0/12]\tLoss: 3.592\n",
      "Epoch [24][0/12]\tLoss: 3.602\n",
      "Epoch [25][0/12]\tLoss: 3.278\n",
      "Epoch [26][0/12]\tLoss: 3.365\n",
      "Epoch [27][0/12]\tLoss: 3.222\n",
      "Epoch [28][0/12]\tLoss: 3.188\n",
      "Epoch [29][0/12]\tLoss: 2.954\n",
      "Epoch [30][0/12]\tLoss: 2.983\n",
      "Epoch [31][0/12]\tLoss: 2.993\n",
      "Epoch [32][0/12]\tLoss: 2.808\n",
      "Epoch [33][0/12]\tLoss: 2.559\n",
      "Epoch [34][0/12]\tLoss: 2.759\n",
      "Epoch [35][0/12]\tLoss: 2.506\n",
      "Epoch [36][0/12]\tLoss: 2.525\n",
      "Epoch [37][0/12]\tLoss: 2.235\n",
      "Epoch [38][0/12]\tLoss: 1.970\n",
      "Epoch [39][0/12]\tLoss: 1.870\n",
      "Epoch [40][0/12]\tLoss: 2.147\n",
      "Epoch [41][0/12]\tLoss: 1.876\n",
      "Epoch [42][0/12]\tLoss: 2.102\n",
      "Epoch [43][0/12]\tLoss: 1.923\n",
      "Epoch [44][0/12]\tLoss: 1.753\n",
      "Epoch [45][0/12]\tLoss: 2.083\n",
      "Epoch [46][0/12]\tLoss: 1.645\n",
      "Epoch [47][0/12]\tLoss: 1.657\n",
      "Epoch [48][0/12]\tLoss: 1.490\n",
      "Epoch [49][0/12]\tLoss: 1.431\n",
      "Epoch [50][0/12]\tLoss: 1.651\n",
      "Epoch [51][0/12]\tLoss: 1.640\n",
      "Epoch [52][0/12]\tLoss: 1.341\n",
      "Epoch [53][0/12]\tLoss: 1.389\n",
      "Epoch [54][0/12]\tLoss: 1.432\n",
      "Epoch [55][0/12]\tLoss: 1.105\n",
      "Epoch [56][0/12]\tLoss: 1.299\n",
      "Epoch [57][0/12]\tLoss: 1.245\n",
      "Epoch [58][0/12]\tLoss: 1.114\n",
      "Epoch [59][0/12]\tLoss: 1.109\n",
      "Epoch [60][0/12]\tLoss: 1.139\n",
      "Epoch [61][0/12]\tLoss: 0.827\n",
      "Epoch [62][0/12]\tLoss: 0.869\n",
      "Epoch [63][0/12]\tLoss: 0.800\n",
      "Epoch [64][0/12]\tLoss: 0.967\n",
      "Epoch [65][0/12]\tLoss: 0.690\n",
      "Epoch [66][0/12]\tLoss: 0.852\n",
      "Epoch [67][0/12]\tLoss: 0.766\n",
      "Epoch [68][0/12]\tLoss: 0.674\n",
      "Epoch [69][0/12]\tLoss: 0.866\n",
      "Epoch [70][0/12]\tLoss: 0.649\n",
      "Epoch [71][0/12]\tLoss: 0.666\n",
      "Epoch [72][0/12]\tLoss: 0.642\n",
      "Epoch [73][0/12]\tLoss: 0.633\n",
      "Epoch [74][0/12]\tLoss: 0.473\n",
      "Epoch [75][0/12]\tLoss: 0.560\n",
      "Epoch [76][0/12]\tLoss: 0.552\n",
      "Epoch [77][0/12]\tLoss: 0.499\n",
      "Epoch [78][0/12]\tLoss: 0.461\n",
      "Epoch [79][0/12]\tLoss: 0.382\n",
      "Epoch [80][0/12]\tLoss: 0.484\n",
      "Epoch [81][0/12]\tLoss: 0.460\n",
      "Epoch [82][0/12]\tLoss: 0.446\n",
      "Epoch [83][0/12]\tLoss: 0.435\n",
      "Epoch [84][0/12]\tLoss: 0.397\n",
      "Epoch [85][0/12]\tLoss: 0.399\n",
      "Epoch [86][0/12]\tLoss: 0.393\n",
      "Epoch [87][0/12]\tLoss: 0.395\n",
      "Epoch [88][0/12]\tLoss: 0.366\n",
      "Epoch [89][0/12]\tLoss: 0.324\n",
      "Epoch [90][0/12]\tLoss: 0.307\n",
      "Epoch [91][0/12]\tLoss: 0.327\n",
      "Epoch [92][0/12]\tLoss: 0.325\n",
      "Epoch [93][0/12]\tLoss: 0.332\n",
      "Epoch [94][0/12]\tLoss: 0.311\n",
      "Epoch [95][0/12]\tLoss: 0.298\n",
      "Epoch [96][0/12]\tLoss: 0.296\n",
      "Epoch [97][0/12]\tLoss: 0.301\n",
      "Epoch [98][0/12]\tLoss: 0.305\n",
      "Epoch [99][0/12]\tLoss: 0.274\n",
      "Epoch [100][0/12]\tLoss: 0.264\n",
      "Epoch [101][0/12]\tLoss: 0.267\n",
      "Epoch [102][0/12]\tLoss: 0.279\n",
      "Epoch [103][0/12]\tLoss: 0.261\n",
      "Epoch [104][0/12]\tLoss: 0.263\n",
      "Epoch [105][0/12]\tLoss: 0.269\n",
      "Epoch [106][0/12]\tLoss: 0.252\n",
      "Epoch [107][0/12]\tLoss: 0.251\n",
      "Epoch [108][0/12]\tLoss: 0.239\n",
      "Epoch [109][0/12]\tLoss: 0.256\n",
      "Epoch [110][0/12]\tLoss: 0.242\n",
      "Epoch [111][0/12]\tLoss: 0.231\n",
      "Epoch [112][0/12]\tLoss: 0.227\n",
      "Epoch [113][0/12]\tLoss: 0.209\n",
      "Epoch [114][0/12]\tLoss: 0.220\n",
      "Epoch [115][0/12]\tLoss: 0.231\n",
      "Epoch [116][0/12]\tLoss: 0.204\n",
      "Epoch [117][0/12]\tLoss: 0.246\n",
      "Epoch [118][0/12]\tLoss: 0.232\n",
      "Epoch [119][0/12]\tLoss: 0.215\n",
      "Epoch [120][0/12]\tLoss: 0.192\n",
      "Epoch [121][0/12]\tLoss: 0.218\n",
      "Epoch [122][0/12]\tLoss: 0.213\n",
      "Epoch [123][0/12]\tLoss: 0.232\n",
      "Epoch [124][0/12]\tLoss: 0.205\n",
      "Epoch [125][0/12]\tLoss: 0.198\n",
      "Epoch [126][0/12]\tLoss: 0.223\n",
      "Epoch [127][0/12]\tLoss: 0.205\n",
      "Epoch [128][0/12]\tLoss: 0.191\n",
      "Epoch [129][0/12]\tLoss: 0.178\n",
      "Epoch [130][0/12]\tLoss: 0.184\n",
      "Epoch [131][0/12]\tLoss: 0.171\n",
      "Epoch [132][0/12]\tLoss: 0.196\n",
      "Epoch [133][0/12]\tLoss: 0.195\n",
      "Epoch [134][0/12]\tLoss: 0.191\n",
      "Epoch [135][0/12]\tLoss: 0.181\n",
      "Epoch [136][0/12]\tLoss: 0.162\n",
      "Epoch [137][0/12]\tLoss: 0.199\n",
      "Epoch [138][0/12]\tLoss: 0.175\n",
      "Epoch [139][0/12]\tLoss: 0.184\n",
      "Epoch [140][0/12]\tLoss: 0.180\n",
      "Epoch [141][0/12]\tLoss: 0.171\n",
      "Epoch [142][0/12]\tLoss: 0.160\n",
      "Epoch [143][0/12]\tLoss: 0.181\n",
      "Epoch [144][0/12]\tLoss: 0.166\n",
      "Epoch [145][0/12]\tLoss: 0.166\n",
      "Epoch [146][0/12]\tLoss: 0.171\n",
      "Epoch [147][0/12]\tLoss: 0.166\n",
      "Epoch [148][0/12]\tLoss: 0.156\n",
      "Epoch [149][0/12]\tLoss: 0.157\n",
      "[neptune] [info   ] Shutting down background jobs, please wait a moment...\n",
      "[neptune] [info   ] Done!\n",
      "[neptune] [info   ] All 0 operations synced, thanks for waiting!\n",
      "[neptune] [info   ] Explore the metadata in the Neptune app: https://app.neptune.ai/andialifs/siet-24/e/SIET-125/metadata\n",
      "\n",
      "Running for experiment 99 with d_model 2048, heads32, num_layers_enc 1, num_layers_dec 1\n",
      "\n",
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/andialifs/siet-24/e/SIET-126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][0/12]\tLoss: 5.932\n",
      "Epoch [1][0/12]\tLoss: 5.914\n",
      "Epoch [2][0/12]\tLoss: 5.863\n",
      "Epoch [3][0/12]\tLoss: 5.792\n",
      "Epoch [4][0/12]\tLoss: 5.654\n",
      "Epoch [5][0/12]\tLoss: 5.534\n",
      "Epoch [6][0/12]\tLoss: 5.300\n",
      "Epoch [7][0/12]\tLoss: 5.139\n",
      "Epoch [8][0/12]\tLoss: 4.857\n",
      "Epoch [9][0/12]\tLoss: 4.789\n",
      "Epoch [10][0/12]\tLoss: 4.558\n",
      "Epoch [11][0/12]\tLoss: 4.267\n",
      "Epoch [12][0/12]\tLoss: 4.273\n",
      "Epoch [13][0/12]\tLoss: 4.138\n",
      "Epoch [14][0/12]\tLoss: 4.088\n",
      "Epoch [15][0/12]\tLoss: 3.875\n",
      "Epoch [16][0/12]\tLoss: 3.749\n",
      "Epoch [17][0/12]\tLoss: 3.725\n",
      "Epoch [18][0/12]\tLoss: 3.562\n",
      "Epoch [19][0/12]\tLoss: 3.417\n",
      "Epoch [20][0/12]\tLoss: 3.261\n",
      "Epoch [21][0/12]\tLoss: 2.954\n",
      "Epoch [22][0/12]\tLoss: 2.883\n",
      "Epoch [23][0/12]\tLoss: 2.784\n",
      "Epoch [24][0/12]\tLoss: 2.830\n",
      "Epoch [25][0/12]\tLoss: 2.872\n",
      "Epoch [26][0/12]\tLoss: 2.845\n",
      "Epoch [27][0/12]\tLoss: 2.346\n",
      "Epoch [28][0/12]\tLoss: 2.081\n",
      "Epoch [29][0/12]\tLoss: 1.751\n",
      "Epoch [30][0/12]\tLoss: 1.813\n",
      "Epoch [31][0/12]\tLoss: 2.145\n",
      "Epoch [32][0/12]\tLoss: 2.097\n",
      "Epoch [33][0/12]\tLoss: 1.782\n",
      "Epoch [34][0/12]\tLoss: 1.383\n",
      "Epoch [35][0/12]\tLoss: 1.673\n",
      "Epoch [36][0/12]\tLoss: 1.479\n",
      "Epoch [37][0/12]\tLoss: 1.326\n",
      "Epoch [38][0/12]\tLoss: 1.278\n",
      "Epoch [39][0/12]\tLoss: 1.361\n",
      "Epoch [40][0/12]\tLoss: 1.250\n",
      "Epoch [41][0/12]\tLoss: 1.024\n",
      "Epoch [42][0/12]\tLoss: 0.954\n",
      "Epoch [43][0/12]\tLoss: 0.919\n",
      "Epoch [44][0/12]\tLoss: 0.839\n",
      "Epoch [45][0/12]\tLoss: 0.744\n",
      "Epoch [46][0/12]\tLoss: 0.604\n",
      "Epoch [47][0/12]\tLoss: 0.713\n",
      "Epoch [48][0/12]\tLoss: 0.612\n",
      "Epoch [49][0/12]\tLoss: 0.543\n",
      "Epoch [50][0/12]\tLoss: 0.484\n",
      "Epoch [51][0/12]\tLoss: 0.523\n",
      "Epoch [52][0/12]\tLoss: 0.462\n",
      "Epoch [53][0/12]\tLoss: 0.428\n",
      "Epoch [54][0/12]\tLoss: 0.411\n",
      "Epoch [55][0/12]\tLoss: 0.421\n",
      "Epoch [56][0/12]\tLoss: 0.447\n",
      "Epoch [57][0/12]\tLoss: 0.362\n",
      "Epoch [58][0/12]\tLoss: 0.366\n",
      "Epoch [59][0/12]\tLoss: 0.313\n",
      "Epoch [60][0/12]\tLoss: 0.319\n",
      "Epoch [61][0/12]\tLoss: 0.301\n",
      "Epoch [62][0/12]\tLoss: 0.318\n",
      "Epoch [63][0/12]\tLoss: 0.313\n",
      "Epoch [64][0/12]\tLoss: 0.322\n",
      "Epoch [65][0/12]\tLoss: 0.295\n",
      "Epoch [66][0/12]\tLoss: 0.261\n",
      "Epoch [67][0/12]\tLoss: 0.247\n",
      "Epoch [68][0/12]\tLoss: 0.294\n",
      "Epoch [69][0/12]\tLoss: 0.238\n",
      "Epoch [70][0/12]\tLoss: 0.274\n",
      "Epoch [71][0/12]\tLoss: 0.260\n",
      "Epoch [72][0/12]\tLoss: 0.267\n",
      "Epoch [73][0/12]\tLoss: 0.228\n",
      "Epoch [74][0/12]\tLoss: 0.257\n",
      "Epoch [75][0/12]\tLoss: 0.246\n",
      "Epoch [76][0/12]\tLoss: 0.216\n",
      "Epoch [77][0/12]\tLoss: 0.235\n",
      "Epoch [78][0/12]\tLoss: 0.202\n",
      "Epoch [79][0/12]\tLoss: 0.208\n",
      "Epoch [80][0/12]\tLoss: 0.226\n",
      "Epoch [81][0/12]\tLoss: 0.216\n",
      "Epoch [82][0/12]\tLoss: 0.212\n",
      "Epoch [83][0/12]\tLoss: 0.213\n",
      "Epoch [84][0/12]\tLoss: 0.221\n",
      "Epoch [85][0/12]\tLoss: 0.215\n",
      "Epoch [86][0/12]\tLoss: 0.214\n",
      "Epoch [87][0/12]\tLoss: 0.228\n",
      "Epoch [88][0/12]\tLoss: 0.207\n",
      "Epoch [89][0/12]\tLoss: 0.210\n",
      "Epoch [90][0/12]\tLoss: 0.180\n",
      "Epoch [91][0/12]\tLoss: 0.170\n",
      "Epoch [92][0/12]\tLoss: 0.191\n",
      "Epoch [93][0/12]\tLoss: 0.191\n",
      "Epoch [94][0/12]\tLoss: 0.190\n",
      "Epoch [95][0/12]\tLoss: 0.202\n",
      "Epoch [96][0/12]\tLoss: 0.202\n",
      "Epoch [97][0/12]\tLoss: 0.188\n",
      "Epoch [98][0/12]\tLoss: 0.192\n",
      "Epoch [99][0/12]\tLoss: 0.222\n",
      "Epoch [100][0/12]\tLoss: 0.213\n",
      "Epoch [101][0/12]\tLoss: 0.178\n",
      "Epoch [102][0/12]\tLoss: 0.195\n",
      "Epoch [103][0/12]\tLoss: 0.183\n",
      "Epoch [104][0/12]\tLoss: 0.182\n",
      "Epoch [105][0/12]\tLoss: 0.217\n",
      "Epoch [106][0/12]\tLoss: 0.201\n",
      "Epoch [107][0/12]\tLoss: 0.210\n",
      "Epoch [108][0/12]\tLoss: 0.223\n",
      "Epoch [109][0/12]\tLoss: 0.182\n",
      "Epoch [110][0/12]\tLoss: 0.170\n",
      "Epoch [111][0/12]\tLoss: 0.185\n",
      "Epoch [112][0/12]\tLoss: 0.207\n",
      "Epoch [113][0/12]\tLoss: 0.188\n",
      "Epoch [114][0/12]\tLoss: 0.184\n",
      "Epoch [115][0/12]\tLoss: 0.178\n",
      "Epoch [116][0/12]\tLoss: 0.171\n",
      "Epoch [117][0/12]\tLoss: 0.184\n",
      "Epoch [118][0/12]\tLoss: 0.173\n",
      "Epoch [119][0/12]\tLoss: 0.190\n",
      "Epoch [120][0/12]\tLoss: 0.193\n",
      "Epoch [121][0/12]\tLoss: 0.186\n",
      "Epoch [122][0/12]\tLoss: 0.159\n",
      "Epoch [123][0/12]\tLoss: 0.153\n",
      "Epoch [124][0/12]\tLoss: 0.164\n",
      "Epoch [125][0/12]\tLoss: 0.163\n",
      "Epoch [126][0/12]\tLoss: 0.181\n",
      "Epoch [127][0/12]\tLoss: 0.170\n",
      "Epoch [128][0/12]\tLoss: 0.163\n",
      "Epoch [129][0/12]\tLoss: 0.168\n",
      "Epoch [130][0/12]\tLoss: 0.166\n",
      "Epoch [131][0/12]\tLoss: 0.188\n",
      "Epoch [132][0/12]\tLoss: 0.154\n",
      "Epoch [133][0/12]\tLoss: 0.160\n",
      "Epoch [134][0/12]\tLoss: 0.169\n",
      "Epoch [135][0/12]\tLoss: 0.171\n",
      "Epoch [136][0/12]\tLoss: 0.173\n",
      "Epoch [137][0/12]\tLoss: 0.165\n",
      "Epoch [138][0/12]\tLoss: 0.183\n",
      "Epoch [139][0/12]\tLoss: 0.180\n",
      "Epoch [140][0/12]\tLoss: 0.204\n",
      "Epoch [141][0/12]\tLoss: 0.151\n",
      "Epoch [142][0/12]\tLoss: 0.153\n",
      "Epoch [143][0/12]\tLoss: 0.185\n",
      "Epoch [144][0/12]\tLoss: 0.175\n",
      "Epoch [145][0/12]\tLoss: 0.180\n",
      "Epoch [146][0/12]\tLoss: 0.159\n",
      "Epoch [147][0/12]\tLoss: 0.179\n",
      "Epoch [148][0/12]\tLoss: 0.179\n",
      "Epoch [149][0/12]\tLoss: 0.154\n",
      "[neptune] [info   ] Shutting down background jobs, please wait a moment...\n",
      "[neptune] [info   ] Done!\n",
      "[neptune] [info   ] All 0 operations synced, thanks for waiting!\n",
      "[neptune] [info   ] Explore the metadata in the Neptune app: https://app.neptune.ai/andialifs/siet-24/e/SIET-126/metadata\n",
      "\n",
      "Running for experiment 100 with d_model 2048, heads32, num_layers_enc 1, num_layers_dec 3\n",
      "\n",
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/andialifs/siet-24/e/SIET-127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][0/12]\tLoss: 5.945\n",
      "Epoch [1][0/12]\tLoss: 5.921\n",
      "Epoch [2][0/12]\tLoss: 5.858\n",
      "Epoch [3][0/12]\tLoss: 5.785\n",
      "Epoch [4][0/12]\tLoss: 5.693\n",
      "Epoch [5][0/12]\tLoss: 5.532\n",
      "Epoch [6][0/12]\tLoss: 5.463\n",
      "Epoch [7][0/12]\tLoss: 5.172\n",
      "Epoch [8][0/12]\tLoss: 5.036\n",
      "Epoch [9][0/12]\tLoss: 4.687\n",
      "Epoch [10][0/12]\tLoss: 4.561\n",
      "Epoch [11][0/12]\tLoss: 4.497\n",
      "Epoch [12][0/12]\tLoss: 4.217\n",
      "Epoch [13][0/12]\tLoss: 4.170\n",
      "Epoch [14][0/12]\tLoss: 3.986\n",
      "Epoch [15][0/12]\tLoss: 3.671\n",
      "Epoch [16][0/12]\tLoss: 3.646\n",
      "Epoch [17][0/12]\tLoss: 3.724\n",
      "Epoch [18][0/12]\tLoss: 3.427\n",
      "Epoch [19][0/12]\tLoss: 2.846\n",
      "Epoch [20][0/12]\tLoss: 3.491\n",
      "Epoch [21][0/12]\tLoss: 3.169\n",
      "Epoch [22][0/12]\tLoss: 2.807\n",
      "Epoch [23][0/12]\tLoss: 2.817\n",
      "Epoch [24][0/12]\tLoss: 2.718\n",
      "Epoch [25][0/12]\tLoss: 2.296\n",
      "Epoch [26][0/12]\tLoss: 2.517\n",
      "Epoch [27][0/12]\tLoss: 2.033\n",
      "Epoch [28][0/12]\tLoss: 2.493\n",
      "Epoch [29][0/12]\tLoss: 2.258\n",
      "Epoch [30][0/12]\tLoss: 2.014\n",
      "Epoch [31][0/12]\tLoss: 2.076\n",
      "Epoch [32][0/12]\tLoss: 1.606\n",
      "Epoch [33][0/12]\tLoss: 1.472\n",
      "Epoch [34][0/12]\tLoss: 1.613\n",
      "Epoch [35][0/12]\tLoss: 1.634\n",
      "Epoch [36][0/12]\tLoss: 1.487\n",
      "Epoch [37][0/12]\tLoss: 1.499\n",
      "Epoch [38][0/12]\tLoss: 1.282\n",
      "Epoch [39][0/12]\tLoss: 1.111\n",
      "Epoch [40][0/12]\tLoss: 1.081\n",
      "Epoch [41][0/12]\tLoss: 0.773\n",
      "Epoch [42][0/12]\tLoss: 0.977\n",
      "Epoch [43][0/12]\tLoss: 0.847\n",
      "Epoch [44][0/12]\tLoss: 0.817\n",
      "Epoch [45][0/12]\tLoss: 0.785\n",
      "Epoch [46][0/12]\tLoss: 0.659\n",
      "Epoch [47][0/12]\tLoss: 0.748\n",
      "Epoch [48][0/12]\tLoss: 0.705\n",
      "Epoch [49][0/12]\tLoss: 0.603\n",
      "Epoch [50][0/12]\tLoss: 0.551\n",
      "Epoch [51][0/12]\tLoss: 0.528\n",
      "Epoch [52][0/12]\tLoss: 0.401\n",
      "Epoch [53][0/12]\tLoss: 0.482\n",
      "Epoch [54][0/12]\tLoss: 0.408\n",
      "Epoch [55][0/12]\tLoss: 0.434\n",
      "Epoch [56][0/12]\tLoss: 0.361\n",
      "Epoch [57][0/12]\tLoss: 0.336\n",
      "Epoch [58][0/12]\tLoss: 0.361\n",
      "Epoch [59][0/12]\tLoss: 0.361\n",
      "Epoch [60][0/12]\tLoss: 0.339\n",
      "Epoch [61][0/12]\tLoss: 0.283\n",
      "Epoch [62][0/12]\tLoss: 0.299\n",
      "Epoch [63][0/12]\tLoss: 0.288\n",
      "Epoch [64][0/12]\tLoss: 0.273\n",
      "Epoch [65][0/12]\tLoss: 0.291\n",
      "Epoch [66][0/12]\tLoss: 0.296\n",
      "Epoch [67][0/12]\tLoss: 0.277\n",
      "Epoch [68][0/12]\tLoss: 0.254\n",
      "Epoch [69][0/12]\tLoss: 0.244\n",
      "Epoch [70][0/12]\tLoss: 0.227\n",
      "Epoch [71][0/12]\tLoss: 0.217\n",
      "Epoch [72][0/12]\tLoss: 0.227\n",
      "Epoch [73][0/12]\tLoss: 0.245\n",
      "Epoch [74][0/12]\tLoss: 0.248\n",
      "Epoch [75][0/12]\tLoss: 0.223\n",
      "Epoch [76][0/12]\tLoss: 0.251\n",
      "Epoch [77][0/12]\tLoss: 0.212\n",
      "Epoch [78][0/12]\tLoss: 0.232\n",
      "Epoch [79][0/12]\tLoss: 0.219\n",
      "Epoch [80][0/12]\tLoss: 0.239\n",
      "Epoch [81][0/12]\tLoss: 0.216\n",
      "Epoch [82][0/12]\tLoss: 0.203\n",
      "Epoch [83][0/12]\tLoss: 0.217\n",
      "Epoch [84][0/12]\tLoss: 0.210\n",
      "Epoch [85][0/12]\tLoss: 0.228\n",
      "Epoch [86][0/12]\tLoss: 0.207\n",
      "Epoch [87][0/12]\tLoss: 0.219\n",
      "Epoch [88][0/12]\tLoss: 0.210\n",
      "Epoch [89][0/12]\tLoss: 0.213\n",
      "Epoch [90][0/12]\tLoss: 0.201\n",
      "Epoch [91][0/12]\tLoss: 0.184\n",
      "Epoch [92][0/12]\tLoss: 0.195\n",
      "Epoch [93][0/12]\tLoss: 0.180\n",
      "Epoch [94][0/12]\tLoss: 0.224\n",
      "Epoch [95][0/12]\tLoss: 0.181\n",
      "Epoch [96][0/12]\tLoss: 0.197\n",
      "Epoch [97][0/12]\tLoss: 0.206\n",
      "Epoch [98][0/12]\tLoss: 0.164\n",
      "Epoch [99][0/12]\tLoss: 0.180\n",
      "Epoch [100][0/12]\tLoss: 0.201\n",
      "Epoch [101][0/12]\tLoss: 0.195\n",
      "Epoch [102][0/12]\tLoss: 0.181\n",
      "Epoch [103][0/12]\tLoss: 0.186\n",
      "Epoch [104][0/12]\tLoss: 0.184\n",
      "Epoch [105][0/12]\tLoss: 0.181\n",
      "Epoch [106][0/12]\tLoss: 0.182\n",
      "Epoch [107][0/12]\tLoss: 0.184\n",
      "Epoch [108][0/12]\tLoss: 0.170\n",
      "Epoch [109][0/12]\tLoss: 0.196\n",
      "Epoch [110][0/12]\tLoss: 0.159\n",
      "Epoch [111][0/12]\tLoss: 0.182\n",
      "Epoch [112][0/12]\tLoss: 0.168\n",
      "Epoch [113][0/12]\tLoss: 0.178\n",
      "Epoch [114][0/12]\tLoss: 0.175\n",
      "Epoch [115][0/12]\tLoss: 0.187\n",
      "Epoch [116][0/12]\tLoss: 0.174\n",
      "Epoch [117][0/12]\tLoss: 0.179\n",
      "Epoch [118][0/12]\tLoss: 0.184\n",
      "Epoch [119][0/12]\tLoss: 0.197\n",
      "Epoch [120][0/12]\tLoss: 0.173\n",
      "Epoch [121][0/12]\tLoss: 0.163\n",
      "Epoch [122][0/12]\tLoss: 0.171\n",
      "Epoch [123][0/12]\tLoss: 0.173\n",
      "Epoch [124][0/12]\tLoss: 0.180\n",
      "Epoch [125][0/12]\tLoss: 0.194\n",
      "Epoch [126][0/12]\tLoss: 0.171\n",
      "Epoch [127][0/12]\tLoss: 0.181\n",
      "Epoch [128][0/12]\tLoss: 0.155\n",
      "Epoch [129][0/12]\tLoss: 0.162\n",
      "Epoch [130][0/12]\tLoss: 0.163\n",
      "Epoch [131][0/12]\tLoss: 0.181\n",
      "Epoch [132][0/12]\tLoss: 0.181\n",
      "Epoch [133][0/12]\tLoss: 0.178\n",
      "Epoch [134][0/12]\tLoss: 0.177\n",
      "Epoch [135][0/12]\tLoss: 0.176\n",
      "Epoch [136][0/12]\tLoss: 0.191\n",
      "Epoch [137][0/12]\tLoss: 0.190\n",
      "Epoch [138][0/12]\tLoss: 0.155\n",
      "Epoch [139][0/12]\tLoss: 0.174\n",
      "Epoch [140][0/12]\tLoss: 0.153\n",
      "Epoch [141][0/12]\tLoss: 0.164\n",
      "Epoch [142][0/12]\tLoss: 0.170\n",
      "Epoch [143][0/12]\tLoss: 0.144\n",
      "Epoch [144][0/12]\tLoss: 0.164\n",
      "Epoch [145][0/12]\tLoss: 0.170\n",
      "Epoch [146][0/12]\tLoss: 0.166\n",
      "Epoch [147][0/12]\tLoss: 0.164\n",
      "Epoch [148][0/12]\tLoss: 0.169\n",
      "Epoch [149][0/12]\tLoss: 0.166\n",
      "[neptune] [info   ] Shutting down background jobs, please wait a moment...\n",
      "[neptune] [info   ] Done!\n",
      "[neptune] [info   ] All 0 operations synced, thanks for waiting!\n",
      "[neptune] [info   ] Explore the metadata in the Neptune app: https://app.neptune.ai/andialifs/siet-24/e/SIET-127/metadata\n",
      "\n",
      "Running for experiment 101 with d_model 2048, heads32, num_layers_enc 1, num_layers_dec 5\n",
      "\n",
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/andialifs/siet-24/e/SIET-128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][0/12]\tLoss: 5.946\n",
      "Epoch [1][0/12]\tLoss: 5.922\n",
      "Epoch [2][0/12]\tLoss: 5.892\n",
      "Epoch [3][0/12]\tLoss: 5.821\n",
      "Epoch [4][0/12]\tLoss: 5.633\n",
      "Epoch [5][0/12]\tLoss: 5.551\n",
      "Epoch [6][0/12]\tLoss: 5.421\n",
      "Epoch [7][0/12]\tLoss: 5.240\n",
      "Epoch [8][0/12]\tLoss: 4.983\n",
      "Epoch [9][0/12]\tLoss: 4.838\n",
      "Epoch [10][0/12]\tLoss: 4.503\n",
      "Epoch [11][0/12]\tLoss: 4.268\n",
      "Epoch [12][0/12]\tLoss: 4.378\n",
      "Epoch [13][0/12]\tLoss: 4.152\n",
      "Epoch [14][0/12]\tLoss: 3.891\n",
      "Epoch [15][0/12]\tLoss: 3.869\n",
      "Epoch [16][0/12]\tLoss: 3.932\n",
      "Epoch [17][0/12]\tLoss: 3.282\n",
      "Epoch [18][0/12]\tLoss: 3.515\n",
      "Epoch [19][0/12]\tLoss: 3.325\n",
      "Epoch [20][0/12]\tLoss: 3.302\n",
      "Epoch [21][0/12]\tLoss: 3.061\n",
      "Epoch [22][0/12]\tLoss: 3.410\n",
      "Epoch [23][0/12]\tLoss: 2.792\n",
      "Epoch [24][0/12]\tLoss: 2.814\n",
      "Epoch [25][0/12]\tLoss: 2.536\n",
      "Epoch [26][0/12]\tLoss: 2.481\n",
      "Epoch [27][0/12]\tLoss: 2.342\n",
      "Epoch [28][0/12]\tLoss: 2.394\n",
      "Epoch [29][0/12]\tLoss: 1.916\n",
      "Epoch [30][0/12]\tLoss: 1.710\n",
      "Epoch [31][0/12]\tLoss: 1.710\n",
      "Epoch [32][0/12]\tLoss: 1.791\n",
      "Epoch [33][0/12]\tLoss: 1.690\n",
      "Epoch [34][0/12]\tLoss: 1.259\n",
      "Epoch [35][0/12]\tLoss: 1.493\n",
      "Epoch [36][0/12]\tLoss: 1.360\n",
      "Epoch [37][0/12]\tLoss: 1.287\n",
      "Epoch [38][0/12]\tLoss: 1.446\n",
      "Epoch [39][0/12]\tLoss: 1.282\n",
      "Epoch [40][0/12]\tLoss: 1.364\n",
      "Epoch [41][0/12]\tLoss: 1.154\n",
      "Epoch [42][0/12]\tLoss: 0.961\n",
      "Epoch [43][0/12]\tLoss: 1.010\n",
      "Epoch [44][0/12]\tLoss: 0.933\n",
      "Epoch [45][0/12]\tLoss: 0.830\n",
      "Epoch [46][0/12]\tLoss: 0.832\n",
      "Epoch [47][0/12]\tLoss: 0.662\n",
      "Epoch [48][0/12]\tLoss: 0.611\n",
      "Epoch [49][0/12]\tLoss: 0.561\n",
      "Epoch [50][0/12]\tLoss: 0.497\n",
      "Epoch [51][0/12]\tLoss: 0.548\n",
      "Epoch [52][0/12]\tLoss: 0.503\n",
      "Epoch [53][0/12]\tLoss: 0.443\n",
      "Epoch [54][0/12]\tLoss: 0.462\n",
      "Epoch [55][0/12]\tLoss: 0.392\n",
      "Epoch [56][0/12]\tLoss: 0.398\n",
      "Epoch [57][0/12]\tLoss: 0.349\n",
      "Epoch [58][0/12]\tLoss: 0.357\n",
      "Epoch [59][0/12]\tLoss: 0.284\n",
      "Epoch [60][0/12]\tLoss: 0.313\n",
      "Epoch [61][0/12]\tLoss: 0.367\n",
      "Epoch [62][0/12]\tLoss: 0.297\n",
      "Epoch [63][0/12]\tLoss: 0.299\n",
      "Epoch [64][0/12]\tLoss: 0.269\n",
      "Epoch [65][0/12]\tLoss: 0.300\n",
      "Epoch [66][0/12]\tLoss: 0.255\n",
      "Epoch [67][0/12]\tLoss: 0.275\n",
      "Epoch [68][0/12]\tLoss: 0.255\n",
      "Epoch [69][0/12]\tLoss: 0.251\n",
      "Epoch [70][0/12]\tLoss: 0.293\n",
      "Epoch [71][0/12]\tLoss: 0.274\n",
      "Epoch [72][0/12]\tLoss: 0.281\n",
      "Epoch [73][0/12]\tLoss: 0.242\n",
      "Epoch [74][0/12]\tLoss: 0.237\n",
      "Epoch [75][0/12]\tLoss: 0.250\n",
      "Epoch [76][0/12]\tLoss: 0.245\n",
      "Epoch [77][0/12]\tLoss: 0.235\n",
      "Epoch [78][0/12]\tLoss: 0.221\n",
      "Epoch [79][0/12]\tLoss: 0.238\n",
      "Epoch [80][0/12]\tLoss: 0.234\n",
      "Epoch [81][0/12]\tLoss: 0.222\n",
      "Epoch [82][0/12]\tLoss: 0.217\n",
      "Epoch [83][0/12]\tLoss: 0.217\n",
      "Epoch [84][0/12]\tLoss: 0.233\n",
      "Epoch [85][0/12]\tLoss: 0.217\n",
      "Epoch [86][0/12]\tLoss: 0.242\n",
      "Epoch [87][0/12]\tLoss: 0.217\n",
      "Epoch [88][0/12]\tLoss: 0.214\n",
      "Epoch [89][0/12]\tLoss: 0.211\n",
      "Epoch [90][0/12]\tLoss: 0.222\n",
      "Epoch [91][0/12]\tLoss: 0.209\n",
      "Epoch [92][0/12]\tLoss: 0.242\n",
      "Epoch [93][0/12]\tLoss: 0.195\n",
      "Epoch [94][0/12]\tLoss: 0.183\n",
      "Epoch [95][0/12]\tLoss: 0.215\n",
      "Epoch [96][0/12]\tLoss: 0.200\n",
      "Epoch [97][0/12]\tLoss: 0.226\n",
      "Epoch [98][0/12]\tLoss: 0.199\n",
      "Epoch [99][0/12]\tLoss: 0.186\n",
      "Epoch [100][0/12]\tLoss: 0.181\n",
      "Epoch [101][0/12]\tLoss: 0.189\n",
      "Epoch [102][0/12]\tLoss: 0.190\n",
      "Epoch [103][0/12]\tLoss: 0.184\n",
      "Epoch [104][0/12]\tLoss: 0.200\n",
      "Epoch [105][0/12]\tLoss: 0.212\n",
      "Epoch [106][0/12]\tLoss: 0.168\n",
      "Epoch [107][0/12]\tLoss: 0.174\n",
      "Epoch [108][0/12]\tLoss: 0.183\n",
      "Epoch [109][0/12]\tLoss: 0.175\n",
      "Epoch [110][0/12]\tLoss: 0.190\n",
      "Epoch [111][0/12]\tLoss: 0.192\n",
      "Epoch [112][0/12]\tLoss: 0.201\n",
      "Epoch [113][0/12]\tLoss: 0.187\n",
      "Epoch [114][0/12]\tLoss: 0.190\n",
      "Epoch [115][0/12]\tLoss: 0.191\n",
      "Epoch [116][0/12]\tLoss: 0.164\n",
      "Epoch [117][0/12]\tLoss: 0.187\n",
      "Epoch [118][0/12]\tLoss: 0.190\n",
      "Epoch [119][0/12]\tLoss: 0.177\n",
      "Epoch [120][0/12]\tLoss: 0.196\n",
      "Epoch [121][0/12]\tLoss: 0.181\n",
      "Epoch [122][0/12]\tLoss: 0.188\n",
      "Epoch [123][0/12]\tLoss: 0.184\n",
      "Epoch [124][0/12]\tLoss: 0.175\n",
      "Epoch [125][0/12]\tLoss: 0.190\n",
      "Epoch [126][0/12]\tLoss: 0.177\n",
      "Epoch [127][0/12]\tLoss: 0.176\n",
      "Epoch [128][0/12]\tLoss: 0.181\n",
      "Epoch [129][0/12]\tLoss: 0.161\n",
      "Epoch [130][0/12]\tLoss: 0.182\n",
      "Epoch [131][0/12]\tLoss: 0.190\n",
      "Epoch [132][0/12]\tLoss: 0.203\n",
      "Epoch [133][0/12]\tLoss: 0.189\n",
      "Epoch [134][0/12]\tLoss: 0.178\n",
      "Epoch [135][0/12]\tLoss: 0.217\n",
      "Epoch [136][0/12]\tLoss: 0.164\n",
      "Epoch [137][0/12]\tLoss: 0.156\n",
      "Epoch [138][0/12]\tLoss: 0.187\n",
      "Epoch [139][0/12]\tLoss: 0.159\n",
      "Epoch [140][0/12]\tLoss: 0.190\n",
      "Epoch [141][0/12]\tLoss: 0.177\n",
      "Epoch [142][0/12]\tLoss: 0.195\n",
      "Epoch [143][0/12]\tLoss: 0.162\n",
      "Epoch [144][0/12]\tLoss: 0.149\n",
      "Epoch [145][0/12]\tLoss: 0.166\n",
      "Epoch [146][0/12]\tLoss: 0.173\n",
      "Epoch [147][0/12]\tLoss: 0.170\n",
      "Epoch [148][0/12]\tLoss: 0.178\n",
      "Epoch [149][0/12]\tLoss: 0.177\n",
      "[neptune] [info   ] Shutting down background jobs, please wait a moment...\n",
      "[neptune] [info   ] Done!\n",
      "[neptune] [info   ] All 0 operations synced, thanks for waiting!\n",
      "[neptune] [info   ] Explore the metadata in the Neptune app: https://app.neptune.ai/andialifs/siet-24/e/SIET-128/metadata\n",
      "\n",
      "Running for experiment 102 with d_model 2048, heads32, num_layers_enc 3, num_layers_dec 1\n",
      "\n",
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/andialifs/siet-24/e/SIET-129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][0/12]\tLoss: 5.930\n",
      "Epoch [1][0/12]\tLoss: 5.854\n",
      "Epoch [2][0/12]\tLoss: 5.663\n",
      "Epoch [3][0/12]\tLoss: 5.432\n",
      "Epoch [4][0/12]\tLoss: 5.061\n",
      "Epoch [5][0/12]\tLoss: 4.956\n",
      "Epoch [6][0/12]\tLoss: 4.798\n",
      "Epoch [7][0/12]\tLoss: 4.825\n",
      "Epoch [8][0/12]\tLoss: 4.646\n",
      "Epoch [9][0/12]\tLoss: 4.621\n",
      "Epoch [10][0/12]\tLoss: 4.544\n",
      "Epoch [11][0/12]\tLoss: 4.559\n",
      "Epoch [12][0/12]\tLoss: 4.247\n",
      "Epoch [13][0/12]\tLoss: 4.162\n",
      "Epoch [14][0/12]\tLoss: 4.075\n",
      "Epoch [15][0/12]\tLoss: 4.055\n",
      "Epoch [16][0/12]\tLoss: 3.838\n",
      "Epoch [17][0/12]\tLoss: 3.453\n",
      "Epoch [18][0/12]\tLoss: 3.683\n",
      "Epoch [19][0/12]\tLoss: 3.452\n",
      "Epoch [20][0/12]\tLoss: 3.412\n",
      "Epoch [21][0/12]\tLoss: 3.072\n",
      "Epoch [22][0/12]\tLoss: 2.981\n",
      "Epoch [23][0/12]\tLoss: 2.653\n",
      "Epoch [24][0/12]\tLoss: 2.727\n",
      "Epoch [25][0/12]\tLoss: 2.620\n",
      "Epoch [26][0/12]\tLoss: 2.227\n",
      "Epoch [27][0/12]\tLoss: 2.574\n",
      "Epoch [28][0/12]\tLoss: 2.544\n",
      "Epoch [29][0/12]\tLoss: 2.446\n",
      "Epoch [30][0/12]\tLoss: 2.447\n",
      "Epoch [31][0/12]\tLoss: 2.128\n",
      "Epoch [32][0/12]\tLoss: 2.539\n",
      "Epoch [33][0/12]\tLoss: 2.281\n",
      "Epoch [34][0/12]\tLoss: 1.919\n",
      "Epoch [35][0/12]\tLoss: 1.974\n",
      "Epoch [36][0/12]\tLoss: 1.751\n",
      "Epoch [37][0/12]\tLoss: 1.926\n",
      "Epoch [38][0/12]\tLoss: 1.565\n",
      "Epoch [39][0/12]\tLoss: 1.672\n",
      "Epoch [40][0/12]\tLoss: 1.625\n",
      "Epoch [41][0/12]\tLoss: 1.474\n",
      "Epoch [42][0/12]\tLoss: 1.281\n",
      "Epoch [43][0/12]\tLoss: 1.265\n",
      "Epoch [44][0/12]\tLoss: 1.217\n",
      "Epoch [45][0/12]\tLoss: 1.384\n",
      "Epoch [46][0/12]\tLoss: 1.149\n",
      "Epoch [47][0/12]\tLoss: 1.136\n",
      "Epoch [48][0/12]\tLoss: 1.050\n",
      "Epoch [49][0/12]\tLoss: 1.083\n",
      "Epoch [50][0/12]\tLoss: 1.047\n",
      "Epoch [51][0/12]\tLoss: 1.017\n",
      "Epoch [52][0/12]\tLoss: 0.803\n",
      "Epoch [53][0/12]\tLoss: 0.860\n",
      "Epoch [54][0/12]\tLoss: 0.826\n",
      "Epoch [55][0/12]\tLoss: 0.790\n",
      "Epoch [56][0/12]\tLoss: 0.562\n",
      "Epoch [57][0/12]\tLoss: 0.669\n",
      "Epoch [58][0/12]\tLoss: 0.723\n",
      "Epoch [59][0/12]\tLoss: 0.550\n",
      "Epoch [60][0/12]\tLoss: 0.543\n",
      "Epoch [61][0/12]\tLoss: 0.530\n",
      "Epoch [62][0/12]\tLoss: 0.509\n",
      "Epoch [63][0/12]\tLoss: 0.448\n",
      "Epoch [64][0/12]\tLoss: 0.477\n",
      "Epoch [65][0/12]\tLoss: 0.414\n",
      "Epoch [66][0/12]\tLoss: 0.446\n",
      "Epoch [67][0/12]\tLoss: 0.389\n",
      "Epoch [68][0/12]\tLoss: 0.375\n",
      "Epoch [69][0/12]\tLoss: 0.335\n",
      "Epoch [70][0/12]\tLoss: 0.354\n",
      "Epoch [71][0/12]\tLoss: 0.337\n",
      "Epoch [72][0/12]\tLoss: 0.347\n",
      "Epoch [73][0/12]\tLoss: 0.304\n",
      "Epoch [74][0/12]\tLoss: 0.286\n",
      "Epoch [75][0/12]\tLoss: 0.285\n",
      "Epoch [76][0/12]\tLoss: 0.267\n",
      "Epoch [77][0/12]\tLoss: 0.260\n",
      "Epoch [78][0/12]\tLoss: 0.269\n",
      "Epoch [79][0/12]\tLoss: 0.276\n",
      "Epoch [80][0/12]\tLoss: 0.287\n",
      "Epoch [81][0/12]\tLoss: 0.243\n",
      "Epoch [82][0/12]\tLoss: 0.219\n",
      "Epoch [83][0/12]\tLoss: 0.220\n",
      "Epoch [84][0/12]\tLoss: 0.258\n",
      "Epoch [85][0/12]\tLoss: 0.242\n",
      "Epoch [86][0/12]\tLoss: 0.229\n",
      "Epoch [87][0/12]\tLoss: 0.221\n",
      "Epoch [88][0/12]\tLoss: 0.226\n",
      "Epoch [89][0/12]\tLoss: 0.207\n",
      "Epoch [90][0/12]\tLoss: 0.218\n",
      "Epoch [91][0/12]\tLoss: 0.240\n",
      "Epoch [92][0/12]\tLoss: 0.193\n",
      "Epoch [93][0/12]\tLoss: 0.205\n",
      "Epoch [94][0/12]\tLoss: 0.193\n",
      "Epoch [95][0/12]\tLoss: 0.219\n",
      "Epoch [96][0/12]\tLoss: 0.219\n",
      "Epoch [97][0/12]\tLoss: 0.185\n",
      "Epoch [98][0/12]\tLoss: 0.179\n",
      "Epoch [99][0/12]\tLoss: 0.199\n",
      "Epoch [100][0/12]\tLoss: 0.193\n",
      "Epoch [101][0/12]\tLoss: 0.187\n",
      "Epoch [102][0/12]\tLoss: 0.209\n",
      "Epoch [103][0/12]\tLoss: 0.189\n",
      "Epoch [104][0/12]\tLoss: 0.184\n",
      "Epoch [105][0/12]\tLoss: 0.182\n",
      "Epoch [106][0/12]\tLoss: 0.190\n",
      "Epoch [107][0/12]\tLoss: 0.193\n",
      "Epoch [108][0/12]\tLoss: 0.180\n",
      "Epoch [109][0/12]\tLoss: 0.154\n",
      "Epoch [110][0/12]\tLoss: 0.170\n",
      "Epoch [111][0/12]\tLoss: 0.154\n",
      "Epoch [112][0/12]\tLoss: 0.172\n",
      "Epoch [113][0/12]\tLoss: 0.152\n",
      "Epoch [114][0/12]\tLoss: 0.161\n",
      "Epoch [115][0/12]\tLoss: 0.152\n",
      "Epoch [116][0/12]\tLoss: 0.150\n",
      "Epoch [117][0/12]\tLoss: 0.155\n",
      "Epoch [118][0/12]\tLoss: 0.150\n",
      "Epoch [119][0/12]\tLoss: 0.175\n",
      "Epoch [120][0/12]\tLoss: 0.155\n",
      "Epoch [121][0/12]\tLoss: 0.143\n",
      "Epoch [122][0/12]\tLoss: 0.134\n",
      "Epoch [123][0/12]\tLoss: 0.130\n",
      "Epoch [124][0/12]\tLoss: 0.165\n",
      "Epoch [125][0/12]\tLoss: 0.145\n",
      "Epoch [126][0/12]\tLoss: 0.152\n",
      "Epoch [127][0/12]\tLoss: 0.142\n",
      "Epoch [128][0/12]\tLoss: 0.152\n",
      "Epoch [129][0/12]\tLoss: 0.138\n",
      "Epoch [130][0/12]\tLoss: 0.137\n",
      "Epoch [131][0/12]\tLoss: 0.133\n",
      "Epoch [132][0/12]\tLoss: 0.134\n",
      "Epoch [133][0/12]\tLoss: 0.139\n",
      "Epoch [134][0/12]\tLoss: 0.128\n",
      "Epoch [135][0/12]\tLoss: 0.135\n",
      "Epoch [136][0/12]\tLoss: 0.133\n",
      "Epoch [137][0/12]\tLoss: 0.145\n",
      "Epoch [138][0/12]\tLoss: 0.141\n",
      "Epoch [139][0/12]\tLoss: 0.127\n",
      "Epoch [140][0/12]\tLoss: 0.134\n",
      "Epoch [141][0/12]\tLoss: 0.138\n",
      "Epoch [142][0/12]\tLoss: 0.122\n",
      "Epoch [143][0/12]\tLoss: 0.124\n",
      "Epoch [144][0/12]\tLoss: 0.152\n",
      "Epoch [145][0/12]\tLoss: 0.133\n",
      "Epoch [146][0/12]\tLoss: 0.112\n",
      "Epoch [147][0/12]\tLoss: 0.151\n",
      "Epoch [148][0/12]\tLoss: 0.108\n",
      "Epoch [149][0/12]\tLoss: 0.134\n",
      "[neptune] [info   ] Shutting down background jobs, please wait a moment...\n",
      "[neptune] [info   ] Done!\n",
      "[neptune] [info   ] All 0 operations synced, thanks for waiting!\n",
      "[neptune] [info   ] Explore the metadata in the Neptune app: https://app.neptune.ai/andialifs/siet-24/e/SIET-129/metadata\n",
      "\n",
      "Running for experiment 103 with d_model 2048, heads32, num_layers_enc 3, num_layers_dec 3\n",
      "\n",
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/andialifs/siet-24/e/SIET-130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][0/12]\tLoss: 5.875\n",
      "Epoch [1][0/12]\tLoss: 5.817\n",
      "Epoch [2][0/12]\tLoss: 5.573\n",
      "Epoch [3][0/12]\tLoss: 5.349\n",
      "Epoch [4][0/12]\tLoss: 5.132\n",
      "Epoch [5][0/12]\tLoss: 4.856\n",
      "Epoch [6][0/12]\tLoss: 4.860\n",
      "Epoch [7][0/12]\tLoss: 4.826\n",
      "Epoch [8][0/12]\tLoss: 4.647\n",
      "Epoch [9][0/12]\tLoss: 4.689\n",
      "Epoch [10][0/12]\tLoss: 4.585\n",
      "Epoch [11][0/12]\tLoss: 4.614\n",
      "Epoch [12][0/12]\tLoss: 4.253\n",
      "Epoch [13][0/12]\tLoss: 4.279\n",
      "Epoch [14][0/12]\tLoss: 4.165\n",
      "Epoch [15][0/12]\tLoss: 4.071\n",
      "Epoch [16][0/12]\tLoss: 3.832\n",
      "Epoch [17][0/12]\tLoss: 3.866\n",
      "Epoch [18][0/12]\tLoss: 3.518\n",
      "Epoch [19][0/12]\tLoss: 3.277\n",
      "Epoch [20][0/12]\tLoss: 3.485\n",
      "Epoch [21][0/12]\tLoss: 3.170\n",
      "Epoch [22][0/12]\tLoss: 3.229\n",
      "Epoch [23][0/12]\tLoss: 3.226\n",
      "Epoch [24][0/12]\tLoss: 2.372\n",
      "Epoch [25][0/12]\tLoss: 2.567\n",
      "Epoch [26][0/12]\tLoss: 2.696\n",
      "Epoch [27][0/12]\tLoss: 2.694\n",
      "Epoch [28][0/12]\tLoss: 2.463\n",
      "Epoch [29][0/12]\tLoss: 2.114\n",
      "Epoch [30][0/12]\tLoss: 2.167\n",
      "Epoch [31][0/12]\tLoss: 2.011\n",
      "Epoch [32][0/12]\tLoss: 2.169\n",
      "Epoch [33][0/12]\tLoss: 2.050\n",
      "Epoch [34][0/12]\tLoss: 1.632\n",
      "Epoch [35][0/12]\tLoss: 2.207\n",
      "Epoch [36][0/12]\tLoss: 1.973\n",
      "Epoch [37][0/12]\tLoss: 1.377\n",
      "Epoch [38][0/12]\tLoss: 1.626\n",
      "Epoch [39][0/12]\tLoss: 1.502\n",
      "Epoch [40][0/12]\tLoss: 1.721\n",
      "Epoch [41][0/12]\tLoss: 1.608\n",
      "Epoch [42][0/12]\tLoss: 1.475\n",
      "Epoch [43][0/12]\tLoss: 1.253\n",
      "Epoch [44][0/12]\tLoss: 1.440\n",
      "Epoch [45][0/12]\tLoss: 1.028\n",
      "Epoch [46][0/12]\tLoss: 1.060\n",
      "Epoch [47][0/12]\tLoss: 1.234\n",
      "Epoch [48][0/12]\tLoss: 1.216\n",
      "Epoch [49][0/12]\tLoss: 0.963\n",
      "Epoch [50][0/12]\tLoss: 0.905\n",
      "Epoch [51][0/12]\tLoss: 0.909\n",
      "Epoch [52][0/12]\tLoss: 0.719\n",
      "Epoch [53][0/12]\tLoss: 0.669\n",
      "Epoch [54][0/12]\tLoss: 0.921\n",
      "Epoch [55][0/12]\tLoss: 0.671\n",
      "Epoch [56][0/12]\tLoss: 0.697\n",
      "Epoch [57][0/12]\tLoss: 0.570\n",
      "Epoch [58][0/12]\tLoss: 0.538\n",
      "Epoch [59][0/12]\tLoss: 0.464\n",
      "Epoch [60][0/12]\tLoss: 0.523\n",
      "Epoch [61][0/12]\tLoss: 0.544\n",
      "Epoch [62][0/12]\tLoss: 0.502\n",
      "Epoch [63][0/12]\tLoss: 0.481\n",
      "Epoch [64][0/12]\tLoss: 0.535\n",
      "Epoch [65][0/12]\tLoss: 0.397\n",
      "Epoch [66][0/12]\tLoss: 0.346\n",
      "Epoch [67][0/12]\tLoss: 0.381\n",
      "Epoch [68][0/12]\tLoss: 0.330\n",
      "Epoch [69][0/12]\tLoss: 0.350\n",
      "Epoch [70][0/12]\tLoss: 0.291\n",
      "Epoch [71][0/12]\tLoss: 0.329\n",
      "Epoch [72][0/12]\tLoss: 0.321\n",
      "Epoch [73][0/12]\tLoss: 0.309\n",
      "Epoch [74][0/12]\tLoss: 0.269\n",
      "Epoch [75][0/12]\tLoss: 0.299\n",
      "Epoch [76][0/12]\tLoss: 0.266\n",
      "Epoch [77][0/12]\tLoss: 0.265\n",
      "Epoch [78][0/12]\tLoss: 0.263\n",
      "Epoch [79][0/12]\tLoss: 0.254\n",
      "Epoch [80][0/12]\tLoss: 0.253\n",
      "Epoch [81][0/12]\tLoss: 0.264\n",
      "Epoch [82][0/12]\tLoss: 0.282\n",
      "Epoch [83][0/12]\tLoss: 0.229\n",
      "Epoch [84][0/12]\tLoss: 0.252\n",
      "Epoch [85][0/12]\tLoss: 0.223\n",
      "Epoch [86][0/12]\tLoss: 0.217\n",
      "Epoch [87][0/12]\tLoss: 0.252\n",
      "Epoch [88][0/12]\tLoss: 0.220\n",
      "Epoch [89][0/12]\tLoss: 0.250\n",
      "Epoch [90][0/12]\tLoss: 0.212\n",
      "Epoch [91][0/12]\tLoss: 0.211\n",
      "Epoch [92][0/12]\tLoss: 0.203\n",
      "Epoch [93][0/12]\tLoss: 0.189\n",
      "Epoch [94][0/12]\tLoss: 0.189\n",
      "Epoch [95][0/12]\tLoss: 0.185\n",
      "Epoch [96][0/12]\tLoss: 0.187\n",
      "Epoch [97][0/12]\tLoss: 0.192\n",
      "Epoch [98][0/12]\tLoss: 0.246\n",
      "Epoch [99][0/12]\tLoss: 0.202\n",
      "Epoch [100][0/12]\tLoss: 0.182\n",
      "Epoch [101][0/12]\tLoss: 0.182\n",
      "Epoch [102][0/12]\tLoss: 0.200\n",
      "Epoch [103][0/12]\tLoss: 0.180\n",
      "Epoch [104][0/12]\tLoss: 0.193\n",
      "Epoch [105][0/12]\tLoss: 0.166\n",
      "Epoch [106][0/12]\tLoss: 0.179\n",
      "Epoch [107][0/12]\tLoss: 0.173\n",
      "Epoch [108][0/12]\tLoss: 0.177\n",
      "Epoch [109][0/12]\tLoss: 0.177\n",
      "Epoch [110][0/12]\tLoss: 0.158\n",
      "Epoch [111][0/12]\tLoss: 0.147\n",
      "Epoch [112][0/12]\tLoss: 0.156\n",
      "Epoch [113][0/12]\tLoss: 0.150\n",
      "Epoch [114][0/12]\tLoss: 0.162\n"
     ]
    }
   ],
   "source": [
    "d_model = [512, 1024, 2048, 4096]\n",
    "encoder_layers = [1, 3, 5]\n",
    "decoder_layers = [1, 3, 5]\n",
    "heads = [4, 8, 16, 32]\n",
    "epochs = 150\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "directory = 'experiment_siet24_288'\n",
    "create_directory(directory)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "with open('WORDMAP_corpus_KBFILKOM.json', 'r') as j:\n",
    "    word_map = json.load(j)\n",
    "\n",
    "transformer_experiment = pd.DataFrame(columns = ['experiment_id', 'd_model', 'heads', 'num_layers', 'train_loss'])\n",
    "\n",
    "experiment_id = -1\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "for d_m in d_model:\n",
    "    for h in heads:\n",
    "        for n_l1 in encoder_layers:\n",
    "            for n_l2 in decoder_layers: \n",
    "                experiment_id += 1\n",
    "\n",
    "                if experiment_id < 79:\n",
    "                    print('Skipping experiment {}'.format(experiment_id))\n",
    "                    continue\n",
    "                \n",
    "                print('\\nRunning for experiment {} with d_model {}, heads{}, num_layers_enc {}, num_layers_dec {}\\n'.format(experiment_id, d_m, h, n_l1, n_l2))\n",
    "                name = directory + str(experiment_id)\n",
    "\n",
    "                run = neptune_init(name)\n",
    "                \n",
    "                transformer_experiment.loc[experiment_id, 'experiment_id'] = 'experiment_{}'.format(str(experiment_id))\n",
    "                transformer_experiment.loc[experiment_id, 'd_model'] = d_m\n",
    "                transformer_experiment.loc[experiment_id, 'heads'] = h\n",
    "                transformer_experiment.loc[experiment_id, 'decoder_layers'] = n_l2\n",
    "                transformer_experiment.loc[experiment_id, 'encoder_layers'] = n_l1\n",
    "\n",
    "                transformer = Transformer(d_model = d_m, \n",
    "                                            heads = h, \n",
    "                                            num_layers_enc=n_l1, \n",
    "                                            num_layers_dec=n_l2,\n",
    "                                            word_map = word_map, \n",
    "                                            max_len=95\n",
    "                                        )\n",
    "                transformer = transformer.to(device)\n",
    "                adam_optimizer = torch.optim.Adam(transformer.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-9)\n",
    "                transformer_optimizer = AdamWarmup(model_size = d_m, warmup_steps = 4000, optimizer = adam_optimizer)\n",
    "                criterion = LossWithLS(len(word_map), 0.2)\n",
    "\n",
    "                loss_list_experiment = []\n",
    "                for epoch in range(epochs):\n",
    "                    loss_train = train(train_loader, transformer, criterion, epoch)\n",
    "\n",
    "                    state = {'epoch': epoch, 'transformer': transformer, 'transformer_optimizer': transformer_optimizer}\n",
    "\n",
    "                    loss_list_experiment.append(loss_train)\n",
    "\n",
    "                    run['train/loss'].append(loss_train)\n",
    "\n",
    "                save_dir = directory + '/experiment_' + str(experiment_id) + '.pth.tar'\n",
    "                parameters = {\n",
    "                    'd_model': d_m,\n",
    "                    'heads': h,\n",
    "                    'encoder_layers': n_l1,\n",
    "                    'decoder_layers': n_l2,\n",
    "                    'model_dir': save_dir,\n",
    "                    'total_params': summary(transformer).trainable_params\n",
    "                }\n",
    "                run['parameters'] = parameters\n",
    "                \n",
    "                torch.save(state, save_dir)\n",
    "\n",
    "                transformer_experiment.loc[experiment_id, 'train_loss'] = loss_train\n",
    "\n",
    "                with open(directory + 'loss_history.yaml', 'w') as file:\n",
    "                    yaml.dump(loss_list_experiment, file)\n",
    "                \n",
    "                run.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T20:16:32.298394Z",
     "iopub.status.busy": "2024-07-02T20:16:32.297969Z",
     "iopub.status.idle": "2024-07-02T20:16:32.305442Z",
     "shell.execute_reply": "2024-07-02T20:16:32.304012Z",
     "shell.execute_reply.started": "2024-07-02T20:16:32.298351Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "d_model = [512, 1024, 2048, 4096]\n",
    "heads = [8, 16, 32]\n",
    "num_layers = [5, 10]\n",
    "epochs = 100\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "with open('WORDMAP_corpus_KBFILKOM.json', 'r') as j:\n",
    "    word_map = json.load(j)\n",
    "\n",
    "transformer_experiment = pd.DataFrame(columns = ['experiment_id', 'd_model', 'heads', 'num_layers', 'train_loss'])\n",
    "loss_history = {}\n",
    "\n",
    "experiment_id = -1\n",
    "\n",
    "for d_m in d_model:\n",
    "    for h in heads:\n",
    "        for n_l in num_layers: \n",
    "            experiment_id += 1\n",
    "            print('\\nRunning for experiment {} with d_model {}, heads{}, num_layers{}\\n'.format(experiment_id, d_m, h, n_l))\n",
    "\n",
    "            run = neptune.init_run(\n",
    "                project=project,\n",
    "                api_token=api_token,\n",
    "                name=\"experiment_1724_\" + str(experiment_id)\n",
    "            ) \n",
    "            run['parameters'] = {\n",
    "                'd_model': d_m,\n",
    "                'heads': h,\n",
    "                'num_layers': n_l\n",
    "            }\n",
    "\n",
    "            transformer_experiment.loc[experiment_id, 'experiment_id'] = 'experiment_{}'.format(str(experiment_id))\n",
    "            transformer_experiment.loc[experiment_id, 'd_model'] = d_m\n",
    "            transformer_experiment.loc[experiment_id, 'heads'] = h\n",
    "            transformer_experiment.loc[experiment_id, 'num_layers'] = n_l\n",
    "\n",
    "            transformer = Transformer(d_model = d_m, heads = h, num_layers = n_l, word_map = word_map, max_len=95)\n",
    "            transformer = transformer.to(device)\n",
    "            adam_optimizer = torch.optim.Adam(transformer.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-9)\n",
    "            transformer_optimizer = AdamWarmup(model_size = d_m, warmup_steps = 4000, optimizer = adam_optimizer)\n",
    "            criterion = LossWithLS(len(word_map), 0.2)\n",
    "\n",
    "            loss_list_experiment = []\n",
    "            for epoch in range(epochs):\n",
    "                loss_train = train(train_loader, transformer, criterion, epoch)\n",
    "\n",
    "                state = {'epoch': epoch, 'transformer': transformer, 'transformer_optimizer': transformer_optimizer}\n",
    "                torch.save(state, 'checkpoint_experiment_' + str(epoch) + 'id_'+ str(experiment_id) +'.pth.tar')\n",
    "\n",
    "                loss_list_experiment.append(loss_train)\n",
    "\n",
    "                run['train/loss'].append(loss_train)\n",
    "\n",
    "            transformer_experiment.loc[experiment_id, 'train_loss'] = loss_train\n",
    "            loss_history['experiment_{}'.format(str(experiment_id))] = loss_list_experiment\n",
    "            \n",
    "            run.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "with open('loss_history_transformer_experiment_KBFILKOM.yaml', 'w') as file:\n",
    "    documents = yaml.dump(loss_history, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_experiment.dropna(inplace=True)\n",
    "transformer_experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "with open('history_rnn_150524.yaml', 'r') as file:\n",
    "    history_rnn = yaml.load(file, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "loss_history_key = list(loss_history.keys())\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.title(\"Training loss vs. Number of Epochs\")\n",
    "plt.xlabel(\"Number of Epochs\")\n",
    "plt.ylabel(\"Training Loss\")\n",
    "z\n",
    "\n",
    "for key in loss_history_key:\n",
    "    loss_list = loss_history[key]\n",
    "    labels = f'd_model: {transformer_experiment[transformer_experiment[\"experiment_id\"] == key][\"d_model\"].values[0]}, heads: {transformer_experiment[transformer_experiment[\"experiment_id\"] == key][\"heads\"].values[0]}, num_layers: {transformer_experiment[transformer_experiment[\"experiment_id\"] == key][\"num_layers\"].values[0]}'\n",
    "    plt.plot(loss_list, label = labels)\n",
    "\n",
    "    \n",
    "plt.plot(history_rnn['loss'], \n",
    "                label = 'LSTM (Baseline FLUENT 2023)', \n",
    "                linestyle='dashed', \n",
    "                color='black', \n",
    "                linewidth=2.5, \n",
    "                alpha=0.7, \n",
    "                marker='o', \n",
    "                markerfacecolor='black', \n",
    "                markersize=5\n",
    "        )\n",
    "\n",
    "plt.legend()\n",
    "torch.cuda.is_available()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'experiment_pretrained_1'\n",
    "\n",
    "d_model = 2048\n",
    "heads = 16\n",
    "num_layers = 5\n",
    "epochs = 100\n",
    "\n",
    "loss_history_pratrained_embed_transformer = []\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "with open('WORDMAP_corpus_KBFILKOM.json', 'r') as j:\n",
    "    word_map = json.load(j)\n",
    "    \n",
    "transformer = TransformerPreTrainedEmbedding(d_model = d_model, heads = heads, num_layers = num_layers, word_map = word_map, max_len=95)\n",
    "transformer = transformer.to(device)\n",
    "adam_optimizer = torch.optim.Adam(transformer.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-9)\n",
    "transformer_optimizer = AdamWarmup(model_size = d_model, warmup_steps = 4000, optimizer = adam_optimizer)\n",
    "criterion = LossWithLS(len(word_map), 0.2)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    loss_train = train(train_loader, transformer, criterion, epoch)\n",
    "\n",
    "    state = {'epoch': epoch, 'transformer': transformer, 'transformer_optimizer': transformer_optimizer}\n",
    "    torch.save(state, directory + '/checkpoint_' + str(epoch) +'.pth.tar')\n",
    "\n",
    "    loss_history_pratrained_embed_transformer.append(loss_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory already exists at experiment_vanilla_16924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0][0/12]\tLoss: 5.963\n",
      "Epoch [1][0/12]\tLoss: 5.892\n",
      "Epoch [2][0/12]\tLoss: 5.783\n",
      "Epoch [3][0/12]\tLoss: 5.676\n",
      "Epoch [4][0/12]\tLoss: 5.332\n",
      "Epoch [5][0/12]\tLoss: 5.375\n",
      "Epoch [6][0/12]\tLoss: 5.223\n",
      "Epoch [7][0/12]\tLoss: 5.108\n",
      "Epoch [8][0/12]\tLoss: 4.970\n",
      "Epoch [9][0/12]\tLoss: 4.986\n",
      "Epoch [10][0/12]\tLoss: 4.910\n",
      "Epoch [11][0/12]\tLoss: 4.953\n",
      "Epoch [12][0/12]\tLoss: 5.018\n",
      "Epoch [13][0/12]\tLoss: 4.785\n",
      "Epoch [14][0/12]\tLoss: 4.743\n",
      "Epoch [15][0/12]\tLoss: 4.580\n",
      "Epoch [16][0/12]\tLoss: 4.689\n",
      "Epoch [17][0/12]\tLoss: 4.605\n",
      "Epoch [18][0/12]\tLoss: 4.653\n",
      "Epoch [19][0/12]\tLoss: 4.580\n",
      "Epoch [20][0/12]\tLoss: 4.589\n",
      "Epoch [21][0/12]\tLoss: 4.304\n",
      "Epoch [22][0/12]\tLoss: 4.352\n",
      "Epoch [23][0/12]\tLoss: 4.330\n",
      "Epoch [24][0/12]\tLoss: 4.269\n",
      "Epoch [25][0/12]\tLoss: 4.302\n",
      "Epoch [26][0/12]\tLoss: 3.933\n",
      "Epoch [27][0/12]\tLoss: 3.966\n",
      "Epoch [28][0/12]\tLoss: 3.989\n",
      "Epoch [29][0/12]\tLoss: 3.769\n",
      "Epoch [30][0/12]\tLoss: 3.865\n",
      "Epoch [31][0/12]\tLoss: 3.654\n",
      "Epoch [32][0/12]\tLoss: 3.581\n",
      "Epoch [33][0/12]\tLoss: 3.753\n",
      "Epoch [34][0/12]\tLoss: 3.545\n",
      "Epoch [35][0/12]\tLoss: 3.480\n",
      "Epoch [36][0/12]\tLoss: 3.473\n",
      "Epoch [37][0/12]\tLoss: 3.380\n",
      "Epoch [38][0/12]\tLoss: 3.299\n",
      "Epoch [39][0/12]\tLoss: 3.241\n",
      "Epoch [40][0/12]\tLoss: 3.146\n",
      "Epoch [41][0/12]\tLoss: 3.104\n",
      "Epoch [42][0/12]\tLoss: 3.049\n",
      "Epoch [43][0/12]\tLoss: 2.835\n",
      "Epoch [44][0/12]\tLoss: 2.849\n",
      "Epoch [45][0/12]\tLoss: 3.006\n",
      "Epoch [46][0/12]\tLoss: 3.104\n",
      "Epoch [47][0/12]\tLoss: 2.803\n",
      "Epoch [48][0/12]\tLoss: 2.836\n",
      "Epoch [49][0/12]\tLoss: 2.362\n",
      "Epoch [50][0/12]\tLoss: 2.721\n",
      "Epoch [51][0/12]\tLoss: 2.591\n",
      "Epoch [52][0/12]\tLoss: 2.636\n",
      "Epoch [53][0/12]\tLoss: 2.550\n",
      "Epoch [54][0/12]\tLoss: 2.065\n",
      "Epoch [55][0/12]\tLoss: 1.945\n",
      "Epoch [56][0/12]\tLoss: 2.299\n",
      "Epoch [57][0/12]\tLoss: 2.452\n",
      "Epoch [58][0/12]\tLoss: 2.379\n",
      "Epoch [59][0/12]\tLoss: 2.391\n",
      "Epoch [60][0/12]\tLoss: 2.098\n",
      "Epoch [61][0/12]\tLoss: 2.034\n",
      "Epoch [62][0/12]\tLoss: 2.081\n",
      "Epoch [63][0/12]\tLoss: 2.331\n",
      "Epoch [64][0/12]\tLoss: 2.219\n",
      "Epoch [65][0/12]\tLoss: 1.938\n",
      "Epoch [66][0/12]\tLoss: 2.189\n",
      "Epoch [67][0/12]\tLoss: 1.943\n",
      "Epoch [68][0/12]\tLoss: 1.877\n",
      "Epoch [69][0/12]\tLoss: 1.835\n",
      "Epoch [70][0/12]\tLoss: 1.589\n",
      "Epoch [71][0/12]\tLoss: 1.593\n",
      "Epoch [72][0/12]\tLoss: 1.927\n",
      "Epoch [73][0/12]\tLoss: 1.710\n",
      "Epoch [74][0/12]\tLoss: 1.418\n",
      "Epoch [75][0/12]\tLoss: 1.677\n",
      "Epoch [76][0/12]\tLoss: 1.499\n",
      "Epoch [77][0/12]\tLoss: 1.461\n",
      "Epoch [78][0/12]\tLoss: 1.487\n",
      "Epoch [79][0/12]\tLoss: 1.331\n",
      "Epoch [80][0/12]\tLoss: 1.439\n",
      "Epoch [81][0/12]\tLoss: 1.372\n",
      "Epoch [82][0/12]\tLoss: 1.466\n",
      "Epoch [83][0/12]\tLoss: 1.236\n",
      "Epoch [84][0/12]\tLoss: 1.161\n",
      "Epoch [85][0/12]\tLoss: 1.390\n",
      "Epoch [86][0/12]\tLoss: 1.273\n",
      "Epoch [87][0/12]\tLoss: 1.138\n",
      "Epoch [88][0/12]\tLoss: 1.260\n",
      "Epoch [89][0/12]\tLoss: 1.304\n",
      "Epoch [90][0/12]\tLoss: 1.002\n",
      "Epoch [91][0/12]\tLoss: 0.912\n",
      "Epoch [92][0/12]\tLoss: 0.963\n",
      "Epoch [93][0/12]\tLoss: 0.769\n",
      "Epoch [94][0/12]\tLoss: 1.066\n",
      "Epoch [95][0/12]\tLoss: 0.909\n",
      "Epoch [96][0/12]\tLoss: 1.096\n",
      "Epoch [97][0/12]\tLoss: 0.900\n",
      "Epoch [98][0/12]\tLoss: 0.818\n",
      "Epoch [99][0/12]\tLoss: 0.955\n",
      "Epoch [100][0/12]\tLoss: 0.925\n",
      "Epoch [101][0/12]\tLoss: 0.831\n",
      "Epoch [102][0/12]\tLoss: 0.825\n",
      "Epoch [103][0/12]\tLoss: 0.727\n",
      "Epoch [104][0/12]\tLoss: 0.688\n",
      "Epoch [105][0/12]\tLoss: 0.636\n",
      "Epoch [106][0/12]\tLoss: 0.809\n",
      "Epoch [107][0/12]\tLoss: 0.653\n",
      "Epoch [108][0/12]\tLoss: 0.705\n",
      "Epoch [109][0/12]\tLoss: 0.646\n",
      "Epoch [110][0/12]\tLoss: 0.575\n",
      "Epoch [111][0/12]\tLoss: 0.691\n",
      "Epoch [112][0/12]\tLoss: 0.661\n",
      "Epoch [113][0/12]\tLoss: 0.570\n",
      "Epoch [114][0/12]\tLoss: 0.614\n",
      "Epoch [115][0/12]\tLoss: 0.623\n",
      "Epoch [116][0/12]\tLoss: 0.514\n",
      "Epoch [117][0/12]\tLoss: 0.550\n",
      "Epoch [118][0/12]\tLoss: 0.569\n",
      "Epoch [119][0/12]\tLoss: 0.525\n",
      "Epoch [120][0/12]\tLoss: 0.534\n",
      "Epoch [121][0/12]\tLoss: 0.499\n",
      "Epoch [122][0/12]\tLoss: 0.489\n",
      "Epoch [123][0/12]\tLoss: 0.447\n",
      "Epoch [124][0/12]\tLoss: 0.497\n",
      "Epoch [125][0/12]\tLoss: 0.487\n",
      "Epoch [126][0/12]\tLoss: 0.496\n",
      "Epoch [127][0/12]\tLoss: 0.438\n",
      "Epoch [128][0/12]\tLoss: 0.437\n",
      "Epoch [129][0/12]\tLoss: 0.451\n",
      "Epoch [130][0/12]\tLoss: 0.433\n",
      "Epoch [131][0/12]\tLoss: 0.381\n",
      "Epoch [132][0/12]\tLoss: 0.408\n",
      "Epoch [133][0/12]\tLoss: 0.441\n",
      "Epoch [134][0/12]\tLoss: 0.427\n",
      "Epoch [135][0/12]\tLoss: 0.410\n",
      "Epoch [136][0/12]\tLoss: 0.406\n",
      "Epoch [137][0/12]\tLoss: 0.403\n",
      "Epoch [138][0/12]\tLoss: 0.363\n",
      "Epoch [139][0/12]\tLoss: 0.397\n",
      "Epoch [140][0/12]\tLoss: 0.388\n",
      "Epoch [141][0/12]\tLoss: 0.389\n",
      "Epoch [142][0/12]\tLoss: 0.364\n",
      "Epoch [143][0/12]\tLoss: 0.341\n",
      "Epoch [144][0/12]\tLoss: 0.340\n",
      "Epoch [145][0/12]\tLoss: 0.332\n",
      "Epoch [146][0/12]\tLoss: 0.363\n",
      "Epoch [147][0/12]\tLoss: 0.319\n",
      "Epoch [148][0/12]\tLoss: 0.333\n",
      "Epoch [149][0/12]\tLoss: 0.313\n",
      "Epoch [150][0/12]\tLoss: 0.310\n",
      "Epoch [151][0/12]\tLoss: 0.336\n",
      "Epoch [152][0/12]\tLoss: 0.313\n",
      "Epoch [153][0/12]\tLoss: 0.340\n",
      "Epoch [154][0/12]\tLoss: 0.311\n",
      "Epoch [155][0/12]\tLoss: 0.303\n",
      "Epoch [156][0/12]\tLoss: 0.312\n",
      "Epoch [157][0/12]\tLoss: 0.324\n",
      "Epoch [158][0/12]\tLoss: 0.296\n",
      "Epoch [159][0/12]\tLoss: 0.271\n",
      "Epoch [160][0/12]\tLoss: 0.270\n",
      "Epoch [161][0/12]\tLoss: 0.282\n",
      "Epoch [162][0/12]\tLoss: 0.269\n",
      "Epoch [163][0/12]\tLoss: 0.290\n",
      "Epoch [164][0/12]\tLoss: 0.274\n",
      "Epoch [165][0/12]\tLoss: 0.310\n",
      "Epoch [166][0/12]\tLoss: 0.290\n",
      "Epoch [167][0/12]\tLoss: 0.263\n",
      "Epoch [168][0/12]\tLoss: 0.255\n",
      "Epoch [169][0/12]\tLoss: 0.250\n",
      "Epoch [170][0/12]\tLoss: 0.266\n",
      "Epoch [171][0/12]\tLoss: 0.276\n",
      "Epoch [172][0/12]\tLoss: 0.257\n",
      "Epoch [173][0/12]\tLoss: 0.255\n",
      "Epoch [174][0/12]\tLoss: 0.275\n",
      "Epoch [175][0/12]\tLoss: 0.255\n",
      "Epoch [176][0/12]\tLoss: 0.257\n",
      "Epoch [177][0/12]\tLoss: 0.240\n"
     ]
    }
   ],
   "source": [
    "# directory = 'experiment_vanilla_16924_Ext1'\n",
    "# create_directory(directory)\n",
    "\n",
    "d_model = 512\n",
    "heads = 4\n",
    "num_layers_enc = 5\n",
    "num_layers_dec = 1\n",
    "epochs = 200\n",
    "\n",
    "run = neptune_init(name)\n",
    "\n",
    "loss_history_vanilla_transformer = []\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "with open('WORDMAP_corpus_KBFILKOM.json', 'r') as j:\n",
    "    word_map = json.load(j)\n",
    "    \n",
    "transformer = Transformer(d_model = d_model, heads = heads, num_layers_enc = num_layers_enc, num_layers_dec= num_layers_dec, word_map = word_map, max_len=95)\n",
    "transformer = transformer.to(device)\n",
    "adam_optimizer = torch.optim.Adam(transformer.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-9)\n",
    "transformer_optimizer = AdamWarmup(model_size = d_model, warmup_steps = 4000, optimizer = adam_optimizer)\n",
    "criterion = LossWithLS(len(word_map), 0.2)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    loss_train = train(train_loader, transformer, criterion, epoch)\n",
    "\n",
    "    # run['train/loss'].append(loss_train)\n",
    "\n",
    "save_dir = directory + '/experiment_' + '.pth.tar'\n",
    "parameters = {\n",
    "    'd_model': d_model,\n",
    "    'heads': heads,\n",
    "    'encoder_layers': num_layers_enc,\n",
    "    'decoder_layers': num_layers_dec,\n",
    "    'model_dir': save_dir,\n",
    "    'total_params': summary(transformer).trainable_params\n",
    "}\n",
    "# run['parameters'] = parameters\n",
    "\n",
    "state = {'epoch': epoch, 'transformer': transformer, 'transformer_optimizer': transformer_optimizer}\n",
    "torch.save(state, save_dir)\n",
    "\n",
    "print('Model saved at epoch: {} name {}'.format(epoch, save_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = 'experiment_vanilla_16924/experiment_.pth.tar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "directory = save_dir\n",
    "checkpoint = torch.load(directory)\n",
    "transformer = checkpoint['transformer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[62, 29]], device='cuda:0')\n",
      "tensor([[[[True, True]]]], device='cuda:0')\n",
      "tensor([[[ 0.6159, -0.3050, -0.9642,  ..., -0.6129,  1.0539,  0.1198],\n",
      "         [ 1.1908, -0.5067, -0.0990,  ..., -0.7826,  0.6089,  0.9292]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "question = \"visi filkom\" \n",
    "max_len = 50\n",
    "enc_qus = [word_map.get(word, word_map['<unk>']) for word in question.split()]\n",
    "question = torch.LongTensor(enc_qus).to(device).unsqueeze(0)\n",
    "print(question)\n",
    "question_mask = (question!=0).to(device).unsqueeze(1).unsqueeze(1)  \n",
    "print(question_mask)\n",
    "encoded = transformer.encode(question, question_mask)\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 512])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "menjadi fakultas yang berdaya saing internasional dan berkontribusi kepada pengembangan teknologi informasi dan ilmu komputer untuk menunjang industri dan pengabdian kepada masyarakat\n"
     ]
    }
   ],
   "source": [
    "question = \"apakah visi filkom\" \n",
    "max_len = 50\n",
    "enc_qus = [word_map.get(word, word_map['<unk>']) for word in question.split()]\n",
    "question = torch.LongTensor(enc_qus).to(device).unsqueeze(0)\n",
    "question_mask = (question!=0).to(device).unsqueeze(1).unsqueeze(1)  \n",
    "sentence = evaluate(transformer, question, question_mask, int(max_len), word_map)\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "menyelenggarakan pendidikan di bidang teknologi informasi dan ilmu komputer yang berkualitas dan berstandar internasional secara berkelanjutan meningkatkan kemampuan sivitas akademika dalam pengembangan penelitian dan pengabdian kepada masyarakat dalam skala nasional dan pengabdian kepada masyarakat dalam pengembangan penelitian dan pengabdian kepada masyarakat dalam pengembangan penelitian dan pengabdian kepada masyarakat dalam\n"
     ]
    }
   ],
   "source": [
    "question = \"misi filkom\" \n",
    "max_len = 50\n",
    "enc_qus = [word_map.get(word, word_map['<unk>']) for word in question.split()]\n",
    "question = torch.LongTensor(enc_qus).to(device).unsqueeze(0)\n",
    "question_mask = (question!=0).to(device).unsqueeze(1).unsqueeze(1)  \n",
    "sentence = evaluate(transformer, question, question_mask, int(max_len), word_map)\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[202, 9, 223, 31]\n",
      "issa arwani skom msc\n"
     ]
    }
   ],
   "source": [
    "question = \"ketua departemen sistem informasi\" \n",
    "max_len = 50\n",
    "enc_qus = [word_map.get(word, word_map['<unk>']) for word in question.split()]\n",
    "print(enc_qus)\n",
    "question = torch.LongTensor(enc_qus).to(device).unsqueeze(0)\n",
    "question_mask = (question!=0).to(device).unsqueeze(1).unsqueeze(1)  \n",
    "sentence = evaluate(transformer, question, question_mask, int(max_len), word_map)\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[206, 9, 844]\n",
      "s teknik informatika\n"
     ]
    }
   ],
   "source": [
    "question = \"sekretaris departemen si\" \n",
    "max_len = 50\n",
    "enc_qus = [word_map.get(word, word_map['<unk>']) for word in question.split()]\n",
    "print(enc_qus)\n",
    "question = torch.LongTensor(enc_qus).to(device).unsqueeze(0)\n",
    "question_mask = (question!=0).to(device).unsqueeze(1).unsqueeze(1)  \n",
    "sentence = evaluate(transformer, question, question_mask, int(max_len), word_map)\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[202, 12, 13, 211, 15, 16]\n",
      "sabriansyah rizqika akbar st meng phd\n"
     ]
    }
   ],
   "source": [
    "question = \"ketua program studi magister ilmu komputer\" \n",
    "max_len = 50\n",
    "enc_qus = [word_map.get(word, word_map['<unk>']) for word in question.split()]\n",
    "print(enc_qus)\n",
    "question = torch.LongTensor(enc_qus).to(device).unsqueeze(0)\n",
    "question_mask = (question!=0).to(device).unsqueeze(1).unsqueeze(1)  \n",
    "sentence = evaluate(transformer, question, question_mask, int(max_len), word_map)\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "menjadi fakultas yang berdaya saing internasional dan berkontribusi kepada pengembangan teknologi informasi dan ilmu komputer untuk menunjang industri dan pengabdian kepada pengembangan teknologi informasi dan pengabdian kepada pengembangan teknologi informasi dan pengabdian kepada pengembangan teknologi informasi dan pengabdian kepada pengembangan teknologi informasi dan pengabdian kepada pengembangan teknologi informasi dan\n"
     ]
    }
   ],
   "source": [
    "question = \"visi filkom ub\" \n",
    "max_len = 50\n",
    "enc_qus = [word_map.get(word, word_map['<unk>']) for word in question.split()]\n",
    "question = torch.LongTensor(enc_qus).to(device).unsqueeze(0)\n",
    "question_mask = (question!=0).to(device).unsqueeze(1).unsqueeze(1)  \n",
    "sentence = evaluate(transformer, question, question_mask, int(max_len), word_map)\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prof ir wayan firdaus mahmudy ssi mt phd\n"
     ]
    }
   ],
   "source": [
    "question = \"apakah visi fakultas ilmu\" \n",
    "max_len = 50\n",
    "enc_qus = [word_map.get(word, word_map['<unk>']) for word in question.split()]\n",
    "question = torch.LongTensor(enc_qus).to(device).unsqueeze(0)\n",
    "question_mask = (question!=0).to(device).unsqueeze(1).unsqueeze(1)  \n",
    "sentence = evaluate(transformer, question, question_mask, int(max_len), word_map)\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "menyelenggarakan pendidikan di bidang teknologi informasi dan ilmu komputer yang berkualitas dan berstandar internasional secara berkelanjutan meningkatkan kemampuan sivitas akademika dalam pengembangan penelitian dan pengabdian yang selaras dengan kebutuhan industri dan pengabdian yang selaras dengan tatakelola organisasi yang berkelanjutan di bidang pendidikan penelitian dan pengabdian yang berkelanjutan di bidang\n"
     ]
    }
   ],
   "source": [
    "question = \"misi fakultas\" \n",
    "max_len = 50\n",
    "enc_qus = [word_map.get(word, word_map['<unk>']) for word in question.split()]\n",
    "question = torch.LongTensor(enc_qus).to(device).unsqueeze(0)\n",
    "question_mask = (question!=0).to(device).unsqueeze(1).unsqueeze(1)  \n",
    "sentence = evaluate(transformer, question, question_mask, int(max_len), word_map)\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oktober\n"
     ]
    }
   ],
   "source": [
    "question = \"tanggal dibentuk\" \n",
    "max_len = 50\n",
    "enc_qus = [word_map.get(word, word_map['<unk>']) for word in question.split()]\n",
    "question = torch.LongTensor(enc_qus).to(device).unsqueeze(0)\n",
    "question_mask = (question!=0).to(device).unsqueeze(1).unsqueeze(1)  \n",
    "sentence = evaluate(transformer, question, question_mask, int(max_len), word_map)\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wiwin lukitohadi spsi prasetyo iskandar st mt phd\n"
     ]
    }
   ],
   "source": [
    "question = \"konselor filkom\" \n",
    "max_len = 50\n",
    "enc_qus = [word_map.get(word, word_map['<unk>']) for word in question.split()]\n",
    "question = torch.LongTensor(enc_qus).to(device).unsqueeze(0)\n",
    "question_mask = (question!=0).to(device).unsqueeze(1).unsqueeze(1)  \n",
    "sentence = evaluate(transformer, question, question_mask, int(max_len), word_map)\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "informasi alumni dapat diakses pada link berikut httpsfilkomubacidkemahasiswaaninfoalumni\n"
     ]
    }
   ],
   "source": [
    "question = \"saya butuh informasi alumni\" \n",
    "max_len = 50\n",
    "enc_qus = [word_map.get(word, word_map['<unk>']) for word in question.split()]\n",
    "question = torch.LongTensor(enc_qus).to(device).unsqueeze(0)\n",
    "question_mask = (question!=0).to(device).unsqueeze(1).unsqueeze(1)  \n",
    "sentence = evaluate(transformer, question, question_mask, int(max_len), word_map)\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "menghasilkan lulusan yang kompeten profesional berbudi pekerti luhur berjiwa entrepreneur dan berdaya saing internasional menghasilkan sivitas akademika yang berorientasi pada pembaruan dan masyarakat yang berdaya saing internasional menghasilkan sivitas akademika yang berdaya saing internasional menghasilkan sivitas akademika yang berdaya saing internasional menghasilkan sivitas akademika yang berdaya saing internasional menghasilkan\n"
     ]
    }
   ],
   "source": [
    "question = \"tujuan filkom\" \n",
    "max_len = 50\n",
    "enc_qus = [word_map.get(word, word_map['<unk>']) for word in question.split()]\n",
    "question = torch.LongTensor(enc_qus).to(device).unsqueeze(0)\n",
    "question_mask = (question!=0).to(device).unsqueeze(1).unsqueeze(1)  \n",
    "sentence = evaluate(transformer, question, question_mask, int(max_len), word_map)\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meningkatkan kompetensi dan kualifikasi pendidikan dosen meningkatkan sarana dan prasarana pembelajaran mengembangkan kurikulum mengkuti perkembangan dan kebutuhan pemangku kepentingan meningkatkan sarana dan kebutuhan pemangku kepentingan meningkatkan sarana dan kebutuhan pemangku kepentingan meningkatkan sarana dan kebutuhan pemangku kepentingan meningkatkan sarana dan kebutuhan pemangku kepentingan meningkatkan sarana dan kebutuhan pemangku kepentingan\n"
     ]
    }
   ],
   "source": [
    "question = \"sasaran pendidikan filkom\" \n",
    "max_len = 50\n",
    "enc_qus = [word_map.get(word, word_map['<unk>']) for word in question.split()]\n",
    "question = torch.LongTensor(enc_qus).to(device).unsqueeze(0)\n",
    "question_mask = (question!=0).to(device).unsqueeze(1).unsqueeze(1)  \n",
    "sentence = evaluate(transformer, question, question_mask, int(max_len), word_map)\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tensor([[ 0,  0,  0,  0,  0,  0,  0,  0, 62, 29]], device='cuda:0')\n",
      "tensor([[[[False, False, False, False, False, False, False, False,  True,  True]]]],\n",
      "       device='cuda:0')\n",
      "tensor([[[-0.0210, -1.4425, -1.5541,  ...,  0.5792, -0.3571,  0.1604],\n",
      "         [-0.0109, -1.4484, -1.5444,  ...,  0.5792, -0.3574,  0.1601],\n",
      "         [-0.0099, -1.4600, -1.5437,  ...,  0.5792, -0.3572,  0.1599],\n",
      "         ...,\n",
      "         [-0.0131, -1.4453, -1.5464,  ...,  0.5797, -0.3567,  0.1602],\n",
      "         [-1.3773,  0.0873, -1.7631,  ..., -1.3856, -1.0629,  0.2968],\n",
      "         [-0.1818, -0.2244,  1.5794,  ..., -0.2919, -0.5273,  1.3134]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0, 2546,   62,   29]],\n",
      "       device='cuda:0')\n",
      "tensor([[[[False, False, False, False, False, False, False,  True,  True,  True]]]],\n",
      "       device='cuda:0')\n",
      "tensor([[[ 0.8684, -1.5382, -1.1784,  ...,  0.3129,  0.2648, -0.2526],\n",
      "         [ 0.8322, -1.4962, -1.2178,  ...,  0.3413,  0.2255, -0.2379],\n",
      "         [ 0.5334, -1.1792, -1.4425,  ...,  0.5213,  0.0079, -0.1859],\n",
      "         ...,\n",
      "         [ 0.3538, -1.3309,  0.0176,  ..., -0.6950,  0.0601,  0.4056],\n",
      "         [-1.2756, -0.2533, -2.0492,  ..., -2.1603, -0.2899, -0.0820],\n",
      "         [ 0.2590, -0.0989,  1.7050,  ...,  0.0184, -0.3413,  1.0072]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "tensor([[ 0,  0,  0,  0,  0,  0,  0,  0, 79, 29]], device='cuda:0')\n",
      "tensor([[[[False, False, False, False, False, False, False, False,  True,  True]]]],\n",
      "       device='cuda:0')\n",
      "tensor([[[ 0.7296, -0.8148, -1.0830,  ...,  1.4099,  0.5622,  0.0843],\n",
      "         [ 0.7383, -0.8199, -1.0744,  ...,  1.4096,  0.5620,  0.0843],\n",
      "         [ 0.7390, -0.8301, -1.0736,  ...,  1.4095,  0.5621,  0.0845],\n",
      "         ...,\n",
      "         [ 0.6216, -1.0369, -1.2433,  ...,  1.1923,  0.2950, -0.0330],\n",
      "         [-0.8870,  0.5626, -0.6908,  ..., -0.2164, -0.5580, -0.7957],\n",
      "         [ 0.0695, -0.0725,  1.3631,  ..., -1.0353, -0.6049,  0.0964]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0, 2546,   79,   29]],\n",
      "       device='cuda:0')\n",
      "tensor([[[[False, False, False, False, False, False, False,  True,  True,  True]]]],\n",
      "       device='cuda:0')\n",
      "tensor([[[ 1.1358, -0.7084, -0.9287,  ...,  1.7955,  0.9542,  0.2644],\n",
      "         [ 1.1453, -0.7138, -0.9194,  ...,  1.7956,  0.9539,  0.2644],\n",
      "         [ 1.1461, -0.7250, -0.9187,  ...,  1.7959,  0.9538,  0.2646],\n",
      "         ...,\n",
      "         [ 0.4781, -0.8332,  0.7150,  ..., -1.0942,  0.0150, -0.3869],\n",
      "         [-1.4233,  0.4938, -1.1733,  ..., -0.4335, -0.4615, -0.4974],\n",
      "         [ 0.5426, -0.2123,  1.2203,  ..., -0.7187, -0.5314,  0.3341]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "tensor([[  0,   0,   0,   0,   0, 104, 105,  77,  81,  29]], device='cuda:0')\n",
      "tensor([[[[False, False, False, False, False,  True,  True,  True,  True,  True]]]],\n",
      "       device='cuda:0')\n",
      "tensor([[[ 0.3693, -1.9869, -1.3178,  ...,  1.0914, -1.1232, -1.0078],\n",
      "         [ 0.3780, -1.9918, -1.3093,  ...,  1.0912, -1.1239, -1.0082],\n",
      "         [ 0.3787, -2.0021, -1.3088,  ...,  1.0913, -1.1241, -1.0084],\n",
      "         ...,\n",
      "         [-0.3056, -0.9224,  0.9782,  ...,  0.1045, -1.0143,  0.2122],\n",
      "         [-0.6984, -0.0638,  0.5014,  ...,  0.9366, -1.2456,  0.1767],\n",
      "         [ 1.1449, -0.7185,  1.2923,  ...,  0.1062, -0.9777,  0.5880]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "tensor([[  0,   0,   0,   0,   0,   0,   0, 105,  77,  29]], device='cuda:0')\n",
      "tensor([[[[False, False, False, False, False, False, False,  True,  True,  True]]]],\n",
      "       device='cuda:0')\n",
      "tensor([[[ 0.1118, -1.7063, -1.1736,  ...,  1.1226, -1.2293, -0.0926],\n",
      "         [ 0.1200, -1.7109, -1.1654,  ...,  1.1225, -1.2298, -0.0929],\n",
      "         [ 0.1205, -1.7206, -1.1650,  ...,  1.1227, -1.2300, -0.0930],\n",
      "         ...,\n",
      "         [ 0.6966, -0.5614, -1.6369,  ..., -0.5432, -0.8879,  0.0920],\n",
      "         [-0.3024, -0.9314,  0.9825,  ...,  0.1045, -1.0138,  0.2125],\n",
      "         [ 1.1185, -0.4881,  1.1324,  ...,  0.0362, -0.8084,  0.4225]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "tensor([[   0,    0,    0,    0,    0,    0,    0, 2529,  171,   29]],\n",
      "       device='cuda:0')\n",
      "tensor([[[[False, False, False, False, False, False, False,  True,  True,  True]]]],\n",
      "       device='cuda:0')\n",
      "tensor([[[ 0.3758, -1.2325, -1.8158,  ...,  0.8807, -0.8698,  0.2190],\n",
      "         [ 0.3870, -1.2389, -1.8049,  ...,  0.8804, -0.8702,  0.2188],\n",
      "         [ 0.3881, -1.2517, -1.8038,  ...,  0.8803, -0.8703,  0.2190],\n",
      "         ...,\n",
      "         [-1.5290, -1.1721,  0.1989,  ..., -2.2435,  1.1281,  0.3913],\n",
      "         [-1.4599, -0.6551, -0.7919,  ...,  0.4859, -0.4659,  0.7329],\n",
      "         [ 0.3694,  0.0041,  1.4515,  ..., -0.4422, -0.0752,  0.8054]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "tensor([[   0,    0,    0,    0, 2529,  273,   12,   13,  223,   31]],\n",
      "       device='cuda:0')\n",
      "tensor([[[[False, False, False, False,  True,  True,  True,  True,  True,  True]]]],\n",
      "       device='cuda:0')\n",
      "tensor([[[ 1.1715, -0.7217, -1.6568,  ...,  0.3906,  0.1942,  0.3038],\n",
      "         [ 1.1822, -0.7278, -1.6463,  ...,  0.3903,  0.1935,  0.3035],\n",
      "         [ 1.1830, -0.7403, -1.6456,  ...,  0.3902,  0.1935,  0.3034],\n",
      "         ...,\n",
      "         [ 0.2072,  1.0720, -0.1556,  ..., -0.5972, -0.3047,  0.7004],\n",
      "         [ 1.5344,  0.6324, -0.4935,  ..., -0.6496,  1.0257,  0.0113],\n",
      "         [ 0.7715,  0.9892,  0.4277,  ..., -1.3592, -1.3548, -0.1491]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "\n",
      "tensor([[   0,    0,    0,    0,    0,    0, 2529, 2546,  223,   31]],\n",
      "       device='cuda:0')\n",
      "tensor([[[[False, False, False, False, False, False,  True,  True,  True,  True]]]],\n",
      "       device='cuda:0')\n",
      "tensor([[[ 1.8763, -0.4662, -1.9506,  ...,  1.5316,  0.4753,  0.0419],\n",
      "         [ 1.8870, -0.4720, -1.9403,  ...,  1.5316,  0.4749,  0.0417],\n",
      "         [ 1.8881, -0.4840, -1.9394,  ...,  1.5319,  0.4748,  0.0416],\n",
      "         ...,\n",
      "         [ 0.6613, -0.0091,  0.3424,  ..., -0.2732,  0.5128,  0.9503],\n",
      "         [ 1.8465,  0.6955, -0.9188,  ..., -0.0382,  0.9276,  0.0233],\n",
      "         [ 0.4020,  0.5665,  0.5089,  ..., -1.4602, -0.7492, -0.2799]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "encoded_play = []\n",
    "\n",
    "words = [\n",
    "    \"visi filkom\",\n",
    "    \"apakah visi filkom\",\n",
    "    \"misi filkom\",\n",
    "    \"apakah misi filkom\",\n",
    "    \"apa tujuan pendidikan di filkom\",\n",
    "    \"tujuan pendidikan filkom\",\n",
    "    \"siapa dekan filkom\",\n",
    "    \"siapa kepala program studi sistem informasi\",\n",
    "    \"siapa kaprodi sistem informasi\",\n",
    "    ]\n",
    "\n",
    "labels_words = [\n",
    "    \"visi_1\",\n",
    "    \"visi_2\",\n",
    "    \"misi_1\",\n",
    "    \"misi_2\",\n",
    "    \"tujuan_pendidikan_1\",\n",
    "    \"tujuan_pendidikan_2\",\n",
    "    \"dekan_1\",\n",
    "    \"kp_si_1\",\n",
    "    \"kp_si_2\",\n",
    "    ]\n",
    "\n",
    "for i in range(len(words)):\n",
    "    words[i] = words[i].lower()\n",
    "    question = words[i] \n",
    "    max_len = 50\n",
    "    enc_qus = [word_map.get(word, word_map['<unk>']) for word in question.split()]\n",
    "    if len(enc_qus) < 10:\n",
    "        left_pad = [word_map['<pad>']] * (10 - len(enc_qus))\n",
    "        enc_qus = left_pad + enc_qus\n",
    "    question = torch.LongTensor(enc_qus).to(device).unsqueeze(0)\n",
    "    print()\n",
    "    print(question)\n",
    "    question_mask = (question!=0).to(device).unsqueeze(1).unsqueeze(1)  \n",
    "    print(question_mask)\n",
    "    encoded = transformer.encode(question, question_mask)\n",
    "    print(encoded)\n",
    "    encoded = encoded.detach().cpu().numpy()\n",
    "    if encoded.shape[0] > 1:\n",
    "        concetaned_encoded = np.concatenate(encoded, axis=0)\n",
    "        encoded = concetaned_encoded\n",
    "    encoded_play.append(encoded[0].flatten())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pca = PCA(n_components=2)\n",
    "encoded_play_pca = pca.fit_transform(encoded_play)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(encoded_play_pca[:,0], encoded_play_pca[:,1], s=100)\n",
    "for i, txt in enumerate(labels_words):\n",
    "    plt.annotate(txt, (encoded_play_pca[i,0], encoded_play_pca[i,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pca = PCA(n_components=2)\n",
    "encoded_play_pca = pca.fit_transform(encoded_play)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(encoded_play_pca[:,0], encoded_play_pca[:,1], s=100)\n",
    "for i, txt in enumerate(labels_words):\n",
    "    plt.annotate(txt, (encoded_play_pca[i,0], encoded_play_pca[i,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reset Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(torch.cuda.memory_summary(device=None, abbreviated=False))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30733,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
