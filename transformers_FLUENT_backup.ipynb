{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:03.273239Z","iopub.status.busy":"2024-07-02T20:17:03.272867Z","iopub.status.idle":"2024-07-02T20:17:27.680245Z","shell.execute_reply":"2024-07-02T20:17:27.679127Z","shell.execute_reply.started":"2024-07-02T20:17:03.273209Z"},"scrolled":true,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[33mWARNING: Ignoring invalid distribution -aitlets (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -atbuffers (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -nsorflow-estimator (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ping-extensions (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -rtifi (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -aitlets (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -atbuffers (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -gments (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -mpy (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -nsorflow-estimator (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ping-extensions (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -rkzeug (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -rtifi (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -0aitlets (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -0atbuffers (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -0ping-extensions (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -1aitlets (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -1atbuffers (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -1ping-extensions (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -2aitlets (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -2atbuffers (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -2ping-extensions (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -3aitlets (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -3atbuffers (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -3ping-extensions (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -3transfer (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -4aitlets (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -4atbuffers (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -4ping-extensions (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -5aitlets (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -5atbuffers (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -5ping-extensions (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -5py (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -6aitlets (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -6atbuffers (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -6ping-extensions (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -7aitlets (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -7atbuffers (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -7ping-extensions (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -8atbuffers (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -8ping-extensions (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -9atbuffers (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -9ping-extensions (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -rkzeug (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -qlalchemy (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -achetools (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ackaging (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -afetensors (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -andas (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -andb (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -angchain (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -arkdown (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -arkupsafe (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -arshmallow (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ast (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -astjsonschema (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -atplotlib (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -atplotlib-inline (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -azy-loader (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -bformat (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -cikit-image (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -cikit-learn (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -cipy (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -cwidth (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -dfplumber (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -dna (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -down (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -eaborn (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ebugpy (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -edi (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -egex (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -enacity (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ensorboard-data-server (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ensorflow-estimator (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ensorflow-io-gcs-filesystem (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -entry-sdk (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -equests (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -eras (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -erkzeug (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ermcolor (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ertifi (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -est-asyncio (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -etproctitle (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -etuptools (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -etworkx (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ffi (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -gboost (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -hapely (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -harset-normalizer (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -hreadpoolctl (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ifffile (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -iktoken (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ilelock (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -imm (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -iohttp (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ip (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ipenv (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -irtualenv (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -istlib (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -itpython (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -itdb (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -iwisolver (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ix (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -latbuffers (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -latformdirs (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -learml (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -lembic (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -lick (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ltralytics (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -lvmlite (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -lxtend (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -maes (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -mageio (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -mmap (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ntlr4-python3-runtime (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oblib (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oguru (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -okenizers (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ontourpy (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ooch (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oogle-auth-oauthlib (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ornado (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ortalocker (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -otocore (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -oxr (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -penai (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -pencv-python (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -pencv-python-headless (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -penpyxl (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ptuna (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -qdm (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -raitlets (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ransformers (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -rapt (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -reenlet (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -riton (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -rllib3 (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -rompt-toolkit (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -rozenlist (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -sgpack (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -sonschema (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -sspec (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -sttokens (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -sutil (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -sync-timeout (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -udioread (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -uggingface-hub (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -umba (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -umexpr (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -umpy (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -upyterlab-widgets (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -xceptiongroup (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -xecuting (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -xml (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -yjwt (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -yyaml (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -yasn1 (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -yasn1-modules (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ycler (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ycparser (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ygments (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -yparsing (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -yping-extensions (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -yrsistent (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ython (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ytz (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -yzmq (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ing-extensions (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ing-extensions (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -0ing-extensions (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -1ing-extensions (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -2ing-extensions (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -3ing-extensions (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -4ing-extensions (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -gments (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -mpy (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -nsorboard-data-server (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -parsing (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -tplotlib (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -width (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n","\u001b[0mCollecting neptune\n","  Using cached neptune-1.10.4-py3-none-any.whl.metadata (16 kB)\n","Requirement already satisfied: GitPython>=2.0.8 in /opt/conda/lib/python3.10/site-packages (from neptune) (3.1.40)\n","Requirement already satisfied: Pillow>=1.1.6 in /opt/conda/lib/python3.10/site-packages (from neptune) (9.4.0)\n","Requirement already satisfied: PyJWT in /opt/conda/lib/python3.10/site-packages (from neptune) (2.8.0)\n","Collecting boto3>=1.28.0 (from neptune)\n","  Downloading boto3-1.34.139-py3-none-any.whl.metadata (6.6 kB)\n","Collecting bravado<12.0.0,>=11.0.0 (from neptune)\n","  Using cached bravado-11.0.3-py2.py3-none-any.whl.metadata (5.9 kB)\n","Requirement already satisfied: click>=7.0 in /opt/conda/lib/python3.10/site-packages (from neptune) (8.1.7)\n","Collecting future>=0.17.1 (from neptune)\n","  Using cached future-1.0.0-py3-none-any.whl.metadata (4.0 kB)\n","Requirement already satisfied: oauthlib>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from neptune) (3.2.2)\n","Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from neptune) (23.0)\n","Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from neptune) (1.5.3)\n","Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from neptune) (5.9.6)\n","Requirement already satisfied: requests>=2.20.0 in /opt/conda/lib/python3.10/site-packages (from neptune) (2.32.3)\n","Requirement already satisfied: requests-oauthlib>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from neptune) (1.3.1)\n","Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from neptune) (1.16.0)\n","Collecting swagger-spec-validator>=2.7.4 (from neptune)\n","  Using cached swagger_spec_validator-3.0.4-py2.py3-none-any.whl.metadata (2.5 kB)\n","Requirement already satisfied: typing-extensions>=3.10.0 in /opt/conda/lib/python3.10/site-packages (from neptune) (4.4.0)\n","Requirement already satisfied: urllib3 in /opt/conda/lib/python3.10/site-packages (from neptune) (2.0.7)\n","Requirement already satisfied: websocket-client!=1.0.0,>=0.35.0 in /opt/conda/lib/python3.10/site-packages (from neptune) (1.8.0)\n","Collecting botocore<1.35.0,>=1.34.139 (from boto3>=1.28.0->neptune)\n","  Downloading botocore-1.34.139-py3-none-any.whl.metadata (5.7 kB)\n","Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from boto3>=1.28.0->neptune) (1.0.1)\n","Collecting s3transfer<0.11.0,>=0.10.0 (from boto3>=1.28.0->neptune)\n","  Using cached s3transfer-0.10.2-py3-none-any.whl.metadata (1.7 kB)\n","Collecting bravado-core>=5.16.1 (from bravado<12.0.0,>=11.0.0->neptune)\n","  Using cached bravado-core-6.1.1.tar.gz (63 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25ldone\n","\u001b[?25hRequirement already satisfied: msgpack in /opt/conda/lib/python3.10/site-packages (from bravado<12.0.0,>=11.0.0->neptune) (1.0.7)\n","Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.10/site-packages (from bravado<12.0.0,>=11.0.0->neptune) (2.8.2)\n","Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from bravado<12.0.0,>=11.0.0->neptune) (6.0.1)\n","Collecting simplejson (from bravado<12.0.0,>=11.0.0->neptune)\n","  Using cached simplejson-3.19.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.1 kB)\n","Collecting monotonic (from bravado<12.0.0,>=11.0.0->neptune)\n","  Using cached monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from GitPython>=2.0.8->neptune) (4.0.11)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.20.0->neptune) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.20.0->neptune) (3.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.20.0->neptune) (2022.6.15)\n","Requirement already satisfied: jsonschema in /opt/conda/lib/python3.10/site-packages (from swagger-spec-validator>=2.7.4->neptune) (4.19.2)\n","Requirement already satisfied: importlib-resources>=1.3 in /opt/conda/lib/python3.10/site-packages (from swagger-spec-validator>=2.7.4->neptune) (6.4.0)\n","Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->neptune) (2022.7.1)\n","Requirement already satisfied: numpy>=1.21.0 in /opt/conda/lib/python3.10/site-packages (from pandas->neptune) (1.24.1)\n","Collecting jsonref (from bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune)\n","  Using cached jsonref-1.1.0-py3-none-any.whl.metadata (2.7 kB)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython>=2.0.8->neptune) (5.0.1)\n","Requirement already satisfied: attrs>=22.2.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune) (23.1.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune) (2023.7.1)\n","Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune) (0.30.2)\n","Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune) (0.10.6)\n","Collecting fqdn (from jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune)\n","  Using cached fqdn-1.5.1-py3-none-any.whl.metadata (1.4 kB)\n","Collecting isoduration (from jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune)\n","  Using cached isoduration-20.11.0-py3-none-any.whl.metadata (5.7 kB)\n","Requirement already satisfied: jsonpointer>1.13 in /opt/conda/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune) (2.4)\n","Collecting rfc3339-validator (from jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune)\n","  Using cached rfc3339_validator-0.1.4-py2.py3-none-any.whl.metadata (1.5 kB)\n","Collecting rfc3986-validator>0.1.0 (from jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune)\n","  Using cached rfc3986_validator-0.1.1-py2.py3-none-any.whl.metadata (1.7 kB)\n","Requirement already satisfied: uri-template in /opt/conda/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune) (1.3.0)\n","Requirement already satisfied: webcolors>=1.11 in /opt/conda/lib/python3.10/site-packages (from jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune) (24.6.0)\n","Collecting arrow>=0.15.0 (from isoduration->jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune)\n","  Using cached arrow-1.3.0-py3-none-any.whl.metadata (7.5 kB)\n","Requirement already satisfied: types-python-dateutil>=2.8.10 in /opt/conda/lib/python3.10/site-packages (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=2.5.1->bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune) (2.9.0.20240316)\n","Using cached neptune-1.10.4-py3-none-any.whl (502 kB)\n","Downloading boto3-1.34.139-py3-none-any.whl (139 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.2/139.2 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n","\u001b[?25hUsing cached bravado-11.0.3-py2.py3-none-any.whl (38 kB)\n","Using cached future-1.0.0-py3-none-any.whl (491 kB)\n","Using cached swagger_spec_validator-3.0.4-py2.py3-none-any.whl (28 kB)\n","Downloading botocore-1.34.139-py3-none-any.whl (12.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n","\u001b[?25hUsing cached s3transfer-0.10.2-py3-none-any.whl (82 kB)\n","Using cached monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n","Using cached simplejson-3.19.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (137 kB)\n","Using cached jsonref-1.1.0-py3-none-any.whl (9.4 kB)\n","Using cached rfc3986_validator-0.1.1-py2.py3-none-any.whl (4.2 kB)\n","Using cached fqdn-1.5.1-py3-none-any.whl (9.1 kB)\n","Using cached isoduration-20.11.0-py3-none-any.whl (11 kB)\n","Using cached rfc3339_validator-0.1.4-py2.py3-none-any.whl (3.5 kB)\n","Using cached arrow-1.3.0-py3-none-any.whl (66 kB)\n","Building wheels for collected packages: bravado-core\n","  Building wheel for bravado-core (setup.py) ... \u001b[?25lerror\n","  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n","  \n","  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n","  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n","  \u001b[31m╰─>\u001b[0m \u001b[31m[47 lines of output]\u001b[0m\n","  \u001b[31m   \u001b[0m /opt/conda/lib/python3.10/site-packages/setuptools/dist.py:476: SetuptoolsDeprecationWarning: Invalid dash-separated options\n","  \u001b[31m   \u001b[0m !!\n","  \u001b[31m   \u001b[0m \n","  \u001b[31m   \u001b[0m         ********************************************************************************\n","  \u001b[31m   \u001b[0m         Usage of dash-separated 'description-file' will not be supported in future\n","  \u001b[31m   \u001b[0m         versions. Please use the underscore name 'description_file' instead.\n","  \u001b[31m   \u001b[0m \n","  \u001b[31m   \u001b[0m         By 2024-Sep-26, you need to update your project and remove deprecated calls\n","  \u001b[31m   \u001b[0m         or your builds will no longer be supported.\n","  \u001b[31m   \u001b[0m \n","  \u001b[31m   \u001b[0m         See https://setuptools.pypa.io/en/latest/userguide/declarative_config.html for details.\n","  \u001b[31m   \u001b[0m         ********************************************************************************\n","  \u001b[31m   \u001b[0m \n","  \u001b[31m   \u001b[0m !!\n","  \u001b[31m   \u001b[0m   opt = self.warn_dash_deprecation(opt, section)\n","  \u001b[31m   \u001b[0m Traceback (most recent call last):\n","  \u001b[31m   \u001b[0m   File \"<string>\", line 2, in <module>\n","  \u001b[31m   \u001b[0m   File \"<pip-setuptools-caller>\", line 34, in <module>\n","  \u001b[31m   \u001b[0m   File \"/tmp/pip-install-g8gjdmme/bravado-core_747c8c11e33f4e8a8b46c0f86e8ea122/setup.py\", line 12, in <module>\n","  \u001b[31m   \u001b[0m     setup(\n","  \u001b[31m   \u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/setuptools/__init__.py\", line 103, in setup\n","  \u001b[31m   \u001b[0m     return distutils.core.setup(**attrs)\n","  \u001b[31m   \u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/setuptools/_distutils/core.py\", line 171, in setup\n","  \u001b[31m   \u001b[0m     ok = dist.parse_command_line()\n","  \u001b[31m   \u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/setuptools/_distutils/dist.py\", line 476, in parse_command_line\n","  \u001b[31m   \u001b[0m     args = self._parse_command_opts(parser, args)\n","  \u001b[31m   \u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/setuptools/dist.py\", line 870, in _parse_command_opts\n","  \u001b[31m   \u001b[0m     nargs = _Distribution._parse_command_opts(self, parser, args)\n","  \u001b[31m   \u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/setuptools/_distutils/dist.py\", line 535, in _parse_command_opts\n","  \u001b[31m   \u001b[0m     cmd_class = self.get_command_class(command)\n","  \u001b[31m   \u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/setuptools/dist.py\", line 715, in get_command_class\n","  \u001b[31m   \u001b[0m     self.cmdclass[command] = cmdclass = ep.load()\n","  \u001b[31m   \u001b[0m   File \"/opt/conda/lib/python3.10/importlib/metadata/__init__.py\", line 171, in load\n","  \u001b[31m   \u001b[0m     module = import_module(match.group('module'))\n","  \u001b[31m   \u001b[0m   File \"/opt/conda/lib/python3.10/importlib/__init__.py\", line 126, in import_module\n","  \u001b[31m   \u001b[0m     return _bootstrap._gcd_import(name[level:], package, level)\n","  \u001b[31m   \u001b[0m   File \"<frozen importlib._bootstrap>\", line 1050, in _gcd_import\n","  \u001b[31m   \u001b[0m   File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n","  \u001b[31m   \u001b[0m   File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\n","  \u001b[31m   \u001b[0m   File \"<frozen importlib._bootstrap>\", line 688, in _load_unlocked\n","  \u001b[31m   \u001b[0m   File \"<frozen importlib._bootstrap_external>\", line 883, in exec_module\n","  \u001b[31m   \u001b[0m   File \"<frozen importlib._bootstrap>\", line 241, in _call_with_frames_removed\n","  \u001b[31m   \u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/wheel/bdist_wheel.py\", line 29, in <module>\n","  \u001b[31m   \u001b[0m     from .wheelfile import WheelFile\n","  \u001b[31m   \u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/wheel/wheelfile.py\", line 14, in <module>\n","  \u001b[31m   \u001b[0m     from wheel.cli import WheelError\n","  \u001b[31m   \u001b[0m ImportError: cannot import name 'WheelError' from 'wheel.cli' (unknown location)\n","  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n","  \n","  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n","\u001b[31m  ERROR: Failed building wheel for bravado-core\u001b[0m\u001b[31m\n","\u001b[0m\u001b[?25h  Running setup.py clean for bravado-core\n","Failed to build bravado-core\n","\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (bravado-core)\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["!pip install neptune"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:27.682419Z","iopub.status.busy":"2024-07-02T20:17:27.682115Z","iopub.status.idle":"2024-07-02T20:17:30.597078Z","shell.execute_reply":"2024-07-02T20:17:30.596113Z","shell.execute_reply.started":"2024-07-02T20:17:27.682391Z"},"trusted":true},"outputs":[],"source":["from collections import Counter\n","import json\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset\n","import torch.utils.data\n","import math\n","import torch.nn.functional as F\n","import neptune\n","import pandas as pd"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:30.598900Z","iopub.status.busy":"2024-07-02T20:17:30.598458Z","iopub.status.idle":"2024-07-02T20:17:32.694354Z","shell.execute_reply":"2024-07-02T20:17:32.693406Z","shell.execute_reply.started":"2024-07-02T20:17:30.598873Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Pertanyaan</th>\n","      <th>Jawaban</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>email Fitra A. Bachtiar</td>\n","      <td>fitra.bachtiar[at]ub.ac.id</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>NIK/NIP Fitra A. Bachtiar</td>\n","      <td>198406282019031006</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>nama lengkap Fitra A. Bachtiar</td>\n","      <td>Dr.Eng. Fitra A. Bachtiar</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Departemen Fitra A. Bachtiar</td>\n","      <td>Departemen Teknik Informatika</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Program Studi Fitra A. Bachtiar</td>\n","      <td>S2 Ilmu Komputer</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1229</th>\n","      <td>Apa Manfaat Konseling FILKOM ?</td>\n","      <td>1. Masalah ditangani oleh ahli yang kompeten d...</td>\n","    </tr>\n","    <tr>\n","      <th>1230</th>\n","      <td>Berikan informasi mengenai Layanan Konseling</td>\n","      <td>Informasi mengenai Layanan Konseling dapat dia...</td>\n","    </tr>\n","    <tr>\n","      <th>1231</th>\n","      <td>Siapa Konselor Bimbingan dan Konseling di FILK...</td>\n","      <td>Ada 2 konselor Bimbingan dan Konseling di FILK...</td>\n","    </tr>\n","    <tr>\n","      <th>1232</th>\n","      <td>Siapa Koordinator Konselor Sebaya ?</td>\n","      <td>Koordinator Konselor Sebaya adalah Muhammad Da...</td>\n","    </tr>\n","    <tr>\n","      <th>1233</th>\n","      <td>Berikan Rincian Layanan ULTKSP</td>\n","      <td>Rincian Layanan ULTKSP dapat diakses pada taut...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1198 rows × 2 columns</p>\n","</div>"],"text/plain":["                                             Pertanyaan  \\\n","0                               email Fitra A. Bachtiar   \n","1                             NIK/NIP Fitra A. Bachtiar   \n","2                        nama lengkap Fitra A. Bachtiar   \n","3                          Departemen Fitra A. Bachtiar   \n","4                       Program Studi Fitra A. Bachtiar   \n","...                                                 ...   \n","1229                     Apa Manfaat Konseling FILKOM ?   \n","1230       Berikan informasi mengenai Layanan Konseling   \n","1231  Siapa Konselor Bimbingan dan Konseling di FILK...   \n","1232                Siapa Koordinator Konselor Sebaya ?   \n","1233                     Berikan Rincian Layanan ULTKSP   \n","\n","                                                Jawaban  \n","0                            fitra.bachtiar[at]ub.ac.id  \n","1                                    198406282019031006  \n","2                             Dr.Eng. Fitra A. Bachtiar  \n","3                         Departemen Teknik Informatika  \n","4                                      S2 Ilmu Komputer  \n","...                                                 ...  \n","1229  1. Masalah ditangani oleh ahli yang kompeten d...  \n","1230  Informasi mengenai Layanan Konseling dapat dia...  \n","1231  Ada 2 konselor Bimbingan dan Konseling di FILK...  \n","1232  Koordinator Konselor Sebaya adalah Muhammad Da...  \n","1233  Rincian Layanan ULTKSP dapat diakses pada taut...  \n","\n","[1198 rows x 2 columns]"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["knowledgebase_url = 'https://github.com/AndiAlifs/FLUENT-Chatbot-2023/raw/main/KnowledgeBaseFilkom.xlsx'\n","knowledgebase = pd.read_excel(knowledgebase_url)\n","\n","qa_paired = knowledgebase.drop(columns=knowledgebase.columns.drop(['Pertanyaan', 'Jawaban']))\n","qa_paired.dropna(inplace=True)\n","qa_paired"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:32.697066Z","iopub.status.busy":"2024-07-02T20:17:32.696633Z","iopub.status.idle":"2024-07-02T20:17:32.702322Z","shell.execute_reply":"2024-07-02T20:17:32.701334Z","shell.execute_reply.started":"2024-07-02T20:17:32.697038Z"},"trusted":true},"outputs":[],"source":["def remove_punc(string):\n","    punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n","    no_punct = \"\"\n","    for char in string:\n","        if char not in punctuations:\n","            no_punct = no_punct + char  # space is also a character\n","    return no_punct.lower()"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:32.703938Z","iopub.status.busy":"2024-07-02T20:17:32.703658Z","iopub.status.idle":"2024-07-02T20:17:32.827025Z","shell.execute_reply":"2024-07-02T20:17:32.825935Z","shell.execute_reply.started":"2024-07-02T20:17:32.703915Z"},"trusted":true},"outputs":[],"source":["pairs = []\n","max_len = 90\n","\n","for line in qa_paired.iterrows():\n","    pertanyaan = line[1]['Pertanyaan']\n","    jawaban = line[1]['Jawaban']\n","    qa_pairs = []\n","    first = remove_punc(pertanyaan.strip())      \n","    second = remove_punc(jawaban.strip())\n","    qa_pairs.append(first.split()[:max_len])\n","    qa_pairs.append(second.split()[:max_len])\n","    pairs.append(qa_pairs)"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:32.828565Z","iopub.status.busy":"2024-07-02T20:17:32.828255Z","iopub.status.idle":"2024-07-02T20:17:32.838911Z","shell.execute_reply":"2024-07-02T20:17:32.837848Z","shell.execute_reply.started":"2024-07-02T20:17:32.828539Z"},"trusted":true},"outputs":[],"source":["word_freq = Counter()\n","for pair in pairs:\n","    word_freq.update(pair[0])\n","    word_freq.update(pair[1])"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:32.840576Z","iopub.status.busy":"2024-07-02T20:17:32.840244Z","iopub.status.idle":"2024-07-02T20:17:32.850265Z","shell.execute_reply":"2024-07-02T20:17:32.849352Z","shell.execute_reply.started":"2024-07-02T20:17:32.840548Z"},"trusted":true},"outputs":[],"source":["min_word_freq = 2\n","words = [w for w in word_freq.keys() if word_freq[w] > min_word_freq]\n","word_map = {k: v + 1 for v, k in enumerate(words)}\n","word_map['<unk>'] = len(word_map) + 1\n","word_map['<start>'] = len(word_map) + 1\n","word_map['<end>'] = len(word_map) + 1\n","word_map['<pad>'] = 0"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:32.851802Z","iopub.status.busy":"2024-07-02T20:17:32.851465Z","iopub.status.idle":"2024-07-02T20:17:32.860482Z","shell.execute_reply":"2024-07-02T20:17:32.859625Z","shell.execute_reply.started":"2024-07-02T20:17:32.851770Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Total words are: 1079\n"]}],"source":["print(\"Total words are: {}\".format(len(word_map)))"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:32.861919Z","iopub.status.busy":"2024-07-02T20:17:32.861579Z","iopub.status.idle":"2024-07-02T20:17:32.872138Z","shell.execute_reply":"2024-07-02T20:17:32.871399Z","shell.execute_reply.started":"2024-07-02T20:17:32.861895Z"},"trusted":true},"outputs":[],"source":["with open('WORDMAP_corpus_KBFILKOM.json', 'w') as j:\n","    json.dump(word_map, j)"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:32.877074Z","iopub.status.busy":"2024-07-02T20:17:32.876749Z","iopub.status.idle":"2024-07-02T20:17:32.884325Z","shell.execute_reply":"2024-07-02T20:17:32.883430Z","shell.execute_reply.started":"2024-07-02T20:17:32.877048Z"},"trusted":true},"outputs":[],"source":["def encode_question(words, word_map):\n","    enc_c = [word_map.get(word, word_map['<unk>']) for word in words] + [word_map['<pad>']] * (max_len - len(words))\n","    return enc_c\n","\n","def encode_reply(words, word_map):\n","    enc_c = [word_map['<start>']] + [word_map.get(word, word_map['<unk>']) for word in words] + \\\n","    [word_map['<end>']] + [word_map['<pad>']] * (max_len - len(words))\n","    return enc_c\n","\n","def encode_reply_with_maxlen(words, word_map):\n","    enc_c = [word_map['<start>']] + [word_map.get(word, word_map['<unk>']) for word in words] + [word_map['<end>']] + [word_map['<pad>']] * (max_len-2 - len(words))\n","    return enc_c"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:32.885632Z","iopub.status.busy":"2024-07-02T20:17:32.885341Z","iopub.status.idle":"2024-07-02T20:17:33.099456Z","shell.execute_reply":"2024-07-02T20:17:33.098486Z","shell.execute_reply.started":"2024-07-02T20:17:32.885608Z"},"trusted":true},"outputs":[],"source":["pairs_encoded = []\n","for pair in pairs:\n","    qus = encode_question(pair[0], word_map)\n","    ans = encode_reply(pair[1], word_map)\n","    pairs_encoded.append([qus, ans])\n","\n","with open('pairs_encoded_kbfilkom.json', 'w') as p:\n","    json.dump(pairs_encoded, p)"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:33.100894Z","iopub.status.busy":"2024-07-02T20:17:33.100609Z","iopub.status.idle":"2024-07-02T20:17:33.308867Z","shell.execute_reply":"2024-07-02T20:17:33.307678Z","shell.execute_reply.started":"2024-07-02T20:17:33.100868Z"},"trusted":true},"outputs":[],"source":["pairs_encoded_same_length = []\n","for pair in pairs:\n","    qus = encode_question(pair[0], word_map)\n","    ans = encode_reply_with_maxlen(pair[1], word_map)\n","    pairs_encoded_same_length.append([qus, ans])\n","\n","with open('pairs_encoded_kbfilkom_same_len.json', 'w') as p:\n","    json.dump(pairs_encoded_same_length, p)"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:33.310393Z","iopub.status.busy":"2024-07-02T20:17:33.310107Z","iopub.status.idle":"2024-07-02T20:17:33.317262Z","shell.execute_reply":"2024-07-02T20:17:33.316249Z","shell.execute_reply.started":"2024-07-02T20:17:33.310368Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'apa tujuan filkom <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>'"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["rev_word_map = {v: k for k, v in word_map.items()}\n","' '.join([rev_word_map[v] for v in pairs_encoded[15][0]])"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:33.318791Z","iopub.status.busy":"2024-07-02T20:17:33.318483Z","iopub.status.idle":"2024-07-02T20:17:33.325793Z","shell.execute_reply":"2024-07-02T20:17:33.325035Z","shell.execute_reply.started":"2024-07-02T20:17:33.318756Z"},"trusted":true},"outputs":[],"source":["class Dataset(Dataset):\n","\n","    def __init__(self):\n","\n","        self.pairs = json.load(open('pairs_encoded_kbfilkom.json'))\n","        self.dataset_size = len(self.pairs)\n","\n","    def __getitem__(self, i):\n","        \n","        question = torch.LongTensor(self.pairs[i][0])\n","        reply = torch.LongTensor(self.pairs[i][1])\n","            \n","        return question, reply\n","\n","    def __len__(self):\n","        return self.dataset_size"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:33.327276Z","iopub.status.busy":"2024-07-02T20:17:33.326939Z","iopub.status.idle":"2024-07-02T20:17:33.336359Z","shell.execute_reply":"2024-07-02T20:17:33.335543Z","shell.execute_reply.started":"2024-07-02T20:17:33.327245Z"},"trusted":true},"outputs":[],"source":["class Dataset_Same_Len(Dataset):\n","\n","    def __init__(self):\n","\n","        self.pairs = json.load(open('pairs_encoded_kbfilkom_same_len.json'))\n","        self.dataset_size = len(self.pairs)\n","\n","    def __getitem__(self, i):\n","        \n","        question = torch.LongTensor(self.pairs[i][0])\n","        reply = torch.LongTensor(self.pairs[i][1])\n","            \n","        return question, reply\n","\n","    def __len__(self):\n","        return self.dataset_size"]},{"cell_type":"markdown","metadata":{},"source":["## Train Loader"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:33.337842Z","iopub.status.busy":"2024-07-02T20:17:33.337481Z","iopub.status.idle":"2024-07-02T20:17:33.379910Z","shell.execute_reply":"2024-07-02T20:17:33.378865Z","shell.execute_reply.started":"2024-07-02T20:17:33.337813Z"},"trusted":true},"outputs":[],"source":["train_loader = torch.utils.data.DataLoader(Dataset(),\n","                                           batch_size = 100, \n","                                           shuffle=True, \n","                                           pin_memory=True)"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:33.381467Z","iopub.status.busy":"2024-07-02T20:17:33.381114Z","iopub.status.idle":"2024-07-02T20:17:33.388963Z","shell.execute_reply":"2024-07-02T20:17:33.388016Z","shell.execute_reply.started":"2024-07-02T20:17:33.381434Z"},"trusted":true},"outputs":[],"source":["def create_masks(question, reply_input, reply_target):\n","    \n","    def subsequent_mask(size):\n","        mask = torch.triu(torch.ones(size, size)).transpose(0, 1).type(dtype=torch.uint8)\n","        return mask.unsqueeze(0)\n","    \n","    question_mask = question!=0\n","    question_mask = question_mask.to(device)\n","    question_mask = question_mask.unsqueeze(1).unsqueeze(1)         # (batch_size, 1, 1, max_words)\n","     \n","    reply_input_mask = reply_input!=0\n","    reply_input_mask = reply_input_mask.unsqueeze(1)  # (batch_size, 1, max_words)\n","    reply_input_mask = reply_input_mask & subsequent_mask(reply_input.size(-1)).type_as(reply_input_mask.data) \n","    reply_input_mask = reply_input_mask.unsqueeze(1) # (batch_size, 1, max_words, max_words)\n","    reply_target_mask = reply_target!=0              # (batch_size, max_words)\n","    \n","    return question_mask, reply_input_mask, reply_target_mask"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:33.390611Z","iopub.status.busy":"2024-07-02T20:17:33.390255Z","iopub.status.idle":"2024-07-02T20:17:33.397673Z","shell.execute_reply":"2024-07-02T20:17:33.396839Z","shell.execute_reply.started":"2024-07-02T20:17:33.390578Z"},"trusted":true},"outputs":[],"source":["import os\n","\n","def create_directory(path):\n","    if not os.path.exists(path):\n","        os.makedirs(path)\n","        print(f\"Directory created at {path}\")\n","    else:\n","        print(f\"Directory already exists at {path}\")"]},{"cell_type":"markdown","metadata":{},"source":["# Architecture"]},{"cell_type":"markdown","metadata":{},"source":["## Embeddings"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:33.399270Z","iopub.status.busy":"2024-07-02T20:17:33.398914Z","iopub.status.idle":"2024-07-02T20:17:33.421634Z","shell.execute_reply":"2024-07-02T20:17:33.420718Z","shell.execute_reply.started":"2024-07-02T20:17:33.399239Z"},"trusted":true},"outputs":[],"source":["class Embeddings(nn.Module):\n","    \"\"\"\n","    Implements embeddings of the words and adds their positional encodings. \n","    \"\"\"\n","    def __init__(self, vocab_size, d_model, max_len = 50, num_layers = 6):\n","        super(Embeddings, self).__init__()\n","        self.d_model = d_model\n","        self.dropout = nn.Dropout(0.1)\n","        self.embed = nn.Embedding(vocab_size, d_model)\n","        self.pe = self.create_positinal_encoding(max_len, self.d_model)     # (1, max_len, d_model)\n","        self.te = self.create_positinal_encoding(num_layers, self.d_model)  # (1, num_layers, d_model)\n","        self.dropout = nn.Dropout(0.1)\n","        \n","    def create_positinal_encoding(self, max_len, d_model):\n","        pe = torch.zeros(max_len, d_model).to(device)\n","        for pos in range(max_len):   # for each position of the word\n","            for i in range(0, d_model, 2):   # for each dimension of the each position\n","                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/d_model)))\n","                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n","        pe = pe.unsqueeze(0)   # include the batch size\n","        return pe\n","        \n","    def forward(self, embedding, layer_idx):\n","        if layer_idx == 0:\n","            embedding = self.embed(embedding) * math.sqrt(self.d_model)\n","        embedding += self.pe[:, :embedding.size(1)]   # pe will automatically be expanded with the same batch size as encoded_words\n","        # embedding: (batch_size, max_len, d_model), te: (batch_size, 1, d_model)\n","        embedding += self.te[:, layer_idx, :].unsqueeze(1).repeat(1, embedding.size(1), 1)\n","        embedding = self.dropout(embedding)\n","        return embedding\n","\n","class EmbeddingsLSTM(nn.Module):\n","    \"\"\"\n","    Implements embeddingsLSTM of the words and adds their positional encodings. \n","    \"\"\"\n","    def __init__(self, vocab_size, d_model, num_layers = 6):\n","        super(EmbeddingsLSTM, self).__init__()\n","        self.d_model = d_model\n","        self.dropout = nn.Dropout(0.1)\n","        self.embed = nn.Embedding(vocab_size, d_model)\n","        self.lstm = nn.LSTM(d_model, d_model, num_layers=num_layers)\n","        \n","    def forward(self, embedding, layer_idx, hidden, cell_state):\n","        if layer_idx == 0:\n","            embedding = self.embed(embedding) * math.sqrt(self.d_model)\n","        # Pass the embeddings through the LSTM\\\n","        embedding, (hidden, cell_state) = self.lstm(embedding, (hidden, cell_state))\n","        print()\n","        print(embedding.size())\n","        print(embedding)\n","        return embedding, hidden, cell_state\n","\n","class PretrainedEmbedding(nn.Module):\n","    \"\"\"\n","    Implements embeddingsLSTM of the words and adds their positional encodings. \n","    \"\"\"\n","    def __init__(self, vocab_size, d_model, max_len = 50, num_layers = 6):\n","        super(PretrainedEmbedding, self).__init__()\n","        self.d_model = d_model\n","        self.dropout = nn.Dropout(0.1)\n","        self.embed = nn.Embedding(vocab_size, d_model)\n","        self.pe = self.create_positinal_encoding(max_len, self.d_model)     # (1, max_len, d_model)\n","        self.te = self.create_positinal_encoding(num_layers, self.d_model)  # (1, num_layers, d_model)\n","        self.dropout = nn.Dropout(0.1)\n","        \n","    def create_positinal_encoding(self, max_len, d_model):\n","        pe = torch.zeros(max_len, d_model).to(device)\n","        for pos in range(max_len):   # for each position of the word\n","            for i in range(0, d_model, 2):   # for each dimension of the each position\n","                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/d_model)))\n","                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n","        pe = pe.unsqueeze(0)   # include the batch size\n","        return pe\n","        \n","    def forward(self, embedding, layer_idx):\n","        if layer_idx == 0:\n","            embedding = self.embed(embedding) * math.sqrt(self.d_model)\n","        embedding += self.pe[:, :embedding.size(1)]   # pe will automatically be expanded with the same batch size as encoded_words\n","        # embedding: (batch_size, max_len, d_model), te: (batch_size, 1, d_model)\n","        embedding += self.te[:, layer_idx, :].unsqueeze(1).repeat(1, embedding.size(1), 1)\n","        embedding = self.dropout(embedding)\n","        print()\n","        print(embedding.size())\n","        print(embedding)\n","        return embedding"]},{"cell_type":"markdown","metadata":{},"source":["## Multi-Head Attention"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:33.423711Z","iopub.status.busy":"2024-07-02T20:17:33.423056Z","iopub.status.idle":"2024-07-02T20:17:33.435766Z","shell.execute_reply":"2024-07-02T20:17:33.435028Z","shell.execute_reply.started":"2024-07-02T20:17:33.423675Z"},"trusted":true},"outputs":[],"source":["class MultiHeadAttention(nn.Module):\n","    \n","    def __init__(self, heads, d_model):\n","        super(MultiHeadAttention, self).__init__()\n","        assert d_model % heads == 0\n","        self.d_k = d_model // heads\n","        self.heads = heads\n","        self.dropout = nn.Dropout(0.1)\n","        self.query = nn.Linear(d_model, d_model)\n","        self.key = nn.Linear(d_model, d_model)\n","        self.value = nn.Linear(d_model, d_model)\n","        self.concat = nn.Linear(d_model, d_model)\n","        \n","    def forward(self, query, key, value, mask):\n","        \"\"\"\n","        query, key, value of shape: (batch_size, max_len, 512)\n","        mask of shape: (batch_size, 1, 1, max_words)\n","        \"\"\"\n","        # (batch_size, max_len, 512)\n","        query = self.query(query)\n","        key = self.key(key)        \n","        value = self.value(value)   \n","        \n","        # (batch_size, max_len, 512) --> (batch_size, max_len, h, d_k) --> (batch_size, h, max_len, d_k)\n","        query = query.view(query.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)   \n","        key = key.view(key.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)  \n","        value = value.view(value.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)  \n","        \n","        # (batch_size, h, max_len, d_k) matmul (batch_size, h, d_k, max_len) --> (batch_size, h, max_len, max_len)\n","        scores = torch.matmul(query, key.permute(0,1,3,2)) / math.sqrt(query.size(-1))\n","        scores = scores.masked_fill(mask == 0, -1e9)    # (batch_size, h, max_len, max_len)\n","        weights = F.softmax(scores, dim = -1)           # (batch_size, h, max_len, max_len)\n","        weights = self.dropout(weights)\n","        # (batch_size, h, max_len, max_len) matmul (batch_size, h, max_len, d_k) --> (batch_size, h, max_len, d_k)\n","        context = torch.matmul(weights, value)\n","        # (batch_size, h, max_len, d_k) --> (batch_size, max_len, h, d_k) --> (batch_size, max_len, h * d_k)\n","        context = context.permute(0,2,1,3).contiguous().view(context.shape[0], -1, self.heads * self.d_k)\n","        # (batch_size, max_len, h * d_k)\n","        interacted = self.concat(context)\n","        return interacted "]},{"cell_type":"markdown","metadata":{},"source":["## Feed Forward Neural Network"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:33.437331Z","iopub.status.busy":"2024-07-02T20:17:33.437039Z","iopub.status.idle":"2024-07-02T20:17:33.450588Z","shell.execute_reply":"2024-07-02T20:17:33.449684Z","shell.execute_reply.started":"2024-07-02T20:17:33.437306Z"},"trusted":true},"outputs":[],"source":["class FeedForward(nn.Module):\n","    def __init__(self, d_model, middle_dim = 2048):\n","        super(FeedForward, self).__init__()\n","        \n","        self.fc1 = nn.Linear(d_model, middle_dim)\n","        self.fc2 = nn.Linear(middle_dim, d_model)\n","        self.dropout = nn.Dropout(0.1)\n","\n","    def forward(self, x):\n","        out = F.relu(self.fc1(x))\n","        out = self.fc2(self.dropout(out))\n","        return out\n","\n","class FeedForwardLSTM(nn.Module):\n","    def __init__(self, d_model, middle_dim = 2048):\n","        super(FeedForwardLSTM, self).__init__()\n","        \n","        self.fc1 = nn.Linear(d_model, middle_dim)\n","        self.lstm = nn.LSTM(middle_dim, middle_dim)\n","        self.fc2 = nn.Linear(middle_dim, d_model)\n","        self.dropout = nn.Dropout(0.1)\n","\n","    def forward(self, x):\n","        out = F.relu(self.fc1(x))\n","        out, _ = self.lstm(out)\n","        out = self.fc2(self.dropout(out))\n","        return out\n","\n","class FeedForwardLayerNoReg(nn.Module):\n","    def __init__(self, d_model, middle_dim = 2048):\n","        super(FeedForward, self).__init__()\n","        \n","        self.fc1 = nn.Linear(d_model, middle_dim)\n","        self.fc2 = nn.Linear(middle_dim, d_model)\n","\n","    def forward(self, x):\n","        out = F.relu(self.fc1(x))\n","        out = self.fc2(out)\n","        return out"]},{"cell_type":"markdown","metadata":{},"source":["## Encoder"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:33.452046Z","iopub.status.busy":"2024-07-02T20:17:33.451709Z","iopub.status.idle":"2024-07-02T20:17:33.461741Z","shell.execute_reply":"2024-07-02T20:17:33.460847Z","shell.execute_reply.started":"2024-07-02T20:17:33.452019Z"},"trusted":true},"outputs":[],"source":["class EncoderLayer(nn.Module):\n","    def __init__(self, d_model, heads):\n","        super(EncoderLayer, self).__init__()\n","        self.layernorm = nn.LayerNorm(d_model)\n","        self.self_multihead = MultiHeadAttention(heads, d_model)\n","        self.feed_forward = FeedForward(d_model)\n","        self.dropout = nn.Dropout(0.1)\n","\n","    def forward(self, embeddings, mask):\n","        interacted = self.dropout(self.self_multihead(embeddings, embeddings, embeddings, mask))\n","        interacted = self.layernorm(interacted + embeddings)\n","        feed_forward_out = self.dropout(self.feed_forward(interacted))\n","        encoded = self.layernorm(feed_forward_out + interacted)\n","        return encoded\n","\n","class EncoderLayerNoReg(nn.Module):\n","    def __init__(self, d_model, heads):\n","        super(EncoderLayerNoReg, self).__init__()\n","        self.layernorm = nn.LayerNorm(d_model)\n","        self.self_multihead = MultiHeadAttention(heads, d_model)\n","        self.feed_forward = FeedForward(d_model)\n","\n","    def forward(self, embeddings, mask):\n","        interacted = self.self_multihead(embeddings, embeddings, embeddings, mask)\n","        interacted = self.layernorm(interacted + embeddings)\n","        feed_forward_out = self.feed_forward(interacted)\n","        encoded = self.layernorm(feed_forward_out + interacted)\n","        return encoded\n","        \n","        "]},{"cell_type":"markdown","metadata":{},"source":["## Decoder"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:33.463365Z","iopub.status.busy":"2024-07-02T20:17:33.463025Z","iopub.status.idle":"2024-07-02T20:17:33.475859Z","shell.execute_reply":"2024-07-02T20:17:33.475008Z","shell.execute_reply.started":"2024-07-02T20:17:33.463339Z"},"trusted":true},"outputs":[],"source":["class DecoderLayer(nn.Module):\n","    \n","    def __init__(self, d_model, heads):\n","        super(DecoderLayer, self).__init__()\n","        self.layernorm = nn.LayerNorm(d_model)\n","        self.self_multihead = MultiHeadAttention(heads, d_model)\n","        self.src_multihead = MultiHeadAttention(heads, d_model)\n","        self.feed_forward = FeedForward(d_model)\n","        self.dropout = nn.Dropout(0.1)\n","        \n","    def forward(self, embeddings, encoded, src_mask, target_mask):\n","        query = self.dropout(self.self_multihead(embeddings, embeddings, embeddings, target_mask))\n","        query = self.layernorm(query + embeddings)\n","        interacted = self.dropout(self.src_multihead(query, encoded, encoded, src_mask))\n","        interacted = self.layernorm(interacted + query)\n","        feed_forward_out = self.dropout(self.feed_forward(interacted))\n","        decoded = self.layernorm(feed_forward_out + interacted)\n","        return decoded\n","\n","class DecoderLayerNoReg(nn.Module):\n","    def __init__(self, d_model, heads):\n","        super(DecoderLayerNoReg, self).__init__()\n","        self.layernorm = nn.LayerNorm(d_model)\n","        self.self_multihead = MultiHeadAttention(heads, d_model)\n","        self.src_multihead = MultiHeadAttention(heads, d_model)\n","        self.feed_forward = FeedForward(d_model)\n","        \n","    def forward(self, embeddings, encoded, src_mask, target_mask):\n","        query = self.self_multihead(embeddings, embeddings, embeddings, target_mask)\n","        query = self.layernorm(query + embeddings)\n","        interacted = self.src_multihead(query, encoded, encoded, src_mask)\n","        interacted = self.layernorm(interacted + query)\n","        feed_forward_out = self.feed_forward(interacted)\n","        decoded = self.layernorm(feed_forward_out + interacted)\n","        return decoded"]},{"cell_type":"markdown","metadata":{},"source":["## Transformer Architecture"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:33.478079Z","iopub.status.busy":"2024-07-02T20:17:33.477782Z","iopub.status.idle":"2024-07-02T20:17:33.514932Z","shell.execute_reply":"2024-07-02T20:17:33.514023Z","shell.execute_reply.started":"2024-07-02T20:17:33.478055Z"},"trusted":true},"outputs":[],"source":["class Transformer(nn.Module):\n","    \n","    def __init__(self, d_model, heads, num_layers, word_map, max_len = 50):\n","        super(Transformer, self).__init__()\n","        \n","        self.d_model = d_model\n","        self.num_layers = num_layers\n","        self.vocab_size = len(word_map)\n","        self.embed = Embeddings(self.vocab_size, d_model, num_layers = num_layers, max_len = max_len)\n","        self.encoder = EncoderLayer(d_model, heads) \n","        self.decoder = DecoderLayer(d_model, heads)\n","        self.logit = nn.Linear(d_model, self.vocab_size)\n","        \n","    def encode(self, src_embeddings, src_mask):\n","        for i in range(self.num_layers):\n","            src_embeddings = self.embed(src_embeddings, i)\n","            src_embeddings = self.encoder(src_embeddings, src_mask)\n","        return src_embeddings\n","    \n","    def decode(self, tgt_embeddings, target_mask, src_embeddings, src_mask):\n","        for i in range(self.num_layers):\n","            tgt_embeddings = self.embed(tgt_embeddings, i)\n","            tgt_embeddings = self.decoder(tgt_embeddings, src_embeddings, src_mask, target_mask)\n","        return tgt_embeddings\n","        \n","    def forward(self, src_words, src_mask, target_words, target_mask):\n","        encoded = self.encode(src_words, src_mask)\n","        decoded = self.decode(target_words, target_mask, encoded, src_mask)\n","        out = F.log_softmax(self.logit(decoded), dim = 2)\n","        return out\n","\n","class TransformerNoReg(nn.Module):\n","    \n","    def __init__(self, d_model, heads, num_layers, word_map, max_len = 50):\n","        super(TransformerNoReg, self).__init__()\n","        \n","        self.d_model = d_model\n","        self.num_layers = num_layers\n","        self.vocab_size = len(word_map)\n","        self.embed = Embeddings(self.vocab_size, d_model, num_layers = num_layers, max_len = max_len)\n","        self.encoder = EncoderLayerNoReg(d_model, heads) \n","        self.decoder = DecoderLayerNoReg(d_model, heads)\n","        self.logit = nn.Linear(d_model, self.vocab_size)\n","        \n","    def encode(self, src_embeddings, src_mask):\n","        for i in range(self.num_layers):\n","            src_embeddings = self.embed(src_embeddings, i)\n","            src_embeddings = self.encoder(src_embeddings, src_mask)\n","        return src_embeddings\n","    \n","    def decode(self, tgt_embeddings, target_mask, src_embeddings, src_mask):\n","        for i in range(self.num_layers):\n","            tgt_embeddings = self.embed(tgt_embeddings, i)\n","            tgt_embeddings = self.decoder(tgt_embeddings, src_embeddings, src_mask, target_mask)\n","        return tgt_embeddings\n","        \n","    def forward(self, src_words, src_mask, target_words, target_mask):\n","        encoded = self.encode(src_words, src_mask)\n","        decoded = self.decode(target_words, target_mask, encoded, src_mask)\n","        out = F.log_softmax(self.logit(decoded), dim = 2)\n","        return out\n","\n","class TransformerLSTM(nn.Module):\n","    \n","    def __init__(self, d_model, heads, num_layers, word_map, max_len = 50):\n","        super(TransformerLSTM, self).__init__()\n","        \n","        self.d_model = d_model\n","        self.num_layers = num_layers\n","        self.vocab_size = len(word_map)\n","        self.embed = EmbeddingsLSTM(self.vocab_size, d_model, num_layers = num_layers)\n","        self.encoder = EncoderLayer(d_model, heads) \n","        self.decoder = DecoderLayer(d_model, heads)\n","        self.logit = nn.Linear(d_model, self.vocab_size)\n","        self.max_len = max_len\n","        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        \n","    def encode(self, src_embeddings, src_mask):\n","        encoder_hidden = torch.zeros([self.num_layers, self.max_len, self.d_model]).to(self.device) # 1 = number of LSTM layers\n","        cell_state = torch.zeros([self.num_layers, self.max_len, self.d_model]).to(self.device)\n","        for i in range(self.num_layers):\n","            src_embeddings, encoder_hidden, cell_state = self.embed(src_embeddings, i, encoder_hidden, cell_state)\n","            src_embeddings = self.encoder(src_embeddings, src_mask)\n","        return src_embeddings, encoder_hidden, cell_state\n","    \n","    def decode(self, tgt_embeddings, target_mask, src_embeddings, src_mask, encoder_hidden, cell_state):\n","        zeros = torch.zeros((self.num_layers, 1, self.d_model), device=encoder_hidden.device)\n","        encoder_hidden = torch.cat((encoder_hidden, zeros), dim=1)\n","        decoder_input = torch.Tensor([[0]]).long().to(self.device) # 0 = SOS_token\n","        decoder_hidden = encoder_hidden\n","        print(decoder_hidden.size())\n","        for i in range(self.num_layers):\n","            tgt_embeddings, decoder_hidden, cell_state = self.embed(tgt_embeddings, i, decoder_hidden, cell_state)\n","            tgt_embeddings = self.decoder(tgt_embeddings, src_embeddings, src_mask, target_mask)\n","        return tgt_embeddings\n","        \n","    def forward(self, src_words, src_mask, target_words, target_mask):\n","        encoded, encoder_hidden, cell_state = self.encode(src_words, src_mask)\n","        decoded = self.decode(target_words, target_mask, encoded, src_mask, encoder_hidden, cell_state)\n","        out = F.log_softmax(self.logit(decoded), dim = 2)\n","        return out\n","\n","class TransformerPreTrainedEmbedding(nn.Module):\n","    def __init__(self, d_model, heads, num_layers, word_map, max_len = 50):\n","        super(TransformerPreTrainedEmbedding, self).__init__()\n","        \n","        self.d_model = d_model\n","        self.num_layers = num_layers\n","        self.vocab_size = len(word_map)\n","        self.embed = PretrainedEmbedding(self.vocab_size, d_model, num_layers = num_layers, max_len = max_len)\n","        self.encoder = EncoderLayer(d_model, heads) \n","        self.decoder = DecoderLayer(d_model, heads)\n","        self.logit = nn.Linear(d_model, self.vocab_size)\n","        \n","    def encode(self, src_embeddings, src_mask):\n","        for i in range(self.num_layers):\n","            src_embeddings = self.embed(src_embeddings, i)\n","            src_embeddings = self.encoder(src_embeddings, src_mask)\n","        return src_embeddings\n","    \n","    def decode(self, tgt_embeddings, target_mask, src_embeddings, src_mask):\n","        for i in range(self.num_layers):\n","            tgt_embeddings = self.embed(tgt_embeddings, i)\n","            tgt_embeddings = self.decoder(tgt_embeddings, src_embeddings, src_mask, target_mask)\n","        return tgt_embeddings\n","        \n","    def forward(self, src_words, src_mask, target_words, target_mask):\n","        encoded = self.encode(src_words, src_mask)\n","        decoded = self.decode(target_words, target_mask, encoded, src_mask)\n","        out = F.log_softmax(self.logit(decoded), dim = 2)\n","        return out\n","\n","class TransformerDecoderOnly(nn.Module):    \n","    def __init__(self, d_model, heads, num_layers, word_map, max_len = 50):\n","        super(TransformerDecoderOnly, self).__init__()\n","        \n","        self.d_model = d_model\n","        self.num_layers = num_layers\n","        self.vocab_size = len(word_map)\n","        self.embed = Embeddings(self.vocab_size, d_model, num_layers = num_layers, max_len = max_len)\n","        self.encoder = EncoderLayer(d_model, heads) \n","        self.decoder = DecoderLayer(d_model, heads)\n","        self.logit = nn.Linear(d_model, self.vocab_size)\n","    \n","    def decode(self, tgt_embeddings, target_mask, src_embeddings, src_mask):\n","        for i in range(self.num_layers):\n","            tgt_embeddings = self.embed(tgt_embeddings, i)\n","            tgt_embeddings = self.decoder(tgt_embeddings, src_embeddings, src_mask, target_mask)\n","        return tgt_embeddings\n","        \n","    def forward(self, src_words, src_mask, target_words, target_mask):\n","        encoded = self.embed(src_words, 0)\n","        decoded = self.decode(target_words, target_mask, encoded, src_mask)\n","        out = F.log_softmax(self.logit(decoded), dim = 2)\n","        return out"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:33.516425Z","iopub.status.busy":"2024-07-02T20:17:33.516094Z","iopub.status.idle":"2024-07-02T20:17:33.527522Z","shell.execute_reply":"2024-07-02T20:17:33.526734Z","shell.execute_reply.started":"2024-07-02T20:17:33.516394Z"},"trusted":true},"outputs":[],"source":["class AdamWarmup:\n","    \n","    def __init__(self, model_size, warmup_steps, optimizer):\n","        \n","        self.model_size = model_size\n","        self.warmup_steps = warmup_steps\n","        self.optimizer = optimizer\n","        self.current_step = 0\n","        self.lr = 0\n","        \n","    def get_lr(self):\n","        return self.model_size ** (-0.5) * min(self.current_step ** (-0.5), self.current_step * self.warmup_steps ** (-1.5))\n","        \n","    def step(self):\n","        # Increment the number of steps each time we call the step function\n","        self.current_step += 1\n","        lr = self.get_lr()\n","        for param_group in self.optimizer.param_groups:\n","            param_group['lr'] = lr\n","        # update the learning rate\n","        self.lr = lr\n","        self.optimizer.step()       \n","        "]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:33.529363Z","iopub.status.busy":"2024-07-02T20:17:33.528622Z","iopub.status.idle":"2024-07-02T20:17:33.540886Z","shell.execute_reply":"2024-07-02T20:17:33.540153Z","shell.execute_reply.started":"2024-07-02T20:17:33.529331Z"},"trusted":true},"outputs":[],"source":["class LossWithLS(nn.Module):\n","\n","    def __init__(self, size, smooth):\n","        super(LossWithLS, self).__init__()\n","        self.criterion = nn.KLDivLoss(size_average=False, reduce=False)\n","        self.confidence = 1.0 - smooth\n","        self.smooth = smooth\n","        self.size = size\n","        \n","    def forward(self, prediction, target, mask):\n","        \"\"\"\n","        prediction of shape: (batch_size, max_words, vocab_size)\n","        target and mask of shape: (batch_size, max_words)\n","        \"\"\"\n","        prediction = prediction.view(-1, prediction.size(-1))   # (batch_size * max_words, vocab_size)\n","        target = target.contiguous().view(-1)   # (batch_size * max_words)\n","        mask = mask.float()\n","        mask = mask.view(-1)       # (batch_size * max_words)\n","        labels = prediction.data.clone()\n","        labels.fill_(self.smooth / (self.size - 1))\n","        labels.scatter_(1, target.data.unsqueeze(1), self.confidence)\n","        loss = self.criterion(prediction, labels)    # (batch_size * max_words, vocab_size)\n","        loss = (loss.sum(1) * mask).sum() / mask.sum()\n","        return loss\n","        "]},{"cell_type":"markdown","metadata":{},"source":["# Define Neptune Experiment"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:33.542496Z","iopub.status.busy":"2024-07-02T20:17:33.542228Z","iopub.status.idle":"2024-07-02T20:17:33.550662Z","shell.execute_reply":"2024-07-02T20:17:33.549840Z","shell.execute_reply.started":"2024-07-02T20:17:33.542473Z"},"trusted":true},"outputs":[],"source":["project = \"andialifs/fluent-tesis-24\"\n","api_token = \"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJjZTY2YWQ3My04OTBkLTQ2OWUtYTc1Ni1jYjk0MGZhMWFiNGEifQ==\"\n","\n","def neptune_init(name):\n","    run = neptune.init_run(\n","        project=project,\n","        api_token=api_token,\n","        name=name\n","    )\n","    return run"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:33.555852Z","iopub.status.busy":"2024-07-02T20:17:33.555608Z","iopub.status.idle":"2024-07-02T20:17:33.563546Z","shell.execute_reply":"2024-07-02T20:17:33.562660Z","shell.execute_reply.started":"2024-07-02T20:17:33.555831Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'max_split_size_mb:8000'"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["import os\n","\n","os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:8000\"\n","os.getenv(\"PYTORCH_CUDA_ALLOC_CONF\")"]},{"cell_type":"markdown","metadata":{},"source":["# Function"]},{"cell_type":"markdown","metadata":{},"source":["## Train"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:33.564996Z","iopub.status.busy":"2024-07-02T20:17:33.564656Z","iopub.status.idle":"2024-07-02T20:17:33.574590Z","shell.execute_reply":"2024-07-02T20:17:33.573653Z","shell.execute_reply.started":"2024-07-02T20:17:33.564947Z"},"trusted":true},"outputs":[],"source":["def train(train_loader, transformer, criterion, epoch):\n","    transformer.train()\n","    sum_loss = 0\n","    count = 0\n","\n","    for i, (question, reply) in enumerate(train_loader):\n","        \n","        samples = question.shape[0]\n","\n","        # Move to device\n","        question = question.to(device)\n","        reply = reply.to(device)\n","\n","        # Prepare Target Data\n","        reply_input = reply[:, :-1]\n","        reply_target = reply[:, 1:]\n","\n","        # Create mask and add dimensions\n","        question_mask, reply_input_mask, reply_target_mask = create_masks(question, reply_input, reply_target)\n","\n","        # Get the transformer outputs\n","        out = transformer(question, question_mask, reply_input, reply_input_mask)\n","\n","        # Compute the loss\n","        loss = criterion(out, reply_target, reply_target_mask)\n","        \n","        # Backprop\n","        transformer_optimizer.optimizer.zero_grad()\n","        loss.backward()\n","        transformer_optimizer.step()\n","        \n","        sum_loss += loss.item() * samples\n","        count += samples\n","        \n","        if i % 100 == 0:\n","            print(\"Epoch [{}][{}/{}]\\tLoss: {:.3f}\".format(epoch, i, len(train_loader), sum_loss/count))\n","    \n","    return sum_loss/count"]},{"cell_type":"markdown","metadata":{},"source":["## Evaluate"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:33.576127Z","iopub.status.busy":"2024-07-02T20:17:33.575799Z","iopub.status.idle":"2024-07-02T20:17:33.587390Z","shell.execute_reply":"2024-07-02T20:17:33.586545Z","shell.execute_reply.started":"2024-07-02T20:17:33.576103Z"},"trusted":true},"outputs":[],"source":["def evaluate(transformer, question, question_mask, max_len, word_map):\n","    \"\"\"\n","    Performs Greedy Decoding with a batch size of 1\n","    \"\"\"\n","    rev_word_map = {v: k for k, v in word_map.items()}\n","    transformer.eval()\n","    start_token = word_map['<start>']\n","    encoded = transformer.encode(question, question_mask)\n","    words = torch.LongTensor([[start_token]]).to(device)\n","    \n","    for step in range(max_len - 1):\n","        size = words.shape[1]\n","        target_mask = torch.triu(torch.ones(size, size)).transpose(0, 1).type(dtype=torch.uint8)\n","        target_mask = target_mask.to(device).unsqueeze(0).unsqueeze(0)\n","        decoded = transformer.decode(words, target_mask, encoded, question_mask)\n","        predictions = transformer.logit(decoded[:, -1])\n","        _, next_word = torch.max(predictions, dim = 1)\n","        next_word = next_word.item()\n","        if next_word == word_map['<end>']:\n","            break\n","        words = torch.cat([words, torch.LongTensor([[next_word]]).to(device)], dim = 1)   # (1,step+2)\n","        \n","    # Construct Sentence\n","    if words.dim() == 2:\n","        words = words.squeeze(0)\n","        words = words.tolist()\n","        \n","    sen_idx = [w for w in words if w not in {word_map['<start>']}]\n","    sentence = ' '.join([rev_word_map[sen_idx[k]] for k in range(len(sen_idx))])\n","    \n","    return sentence"]},{"cell_type":"markdown","metadata":{},"source":["# Experiment"]},{"cell_type":"markdown","metadata":{},"source":["## Transformers without reg"]},{"cell_type":"code","execution_count":38,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:33.588788Z","iopub.status.busy":"2024-07-02T20:17:33.588528Z","iopub.status.idle":"2024-07-02T20:17:47.395240Z","shell.execute_reply":"2024-07-02T20:17:47.393722Z","shell.execute_reply.started":"2024-07-02T20:17:33.588766Z"},"scrolled":true,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\\Skipping experiment 0 with d_model 512, heads8, num_layers5\n","\n","\\Skipping experiment 1 with d_model 512, heads8, num_layers10\n","\n","\\Skipping experiment 2 with d_model 512, heads16, num_layers5\n","\n","\\Skipping experiment 3 with d_model 512, heads16, num_layers10\n","\n","\\Skipping experiment 4 with d_model 512, heads32, num_layers5\n","\n","\\Skipping experiment 5 with d_model 512, heads32, num_layers10\n","\n","\\Skipping experiment 6 with d_model 1024, heads8, num_layers5\n","\n","\\Skipping experiment 7 with d_model 1024, heads8, num_layers10\n","\n","\\Skipping experiment 8 with d_model 1024, heads16, num_layers5\n","\n","\\Skipping experiment 9 with d_model 1024, heads16, num_layers10\n","\n","\\Skipping experiment 10 with d_model 1024, heads32, num_layers5\n","\n","\\Skipping experiment 11 with d_model 1024, heads32, num_layers10\n","\n","\\Skipping experiment 12 with d_model 2048, heads8, num_layers5\n","\n","\n","Running for experiment 13 with d_model 2048, heads8, num_layers10\n","\n","[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/andialifs/fluent-tesis-24/e/FLUENT24-72\n"]},{"name":"stderr","output_type":"stream","text":["/home/andyalyfsyah/.conda/envs/myenv/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [0][0/12]\tLoss: 5.316\n","Epoch [1][0/12]\tLoss: 5.172\n","Epoch [2][0/12]\tLoss: 4.794\n","Epoch [3][0/12]\tLoss: 4.388\n","Epoch [4][0/12]\tLoss: 4.277\n","Epoch [5][0/12]\tLoss: 4.096\n","Epoch [6][0/12]\tLoss: 4.054\n","Epoch [7][0/12]\tLoss: 4.016\n","Epoch [8][0/12]\tLoss: 4.039\n","Epoch [9][0/12]\tLoss: 3.996\n","Epoch [10][0/12]\tLoss: 3.987\n","Epoch [11][0/12]\tLoss: 3.953\n","Epoch [12][0/12]\tLoss: 3.956\n","Epoch [13][0/12]\tLoss: 3.985\n","Epoch [14][0/12]\tLoss: 3.910\n","Epoch [15][0/12]\tLoss: 3.954\n","Epoch [16][0/12]\tLoss: 3.935\n","Epoch [17][0/12]\tLoss: 3.714\n","Epoch [18][0/12]\tLoss: 3.813\n","Epoch [19][0/12]\tLoss: 3.785\n","Epoch [20][0/12]\tLoss: 3.831\n","Epoch [21][0/12]\tLoss: 3.673\n","Epoch [22][0/12]\tLoss: 3.745\n","Epoch [23][0/12]\tLoss: 3.641\n","Epoch [24][0/12]\tLoss: 3.572\n","Epoch [25][0/12]\tLoss: 3.588\n","Epoch [26][0/12]\tLoss: 3.553\n","Epoch [27][0/12]\tLoss: 3.565\n","Epoch [28][0/12]\tLoss: 3.427\n","Epoch [29][0/12]\tLoss: 3.531\n","Epoch [30][0/12]\tLoss: 3.219\n","Epoch [31][0/12]\tLoss: 3.407\n","Epoch [32][0/12]\tLoss: 3.051\n","Epoch [33][0/12]\tLoss: 3.375\n","Epoch [34][0/12]\tLoss: 3.234\n","Epoch [35][0/12]\tLoss: 3.081\n","Epoch [36][0/12]\tLoss: 2.986\n","Epoch [37][0/12]\tLoss: 2.987\n","Epoch [38][0/12]\tLoss: 2.787\n","Epoch [39][0/12]\tLoss: 3.200\n","Epoch [40][0/12]\tLoss: 3.130\n","Epoch [41][0/12]\tLoss: 3.092\n","Epoch [42][0/12]\tLoss: 2.891\n","Epoch [43][0/12]\tLoss: 2.947\n","Epoch [44][0/12]\tLoss: 2.989\n","Epoch [45][0/12]\tLoss: 2.848\n","Epoch [46][0/12]\tLoss: 2.886\n","Epoch [47][0/12]\tLoss: 2.696\n","Epoch [48][0/12]\tLoss: 2.990\n","Epoch [49][0/12]\tLoss: 2.601\n"]}],"source":["d_model = [512, 1024, 2048, 4096]\n","heads = [8, 16, 32]\n","num_layers = [5, 10]\n","epochs = 100\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","with open('WORDMAP_corpus_KBFILKOM.json', 'r') as j:\n","    word_map = json.load(j)\n","\n","transformer_experiment = pd.DataFrame(columns = ['experiment_id', 'd_model', 'heads', 'num_layers', 'train_loss'])\n","loss_history = {}\n","\n","experiment_id = -1\n","\n","torch.cuda.empty_cache()\n","\n","for d_m in d_model:\n","    for h in heads:\n","        for n_l in num_layers: \n","            parameters = {\n","                'd_model': d_m,\n","                'heads': h,\n","                'num_layers': n_l\n","            }\n","            \n","            experiment_id += 1\n","            \n","            if experiment_id < 13:\n","                print('\\Skipping experiment {} with d_model {}, heads{}, num_layers{}\\n'.format(experiment_id, d_m, h, n_l))\n","                continue\n","            \n","            print('\\nRunning for experiment {} with d_model {}, heads{}, num_layers{}\\n'.format(experiment_id, d_m, h, n_l))\n","            name = \"experiment_2724_noreg_\" + str(experiment_id)\n","\n","            run = neptune_init(name)\n","            run['parameters'] = parameters\n","            run['group_tags'] = \"transformers-vanilla-no-reg\"\n","            \n","            transformer_experiment.loc[experiment_id, 'experiment_id'] = 'experiment_{}'.format(str(experiment_id))\n","            transformer_experiment.loc[experiment_id, 'd_model'] = d_m\n","            transformer_experiment.loc[experiment_id, 'heads'] = h\n","            transformer_experiment.loc[experiment_id, 'num_layers'] = n_l\n","\n","            transformer = Transformer(d_model = d_m, heads = h, num_layers = n_l, word_map = word_map, max_len=95)\n","            transformer = transformer.to(device)\n","            adam_optimizer = torch.optim.Adam(transformer.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-9)\n","            transformer_optimizer = AdamWarmup(model_size = d_m, warmup_steps = 4000, optimizer = adam_optimizer)\n","            criterion = LossWithLS(len(word_map), 0.2)\n","\n","            loss_list_experiment = []\n","            for epoch in range(epochs):\n","                loss_train = train(train_loader, transformer, criterion, epoch)\n","\n","                state = {'epoch': epoch, 'transformer': transformer, 'transformer_optimizer': transformer_optimizer}\n","                # torch.save(state, 'checkpoint_experiment_' + str(epoch) + 'id_'+ str(experiment_id) +'.pth.tar')\n","\n","                loss_list_experiment.append(loss_train)\n","\n","                run['train/loss'].append(loss_train)\n","\n","            transformer_experiment.loc[experiment_id, 'train_loss'] = loss_train\n","            loss_history['experiment_{}'.format(str(experiment_id))] = loss_list_experiment\n","            \n","            # torch.cuda.empty_cache() \n","            run.stop()\n"]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:16:32.298394Z","iopub.status.busy":"2024-07-02T20:16:32.297969Z","iopub.status.idle":"2024-07-02T20:16:32.305442Z","shell.execute_reply":"2024-07-02T20:16:32.304012Z","shell.execute_reply.started":"2024-07-02T20:16:32.298351Z"},"trusted":true},"outputs":[],"source":["torch.cuda.empty_cache() "]},{"cell_type":"markdown","metadata":{},"source":["## Transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"trusted":true},"outputs":[],"source":["d_model = [512, 1024, 2048, 4096]\n","heads = [8, 16, 32]\n","num_layers = [5, 10]\n","epochs = 100\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","with open('WORDMAP_corpus_KBFILKOM.json', 'r') as j:\n","    word_map = json.load(j)\n","\n","transformer_experiment = pd.DataFrame(columns = ['experiment_id', 'd_model', 'heads', 'num_layers', 'train_loss'])\n","loss_history = {}\n","\n","experiment_id = -1\n","\n","for d_m in d_model:\n","    for h in heads:\n","        for n_l in num_layers: \n","            experiment_id += 1\n","            print('\\nRunning for experiment {} with d_model {}, heads{}, num_layers{}\\n'.format(experiment_id, d_m, h, n_l))\n","\n","            run = neptune.init_run(\n","                project=project,\n","                api_token=api_token,\n","                name=\"experiment_1724_\" + str(experiment_id)\n","            ) \n","            run['parameters'] = {\n","                'd_model': d_m,\n","                'heads': h,\n","                'num_layers': n_l\n","            }\n","\n","            transformer_experiment.loc[experiment_id, 'experiment_id'] = 'experiment_{}'.format(str(experiment_id))\n","            transformer_experiment.loc[experiment_id, 'd_model'] = d_m\n","            transformer_experiment.loc[experiment_id, 'heads'] = h\n","            transformer_experiment.loc[experiment_id, 'num_layers'] = n_l\n","\n","            transformer = Transformer(d_model = d_m, heads = h, num_layers = n_l, word_map = word_map, max_len=95)\n","            transformer = transformer.to(device)\n","            adam_optimizer = torch.optim.Adam(transformer.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-9)\n","            transformer_optimizer = AdamWarmup(model_size = d_m, warmup_steps = 4000, optimizer = adam_optimizer)\n","            criterion = LossWithLS(len(word_map), 0.2)\n","\n","            loss_list_experiment = []\n","            for epoch in range(epochs):\n","                loss_train = train(train_loader, transformer, criterion, epoch)\n","\n","                state = {'epoch': epoch, 'transformer': transformer, 'transformer_optimizer': transformer_optimizer}\n","                torch.save(state, 'checkpoint_experiment_' + str(epoch) + 'id_'+ str(experiment_id) +'.pth.tar')\n","\n","                loss_list_experiment.append(loss_train)\n","\n","                run['train/loss'].append(loss_train)\n","\n","            transformer_experiment.loc[experiment_id, 'train_loss'] = loss_train\n","            loss_history['experiment_{}'.format(str(experiment_id))] = loss_list_experiment\n","            \n","            run.stop()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import yaml\n","\n","with open('loss_history_transformer_experiment_KBFILKOM.yaml', 'w') as file:\n","    documents = yaml.dump(loss_history, file)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["transformer_experiment.dropna(inplace=True)\n","transformer_experiment"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import yaml\n","\n","with open('history_rnn_150524.yaml', 'r') as file:\n","    history_rnn = yaml.load(file, Loader=yaml.FullLoader)"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true},"outputs":[],"source":["import matplotlib \n","import matplotlib.pyplot as plt\n","\n","loss_history_key = list(loss_history.keys())\n","\n","plt.figure(figsize=(15,10))\n","plt.title(\"Training loss vs. Number of Epochs\")\n","plt.xlabel(\"Number of Epochs\")\n","plt.ylabel(\"Training Loss\")\n","z\n","\n","for key in loss_history_key:\n","    loss_list = loss_history[key]\n","    labels = f'd_model: {transformer_experiment[transformer_experiment[\"experiment_id\"] == key][\"d_model\"].values[0]}, heads: {transformer_experiment[transformer_experiment[\"experiment_id\"] == key][\"heads\"].values[0]}, num_layers: {transformer_experiment[transformer_experiment[\"experiment_id\"] == key][\"num_layers\"].values[0]}'\n","    plt.plot(loss_list, label = labels)\n","\n","    \n","plt.plot(history_rnn['loss'], \n","                label = 'LSTM (Baseline FLUENT 2023)', \n","                linestyle='dashed', \n","                color='black', \n","                linewidth=2.5, \n","                alpha=0.7, \n","                marker='o', \n","                markerfacecolor='black', \n","                markersize=5\n","        )\n","\n","plt.legend()\n","torch.cuda.is_available()\n","plt.grid()"]},{"cell_type":"markdown","metadata":{},"source":["# Training"]},{"cell_type":"markdown","metadata":{},"source":["## Pretrained"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["directory = 'experiment_pretrained_1'\n","\n","d_model = 2048\n","heads = 16\n","num_layers = 5\n","epochs = 100\n","\n","loss_history_pratrained_embed_transformer = []\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","with open('WORDMAP_corpus_KBFILKOM.json', 'r') as j:\n","    word_map = json.load(j)\n","    \n","transformer = TransformerPreTrainedEmbedding(d_model = d_model, heads = heads, num_layers = num_layers, word_map = word_map, max_len=95)\n","transformer = transformer.to(device)\n","adam_optimizer = torch.optim.Adam(transformer.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-9)\n","transformer_optimizer = AdamWarmup(model_size = d_model, warmup_steps = 4000, optimizer = adam_optimizer)\n","criterion = LossWithLS(len(word_map), 0.2)\n","\n","for epoch in range(epochs):\n","    loss_train = train(train_loader, transformer, criterion, epoch)\n","\n","    state = {'epoch': epoch, 'transformer': transformer, 'transformer_optimizer': transformer_optimizer}\n","    torch.save(state, directory + '/checkpoint_' + str(epoch) +'.pth.tar')\n","\n","    loss_history_pratrained_embed_transformer.append(loss_train)"]},{"cell_type":"markdown","metadata":{},"source":["## Vanilla"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["directory = 'experiment_vanilla_30624'\n","create_directory(directory)\n","\n","d_model = 2048\n","heads = 16\n","num_layers = 5\n","epochs = 100\n","\n","loss_history_vanilla_transformer = []\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","with open('WORDMAP_corpus_KBFILKOM.json', 'r') as j:\n","    word_map = json.load(j)\n","    \n","transformer = Transformer(d_model = d_model, heads = heads, num_layers = num_layers, word_map = word_map, max_len=95)\n","transformer = transformer.to(device)\n","adam_optimizer = torch.optim.Adam(transformer.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-9)\n","transformer_optimizer = AdamWarmup(model_size = d_model, warmup_steps = 4000, optimizer = adam_optimizer)\n","criterion = LossWithLS(len(word_map), 0.2)\n","\n","for epoch in range(epochs):\n","    loss_train = train(train_loader, transformer, criterion, epoch)\n","\n","    state = {'epoch': epoch, 'transformer': transformer, 'transformer_optimizer': transformer_optimizer}\n","    torch.save(state, directory + '/checkpoint_' + str(epoch) +'.pth.tar')\n","\n","    loss_history_vanilla_transformer.append(loss_train)\n","\n","import yaml \n","\n","with open(directory + '/loss_history_vanilla_transformer.yaml', 'w') as file:\n","    yaml.dump(loss_history_vanilla_transformer, file)"]},{"cell_type":"markdown","metadata":{},"source":["## Vanilla without Regularization"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Directory already exists at transformers_vanillanoreg_01724\n"]},{"name":"stderr","output_type":"stream","text":["/home/andyalyfsyah/.conda/envs/myenv/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n"]},{"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 72.00 MiB (GPU 0; 79.35 GiB total capacity; 10.14 GiB already allocated; 29.19 MiB free; 10.40 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","Cell \u001b[0;32mIn[32], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m criterion \u001b[38;5;241m=\u001b[39m LossWithLS(\u001b[38;5;28mlen\u001b[39m(word_map), \u001b[38;5;241m0.2\u001b[39m)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(parameters[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[0;32m---> 29\u001b[0m     loss_train \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     state \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m: epoch, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer\u001b[39m\u001b[38;5;124m'\u001b[39m: transformer, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransformer_optimizer\u001b[39m\u001b[38;5;124m'\u001b[39m: transformer_optimizer}\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;66;03m# torch.save(state, directory + '/checkpoint_' + str(epoch) +'.pth.tar')\u001b[39;00m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m# run['model_checkpoint'].upload(directory + '/checkpoint_' + str(epoch) +'.pth.tar')\u001b[39;00m\n","Cell \u001b[0;32mIn[29], line 22\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(train_loader, transformer, criterion, epoch)\u001b[0m\n\u001b[1;32m     19\u001b[0m question_mask, reply_input_mask, reply_target_mask \u001b[38;5;241m=\u001b[39m create_masks(question, reply_input, reply_target)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Get the transformer outputs\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreply_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreply_input_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Compute the loss\u001b[39;00m\n\u001b[1;32m     25\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(out, reply_target, reply_target_mask)\n","File \u001b[0;32m~/.conda/envs/myenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","Cell \u001b[0;32mIn[24], line 58\u001b[0m, in \u001b[0;36mTransformerNoReg.forward\u001b[0;34m(self, src_words, src_mask, target_words, target_mask)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, src_words, src_mask, target_words, target_mask):\n\u001b[0;32m---> 58\u001b[0m     encoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc_words\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m     decoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode(target_words, target_mask, encoded, src_mask)\n\u001b[1;32m     60\u001b[0m     out \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mlog_softmax(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogit(decoded), dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m)\n","Cell \u001b[0;32mIn[24], line 48\u001b[0m, in \u001b[0;36mTransformerNoReg.encode\u001b[0;34m(self, src_embeddings, src_mask)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers):\n\u001b[1;32m     47\u001b[0m     src_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed(src_embeddings, i)\n\u001b[0;32m---> 48\u001b[0m     src_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m src_embeddings\n","File \u001b[0;32m~/.conda/envs/myenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","Cell \u001b[0;32mIn[22], line 24\u001b[0m, in \u001b[0;36mEncoderLayerNoReg.forward\u001b[0;34m(self, embeddings, mask)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, embeddings, mask):\n\u001b[0;32m---> 24\u001b[0m     interacted \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_multihead\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m     interacted \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayernorm(interacted \u001b[38;5;241m+\u001b[39m embeddings)\n\u001b[1;32m     26\u001b[0m     feed_forward_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeed_forward(interacted)\n","File \u001b[0;32m~/.conda/envs/myenv/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","Cell \u001b[0;32mIn[20], line 37\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[0;34m(self, query, key, value, mask)\u001b[0m\n\u001b[1;32m     35\u001b[0m context \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(weights, value)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# (batch_size, h, max_len, d_k) --> (batch_size, max_len, h, d_k) --> (batch_size, max_len, h * d_k)\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m context \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontiguous\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mview(context\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheads \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_k)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# (batch_size, max_len, h * d_k)\u001b[39;00m\n\u001b[1;32m     39\u001b[0m interacted \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconcat(context)\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 72.00 MiB (GPU 0; 79.35 GiB total capacity; 10.14 GiB already allocated; 29.19 MiB free; 10.40 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"]}],"source":["directory = 'transformers_vanillanoreg_01724'\n","create_directory(directory)\n","\n","# run = neptune_init(directory)\n","\n","parameters = {\n","    'd_model': 2048,\n","    'heads': 16,\n","    'num_layers': 15,\n","    'epochs': 100,\n","}\n","# run['parameters'] = parameters\n","\n","loss_history_vanillanoreg_transformer = []\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n","# device = \"cpu\"\n","\n","with open('WORDMAP_corpus_KBFILKOM.json', 'r') as j:\n","    word_map = json.load(j)\n","    \n","transformer = TransformerNoReg(d_model = parameters['d_model'], heads = parameters['heads'], num_layers = parameters['num_layers'], word_map = word_map, max_len=95)\n","transformer = transformer.to(device)\n","adam_optimizer = torch.optim.Adam(transformer.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-9)\n","transformer_optimizer = AdamWarmup(model_size = parameters['d_model'], warmup_steps = 4000, optimizer = adam_optimizer)\n","criterion = LossWithLS(len(word_map), 0.2)\n","\n","for epoch in range(parameters['epochs']):\n","    loss_train = train(train_loader, transformer, criterion, epoch)\n","\n","    state = {'epoch': epoch, 'transformer': transformer, 'transformer_optimizer': transformer_optimizer}\n","    # torch.save(state, directory + '/checkpoint_' + str(epoch) +'.pth.tar')\n","    # run['model_checkpoint'].upload(directory + '/checkpoint_' + str(epoch) +'.pth.tar')\n","\n","    loss_history_vanillanoreg_transformer.append(loss_train)\n","    # run['train/loss'].append(loss_train)\n","\n","with open(directory + '/loss_history_vanillanoreg_transformer.yaml', 'w') as file:\n","    yaml.dump(loss_history_vanilla_transformer, file)\n","\n","# run.stop()"]},{"cell_type":"markdown","metadata":{},"source":["## Decoder Only"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["directory = 'experiment_deconly_17624'\n","create_directory(directory)\n","\n","d_model = 2048\n","heads = 16\n","num_layers = 5\n","epochs = 100\n","\n","loss_history_decoder_transformer = []\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","with open('WORDMAP_corpus_KBFILKOM.json', 'r') as j:\n","    word_map = json.load(j)\n","    \n","transformer = TransformerDecoderOnly(d_model = d_model, heads = heads, num_layers = num_layers, word_map = word_map, max_len=95)\n","transformer = transformer.to(device)\n","adam_optimizer = torch.optim.Adam(transformer.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-9)\n","transformer_optimizer = AdamWarmup(model_size = d_model, warmup_steps = 4000, optimizer = adam_optimizer)\n","criterion = LossWithLS(len(word_map), 0.2)\n","\n","for epoch in range(epochs):\n","    loss_train = train(train_loader, transformer, criterion, epoch)\n","\n","    state = {'epoch': epoch, 'transformer': transformer, 'transformer_optimizer': transformer_optimizer}\n","    torch.save(state, directory + '/checkpoint_' + str(epoch) +'.pth.tar')\n","\n","    loss_history_decoder_transformer.append(loss_train)"]},{"cell_type":"markdown","metadata":{},"source":["## LSTM_Transformer"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["directory = 'experiment_2_lstm'\n","\n","d_model = 1024\n","heads = 32\n","num_layers = 10\n","epochs = 100\n","\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","loss_history_lstm_transformer = []\n","\n","with open('WORDMAP_corpus_KBFILKOM.json', 'r') as j:\n","    word_map = json.load(j)\n","    \n","transformer = TransformerLSTM(d_model = d_model, heads = heads, num_layers = num_layers, word_map = word_map, max_len=90)\n","transformer = transformer.to(device)\n","adam_optimizer = torch.optim.Adam(transformer.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-9)\n","transformer_optimizer = AdamWarmup(model_size = d_model, warmup_steps = 4000, optimizer = adam_optimizer)\n","criterion = LossWithLS(len(word_map), 0.2)\n","\n","\n","for epoch in range(epochs):\n","    loss_train = train(train_loader, transformer, criterion, epoch)\n","\n","    state = {'epoch': epoch, 'transformer': transformer, 'transformer_optimizer': transformer_optimizer}\n","    torch.save(state, directory + '/checkpoint_' + str(epoch) +'.pth.tar')\n","\n","    loss_history_lstm_transformer.append(loss_train)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import yaml\n","\n","with open(directory + '/loss_history_lstm_transformer.yaml', 'w') as file:\n","    yaml.dump(loss_history_lstm_transformer, file)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["directory = 'experiment_1'\n","checkpoint = torch.load(directory + '/checkpoint_99.pth.tar')\n","transformer = checkpoint['transformer']\n","\n","question = \"Visi FILKOM\" \n","max_len = 50\n","enc_qus = [word_map.get(word, word_map['<unk>']) for word in question.split()]\n","question = torch.LongTensor(enc_qus).to(device).unsqueeze(0)\n","question_mask = (question!=0).to(device).unsqueeze(1).unsqueeze(1)  \n","sentence = evaluate(transformer, question, question_mask, int(max_len), word_map)\n","print(sentence)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["checkpoint"]},{"cell_type":"markdown","metadata":{},"source":["# Reset Cache"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","\n","torch.cuda.empty_cache()\n","\n","print(torch.cuda.memory_summary(device=None, abbreviated=False))"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30733,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"My Environment","language":"python","name":"myenv"}},"nbformat":4,"nbformat_minor":4}
