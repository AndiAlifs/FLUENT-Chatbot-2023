{"cells":[{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:27.682419Z","iopub.status.busy":"2024-07-02T20:17:27.682115Z","iopub.status.idle":"2024-07-02T20:17:30.597078Z","shell.execute_reply":"2024-07-02T20:17:30.596113Z","shell.execute_reply.started":"2024-07-02T20:17:27.682391Z"},"trusted":true},"outputs":[],"source":["import json\n","import torch\n","import torch.nn as nn\n","import torch.utils.data\n","import math\n","import torch.nn.functional as F\n","import neptune\n","import pandas as pd\n","from collections import Counter\n","from torch.utils.data import Dataset"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:30.598900Z","iopub.status.busy":"2024-07-02T20:17:30.598458Z","iopub.status.idle":"2024-07-02T20:17:32.694354Z","shell.execute_reply":"2024-07-02T20:17:32.693406Z","shell.execute_reply.started":"2024-07-02T20:17:30.598873Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Pertanyaan</th>\n","      <th>Jawaban</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>visi FILKOM</td>\n","      <td>Menjadi Fakultas yang berdaya saing internasio...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>misi FILKOM</td>\n","      <td>Menyelenggarakan pendidikan di bidang Teknolog...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>apa tujuan filkom?</td>\n","      <td>Menghasilkan lulusan yang kompeten , profesion...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>sasaran pendidikan FILKOM</td>\n","      <td>1. Meningkatkan kompetensi dan kualifikasi pen...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                  Pertanyaan  \\\n","0                visi FILKOM   \n","1                misi FILKOM   \n","2         apa tujuan filkom?   \n","3  sasaran pendidikan FILKOM   \n","\n","                                             Jawaban  \n","0  Menjadi Fakultas yang berdaya saing internasio...  \n","1  Menyelenggarakan pendidikan di bidang Teknolog...  \n","2  Menghasilkan lulusan yang kompeten , profesion...  \n","3  1. Meningkatkan kompetensi dan kualifikasi pen...  "]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["# knowledgebase_url = 'https://github.com/AndiAlifs/FLUENT-Chatbot-2023/raw/main/KnowledgeBaseFilkom.xlsx'\n","knowledgebase_url = 'https://github.com/AndiAlifs/FLUENT-Chatbot-2023/raw/main/KnowledgeBaseFilkom_simple.xlsx'\n","knowledgebase = pd.read_excel(knowledgebase_url)\n","\n","qa_paired = knowledgebase.drop(columns=knowledgebase.columns.drop(['Pertanyaan', 'Jawaban']))\n","qa_paired.dropna(inplace=True)\n","qa_paired"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:32.697066Z","iopub.status.busy":"2024-07-02T20:17:32.696633Z","iopub.status.idle":"2024-07-02T20:17:32.702322Z","shell.execute_reply":"2024-07-02T20:17:32.701334Z","shell.execute_reply.started":"2024-07-02T20:17:32.697038Z"},"trusted":true},"outputs":[],"source":["def remove_punc(string):\n","    punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n","    no_punct = \"\"\n","    for char in string:\n","        if char not in punctuations:\n","            no_punct = no_punct + char  # space is also a character\n","    return no_punct.lower()"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:32.703938Z","iopub.status.busy":"2024-07-02T20:17:32.703658Z","iopub.status.idle":"2024-07-02T20:17:32.827025Z","shell.execute_reply":"2024-07-02T20:17:32.825935Z","shell.execute_reply.started":"2024-07-02T20:17:32.703915Z"},"trusted":true},"outputs":[],"source":["pairs = []\n","max_len = 90\n","\n","for line in qa_paired.iterrows():\n","    pertanyaan = line[1]['Pertanyaan']\n","    jawaban = line[1]['Jawaban']\n","    qa_pairs = []\n","    first = remove_punc(pertanyaan.strip())      \n","    second = remove_punc(jawaban.strip())\n","    qa_pairs.append(first.split()[:max_len])\n","    qa_pairs.append(second.split()[:max_len])\n","    pairs.append(qa_pairs)"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:32.828565Z","iopub.status.busy":"2024-07-02T20:17:32.828255Z","iopub.status.idle":"2024-07-02T20:17:32.838911Z","shell.execute_reply":"2024-07-02T20:17:32.837848Z","shell.execute_reply.started":"2024-07-02T20:17:32.828539Z"},"trusted":true},"outputs":[],"source":["word_freq = Counter()\n","for pair in pairs:\n","    word_freq.update(pair[0])\n","    word_freq.update(pair[1])"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:32.840576Z","iopub.status.busy":"2024-07-02T20:17:32.840244Z","iopub.status.idle":"2024-07-02T20:17:32.850265Z","shell.execute_reply":"2024-07-02T20:17:32.849352Z","shell.execute_reply.started":"2024-07-02T20:17:32.840548Z"},"trusted":true},"outputs":[],"source":["min_word_freq = 0\n","words = [w for w in word_freq.keys() if word_freq[w] > min_word_freq]\n","word_map = {k: v + 1 for v, k in enumerate(words)}\n","word_map['<unk>'] = len(word_map) + 1\n","word_map['<start>'] = len(word_map) + 1\n","word_map['<end>'] = len(word_map) + 1\n","word_map['<pad>'] = 0"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:32.851802Z","iopub.status.busy":"2024-07-02T20:17:32.851465Z","iopub.status.idle":"2024-07-02T20:17:32.860482Z","shell.execute_reply":"2024-07-02T20:17:32.859625Z","shell.execute_reply.started":"2024-07-02T20:17:32.851770Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Total words are: 116\n"]}],"source":["print(\"Total words are: {}\".format(len(word_map)))"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:32.861919Z","iopub.status.busy":"2024-07-02T20:17:32.861579Z","iopub.status.idle":"2024-07-02T20:17:32.872138Z","shell.execute_reply":"2024-07-02T20:17:32.871399Z","shell.execute_reply.started":"2024-07-02T20:17:32.861895Z"},"trusted":true},"outputs":[],"source":["with open('WORDMAP_corpus_KBFILKOM.json', 'w') as j:\n","    json.dump(word_map, j)"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:32.877074Z","iopub.status.busy":"2024-07-02T20:17:32.876749Z","iopub.status.idle":"2024-07-02T20:17:32.884325Z","shell.execute_reply":"2024-07-02T20:17:32.883430Z","shell.execute_reply.started":"2024-07-02T20:17:32.877048Z"},"trusted":true},"outputs":[],"source":["def encode_question(words, word_map):\n","    enc_c = [word_map.get(word, word_map['<unk>']) for word in words] + [word_map['<pad>']] * (max_len - len(words))\n","    return enc_c\n","\n","\n","def encode_question_left(words, word_map):\n","    enc_c = [word_map['<pad>']] * (max_len - len(words)) + [word_map.get(word, word_map['<unk>']) for word in words]\n","    return enc_c\n","\n","def encode_reply(words, word_map):\n","    enc_c = [word_map['<start>']] + [word_map.get(word, word_map['<unk>']) for word in words] + \\\n","    [word_map['<end>']] + [word_map['<pad>']] * (max_len - len(words))\n","    return enc_c\n","\n","def encode_reply_with_maxlen(words, word_map):\n","    enc_c = [word_map['<start>']] + [word_map.get(word, word_map['<unk>']) for word in words] + [word_map['<end>']] + [word_map['<pad>']] * (max_len-2 - len(words))\n","    return enc_c"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:32.885632Z","iopub.status.busy":"2024-07-02T20:17:32.885341Z","iopub.status.idle":"2024-07-02T20:17:33.099456Z","shell.execute_reply":"2024-07-02T20:17:33.098486Z","shell.execute_reply.started":"2024-07-02T20:17:32.885608Z"},"trusted":true},"outputs":[],"source":["pairs_encoded = []\n","for pair in pairs:\n","    qus = encode_question(pair[0], word_map)\n","    # qus = encode_question_left(pair[0], word_map)\n","    ans = encode_reply(pair[1], word_map)\n","    pairs_encoded.append([qus, ans])\n","\n","with open('pairs_encoded_kbfilkom.json', 'w') as p:\n","    json.dump(pairs_encoded, p)"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"data":{"text/plain":["90"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["len( pairs_encoded[1][0])"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:33.310393Z","iopub.status.busy":"2024-07-02T20:17:33.310107Z","iopub.status.idle":"2024-07-02T20:17:33.317262Z","shell.execute_reply":"2024-07-02T20:17:33.316249Z","shell.execute_reply.started":"2024-07-02T20:17:33.310368Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'misi filkom <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>'"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["rev_word_map = {v: k for k, v in word_map.items()}\n","' '.join([rev_word_map[v] for v in pairs_encoded[1][0]])"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:33.318791Z","iopub.status.busy":"2024-07-02T20:17:33.318483Z","iopub.status.idle":"2024-07-02T20:17:33.325793Z","shell.execute_reply":"2024-07-02T20:17:33.325035Z","shell.execute_reply.started":"2024-07-02T20:17:33.318756Z"},"trusted":true},"outputs":[],"source":["class Dataset(Dataset):\n","\n","    def __init__(self):\n","\n","        self.pairs = json.load(open('pairs_encoded_kbfilkom.json'))\n","        self.dataset_size = len(self.pairs)\n","\n","    def __getitem__(self, i):\n","        \n","        question = torch.LongTensor(self.pairs[i][0])\n","        reply = torch.LongTensor(self.pairs[i][1])\n","            \n","        return question, reply\n","\n","    def __len__(self):\n","        return self.dataset_size"]},{"cell_type":"markdown","metadata":{},"source":["## Train Loader"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:33.337842Z","iopub.status.busy":"2024-07-02T20:17:33.337481Z","iopub.status.idle":"2024-07-02T20:17:33.379910Z","shell.execute_reply":"2024-07-02T20:17:33.378865Z","shell.execute_reply.started":"2024-07-02T20:17:33.337813Z"},"trusted":true},"outputs":[],"source":["train_loader = torch.utils.data.DataLoader(Dataset(),\n","                                           batch_size = 100, \n","                                           shuffle=True, \n","                                           pin_memory=True)"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:33.381467Z","iopub.status.busy":"2024-07-02T20:17:33.381114Z","iopub.status.idle":"2024-07-02T20:17:33.388963Z","shell.execute_reply":"2024-07-02T20:17:33.388016Z","shell.execute_reply.started":"2024-07-02T20:17:33.381434Z"},"trusted":true},"outputs":[],"source":["def create_masks(question, reply_input, reply_target):\n","    \n","    def subsequent_mask(size):\n","        mask = torch.triu(torch.ones(size, size)).transpose(0, 1).type(dtype=torch.uint8)\n","        return mask.unsqueeze(0)\n","    \n","    question_mask = question!=0\n","    question_mask = question_mask.to(device)\n","    question_mask = question_mask.unsqueeze(1).unsqueeze(1)         # (batch_size, 1, 1, max_words)\n","     \n","    reply_input_mask = reply_input!=0\n","    reply_input_mask = reply_input_mask.unsqueeze(1)  # (batch_size, 1, max_words)\n","    reply_input_mask = reply_input_mask & subsequent_mask(reply_input.size(-1)).type_as(reply_input_mask.data) \n","    reply_input_mask = reply_input_mask.unsqueeze(1) # (batch_size, 1, max_words, max_words)\n","    reply_target_mask = reply_target!=0              # (batch_size, max_words)\n","    \n","    return question_mask, reply_input_mask, reply_target_mask"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:33.390611Z","iopub.status.busy":"2024-07-02T20:17:33.390255Z","iopub.status.idle":"2024-07-02T20:17:33.397673Z","shell.execute_reply":"2024-07-02T20:17:33.396839Z","shell.execute_reply.started":"2024-07-02T20:17:33.390578Z"},"trusted":true},"outputs":[],"source":["import os\n","\n","def create_directory(path):\n","    if not os.path.exists(path):\n","        os.makedirs(path)\n","        print(f\"Directory created at {path}\")\n","    else:\n","        print(f\"Directory already exists at {path}\")"]},{"cell_type":"markdown","metadata":{},"source":["# Architecture"]},{"cell_type":"markdown","metadata":{},"source":["## Embeddings"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:33.399270Z","iopub.status.busy":"2024-07-02T20:17:33.398914Z","iopub.status.idle":"2024-07-02T20:17:33.421634Z","shell.execute_reply":"2024-07-02T20:17:33.420718Z","shell.execute_reply.started":"2024-07-02T20:17:33.399239Z"},"trusted":true},"outputs":[],"source":["class Embeddings(nn.Module):\n","    \"\"\"\n","    Implements embeddings of the words and adds their positional encodings. \n","    \"\"\"\n","    def __init__(self, vocab_size, d_model, max_len = 50, num_layers = 6):\n","        super(Embeddings, self).__init__()\n","        self.d_model = d_model\n","        self.dropout = nn.Dropout(0.1)\n","        self.embed = nn.Embedding(vocab_size, d_model)\n","        self.pe = self.create_positinal_encoding(max_len, self.d_model)     # (1, max_len, d_model)\n","        self.te = self.create_positinal_encoding(num_layers, self.d_model)  # (1, num_layers, d_model)\n","        self.dropout = nn.Dropout(0.1)\n","        \n","    def create_positinal_encoding(self, max_len, d_model):\n","        pe = torch.zeros(max_len, d_model).to(device)\n","        for pos in range(max_len):   # for each position of the word\n","            for i in range(0, d_model, 2):   # for each dimension of the each position\n","                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/d_model)))\n","                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n","        pe = pe.unsqueeze(0)   # include the batch size\n","        return pe\n","        \n","    def forward(self, embedding, layer_idx):\n","        if layer_idx == 0:\n","            embedding = self.embed(embedding) * math.sqrt(self.d_model)\n","        embedding += self.pe[:, :embedding.size(1)]   # pe will automatically be expanded with the same batch size as encoded_words\n","        # embedding: (batch_size, max_len, d_model), te: (batch_size, 1, d_model)\n","        embedding += self.te[:, layer_idx, :].unsqueeze(1).repeat(1, embedding.size(1), 1)\n","        embedding = self.dropout(embedding)\n","        return embedding\n","\n","class EmbeddingsLSTM(nn.Module):\n","    \"\"\"\n","    Implements embeddingsLSTM of the words and adds their positional encodings. \n","    \"\"\"\n","    def __init__(self, vocab_size, d_model, num_layers = 6):\n","        super(EmbeddingsLSTM, self).__init__()\n","        self.d_model = d_model\n","        self.dropout = nn.Dropout(0.1)\n","        self.embed = nn.Embedding(vocab_size, d_model)\n","        self.lstm = nn.LSTM(d_model, d_model, num_layers=num_layers)\n","        \n","    def forward(self, embedding, layer_idx, hidden, cell_state):\n","        if layer_idx == 0:\n","            embedding = self.embed(embedding) * math.sqrt(self.d_model)\n","        # Pass the embeddings through the LSTM\\\n","        embedding, (hidden, cell_state) = self.lstm(embedding, (hidden, cell_state))\n","        print()\n","        print(embedding.size())\n","        print(embedding)\n","        return embedding, hidden, cell_state\n","\n","class PretrainedEmbedding(nn.Module):\n","    \"\"\"\n","    Implements embeddingsLSTM of the words and adds their positional encodings. \n","    \"\"\"\n","    def __init__(self, vocab_size, d_model, max_len = 50, num_layers = 6):\n","        super(PretrainedEmbedding, self).__init__()\n","        self.d_model = d_model\n","        self.dropout = nn.Dropout(0.1)\n","        self.embed = nn.Embedding(vocab_size, d_model)\n","        self.pe = self.create_positinal_encoding(max_len, self.d_model)     # (1, max_len, d_model)\n","        self.te = self.create_positinal_encoding(num_layers, self.d_model)  # (1, num_layers, d_model)\n","        self.dropout = nn.Dropout(0.1)\n","        \n","    def create_positinal_encoding(self, max_len, d_model):\n","        pe = torch.zeros(max_len, d_model).to(device)\n","        for pos in range(max_len):   # for each position of the word\n","            for i in range(0, d_model, 2):   # for each dimension of the each position\n","                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/d_model)))\n","                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n","        pe = pe.unsqueeze(0)   # include the batch size\n","        return pe\n","        \n","    def forward(self, embedding, layer_idx):\n","        if layer_idx == 0:\n","            embedding = self.embed(embedding) * math.sqrt(self.d_model)\n","        embedding += self.pe[:, :embedding.size(1)]   # pe will automatically be expanded with the same batch size as encoded_words\n","        # embedding: (batch_size, max_len, d_model), te: (batch_size, 1, d_model)\n","        embedding += self.te[:, layer_idx, :].unsqueeze(1).repeat(1, embedding.size(1), 1)\n","        embedding = self.dropout(embedding)\n","        print()\n","        print(embedding.size())\n","        print(embedding)\n","        return embedding"]},{"cell_type":"markdown","metadata":{},"source":["## Multi-Head Attention"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:33.423711Z","iopub.status.busy":"2024-07-02T20:17:33.423056Z","iopub.status.idle":"2024-07-02T20:17:33.435766Z","shell.execute_reply":"2024-07-02T20:17:33.435028Z","shell.execute_reply.started":"2024-07-02T20:17:33.423675Z"},"trusted":true},"outputs":[],"source":["class MultiHeadAttention(nn.Module):\n","    \n","    def __init__(self, heads, d_model):\n","        super(MultiHeadAttention, self).__init__()\n","        assert d_model % heads == 0\n","        self.d_k = d_model // heads\n","        self.heads = heads\n","        self.dropout = nn.Dropout(0.1)\n","        self.query = nn.Linear(d_model, d_model)\n","        self.key = nn.Linear(d_model, d_model)\n","        self.value = nn.Linear(d_model, d_model)\n","        self.concat = nn.Linear(d_model, d_model)\n","        \n","    def forward(self, query, key, value, mask):\n","        \"\"\"\n","        query, key, value of shape: (batch_size, max_len, 512)\n","        mask of shape: (batch_size, 1, 1, max_words)\n","        \"\"\"\n","        # (batch_size, max_len, 512)\n","        query = self.query(query)\n","        key = self.key(key)        \n","        value = self.value(value)   \n","        \n","        # (batch_size, max_len, 512) --> (batch_size, max_len, h, d_k) --> (batch_size, h, max_len, d_k)\n","        query = query.view(query.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)   \n","        key = key.view(key.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)  \n","        value = value.view(value.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)  \n","        \n","        # (batch_size, h, max_len, d_k) matmul (batch_size, h, d_k, max_len) --> (batch_size, h, max_len, max_len)\n","        scores = torch.matmul(query, key.permute(0,1,3,2)) / math.sqrt(query.size(-1))\n","        scores = scores.masked_fill(mask == 0, -1e9)    # (batch_size, h, max_len, max_len)\n","        weights = F.softmax(scores, dim = -1)           # (batch_size, h, max_len, max_len)\n","        weights = self.dropout(weights)\n","        # (batch_size, h, max_len, max_len) matmul (batch_size, h, max_len, d_k) --> (batch_size, h, max_len, d_k)\n","        context = torch.matmul(weights, value)\n","        # (batch_size, h, max_len, d_k) --> (batch_size, max_len, h, d_k) --> (batch_size, max_len, h * d_k)\n","        context = context.permute(0,2,1,3).contiguous().view(context.shape[0], -1, self.heads * self.d_k)\n","        # (batch_size, max_len, h * d_k)\n","        interacted = self.concat(context)\n","        return interacted "]},{"cell_type":"markdown","metadata":{},"source":["## Feed Forward Neural Network"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:33.437331Z","iopub.status.busy":"2024-07-02T20:17:33.437039Z","iopub.status.idle":"2024-07-02T20:17:33.450588Z","shell.execute_reply":"2024-07-02T20:17:33.449684Z","shell.execute_reply.started":"2024-07-02T20:17:33.437306Z"},"trusted":true},"outputs":[],"source":["class FeedForward(nn.Module):\n","    def __init__(self, d_model, middle_dim = 2048):\n","        super(FeedForward, self).__init__()\n","        \n","        self.fc1 = nn.Linear(d_model, middle_dim)\n","        self.fc2 = nn.Linear(middle_dim, d_model)\n","        self.dropout = nn.Dropout(0.1)\n","\n","    def forward(self, x):\n","        out = F.relu(self.fc1(x))\n","        out = self.fc2(self.dropout(out))\n","        return out\n","\n","class FeedForwardLSTM(nn.Module):\n","    def __init__(self, d_model, middle_dim = 2048):\n","        super(FeedForwardLSTM, self).__init__()\n","        \n","        self.fc1 = nn.Linear(d_model, middle_dim)\n","        self.lstm = nn.LSTM(middle_dim, middle_dim)\n","        self.fc2 = nn.Linear(middle_dim, d_model)\n","        self.dropout = nn.Dropout(0.1)\n","\n","    def forward(self, x):\n","        out = F.relu(self.fc1(x))\n","        out, _ = self.lstm(out)\n","        out = self.fc2(self.dropout(out))\n","        return out\n","\n","class FeedForwardLayerNoReg(nn.Module):\n","    def __init__(self, d_model, middle_dim = 2048):\n","        super(FeedForward, self).__init__()\n","        \n","        self.fc1 = nn.Linear(d_model, middle_dim)\n","        self.fc2 = nn.Linear(middle_dim, d_model)\n","\n","    def forward(self, x):\n","        out = F.relu(self.fc1(x))\n","        out = self.fc2(out)\n","        return out"]},{"cell_type":"markdown","metadata":{},"source":["## Encoder"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:33.452046Z","iopub.status.busy":"2024-07-02T20:17:33.451709Z","iopub.status.idle":"2024-07-02T20:17:33.461741Z","shell.execute_reply":"2024-07-02T20:17:33.460847Z","shell.execute_reply.started":"2024-07-02T20:17:33.452019Z"},"trusted":true},"outputs":[],"source":["class EncoderLayer(nn.Module):\n","    def __init__(self, d_model, heads):\n","        super(EncoderLayer, self).__init__()\n","        self.layernorm = nn.LayerNorm(d_model)\n","        self.self_multihead = MultiHeadAttention(heads, d_model)\n","        self.feed_forward = FeedForward(d_model)\n","        self.dropout = nn.Dropout(0.1)\n","\n","    def forward(self, embeddings, mask):\n","        interacted = self.dropout(self.self_multihead(embeddings, embeddings, embeddings, mask))\n","        interacted = self.layernorm(interacted + embeddings)\n","        feed_forward_out = self.dropout(self.feed_forward(interacted))\n","        encoded = self.layernorm(feed_forward_out + interacted)\n","        return encoded\n","\n","class EncoderLayerNoReg(nn.Module):\n","    def __init__(self, d_model, heads):\n","        super(EncoderLayerNoReg, self).__init__()\n","        self.layernorm = nn.LayerNorm(d_model)\n","        self.self_multihead = MultiHeadAttention(heads, d_model)\n","        self.feed_forward = FeedForward(d_model)\n","\n","    def forward(self, embeddings, mask):\n","        interacted = self.self_multihead(embeddings, embeddings, embeddings, mask)\n","        interacted = self.layernorm(interacted + embeddings)\n","        feed_forward_out = self.feed_forward(interacted)\n","        encoded = self.layernorm(feed_forward_out + interacted)\n","        return encoded\n","        \n","        "]},{"cell_type":"markdown","metadata":{},"source":["## Decoder"]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:33.463365Z","iopub.status.busy":"2024-07-02T20:17:33.463025Z","iopub.status.idle":"2024-07-02T20:17:33.475859Z","shell.execute_reply":"2024-07-02T20:17:33.475008Z","shell.execute_reply.started":"2024-07-02T20:17:33.463339Z"},"trusted":true},"outputs":[],"source":["class DecoderLayer(nn.Module):\n","    \n","    def __init__(self, d_model, heads):\n","        super(DecoderLayer, self).__init__()\n","        self.layernorm = nn.LayerNorm(d_model)\n","        self.self_multihead = MultiHeadAttention(heads, d_model)\n","        self.src_multihead = MultiHeadAttention(heads, d_model)\n","        self.feed_forward = FeedForward(d_model)\n","        self.dropout = nn.Dropout(0.1)\n","        \n","    def forward(self, embeddings, encoded, src_mask, target_mask):\n","        query = self.dropout(self.self_multihead(embeddings, embeddings, embeddings, target_mask))\n","        query = self.layernorm(query + embeddings)\n","        interacted = self.dropout(self.src_multihead(query, encoded, encoded, src_mask))\n","        interacted = self.layernorm(interacted + query)\n","        feed_forward_out = self.dropout(self.feed_forward(interacted))\n","        decoded = self.layernorm(feed_forward_out + interacted)\n","        return decoded\n","\n","class DecoderLayerNoReg(nn.Module):\n","    def __init__(self, d_model, heads):\n","        super(DecoderLayerNoReg, self).__init__()\n","        self.layernorm = nn.LayerNorm(d_model)\n","        self.self_multihead = MultiHeadAttention(heads, d_model)\n","        self.src_multihead = MultiHeadAttention(heads, d_model)\n","        self.feed_forward = FeedForward(d_model)\n","        \n","    def forward(self, embeddings, encoded, src_mask, target_mask):\n","        query = self.self_multihead(embeddings, embeddings, embeddings, target_mask)\n","        query = self.layernorm(query + embeddings)\n","        interacted = self.src_multihead(query, encoded, encoded, src_mask)\n","        interacted = self.layernorm(interacted + query)\n","        feed_forward_out = self.feed_forward(interacted)\n","        decoded = self.layernorm(feed_forward_out + interacted)\n","        return decoded"]},{"cell_type":"markdown","metadata":{},"source":["## Transformer Architecture"]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:33.478079Z","iopub.status.busy":"2024-07-02T20:17:33.477782Z","iopub.status.idle":"2024-07-02T20:17:33.514932Z","shell.execute_reply":"2024-07-02T20:17:33.514023Z","shell.execute_reply.started":"2024-07-02T20:17:33.478055Z"},"trusted":true},"outputs":[],"source":["class Transformer(nn.Module):\n","    \n","    def __init__(self, d_model, heads, num_layers, word_map, max_len = 50):\n","        super(Transformer, self).__init__()\n","        \n","        self.d_model = d_model\n","        self.num_layers = num_layers\n","        self.vocab_size = len(word_map)\n","        self.embed = Embeddings(self.vocab_size, d_model, num_layers = num_layers, max_len = max_len)\n","        self.encoder = EncoderLayer(d_model, heads) \n","        self.decoder = DecoderLayer(d_model, heads)\n","        self.logit = nn.Linear(d_model, self.vocab_size)\n","        \n","    def encode(self, src_embeddings, src_mask):\n","        for i in range(self.num_layers):\n","            src_embeddings = self.embed(src_embeddings, i)\n","            src_embeddings = self.encoder(src_embeddings, src_mask)\n","        return src_embeddings\n","    \n","    def decode(self, tgt_embeddings, target_mask, src_embeddings, src_mask):\n","        for i in range(self.num_layers):\n","            tgt_embeddings = self.embed(tgt_embeddings, i)\n","            tgt_embeddings = self.decoder(tgt_embeddings, src_embeddings, src_mask, target_mask)\n","        return tgt_embeddings\n","        \n","    def forward(self, src_words, src_mask, target_words, target_mask):\n","        encoded = self.encode(src_words, src_mask)\n","        decoded = self.decode(target_words, target_mask, encoded, src_mask)\n","        out = F.log_softmax(self.logit(decoded), dim = 2)\n","        return out\n","\n","class TransformerNoReg(nn.Module):\n","    \n","    def __init__(self, d_model, heads, num_layers, word_map, max_len = 50):\n","        super(TransformerNoReg, self).__init__()\n","        \n","        self.d_model = d_model\n","        self.num_layers = num_layers\n","        self.vocab_size = len(word_map)\n","        self.embed = Embeddings(self.vocab_size, d_model, num_layers = num_layers, max_len = max_len)\n","        self.encoder = EncoderLayerNoReg(d_model, heads) \n","        self.decoder = DecoderLayerNoReg(d_model, heads)\n","        self.logit = nn.Linear(d_model, self.vocab_size)\n","        \n","    def encode(self, src_embeddings, src_mask):\n","        for i in range(self.num_layers):\n","            src_embeddings = self.embed(src_embeddings, i)\n","            src_embeddings = self.encoder(src_embeddings, src_mask)\n","        return src_embeddings\n","    \n","    def decode(self, tgt_embeddings, target_mask, src_embeddings, src_mask):\n","        for i in range(self.num_layers):\n","            tgt_embeddings = self.embed(tgt_embeddings, i)\n","            tgt_embeddings = self.decoder(tgt_embeddings, src_embeddings, src_mask, target_mask)\n","        return tgt_embeddings\n","        \n","    def forward(self, src_words, src_mask, target_words, target_mask):\n","        encoded = self.encode(src_words, src_mask)\n","        decoded = self.decode(target_words, target_mask, encoded, src_mask)\n","        out = F.log_softmax(self.logit(decoded), dim = 2)\n","        return out\n","\n","class TransformerLSTM(nn.Module):\n","    \n","    def __init__(self, d_model, heads, num_layers, word_map, max_len = 50):\n","        super(TransformerLSTM, self).__init__()\n","        \n","        self.d_model = d_model\n","        self.num_layers = num_layers\n","        self.vocab_size = len(word_map)\n","        self.embed = EmbeddingsLSTM(self.vocab_size, d_model, num_layers = num_layers)\n","        self.encoder = EncoderLayer(d_model, heads) \n","        self.decoder = DecoderLayer(d_model, heads)\n","        self.logit = nn.Linear(d_model, self.vocab_size)\n","        self.max_len = max_len\n","        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        \n","    def encode(self, src_embeddings, src_mask):\n","        encoder_hidden = torch.zeros([self.num_layers, self.max_len, self.d_model]).to(self.device) # 1 = number of LSTM layers\n","        cell_state = torch.zeros([self.num_layers, self.max_len, self.d_model]).to(self.device)\n","        for i in range(self.num_layers):\n","            src_embeddings, encoder_hidden, cell_state = self.embed(src_embeddings, i, encoder_hidden, cell_state)\n","            src_embeddings = self.encoder(src_embeddings, src_mask)\n","        return src_embeddings, encoder_hidden, cell_state\n","    \n","    def decode(self, tgt_embeddings, target_mask, src_embeddings, src_mask, encoder_hidden, cell_state):\n","        zeros = torch.zeros((self.num_layers, 1, self.d_model), device=encoder_hidden.device)\n","        encoder_hidden = torch.cat((encoder_hidden, zeros), dim=1)\n","        decoder_input = torch.Tensor([[0]]).long().to(self.device) # 0 = SOS_token\n","        decoder_hidden = encoder_hidden\n","        print(decoder_hidden.size())\n","        for i in range(self.num_layers):\n","            tgt_embeddings, decoder_hidden, cell_state = self.embed(tgt_embeddings, i, decoder_hidden, cell_state)\n","            tgt_embeddings = self.decoder(tgt_embeddings, src_embeddings, src_mask, target_mask)\n","        return tgt_embeddings\n","        \n","    def forward(self, src_words, src_mask, target_words, target_mask):\n","        encoded, encoder_hidden, cell_state = self.encode(src_words, src_mask)\n","        decoded = self.decode(target_words, target_mask, encoded, src_mask, encoder_hidden, cell_state)\n","        out = F.log_softmax(self.logit(decoded), dim = 2)\n","        return out\n","\n","class TransformerPreTrainedEmbedding(nn.Module):\n","    def __init__(self, d_model, heads, num_layers, word_map, max_len = 50):\n","        super(TransformerPreTrainedEmbedding, self).__init__()\n","        \n","        self.d_model = d_model\n","        self.num_layers = num_layers\n","        self.vocab_size = len(word_map)\n","        self.embed = PretrainedEmbedding(self.vocab_size, d_model, num_layers = num_layers, max_len = max_len)\n","        self.encoder = EncoderLayer(d_model, heads) \n","        self.decoder = DecoderLayer(d_model, heads)\n","        self.logit = nn.Linear(d_model, self.vocab_size)\n","        \n","    def encode(self, src_embeddings, src_mask):\n","        for i in range(self.num_layers):\n","            src_embeddings = self.embed(src_embeddings, i)\n","            src_embeddings = self.encoder(src_embeddings, src_mask)\n","        return src_embeddings\n","    \n","    def decode(self, tgt_embeddings, target_mask, src_embeddings, src_mask):\n","        for i in range(self.num_layers):\n","            tgt_embeddings = self.embed(tgt_embeddings, i)\n","            tgt_embeddings = self.decoder(tgt_embeddings, src_embeddings, src_mask, target_mask)\n","        return tgt_embeddings\n","        \n","    def forward(self, src_words, src_mask, target_words, target_mask):\n","        encoded = self.encode(src_words, src_mask)\n","        decoded = self.decode(target_words, target_mask, encoded, src_mask)\n","        out = F.log_softmax(self.logit(decoded), dim = 2)\n","        return out\n","\n","class TransformerDecoderOnly(nn.Module):    \n","    def __init__(self, d_model, heads, num_layers, word_map, max_len = 50):\n","        super(TransformerDecoderOnly, self).__init__()\n","        \n","        self.d_model = d_model\n","        self.num_layers = num_layers\n","        self.vocab_size = len(word_map)\n","        self.embed = Embeddings(self.vocab_size, d_model, num_layers = num_layers, max_len = max_len)\n","        self.encoder = EncoderLayer(d_model, heads) \n","        self.decoder = DecoderLayer(d_model, heads)\n","        self.logit = nn.Linear(d_model, self.vocab_size)\n","    \n","    def decode(self, tgt_embeddings, target_mask, src_embeddings, src_mask):\n","        for i in range(self.num_layers):\n","            tgt_embeddings = self.embed(tgt_embeddings, i)\n","            tgt_embeddings = self.decoder(tgt_embeddings, src_embeddings, src_mask, target_mask)\n","        return tgt_embeddings\n","        \n","    def forward(self, src_words, src_mask, target_words, target_mask):\n","        encoded = self.embed(src_words, 0)\n","        decoded = self.decode(target_words, target_mask, encoded, src_mask)\n","        out = F.log_softmax(self.logit(decoded), dim = 2)\n","        return out"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:33.516425Z","iopub.status.busy":"2024-07-02T20:17:33.516094Z","iopub.status.idle":"2024-07-02T20:17:33.527522Z","shell.execute_reply":"2024-07-02T20:17:33.526734Z","shell.execute_reply.started":"2024-07-02T20:17:33.516394Z"},"trusted":true},"outputs":[],"source":["class AdamWarmup:\n","    \n","    def __init__(self, model_size, warmup_steps, optimizer):\n","        \n","        self.model_size = model_size\n","        self.warmup_steps = warmup_steps\n","        self.optimizer = optimizer\n","        self.current_step = 0\n","        self.lr = 0\n","        \n","    def get_lr(self):\n","        return self.model_size ** (-0.5) * min(self.current_step ** (-0.5), self.current_step * self.warmup_steps ** (-1.5))\n","        \n","    def step(self):\n","        # Increment the number of steps each time we call the step function\n","        self.current_step += 1\n","        lr = self.get_lr()\n","        for param_group in self.optimizer.param_groups:\n","            param_group['lr'] = lr\n","        # update the learning rate\n","        self.lr = lr\n","        self.optimizer.step()       \n","        "]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:33.529363Z","iopub.status.busy":"2024-07-02T20:17:33.528622Z","iopub.status.idle":"2024-07-02T20:17:33.540886Z","shell.execute_reply":"2024-07-02T20:17:33.540153Z","shell.execute_reply.started":"2024-07-02T20:17:33.529331Z"},"trusted":true},"outputs":[],"source":["class LossWithLS(nn.Module):\n","\n","    def __init__(self, size, smooth):\n","        super(LossWithLS, self).__init__()\n","        self.criterion = nn.KLDivLoss(size_average=False, reduce=False)\n","        self.confidence = 1.0 - smooth\n","        self.smooth = smooth\n","        self.size = size\n","        \n","    def forward(self, prediction, target, mask):\n","        \"\"\"\n","        prediction of shape: (batch_size, max_words, vocab_size)\n","        target and mask of shape: (batch_size, max_words)\n","        \"\"\"\n","        prediction = prediction.view(-1, prediction.size(-1))   # (batch_size * max_words, vocab_size)\n","        target = target.contiguous().view(-1)   # (batch_size * max_words)\n","        mask = mask.float()\n","        mask = mask.view(-1)       # (batch_size * max_words)\n","        labels = prediction.data.clone()\n","        labels.fill_(self.smooth / (self.size - 1))\n","        labels.scatter_(1, target.data.unsqueeze(1), self.confidence)\n","        loss = self.criterion(prediction, labels)    # (batch_size * max_words, vocab_size)\n","        loss = (loss.sum(1) * mask).sum() / mask.sum()\n","        return loss\n","        "]},{"cell_type":"markdown","metadata":{},"source":["# Define Neptune Experiment"]},{"cell_type":"code","execution_count":37,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:33.542496Z","iopub.status.busy":"2024-07-02T20:17:33.542228Z","iopub.status.idle":"2024-07-02T20:17:33.550662Z","shell.execute_reply":"2024-07-02T20:17:33.549840Z","shell.execute_reply.started":"2024-07-02T20:17:33.542473Z"},"trusted":true},"outputs":[],"source":["project = \"andialifs/fluent-tesis-24\"\n","api_token = \"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJjZTY2YWQ3My04OTBkLTQ2OWUtYTc1Ni1jYjk0MGZhMWFiNGEifQ==\"\n","\n","def neptune_init(name):\n","    run = neptune.init_run(\n","        project=project,\n","        api_token=api_token,\n","        name=name\n","    )\n","    return run"]},{"cell_type":"markdown","metadata":{},"source":["# Function"]},{"cell_type":"markdown","metadata":{},"source":["## Train"]},{"cell_type":"code","execution_count":38,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:33.564996Z","iopub.status.busy":"2024-07-02T20:17:33.564656Z","iopub.status.idle":"2024-07-02T20:17:33.574590Z","shell.execute_reply":"2024-07-02T20:17:33.573653Z","shell.execute_reply.started":"2024-07-02T20:17:33.564947Z"},"trusted":true},"outputs":[],"source":["def train(train_loader, transformer, criterion, epoch):\n","    transformer.train()\n","    sum_loss = 0\n","    count = 0\n","\n","    for i, (question, reply) in enumerate(train_loader):\n","        \n","        samples = question.shape[0]\n","\n","        # Move to device\n","        question = question.to(device)\n","        reply = reply.to(device)\n","\n","        # Prepare Target Data\n","        reply_input = reply[:, :-1]\n","        reply_target = reply[:, 1:]\n","\n","        # Create mask and add dimensions\n","        question_mask, reply_input_mask, reply_target_mask = create_masks(question, reply_input, reply_target)\n","\n","        # Get the transformer outputs\n","        out = transformer(question, question_mask, reply_input, reply_input_mask)\n","\n","        # Compute the loss\n","        loss = criterion(out, reply_target, reply_target_mask)\n","        \n","        # Backprop\n","        transformer_optimizer.optimizer.zero_grad()\n","        loss.backward()\n","        transformer_optimizer.step()\n","        \n","        sum_loss += loss.item() * samples\n","        count += samples\n","        \n","        if i % 100 == 0:\n","            print(\"Epoch [{}][{}/{}]\\tLoss: {:.3f}\".format(epoch, i, len(train_loader), sum_loss/count))\n","    \n","    return sum_loss/count"]},{"cell_type":"markdown","metadata":{},"source":["## Evaluate"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":39,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:33.576127Z","iopub.status.busy":"2024-07-02T20:17:33.575799Z","iopub.status.idle":"2024-07-02T20:17:33.587390Z","shell.execute_reply":"2024-07-02T20:17:33.586545Z","shell.execute_reply.started":"2024-07-02T20:17:33.576103Z"},"trusted":true},"outputs":[],"source":["def evaluate(transformer, question, question_mask, max_len, word_map):\n","    \"\"\"\n","    Performs Greedy Decoding with a batch size of 1\n","    \"\"\"\n","    rev_word_map = {v: k for k, v in word_map.items()}\n","    transformer.eval()\n","    start_token = word_map['<start>']\n","    encoded = transformer.encode(question, question_mask)\n","    words = torch.LongTensor([[start_token]]).to(device)\n","    \n","    for step in range(max_len - 1):\n","        size = words.shape[1]\n","        target_mask = torch.triu(torch.ones(size, size)).transpose(0, 1).type(dtype=torch.uint8)\n","        target_mask = target_mask.to(device).unsqueeze(0).unsqueeze(0)\n","        decoded = transformer.decode(words, target_mask, encoded, question_mask)\n","        predictions = transformer.logit(decoded[:, -1])\n","        _, next_word = torch.max(predictions, dim = 1)\n","        next_word = next_word.item()\n","        if next_word == word_map['<end>']:\n","            break\n","        words = torch.cat([words, torch.LongTensor([[next_word]]).to(device)], dim = 1)   # (1,step+2)\n","        \n","    # Construct Sentence\n","    if words.dim() == 2:\n","        words = words.squeeze(0)\n","        words = words.tolist()\n","        \n","    sen_idx = [w for w in words if w not in {word_map['<start>']}]\n","    sentence = ' '.join([rev_word_map[sen_idx[k]] for k in range(len(sen_idx))])\n","    \n","    return sentence"]},{"cell_type":"markdown","metadata":{},"source":["# Experiment"]},{"cell_type":"markdown","metadata":{},"source":["## Transformers without reg"]},{"cell_type":"code","execution_count":38,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:33.588788Z","iopub.status.busy":"2024-07-02T20:17:33.588528Z","iopub.status.idle":"2024-07-02T20:17:47.395240Z","shell.execute_reply":"2024-07-02T20:17:47.393722Z","shell.execute_reply.started":"2024-07-02T20:17:33.588766Z"},"scrolled":true,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\\Skipping experiment 0 with d_model 512, heads8, num_layers5\n","\n","\\Skipping experiment 1 with d_model 512, heads8, num_layers10\n","\n","\\Skipping experiment 2 with d_model 512, heads16, num_layers5\n","\n","\\Skipping experiment 3 with d_model 512, heads16, num_layers10\n","\n","\\Skipping experiment 4 with d_model 512, heads32, num_layers5\n","\n","\\Skipping experiment 5 with d_model 512, heads32, num_layers10\n","\n","\\Skipping experiment 6 with d_model 1024, heads8, num_layers5\n","\n","\\Skipping experiment 7 with d_model 1024, heads8, num_layers10\n","\n","\\Skipping experiment 8 with d_model 1024, heads16, num_layers5\n","\n","\\Skipping experiment 9 with d_model 1024, heads16, num_layers10\n","\n","\\Skipping experiment 10 with d_model 1024, heads32, num_layers5\n","\n","\\Skipping experiment 11 with d_model 1024, heads32, num_layers10\n","\n","\\Skipping experiment 12 with d_model 2048, heads8, num_layers5\n","\n","\n","Running for experiment 13 with d_model 2048, heads8, num_layers10\n","\n","[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/andialifs/fluent-tesis-24/e/FLUENT24-72\n"]},{"name":"stderr","output_type":"stream","text":["/home/andyalyfsyah/.conda/envs/myenv/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [0][0/12]\tLoss: 5.316\n","Epoch [1][0/12]\tLoss: 5.172\n","Epoch [2][0/12]\tLoss: 4.794\n","Epoch [3][0/12]\tLoss: 4.388\n","Epoch [4][0/12]\tLoss: 4.277\n","Epoch [5][0/12]\tLoss: 4.096\n","Epoch [6][0/12]\tLoss: 4.054\n","Epoch [7][0/12]\tLoss: 4.016\n","Epoch [8][0/12]\tLoss: 4.039\n","Epoch [9][0/12]\tLoss: 3.996\n","Epoch [10][0/12]\tLoss: 3.987\n","Epoch [11][0/12]\tLoss: 3.953\n","Epoch [12][0/12]\tLoss: 3.956\n","Epoch [13][0/12]\tLoss: 3.985\n","Epoch [14][0/12]\tLoss: 3.910\n","Epoch [15][0/12]\tLoss: 3.954\n","Epoch [16][0/12]\tLoss: 3.935\n","Epoch [17][0/12]\tLoss: 3.714\n","Epoch [18][0/12]\tLoss: 3.813\n","Epoch [19][0/12]\tLoss: 3.785\n","Epoch [20][0/12]\tLoss: 3.831\n","Epoch [21][0/12]\tLoss: 3.673\n","Epoch [22][0/12]\tLoss: 3.745\n","Epoch [23][0/12]\tLoss: 3.641\n","Epoch [24][0/12]\tLoss: 3.572\n","Epoch [25][0/12]\tLoss: 3.588\n","Epoch [26][0/12]\tLoss: 3.553\n","Epoch [27][0/12]\tLoss: 3.565\n","Epoch [28][0/12]\tLoss: 3.427\n","Epoch [29][0/12]\tLoss: 3.531\n","Epoch [30][0/12]\tLoss: 3.219\n","Epoch [31][0/12]\tLoss: 3.407\n","Epoch [32][0/12]\tLoss: 3.051\n","Epoch [33][0/12]\tLoss: 3.375\n","Epoch [34][0/12]\tLoss: 3.234\n","Epoch [35][0/12]\tLoss: 3.081\n","Epoch [36][0/12]\tLoss: 2.986\n","Epoch [37][0/12]\tLoss: 2.987\n","Epoch [38][0/12]\tLoss: 2.787\n","Epoch [39][0/12]\tLoss: 3.200\n","Epoch [40][0/12]\tLoss: 3.130\n","Epoch [41][0/12]\tLoss: 3.092\n","Epoch [42][0/12]\tLoss: 2.891\n","Epoch [43][0/12]\tLoss: 2.947\n","Epoch [44][0/12]\tLoss: 2.989\n","Epoch [45][0/12]\tLoss: 2.848\n","Epoch [46][0/12]\tLoss: 2.886\n","Epoch [47][0/12]\tLoss: 2.696\n","Epoch [48][0/12]\tLoss: 2.990\n","Epoch [49][0/12]\tLoss: 2.601\n"]}],"source":["d_model = [512, 1024, 2048, 4096]\n","heads = [8, 16, 32]\n","num_layers = [5, 10]\n","epochs = 100\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","with open('WORDMAP_corpus_KBFILKOM.json', 'r') as j:\n","    word_map = json.load(j)\n","\n","transformer_experiment = pd.DataFrame(columns = ['experiment_id', 'd_model', 'heads', 'num_layers', 'train_loss'])\n","loss_history = {}\n","\n","experiment_id = -1\n","\n","torch.cuda.empty_cache()\n","\n","for d_m in d_model:\n","    for h in heads:\n","        for n_l in num_layers: \n","            parameters = {\n","                'd_model': d_m,\n","                'heads': h,\n","                'num_layers': n_l\n","            }\n","            \n","            experiment_id += 1\n","            \n","            if experiment_id < 13:\n","                print('\\Skipping experiment {} with d_model {}, heads{}, num_layers{}\\n'.format(experiment_id, d_m, h, n_l))\n","                continue\n","            \n","            print('\\nRunning for experiment {} with d_model {}, heads{}, num_layers{}\\n'.format(experiment_id, d_m, h, n_l))\n","            name = \"experiment_2724_noreg_\" + str(experiment_id)\n","\n","            run = neptune_init(name)\n","            run['parameters'] = parameters\n","            run['group_tags'] = \"transformers-vanilla-no-reg\"\n","            \n","            transformer_experiment.loc[experiment_id, 'experiment_id'] = 'experiment_{}'.format(str(experiment_id))\n","            transformer_experiment.loc[experiment_id, 'd_model'] = d_m\n","            transformer_experiment.loc[experiment_id, 'heads'] = h\n","            transformer_experiment.loc[experiment_id, 'num_layers'] = n_l\n","\n","            transformer = Transformer(d_model = d_m, heads = h, num_layers = n_l, word_map = word_map, max_len=95)\n","            transformer = transformer.to(device)\n","            adam_optimizer = torch.optim.Adam(transformer.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-9)\n","            transformer_optimizer = AdamWarmup(model_size = d_m, warmup_steps = 4000, optimizer = adam_optimizer)\n","            criterion = LossWithLS(len(word_map), 0.2)\n","\n","            loss_list_experiment = []\n","            for epoch in range(epochs):\n","                loss_train = train(train_loader, transformer, criterion, epoch)\n","\n","                state = {'epoch': epoch, 'transformer': transformer, 'transformer_optimizer': transformer_optimizer}\n","                # torch.save(state, 'checkpoint_experiment_' + str(epoch) + 'id_'+ str(experiment_id) +'.pth.tar')\n","\n","                loss_list_experiment.append(loss_train)\n","\n","                run['train/loss'].append(loss_train)\n","\n","            transformer_experiment.loc[experiment_id, 'train_loss'] = loss_train\n","            loss_history['experiment_{}'.format(str(experiment_id))] = loss_list_experiment\n","            \n","            # torch.cuda.empty_cache() \n","            run.stop()\n"]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:16:32.298394Z","iopub.status.busy":"2024-07-02T20:16:32.297969Z","iopub.status.idle":"2024-07-02T20:16:32.305442Z","shell.execute_reply":"2024-07-02T20:16:32.304012Z","shell.execute_reply.started":"2024-07-02T20:16:32.298351Z"},"trusted":true},"outputs":[],"source":["torch.cuda.empty_cache() "]},{"cell_type":"markdown","metadata":{},"source":["## Transformers"]},{"cell_type":"code","execution_count":8,"metadata":{"_kg_hide-input":true,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Running for experiment 0 with d_model 512, heads8, num_layers5\n","\n"]},{"ename":"NameError","evalue":"name 'project' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m experiment_id \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mRunning for experiment \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m with d_model \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, heads\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, num_layers\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(experiment_id, d_m, h, n_l))\n\u001b[1;32m     22\u001b[0m run \u001b[38;5;241m=\u001b[39m neptune\u001b[38;5;241m.\u001b[39minit_run(\n\u001b[0;32m---> 23\u001b[0m     project\u001b[38;5;241m=\u001b[39m\u001b[43mproject\u001b[49m,\n\u001b[1;32m     24\u001b[0m     api_token\u001b[38;5;241m=\u001b[39mapi_token,\n\u001b[1;32m     25\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexperiment_1724_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(experiment_id)\n\u001b[1;32m     26\u001b[0m ) \n\u001b[1;32m     27\u001b[0m run[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparameters\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124md_model\u001b[39m\u001b[38;5;124m'\u001b[39m: d_m,\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheads\u001b[39m\u001b[38;5;124m'\u001b[39m: h,\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_layers\u001b[39m\u001b[38;5;124m'\u001b[39m: n_l\n\u001b[1;32m     31\u001b[0m }\n\u001b[1;32m     33\u001b[0m transformer_experiment\u001b[38;5;241m.\u001b[39mloc[experiment_id, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexperiment_id\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexperiment_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mstr\u001b[39m(experiment_id))\n","\u001b[0;31mNameError\u001b[0m: name 'project' is not defined"]}],"source":["d_model = [512, 1024, 2048, 4096]\n","heads = [8, 16, 32]\n","num_layers = [5, 10]\n","epochs = 100\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","with open('WORDMAP_corpus_KBFILKOM.json', 'r') as j:\n","    word_map = json.load(j)\n","\n","transformer_experiment = pd.DataFrame(columns = ['experiment_id', 'd_model', 'heads', 'num_layers', 'train_loss'])\n","loss_history = {}\n","\n","experiment_id = -1\n","\n","for d_m in d_model:\n","    for h in heads:\n","        for n_l in num_layers: \n","            experiment_id += 1\n","            print('\\nRunning for experiment {} with d_model {}, heads{}, num_layers{}\\n'.format(experiment_id, d_m, h, n_l))\n","\n","            run = neptune.init_run(\n","                project=project,\n","                api_token=api_token,\n","                name=\"experiment_1724_\" + str(experiment_id)\n","            ) \n","            run['parameters'] = {\n","                'd_model': d_m,\n","                'heads': h,\n","                'num_layers': n_l\n","            }\n","\n","            transformer_experiment.loc[experiment_id, 'experiment_id'] = 'experiment_{}'.format(str(experiment_id))\n","            transformer_experiment.loc[experiment_id, 'd_model'] = d_m\n","            transformer_experiment.loc[experiment_id, 'heads'] = h\n","            transformer_experiment.loc[experiment_id, 'num_layers'] = n_l\n","\n","            transformer = Transformer(d_model = d_m, heads = h, num_layers = n_l, word_map = word_map, max_len=95)\n","            transformer = transformer.to(device)\n","            adam_optimizer = torch.optim.Adam(transformer.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-9)\n","            transformer_optimizer = AdamWarmup(model_size = d_m, warmup_steps = 4000, optimizer = adam_optimizer)\n","            criterion = LossWithLS(len(word_map), 0.2)\n","\n","            loss_list_experiment = []\n","            for epoch in range(epochs):\n","                loss_train = train(train_loader, transformer, criterion, epoch)\n","\n","                state = {'epoch': epoch, 'transformer': transformer, 'transformer_optimizer': transformer_optimizer}\n","                torch.save(state, 'checkpoint_experiment_' + str(epoch) + 'id_'+ str(experiment_id) +'.pth.tar')\n","\n","                loss_list_experiment.append(loss_train)\n","\n","                run['train/loss'].append(loss_train)\n","\n","            transformer_experiment.loc[experiment_id, 'train_loss'] = loss_train\n","            loss_history['experiment_{}'.format(str(experiment_id))] = loss_list_experiment\n","            \n","            run.stop()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import yaml\n","\n","with open('loss_history_transformer_experiment_KBFILKOM.yaml', 'w') as file:\n","    documents = yaml.dump(loss_history, file)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["transformer_experiment.dropna(inplace=True)\n","transformer_experiment"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import yaml\n","\n","with open('history_rnn_150524.yaml', 'r') as file:\n","    history_rnn = yaml.load(file, Loader=yaml.FullLoader)"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true},"outputs":[],"source":["import matplotlib \n","import matplotlib.pyplot as plt\n","\n","loss_history_key = list(loss_history.keys())\n","\n","plt.figure(figsize=(15,10))\n","plt.title(\"Training loss vs. Number of Epochs\")\n","plt.xlabel(\"Number of Epochs\")\n","plt.ylabel(\"Training Loss\")\n","z\n","\n","for key in loss_history_key:\n","    loss_list = loss_history[key]\n","    labels = f'd_model: {transformer_experiment[transformer_experiment[\"experiment_id\"] == key][\"d_model\"].values[0]}, heads: {transformer_experiment[transformer_experiment[\"experiment_id\"] == key][\"heads\"].values[0]}, num_layers: {transformer_experiment[transformer_experiment[\"experiment_id\"] == key][\"num_layers\"].values[0]}'\n","    plt.plot(loss_list, label = labels)\n","\n","    \n","plt.plot(history_rnn['loss'], \n","                label = 'LSTM (Baseline FLUENT 2023)', \n","                linestyle='dashed', \n","                color='black', \n","                linewidth=2.5, \n","                alpha=0.7, \n","                marker='o', \n","                markerfacecolor='black', \n","                markersize=5\n","        )\n","\n","plt.legend()\n","torch.cuda.is_available()\n","plt.grid()"]},{"cell_type":"markdown","metadata":{},"source":["# Training"]},{"cell_type":"markdown","metadata":{},"source":["## Pretrained"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["directory = 'experiment_pretrained_1'\n","\n","d_model = 2048\n","heads = 16\n","num_layers = 5\n","epochs = 100\n","\n","loss_history_pratrained_embed_transformer = []\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","with open('WORDMAP_corpus_KBFILKOM.json', 'r') as j:\n","    word_map = json.load(j)\n","    \n","transformer = TransformerPreTrainedEmbedding(d_model = d_model, heads = heads, num_layers = num_layers, word_map = word_map, max_len=95)\n","transformer = transformer.to(device)\n","adam_optimizer = torch.optim.Adam(transformer.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-9)\n","transformer_optimizer = AdamWarmup(model_size = d_model, warmup_steps = 4000, optimizer = adam_optimizer)\n","criterion = LossWithLS(len(word_map), 0.2)\n","\n","for epoch in range(epochs):\n","    loss_train = train(train_loader, transformer, criterion, epoch)\n","\n","    state = {'epoch': epoch, 'transformer': transformer, 'transformer_optimizer': transformer_optimizer}\n","    torch.save(state, directory + '/checkpoint_' + str(epoch) +'.pth.tar')\n","\n","    loss_history_pratrained_embed_transformer.append(loss_train)"]},{"cell_type":"markdown","metadata":{},"source":["## Vanilla"]},{"cell_type":"code","execution_count":46,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Directory created at experiment_vanilla_18824\n","Epoch [0][0/1]\tLoss: 5.214\n","Saving checkpoint for epoch 0\n","Checkpoint saved with name experiment_vanilla_18824/checkpoint_0.pth.tar\n","Epoch [1][0/1]\tLoss: 5.202\n","Epoch [2][0/1]\tLoss: 5.191\n","Epoch [3][0/1]\tLoss: 5.207\n","Epoch [4][0/1]\tLoss: 5.147\n","Epoch [5][0/1]\tLoss: 5.185\n","Epoch [6][0/1]\tLoss: 5.143\n","Epoch [7][0/1]\tLoss: 5.120\n","Epoch [8][0/1]\tLoss: 5.100\n","Epoch [9][0/1]\tLoss: 5.058\n","Epoch [10][0/1]\tLoss: 5.025\n","Epoch [11][0/1]\tLoss: 4.954\n","Epoch [12][0/1]\tLoss: 4.949\n","Epoch [13][0/1]\tLoss: 4.901\n","Epoch [14][0/1]\tLoss: 4.845\n","Epoch [15][0/1]\tLoss: 4.801\n","Epoch [16][0/1]\tLoss: 4.738\n","Epoch [17][0/1]\tLoss: 4.684\n","Epoch [18][0/1]\tLoss: 4.635\n","Epoch [19][0/1]\tLoss: 4.560\n","Epoch [20][0/1]\tLoss: 4.503\n","Epoch [21][0/1]\tLoss: 4.451\n","Epoch [22][0/1]\tLoss: 4.408\n","Epoch [23][0/1]\tLoss: 4.366\n","Epoch [24][0/1]\tLoss: 4.320\n","Epoch [25][0/1]\tLoss: 4.287\n","Epoch [26][0/1]\tLoss: 4.223\n","Epoch [27][0/1]\tLoss: 4.178\n","Epoch [28][0/1]\tLoss: 4.145\n","Epoch [29][0/1]\tLoss: 4.083\n","Epoch [30][0/1]\tLoss: 4.050\n","Epoch [31][0/1]\tLoss: 4.017\n","Epoch [32][0/1]\tLoss: 3.959\n","Epoch [33][0/1]\tLoss: 3.940\n","Epoch [34][0/1]\tLoss: 3.902\n","Epoch [35][0/1]\tLoss: 3.852\n","Epoch [36][0/1]\tLoss: 3.829\n","Epoch [37][0/1]\tLoss: 3.798\n","Epoch [38][0/1]\tLoss: 3.784\n","Epoch [39][0/1]\tLoss: 3.734\n","Epoch [40][0/1]\tLoss: 3.700\n","Epoch [41][0/1]\tLoss: 3.674\n","Epoch [42][0/1]\tLoss: 3.666\n","Epoch [43][0/1]\tLoss: 3.640\n","Epoch [44][0/1]\tLoss: 3.627\n","Epoch [45][0/1]\tLoss: 3.618\n","Epoch [46][0/1]\tLoss: 3.588\n","Epoch [47][0/1]\tLoss: 3.568\n","Epoch [48][0/1]\tLoss: 3.531\n","Epoch [49][0/1]\tLoss: 3.501\n","Epoch [50][0/1]\tLoss: 3.505\n","Epoch [51][0/1]\tLoss: 3.482\n","Epoch [52][0/1]\tLoss: 3.468\n","Epoch [53][0/1]\tLoss: 3.471\n","Epoch [54][0/1]\tLoss: 3.455\n","Epoch [55][0/1]\tLoss: 3.456\n","Epoch [56][0/1]\tLoss: 3.426\n","Epoch [57][0/1]\tLoss: 3.437\n","Epoch [58][0/1]\tLoss: 3.401\n","Epoch [59][0/1]\tLoss: 3.402\n","Epoch [60][0/1]\tLoss: 3.386\n","Epoch [61][0/1]\tLoss: 3.391\n","Epoch [62][0/1]\tLoss: 3.382\n","Epoch [63][0/1]\tLoss: 3.382\n","Epoch [64][0/1]\tLoss: 3.351\n","Epoch [65][0/1]\tLoss: 3.365\n","Epoch [66][0/1]\tLoss: 3.327\n","Epoch [67][0/1]\tLoss: 3.341\n","Epoch [68][0/1]\tLoss: 3.312\n","Epoch [69][0/1]\tLoss: 3.334\n","Epoch [70][0/1]\tLoss: 3.320\n","Epoch [71][0/1]\tLoss: 3.318\n","Epoch [72][0/1]\tLoss: 3.294\n","Epoch [73][0/1]\tLoss: 3.311\n","Epoch [74][0/1]\tLoss: 3.275\n","Epoch [75][0/1]\tLoss: 3.290\n","Epoch [76][0/1]\tLoss: 3.297\n","Epoch [77][0/1]\tLoss: 3.293\n","Epoch [78][0/1]\tLoss: 3.289\n","Epoch [79][0/1]\tLoss: 3.268\n","Epoch [80][0/1]\tLoss: 3.286\n","Epoch [81][0/1]\tLoss: 3.268\n","Epoch [82][0/1]\tLoss: 3.264\n","Epoch [83][0/1]\tLoss: 3.275\n","Epoch [84][0/1]\tLoss: 3.272\n","Epoch [85][0/1]\tLoss: 3.250\n","Epoch [86][0/1]\tLoss: 3.244\n","Epoch [87][0/1]\tLoss: 3.245\n","Epoch [88][0/1]\tLoss: 3.230\n","Epoch [89][0/1]\tLoss: 3.244\n","Epoch [90][0/1]\tLoss: 3.245\n","Epoch [91][0/1]\tLoss: 3.222\n","Epoch [92][0/1]\tLoss: 3.235\n","Epoch [93][0/1]\tLoss: 3.206\n","Epoch [94][0/1]\tLoss: 3.222\n","Epoch [95][0/1]\tLoss: 3.208\n","Epoch [96][0/1]\tLoss: 3.202\n","Epoch [97][0/1]\tLoss: 3.178\n","Epoch [98][0/1]\tLoss: 3.182\n","Epoch [99][0/1]\tLoss: 3.177\n","Epoch [100][0/1]\tLoss: 3.171\n","Saving checkpoint for epoch 100\n","Checkpoint saved with name experiment_vanilla_18824/checkpoint_100.pth.tar\n","Epoch [101][0/1]\tLoss: 3.161\n","Epoch [102][0/1]\tLoss: 3.159\n","Epoch [103][0/1]\tLoss: 3.173\n","Epoch [104][0/1]\tLoss: 3.144\n","Epoch [105][0/1]\tLoss: 3.147\n","Epoch [106][0/1]\tLoss: 3.142\n","Epoch [107][0/1]\tLoss: 3.117\n","Epoch [108][0/1]\tLoss: 3.116\n","Epoch [109][0/1]\tLoss: 3.128\n","Epoch [110][0/1]\tLoss: 3.089\n","Epoch [111][0/1]\tLoss: 3.077\n","Epoch [112][0/1]\tLoss: 3.089\n","Epoch [113][0/1]\tLoss: 3.032\n","Epoch [114][0/1]\tLoss: 3.052\n","Epoch [115][0/1]\tLoss: 3.031\n","Epoch [116][0/1]\tLoss: 3.017\n","Epoch [117][0/1]\tLoss: 2.985\n","Epoch [118][0/1]\tLoss: 2.950\n","Epoch [119][0/1]\tLoss: 2.971\n","Epoch [120][0/1]\tLoss: 2.945\n","Epoch [121][0/1]\tLoss: 2.917\n","Epoch [122][0/1]\tLoss: 2.927\n","Epoch [123][0/1]\tLoss: 2.882\n","Epoch [124][0/1]\tLoss: 2.884\n","Epoch [125][0/1]\tLoss: 2.842\n","Epoch [126][0/1]\tLoss: 2.816\n","Epoch [127][0/1]\tLoss: 2.823\n","Epoch [128][0/1]\tLoss: 2.782\n","Epoch [129][0/1]\tLoss: 2.755\n","Epoch [130][0/1]\tLoss: 2.739\n","Epoch [131][0/1]\tLoss: 2.741\n","Epoch [132][0/1]\tLoss: 2.704\n","Epoch [133][0/1]\tLoss: 2.665\n","Epoch [134][0/1]\tLoss: 2.665\n","Epoch [135][0/1]\tLoss: 2.648\n","Epoch [136][0/1]\tLoss: 2.621\n","Epoch [137][0/1]\tLoss: 2.617\n","Epoch [138][0/1]\tLoss: 2.559\n","Epoch [139][0/1]\tLoss: 2.532\n","Epoch [140][0/1]\tLoss: 2.534\n","Epoch [141][0/1]\tLoss: 2.540\n","Epoch [142][0/1]\tLoss: 2.502\n","Epoch [143][0/1]\tLoss: 2.495\n","Epoch [144][0/1]\tLoss: 2.480\n","Epoch [145][0/1]\tLoss: 2.472\n","Epoch [146][0/1]\tLoss: 2.453\n","Epoch [147][0/1]\tLoss: 2.410\n","Epoch [148][0/1]\tLoss: 2.379\n","Epoch [149][0/1]\tLoss: 2.375\n","Epoch [150][0/1]\tLoss: 2.353\n","Epoch [151][0/1]\tLoss: 2.306\n","Epoch [152][0/1]\tLoss: 2.304\n","Epoch [153][0/1]\tLoss: 2.284\n","Epoch [154][0/1]\tLoss: 2.270\n","Epoch [155][0/1]\tLoss: 2.269\n","Epoch [156][0/1]\tLoss: 2.225\n","Epoch [157][0/1]\tLoss: 2.217\n","Epoch [158][0/1]\tLoss: 2.188\n","Epoch [159][0/1]\tLoss: 2.137\n","Epoch [160][0/1]\tLoss: 2.160\n","Epoch [161][0/1]\tLoss: 2.145\n","Epoch [162][0/1]\tLoss: 2.150\n","Epoch [163][0/1]\tLoss: 2.137\n","Epoch [164][0/1]\tLoss: 2.111\n","Epoch [165][0/1]\tLoss: 2.106\n","Epoch [166][0/1]\tLoss: 2.070\n","Epoch [167][0/1]\tLoss: 2.026\n","Epoch [168][0/1]\tLoss: 2.003\n","Epoch [169][0/1]\tLoss: 2.006\n","Epoch [170][0/1]\tLoss: 1.996\n","Epoch [171][0/1]\tLoss: 1.975\n","Epoch [172][0/1]\tLoss: 1.965\n","Epoch [173][0/1]\tLoss: 1.969\n","Epoch [174][0/1]\tLoss: 1.927\n","Epoch [175][0/1]\tLoss: 1.946\n","Epoch [176][0/1]\tLoss: 1.866\n","Epoch [177][0/1]\tLoss: 1.884\n","Epoch [178][0/1]\tLoss: 1.886\n","Epoch [179][0/1]\tLoss: 1.863\n","Epoch [180][0/1]\tLoss: 1.853\n","Epoch [181][0/1]\tLoss: 1.820\n","Epoch [182][0/1]\tLoss: 1.828\n","Epoch [183][0/1]\tLoss: 1.774\n","Epoch [184][0/1]\tLoss: 1.791\n","Epoch [185][0/1]\tLoss: 1.780\n","Epoch [186][0/1]\tLoss: 1.767\n","Epoch [187][0/1]\tLoss: 1.745\n","Epoch [188][0/1]\tLoss: 1.740\n","Epoch [189][0/1]\tLoss: 1.739\n","Epoch [190][0/1]\tLoss: 1.742\n","Epoch [191][0/1]\tLoss: 1.684\n","Epoch [192][0/1]\tLoss: 1.680\n","Epoch [193][0/1]\tLoss: 1.680\n","Epoch [194][0/1]\tLoss: 1.674\n","Epoch [195][0/1]\tLoss: 1.670\n","Epoch [196][0/1]\tLoss: 1.682\n","Epoch [197][0/1]\tLoss: 1.624\n","Epoch [198][0/1]\tLoss: 1.658\n","Epoch [199][0/1]\tLoss: 1.597\n","Epoch [200][0/1]\tLoss: 1.670\n","Saving checkpoint for epoch 200\n","Checkpoint saved with name experiment_vanilla_18824/checkpoint_200.pth.tar\n","Epoch [201][0/1]\tLoss: 1.593\n","Epoch [202][0/1]\tLoss: 1.586\n","Epoch [203][0/1]\tLoss: 1.582\n","Epoch [204][0/1]\tLoss: 1.587\n","Epoch [205][0/1]\tLoss: 1.571\n","Epoch [206][0/1]\tLoss: 1.534\n","Epoch [207][0/1]\tLoss: 1.524\n","Epoch [208][0/1]\tLoss: 1.481\n","Epoch [209][0/1]\tLoss: 1.570\n","Epoch [210][0/1]\tLoss: 1.482\n","Epoch [211][0/1]\tLoss: 1.497\n","Epoch [212][0/1]\tLoss: 1.476\n","Epoch [213][0/1]\tLoss: 1.454\n","Epoch [214][0/1]\tLoss: 1.457\n","Epoch [215][0/1]\tLoss: 1.462\n","Epoch [216][0/1]\tLoss: 1.400\n","Epoch [217][0/1]\tLoss: 1.397\n","Epoch [218][0/1]\tLoss: 1.417\n","Epoch [219][0/1]\tLoss: 1.393\n","Epoch [220][0/1]\tLoss: 1.396\n","Epoch [221][0/1]\tLoss: 1.368\n","Epoch [222][0/1]\tLoss: 1.393\n","Epoch [223][0/1]\tLoss: 1.373\n","Epoch [224][0/1]\tLoss: 1.385\n","Epoch [225][0/1]\tLoss: 1.326\n","Epoch [226][0/1]\tLoss: 1.365\n","Epoch [227][0/1]\tLoss: 1.330\n","Epoch [228][0/1]\tLoss: 1.331\n","Epoch [229][0/1]\tLoss: 1.311\n","Epoch [230][0/1]\tLoss: 1.317\n","Epoch [231][0/1]\tLoss: 1.306\n","Epoch [232][0/1]\tLoss: 1.287\n","Epoch [233][0/1]\tLoss: 1.250\n","Epoch [234][0/1]\tLoss: 1.279\n","Epoch [235][0/1]\tLoss: 1.253\n","Epoch [236][0/1]\tLoss: 1.250\n","Epoch [237][0/1]\tLoss: 1.267\n","Epoch [238][0/1]\tLoss: 1.228\n","Epoch [239][0/1]\tLoss: 1.256\n","Epoch [240][0/1]\tLoss: 1.196\n","Epoch [241][0/1]\tLoss: 1.332\n","Epoch [242][0/1]\tLoss: 1.215\n","Epoch [243][0/1]\tLoss: 1.282\n","Epoch [244][0/1]\tLoss: 1.206\n","Epoch [245][0/1]\tLoss: 1.184\n","Epoch [246][0/1]\tLoss: 1.210\n","Epoch [247][0/1]\tLoss: 1.155\n","Epoch [248][0/1]\tLoss: 1.143\n","Epoch [249][0/1]\tLoss: 1.185\n","Epoch [250][0/1]\tLoss: 1.169\n","Epoch [251][0/1]\tLoss: 1.138\n","Epoch [252][0/1]\tLoss: 1.156\n","Epoch [253][0/1]\tLoss: 1.157\n","Epoch [254][0/1]\tLoss: 1.119\n","Epoch [255][0/1]\tLoss: 1.090\n","Epoch [256][0/1]\tLoss: 1.108\n","Epoch [257][0/1]\tLoss: 1.060\n","Epoch [258][0/1]\tLoss: 1.059\n","Epoch [259][0/1]\tLoss: 1.053\n","Epoch [260][0/1]\tLoss: 1.008\n","Epoch [261][0/1]\tLoss: 1.038\n","Epoch [262][0/1]\tLoss: 1.021\n","Epoch [263][0/1]\tLoss: 0.984\n","Epoch [264][0/1]\tLoss: 0.989\n","Epoch [265][0/1]\tLoss: 0.953\n","Epoch [266][0/1]\tLoss: 0.948\n","Epoch [267][0/1]\tLoss: 1.014\n","Epoch [268][0/1]\tLoss: 0.988\n","Epoch [269][0/1]\tLoss: 0.994\n","Epoch [270][0/1]\tLoss: 0.943\n","Epoch [271][0/1]\tLoss: 0.882\n","Epoch [272][0/1]\tLoss: 0.928\n","Epoch [273][0/1]\tLoss: 0.896\n","Epoch [274][0/1]\tLoss: 0.932\n","Epoch [275][0/1]\tLoss: 0.920\n","Epoch [276][0/1]\tLoss: 0.872\n","Epoch [277][0/1]\tLoss: 0.867\n","Epoch [278][0/1]\tLoss: 0.857\n","Epoch [279][0/1]\tLoss: 0.815\n","Epoch [280][0/1]\tLoss: 0.863\n","Epoch [281][0/1]\tLoss: 0.848\n","Epoch [282][0/1]\tLoss: 0.806\n","Epoch [283][0/1]\tLoss: 0.872\n","Epoch [284][0/1]\tLoss: 0.801\n","Epoch [285][0/1]\tLoss: 0.834\n","Epoch [286][0/1]\tLoss: 0.815\n","Epoch [287][0/1]\tLoss: 0.776\n","Epoch [288][0/1]\tLoss: 0.765\n","Epoch [289][0/1]\tLoss: 0.792\n","Epoch [290][0/1]\tLoss: 0.750\n","Epoch [291][0/1]\tLoss: 0.741\n","Epoch [292][0/1]\tLoss: 0.727\n","Epoch [293][0/1]\tLoss: 0.727\n","Epoch [294][0/1]\tLoss: 0.722\n","Epoch [295][0/1]\tLoss: 0.740\n","Epoch [296][0/1]\tLoss: 0.713\n","Epoch [297][0/1]\tLoss: 0.679\n","Epoch [298][0/1]\tLoss: 0.735\n","Epoch [299][0/1]\tLoss: 0.708\n","Epoch [300][0/1]\tLoss: 0.685\n","Saving checkpoint for epoch 300\n","Checkpoint saved with name experiment_vanilla_18824/checkpoint_300.pth.tar\n","Epoch [301][0/1]\tLoss: 0.723\n","Epoch [302][0/1]\tLoss: 0.699\n","Epoch [303][0/1]\tLoss: 0.678\n","Epoch [304][0/1]\tLoss: 0.692\n","Epoch [305][0/1]\tLoss: 0.675\n","Epoch [306][0/1]\tLoss: 0.631\n","Epoch [307][0/1]\tLoss: 0.617\n","Epoch [308][0/1]\tLoss: 0.635\n","Epoch [309][0/1]\tLoss: 0.661\n","Epoch [310][0/1]\tLoss: 0.624\n","Epoch [311][0/1]\tLoss: 0.731\n","Epoch [312][0/1]\tLoss: 0.647\n","Epoch [313][0/1]\tLoss: 0.636\n","Epoch [314][0/1]\tLoss: 0.640\n","Epoch [315][0/1]\tLoss: 0.612\n","Epoch [316][0/1]\tLoss: 0.581\n","Epoch [317][0/1]\tLoss: 0.578\n","Epoch [318][0/1]\tLoss: 0.602\n","Epoch [319][0/1]\tLoss: 0.566\n","Epoch [320][0/1]\tLoss: 0.552\n","Epoch [321][0/1]\tLoss: 0.582\n","Epoch [322][0/1]\tLoss: 0.518\n","Epoch [323][0/1]\tLoss: 0.574\n","Epoch [324][0/1]\tLoss: 0.516\n","Epoch [325][0/1]\tLoss: 0.555\n","Epoch [326][0/1]\tLoss: 0.516\n","Epoch [327][0/1]\tLoss: 0.540\n","Epoch [328][0/1]\tLoss: 0.492\n","Epoch [329][0/1]\tLoss: 0.502\n","Epoch [330][0/1]\tLoss: 0.508\n","Epoch [331][0/1]\tLoss: 0.524\n","Epoch [332][0/1]\tLoss: 0.477\n","Epoch [333][0/1]\tLoss: 0.488\n","Epoch [334][0/1]\tLoss: 0.497\n","Epoch [335][0/1]\tLoss: 0.474\n","Epoch [336][0/1]\tLoss: 0.495\n","Epoch [337][0/1]\tLoss: 0.424\n","Epoch [338][0/1]\tLoss: 0.483\n","Epoch [339][0/1]\tLoss: 0.445\n","Epoch [340][0/1]\tLoss: 0.476\n","Epoch [341][0/1]\tLoss: 0.450\n","Epoch [342][0/1]\tLoss: 0.458\n","Epoch [343][0/1]\tLoss: 0.475\n","Epoch [344][0/1]\tLoss: 0.407\n","Epoch [345][0/1]\tLoss: 0.462\n","Epoch [346][0/1]\tLoss: 0.438\n","Epoch [347][0/1]\tLoss: 0.436\n","Epoch [348][0/1]\tLoss: 0.461\n","Epoch [349][0/1]\tLoss: 0.406\n","Epoch [350][0/1]\tLoss: 0.429\n","Epoch [351][0/1]\tLoss: 0.384\n","Epoch [352][0/1]\tLoss: 0.429\n","Epoch [353][0/1]\tLoss: 0.379\n","Epoch [354][0/1]\tLoss: 0.404\n","Epoch [355][0/1]\tLoss: 0.370\n","Epoch [356][0/1]\tLoss: 0.404\n","Epoch [357][0/1]\tLoss: 0.369\n","Epoch [358][0/1]\tLoss: 0.406\n","Epoch [359][0/1]\tLoss: 0.373\n","Epoch [360][0/1]\tLoss: 0.370\n","Epoch [361][0/1]\tLoss: 0.404\n","Epoch [362][0/1]\tLoss: 0.382\n","Epoch [363][0/1]\tLoss: 0.413\n","Epoch [364][0/1]\tLoss: 0.324\n","Epoch [365][0/1]\tLoss: 0.442\n","Epoch [366][0/1]\tLoss: 0.345\n","Epoch [367][0/1]\tLoss: 0.372\n","Epoch [368][0/1]\tLoss: 0.365\n","Epoch [369][0/1]\tLoss: 0.320\n","Epoch [370][0/1]\tLoss: 0.329\n","Epoch [371][0/1]\tLoss: 0.330\n","Epoch [372][0/1]\tLoss: 0.324\n","Epoch [373][0/1]\tLoss: 0.343\n","Epoch [374][0/1]\tLoss: 0.354\n","Epoch [375][0/1]\tLoss: 0.302\n","Epoch [376][0/1]\tLoss: 0.318\n","Epoch [377][0/1]\tLoss: 0.329\n","Epoch [378][0/1]\tLoss: 0.289\n","Epoch [379][0/1]\tLoss: 0.298\n","Epoch [380][0/1]\tLoss: 0.305\n","Epoch [381][0/1]\tLoss: 0.281\n","Epoch [382][0/1]\tLoss: 0.316\n","Epoch [383][0/1]\tLoss: 0.318\n","Epoch [384][0/1]\tLoss: 0.299\n","Epoch [385][0/1]\tLoss: 0.288\n","Epoch [386][0/1]\tLoss: 0.285\n","Epoch [387][0/1]\tLoss: 0.294\n","Epoch [388][0/1]\tLoss: 0.283\n","Epoch [389][0/1]\tLoss: 0.262\n","Epoch [390][0/1]\tLoss: 0.275\n","Epoch [391][0/1]\tLoss: 0.268\n","Epoch [392][0/1]\tLoss: 0.249\n","Epoch [393][0/1]\tLoss: 0.263\n","Epoch [394][0/1]\tLoss: 0.246\n","Epoch [395][0/1]\tLoss: 0.242\n","Epoch [396][0/1]\tLoss: 0.241\n","Epoch [397][0/1]\tLoss: 0.245\n","Epoch [398][0/1]\tLoss: 0.249\n","Epoch [399][0/1]\tLoss: 0.231\n","Epoch [400][0/1]\tLoss: 0.227\n","Saving checkpoint for epoch 400\n","Checkpoint saved with name experiment_vanilla_18824/checkpoint_400.pth.tar\n","Epoch [401][0/1]\tLoss: 0.239\n","Epoch [402][0/1]\tLoss: 0.222\n","Epoch [403][0/1]\tLoss: 0.220\n","Epoch [404][0/1]\tLoss: 0.217\n","Epoch [405][0/1]\tLoss: 0.244\n","Epoch [406][0/1]\tLoss: 0.221\n","Epoch [407][0/1]\tLoss: 0.246\n","Epoch [408][0/1]\tLoss: 0.240\n","Epoch [409][0/1]\tLoss: 0.226\n","Epoch [410][0/1]\tLoss: 0.225\n","Epoch [411][0/1]\tLoss: 0.210\n","Epoch [412][0/1]\tLoss: 0.213\n","Epoch [413][0/1]\tLoss: 0.210\n","Epoch [414][0/1]\tLoss: 0.207\n","Epoch [415][0/1]\tLoss: 0.227\n","Epoch [416][0/1]\tLoss: 0.189\n","Epoch [417][0/1]\tLoss: 0.210\n","Epoch [418][0/1]\tLoss: 0.233\n","Epoch [419][0/1]\tLoss: 0.216\n","Epoch [420][0/1]\tLoss: 0.204\n","Epoch [421][0/1]\tLoss: 0.189\n","Epoch [422][0/1]\tLoss: 0.204\n","Epoch [423][0/1]\tLoss: 0.191\n","Epoch [424][0/1]\tLoss: 0.201\n","Epoch [425][0/1]\tLoss: 0.191\n","Epoch [426][0/1]\tLoss: 0.170\n","Epoch [427][0/1]\tLoss: 0.188\n","Epoch [428][0/1]\tLoss: 0.187\n","Epoch [429][0/1]\tLoss: 0.180\n","Epoch [430][0/1]\tLoss: 0.179\n","Epoch [431][0/1]\tLoss: 0.178\n","Epoch [432][0/1]\tLoss: 0.173\n","Epoch [433][0/1]\tLoss: 0.172\n","Epoch [434][0/1]\tLoss: 0.158\n","Epoch [435][0/1]\tLoss: 0.163\n","Epoch [436][0/1]\tLoss: 0.173\n","Epoch [437][0/1]\tLoss: 0.173\n","Epoch [438][0/1]\tLoss: 0.165\n","Epoch [439][0/1]\tLoss: 0.176\n","Epoch [440][0/1]\tLoss: 0.148\n","Epoch [441][0/1]\tLoss: 0.158\n","Epoch [442][0/1]\tLoss: 0.179\n","Epoch [443][0/1]\tLoss: 0.163\n","Epoch [444][0/1]\tLoss: 0.166\n","Epoch [445][0/1]\tLoss: 0.171\n","Epoch [446][0/1]\tLoss: 0.153\n","Epoch [447][0/1]\tLoss: 0.158\n","Epoch [448][0/1]\tLoss: 0.156\n","Epoch [449][0/1]\tLoss: 0.147\n","Epoch [450][0/1]\tLoss: 0.166\n","Epoch [451][0/1]\tLoss: 0.160\n","Epoch [452][0/1]\tLoss: 0.154\n","Epoch [453][0/1]\tLoss: 0.150\n","Epoch [454][0/1]\tLoss: 0.142\n","Epoch [455][0/1]\tLoss: 0.144\n","Epoch [456][0/1]\tLoss: 0.145\n","Epoch [457][0/1]\tLoss: 0.140\n","Epoch [458][0/1]\tLoss: 0.150\n","Epoch [459][0/1]\tLoss: 0.141\n","Epoch [460][0/1]\tLoss: 0.149\n","Epoch [461][0/1]\tLoss: 0.139\n","Epoch [462][0/1]\tLoss: 0.147\n","Epoch [463][0/1]\tLoss: 0.149\n","Epoch [464][0/1]\tLoss: 0.137\n","Epoch [465][0/1]\tLoss: 0.143\n","Epoch [466][0/1]\tLoss: 0.136\n","Epoch [467][0/1]\tLoss: 0.140\n","Epoch [468][0/1]\tLoss: 0.132\n","Epoch [469][0/1]\tLoss: 0.134\n","Epoch [470][0/1]\tLoss: 0.137\n","Epoch [471][0/1]\tLoss: 0.140\n","Epoch [472][0/1]\tLoss: 0.128\n","Epoch [473][0/1]\tLoss: 0.137\n","Epoch [474][0/1]\tLoss: 0.134\n","Epoch [475][0/1]\tLoss: 0.126\n","Epoch [476][0/1]\tLoss: 0.146\n","Epoch [477][0/1]\tLoss: 0.134\n","Epoch [478][0/1]\tLoss: 0.134\n","Epoch [479][0/1]\tLoss: 0.135\n","Epoch [480][0/1]\tLoss: 0.134\n","Epoch [481][0/1]\tLoss: 0.135\n","Epoch [482][0/1]\tLoss: 0.128\n","Epoch [483][0/1]\tLoss: 0.140\n","Epoch [484][0/1]\tLoss: 0.152\n","Epoch [485][0/1]\tLoss: 0.134\n","Epoch [486][0/1]\tLoss: 0.141\n","Epoch [487][0/1]\tLoss: 0.132\n","Epoch [488][0/1]\tLoss: 0.189\n","Epoch [489][0/1]\tLoss: 0.359\n","Epoch [490][0/1]\tLoss: 0.670\n","Epoch [491][0/1]\tLoss: 0.672\n","Epoch [492][0/1]\tLoss: 1.421\n","Epoch [493][0/1]\tLoss: 0.298\n","Epoch [494][0/1]\tLoss: 1.034\n","Epoch [495][0/1]\tLoss: 0.668\n","Epoch [496][0/1]\tLoss: 0.615\n","Epoch [497][0/1]\tLoss: 0.483\n","Epoch [498][0/1]\tLoss: 0.408\n","Epoch [499][0/1]\tLoss: 0.496\n","Epoch [500][0/1]\tLoss: 0.345\n","Saving checkpoint for epoch 500\n","Checkpoint saved with name experiment_vanilla_18824/checkpoint_500.pth.tar\n","Epoch [501][0/1]\tLoss: 0.298\n","Epoch [502][0/1]\tLoss: 0.351\n","Epoch [503][0/1]\tLoss: 0.308\n","Epoch [504][0/1]\tLoss: 0.233\n","Epoch [505][0/1]\tLoss: 0.245\n","Epoch [506][0/1]\tLoss: 0.256\n","Epoch [507][0/1]\tLoss: 0.233\n","Epoch [508][0/1]\tLoss: 0.230\n","Epoch [509][0/1]\tLoss: 0.224\n","Epoch [510][0/1]\tLoss: 0.176\n","Epoch [511][0/1]\tLoss: 0.170\n","Epoch [512][0/1]\tLoss: 0.184\n","Epoch [513][0/1]\tLoss: 0.164\n","Epoch [514][0/1]\tLoss: 0.180\n","Epoch [515][0/1]\tLoss: 0.164\n","Epoch [516][0/1]\tLoss: 0.138\n","Epoch [517][0/1]\tLoss: 0.140\n","Epoch [518][0/1]\tLoss: 0.142\n","Epoch [519][0/1]\tLoss: 0.141\n","Epoch [520][0/1]\tLoss: 0.145\n","Epoch [521][0/1]\tLoss: 0.134\n","Epoch [522][0/1]\tLoss: 0.129\n","Epoch [523][0/1]\tLoss: 0.139\n","Epoch [524][0/1]\tLoss: 0.122\n","Epoch [525][0/1]\tLoss: 0.120\n","Epoch [526][0/1]\tLoss: 0.129\n","Epoch [527][0/1]\tLoss: 0.127\n","Epoch [528][0/1]\tLoss: 0.118\n","Epoch [529][0/1]\tLoss: 0.119\n","Epoch [530][0/1]\tLoss: 0.113\n","Epoch [531][0/1]\tLoss: 0.112\n","Epoch [532][0/1]\tLoss: 0.115\n","Epoch [533][0/1]\tLoss: 0.114\n","Epoch [534][0/1]\tLoss: 0.115\n","Epoch [535][0/1]\tLoss: 0.107\n","Epoch [536][0/1]\tLoss: 0.103\n","Epoch [537][0/1]\tLoss: 0.107\n","Epoch [538][0/1]\tLoss: 0.106\n","Epoch [539][0/1]\tLoss: 0.105\n","Epoch [540][0/1]\tLoss: 0.106\n","Epoch [541][0/1]\tLoss: 0.104\n","Epoch [542][0/1]\tLoss: 0.103\n","Epoch [543][0/1]\tLoss: 0.102\n","Epoch [544][0/1]\tLoss: 0.096\n","Epoch [545][0/1]\tLoss: 0.100\n","Epoch [546][0/1]\tLoss: 0.103\n","Epoch [547][0/1]\tLoss: 0.104\n","Epoch [548][0/1]\tLoss: 0.102\n","Epoch [549][0/1]\tLoss: 0.095\n","Epoch [550][0/1]\tLoss: 0.094\n","Epoch [551][0/1]\tLoss: 0.099\n","Epoch [552][0/1]\tLoss: 0.095\n","Epoch [553][0/1]\tLoss: 0.097\n","Epoch [554][0/1]\tLoss: 0.099\n","Epoch [555][0/1]\tLoss: 0.095\n","Epoch [556][0/1]\tLoss: 0.095\n","Epoch [557][0/1]\tLoss: 0.093\n","Epoch [558][0/1]\tLoss: 0.089\n","Epoch [559][0/1]\tLoss: 0.091\n","Epoch [560][0/1]\tLoss: 0.092\n","Epoch [561][0/1]\tLoss: 0.094\n","Epoch [562][0/1]\tLoss: 0.089\n","Epoch [563][0/1]\tLoss: 0.090\n","Epoch [564][0/1]\tLoss: 0.095\n","Epoch [565][0/1]\tLoss: 0.092\n","Epoch [566][0/1]\tLoss: 0.092\n","Epoch [567][0/1]\tLoss: 0.084\n","Epoch [568][0/1]\tLoss: 0.093\n","Epoch [569][0/1]\tLoss: 0.085\n","Epoch [570][0/1]\tLoss: 0.089\n","Epoch [571][0/1]\tLoss: 0.086\n","Epoch [572][0/1]\tLoss: 0.091\n","Epoch [573][0/1]\tLoss: 0.086\n","Epoch [574][0/1]\tLoss: 0.086\n","Epoch [575][0/1]\tLoss: 0.086\n","Epoch [576][0/1]\tLoss: 0.083\n","Epoch [577][0/1]\tLoss: 0.088\n","Epoch [578][0/1]\tLoss: 0.083\n","Epoch [579][0/1]\tLoss: 0.088\n","Epoch [580][0/1]\tLoss: 0.084\n","Epoch [581][0/1]\tLoss: 0.083\n","Epoch [582][0/1]\tLoss: 0.081\n","Epoch [583][0/1]\tLoss: 0.086\n","Epoch [584][0/1]\tLoss: 0.086\n","Epoch [585][0/1]\tLoss: 0.086\n","Epoch [586][0/1]\tLoss: 0.078\n","Epoch [587][0/1]\tLoss: 0.079\n","Epoch [588][0/1]\tLoss: 0.084\n","Epoch [589][0/1]\tLoss: 0.081\n","Epoch [590][0/1]\tLoss: 0.085\n","Epoch [591][0/1]\tLoss: 0.081\n","Epoch [592][0/1]\tLoss: 0.080\n","Epoch [593][0/1]\tLoss: 0.084\n","Epoch [594][0/1]\tLoss: 0.080\n","Epoch [595][0/1]\tLoss: 0.086\n","Epoch [596][0/1]\tLoss: 0.079\n","Epoch [597][0/1]\tLoss: 0.080\n","Epoch [598][0/1]\tLoss: 0.084\n","Epoch [599][0/1]\tLoss: 0.089\n","Epoch [600][0/1]\tLoss: 0.093\n","Saving checkpoint for epoch 600\n","Checkpoint saved with name experiment_vanilla_18824/checkpoint_600.pth.tar\n","Epoch [601][0/1]\tLoss: 0.115\n","Epoch [602][0/1]\tLoss: 0.091\n","Epoch [603][0/1]\tLoss: 0.109\n","Epoch [604][0/1]\tLoss: 0.112\n","Epoch [605][0/1]\tLoss: 0.141\n","Epoch [606][0/1]\tLoss: 0.117\n","Epoch [607][0/1]\tLoss: 0.118\n","Epoch [608][0/1]\tLoss: 0.112\n","Epoch [609][0/1]\tLoss: 0.112\n","Epoch [610][0/1]\tLoss: 0.105\n","Epoch [611][0/1]\tLoss: 0.109\n","Epoch [612][0/1]\tLoss: 0.123\n","Epoch [613][0/1]\tLoss: 0.101\n","Epoch [614][0/1]\tLoss: 0.099\n","Epoch [615][0/1]\tLoss: 0.113\n","Epoch [616][0/1]\tLoss: 0.112\n","Epoch [617][0/1]\tLoss: 0.100\n","Epoch [618][0/1]\tLoss: 0.100\n","Epoch [619][0/1]\tLoss: 0.097\n","Epoch [620][0/1]\tLoss: 0.099\n","Epoch [621][0/1]\tLoss: 0.100\n","Epoch [622][0/1]\tLoss: 0.095\n","Epoch [623][0/1]\tLoss: 0.113\n","Epoch [624][0/1]\tLoss: 0.092\n","Epoch [625][0/1]\tLoss: 0.100\n","Epoch [626][0/1]\tLoss: 0.099\n","Epoch [627][0/1]\tLoss: 0.090\n","Epoch [628][0/1]\tLoss: 0.103\n","Epoch [629][0/1]\tLoss: 0.097\n","Epoch [630][0/1]\tLoss: 0.132\n","Epoch [631][0/1]\tLoss: 0.106\n","Epoch [632][0/1]\tLoss: 0.110\n","Epoch [633][0/1]\tLoss: 0.109\n","Epoch [634][0/1]\tLoss: 0.108\n","Epoch [635][0/1]\tLoss: 0.110\n","Epoch [636][0/1]\tLoss: 0.101\n","Epoch [637][0/1]\tLoss: 0.098\n","Epoch [638][0/1]\tLoss: 0.092\n","Epoch [639][0/1]\tLoss: 0.092\n","Epoch [640][0/1]\tLoss: 0.100\n","Epoch [641][0/1]\tLoss: 0.098\n","Epoch [642][0/1]\tLoss: 0.091\n","Epoch [643][0/1]\tLoss: 0.089\n","Epoch [644][0/1]\tLoss: 0.091\n","Epoch [645][0/1]\tLoss: 0.085\n","Epoch [646][0/1]\tLoss: 0.089\n","Epoch [647][0/1]\tLoss: 0.092\n","Epoch [648][0/1]\tLoss: 0.082\n","Epoch [649][0/1]\tLoss: 0.086\n","Epoch [650][0/1]\tLoss: 0.082\n","Epoch [651][0/1]\tLoss: 0.084\n","Epoch [652][0/1]\tLoss: 0.077\n","Epoch [653][0/1]\tLoss: 0.094\n","Epoch [654][0/1]\tLoss: 0.089\n","Epoch [655][0/1]\tLoss: 0.076\n","Epoch [656][0/1]\tLoss: 0.087\n","Epoch [657][0/1]\tLoss: 0.082\n","Epoch [658][0/1]\tLoss: 0.078\n","Epoch [659][0/1]\tLoss: 0.082\n","Epoch [660][0/1]\tLoss: 0.085\n","Epoch [661][0/1]\tLoss: 0.081\n","Epoch [662][0/1]\tLoss: 0.086\n","Epoch [663][0/1]\tLoss: 0.076\n","Epoch [664][0/1]\tLoss: 0.120\n","Epoch [665][0/1]\tLoss: 0.114\n","Epoch [666][0/1]\tLoss: 0.111\n","Epoch [667][0/1]\tLoss: 0.122\n","Epoch [668][0/1]\tLoss: 0.105\n","Epoch [669][0/1]\tLoss: 0.152\n","Epoch [670][0/1]\tLoss: 0.148\n","Epoch [671][0/1]\tLoss: 0.151\n","Epoch [672][0/1]\tLoss: 0.139\n","Epoch [673][0/1]\tLoss: 0.130\n","Epoch [674][0/1]\tLoss: 0.132\n","Epoch [675][0/1]\tLoss: 0.131\n","Epoch [676][0/1]\tLoss: 0.126\n","Epoch [677][0/1]\tLoss: 0.142\n","Epoch [678][0/1]\tLoss: 0.104\n","Epoch [679][0/1]\tLoss: 0.121\n","Epoch [680][0/1]\tLoss: 0.114\n","Epoch [681][0/1]\tLoss: 0.119\n","Epoch [682][0/1]\tLoss: 0.119\n","Epoch [683][0/1]\tLoss: 0.101\n","Epoch [684][0/1]\tLoss: 0.116\n","Epoch [685][0/1]\tLoss: 0.105\n","Epoch [686][0/1]\tLoss: 0.113\n","Epoch [687][0/1]\tLoss: 0.098\n","Epoch [688][0/1]\tLoss: 0.090\n","Epoch [689][0/1]\tLoss: 0.090\n","Epoch [690][0/1]\tLoss: 0.092\n","Epoch [691][0/1]\tLoss: 0.096\n","Epoch [692][0/1]\tLoss: 0.083\n","Epoch [693][0/1]\tLoss: 0.085\n","Epoch [694][0/1]\tLoss: 0.084\n","Epoch [695][0/1]\tLoss: 0.076\n","Epoch [696][0/1]\tLoss: 0.097\n","Epoch [697][0/1]\tLoss: 0.077\n","Epoch [698][0/1]\tLoss: 0.090\n","Epoch [699][0/1]\tLoss: 0.082\n","Epoch [700][0/1]\tLoss: 0.081\n","Saving checkpoint for epoch 700\n","Checkpoint saved with name experiment_vanilla_18824/checkpoint_700.pth.tar\n","Epoch [701][0/1]\tLoss: 0.084\n","Epoch [702][0/1]\tLoss: 0.079\n","Epoch [703][0/1]\tLoss: 0.073\n","Epoch [704][0/1]\tLoss: 0.081\n","Epoch [705][0/1]\tLoss: 0.076\n","Epoch [706][0/1]\tLoss: 0.077\n","Epoch [707][0/1]\tLoss: 0.074\n","Epoch [708][0/1]\tLoss: 0.070\n","Epoch [709][0/1]\tLoss: 0.078\n","Epoch [710][0/1]\tLoss: 0.071\n","Epoch [711][0/1]\tLoss: 0.077\n","Epoch [712][0/1]\tLoss: 0.076\n","Epoch [713][0/1]\tLoss: 0.068\n","Epoch [714][0/1]\tLoss: 0.074\n","Epoch [715][0/1]\tLoss: 0.074\n","Epoch [716][0/1]\tLoss: 0.074\n","Epoch [717][0/1]\tLoss: 0.069\n","Epoch [718][0/1]\tLoss: 0.073\n","Epoch [719][0/1]\tLoss: 0.077\n","Epoch [720][0/1]\tLoss: 0.074\n","Epoch [721][0/1]\tLoss: 0.087\n","Epoch [722][0/1]\tLoss: 0.082\n","Epoch [723][0/1]\tLoss: 0.107\n","Epoch [724][0/1]\tLoss: 0.099\n","Epoch [725][0/1]\tLoss: 0.138\n","Epoch [726][0/1]\tLoss: 0.118\n","Epoch [727][0/1]\tLoss: 0.117\n","Epoch [728][0/1]\tLoss: 0.147\n","Epoch [729][0/1]\tLoss: 0.088\n","Epoch [730][0/1]\tLoss: 0.110\n","Epoch [731][0/1]\tLoss: 0.101\n","Epoch [732][0/1]\tLoss: 0.107\n","Epoch [733][0/1]\tLoss: 0.111\n","Epoch [734][0/1]\tLoss: 0.105\n","Epoch [735][0/1]\tLoss: 0.104\n","Epoch [736][0/1]\tLoss: 0.091\n","Epoch [737][0/1]\tLoss: 0.105\n","Epoch [738][0/1]\tLoss: 0.090\n","Epoch [739][0/1]\tLoss: 0.113\n","Epoch [740][0/1]\tLoss: 0.092\n","Epoch [741][0/1]\tLoss: 0.318\n","Epoch [742][0/1]\tLoss: 0.214\n","Epoch [743][0/1]\tLoss: 0.186\n","Epoch [744][0/1]\tLoss: 0.226\n","Epoch [745][0/1]\tLoss: 0.482\n","Epoch [746][0/1]\tLoss: 0.375\n","Epoch [747][0/1]\tLoss: 0.376\n","Epoch [748][0/1]\tLoss: 0.225\n","Epoch [749][0/1]\tLoss: 0.548\n","Epoch [750][0/1]\tLoss: 0.295\n","Epoch [751][0/1]\tLoss: 0.322\n","Epoch [752][0/1]\tLoss: 0.356\n","Epoch [753][0/1]\tLoss: 0.205\n","Epoch [754][0/1]\tLoss: 0.258\n","Epoch [755][0/1]\tLoss: 0.200\n","Epoch [756][0/1]\tLoss: 0.187\n","Epoch [757][0/1]\tLoss: 0.199\n","Epoch [758][0/1]\tLoss: 0.156\n","Epoch [759][0/1]\tLoss: 0.150\n","Epoch [760][0/1]\tLoss: 0.137\n","Epoch [761][0/1]\tLoss: 0.136\n","Epoch [762][0/1]\tLoss: 0.120\n","Epoch [763][0/1]\tLoss: 0.138\n","Epoch [764][0/1]\tLoss: 0.114\n","Epoch [765][0/1]\tLoss: 0.109\n","Epoch [766][0/1]\tLoss: 0.109\n","Epoch [767][0/1]\tLoss: 0.131\n","Epoch [768][0/1]\tLoss: 0.113\n","Epoch [769][0/1]\tLoss: 0.101\n","Epoch [770][0/1]\tLoss: 0.106\n","Epoch [771][0/1]\tLoss: 0.117\n","Epoch [772][0/1]\tLoss: 0.087\n","Epoch [773][0/1]\tLoss: 0.099\n","Epoch [774][0/1]\tLoss: 0.120\n","Epoch [775][0/1]\tLoss: 0.104\n","Epoch [776][0/1]\tLoss: 0.098\n","Epoch [777][0/1]\tLoss: 0.088\n","Epoch [778][0/1]\tLoss: 0.102\n","Epoch [779][0/1]\tLoss: 0.091\n","Epoch [780][0/1]\tLoss: 0.087\n","Epoch [781][0/1]\tLoss: 0.089\n","Epoch [782][0/1]\tLoss: 0.085\n","Epoch [783][0/1]\tLoss: 0.080\n","Epoch [784][0/1]\tLoss: 0.099\n","Epoch [785][0/1]\tLoss: 0.082\n","Epoch [786][0/1]\tLoss: 0.077\n","Epoch [787][0/1]\tLoss: 0.086\n","Epoch [788][0/1]\tLoss: 0.085\n","Epoch [789][0/1]\tLoss: 0.074\n","Epoch [790][0/1]\tLoss: 0.105\n","Epoch [791][0/1]\tLoss: 0.105\n","Epoch [792][0/1]\tLoss: 0.101\n","Epoch [793][0/1]\tLoss: 0.088\n","Epoch [794][0/1]\tLoss: 0.082\n","Epoch [795][0/1]\tLoss: 0.091\n","Epoch [796][0/1]\tLoss: 0.105\n","Epoch [797][0/1]\tLoss: 0.083\n","Epoch [798][0/1]\tLoss: 0.086\n","Epoch [799][0/1]\tLoss: 0.095\n"]}],"source":["directory = 'experiment_vanilla_18824'\n","create_directory(directory)\n","\n","d_model = 4096\n","heads = 32\n","num_layers = 15\n","epochs = 800\n","\n","loss_history_vanilla_transformer = []\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","with open('WORDMAP_corpus_KBFILKOM.json', 'r') as j:\n","    word_map = json.load(j)\n","    \n","transformer = Transformer(d_model = d_model, heads = heads, num_layers = num_layers, word_map = word_map, max_len=95)\n","transformer = transformer.to(device)\n","adam_optimizer = torch.optim.Adam(transformer.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-9)\n","transformer_optimizer = AdamWarmup(model_size = d_model, warmup_steps = 4000, optimizer = adam_optimizer)\n","criterion = LossWithLS(len(word_map), 0.2)\n","\n","for epoch in range(epochs):\n","    loss_train = train(train_loader, transformer, criterion, epoch)\n","\n","    state = {'epoch': epoch, 'transformer': transformer, 'transformer_optimizer': transformer_optimizer}\n","    \n","    if epoch % 100 == 0:\n","        print('Saving checkpoint for epoch {}'.format(epoch))\n","        torch.save(state, directory + '/checkpoint_' + str(epoch) +'.pth.tar')\n","        print('Checkpoint saved with name {}'.format(directory + '/checkpoint_' + str(epoch) +'.pth.tar'))\n","\n","    loss_history_vanilla_transformer.append(loss_train)\n","\n","import yaml \n","\n","# with open(directory + '/loss_history_vanilla_transformer.yaml', 'w') as file:\n","#     yaml.dump(loss_history_vanilla_transformer, file)"]},{"cell_type":"code","execution_count":44,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[56, 25]], device='cuda:0')\n","tensor([[[[True, True]]]], device='cuda:0')\n","a bachtiar nama lengkap dreng departemen teknik informatika program studi s2 ilmu teknik komputer bidang penelitian computing engineering teknik intelligent system data mining fakultas filkom teknik teknologi program intelligent 3 3 intelligent fakultas fakultas fakultas filkom teknik teknologi program intelligent nama 2011 system perangkat lunak nama dari menjadi 10 teknik desember 2014 no tentang nama dikti ptiik rujukan fakultas filkom teknik teknologi program intelligent nomor organisasi tata teknik departemen 3 3 fakultas filkom teknik teknologi program intelligent nomor organisasi tata teknik departemen 3 3 3 3 3 3 3\n"]}],"source":["question = \"visi filkom\" \n","max_len = 90\n","enc_qus = [word_map.get(word, word_map['<unk>']) for word in question.split()]\n","question = torch.LongTensor(enc_qus).to(device).unsqueeze(0)\n","print(question)\n","question_mask = (question!=0).to(device).unsqueeze(1).unsqueeze(1)  \n","print(question_mask)\n","sentence = evaluate(transformer, question, question_mask, int(max_len), word_map)\n","print(sentence)"]},{"cell_type":"markdown","metadata":{},"source":["## Vanilla without Regularization"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Directory already exists at transformers_vanillanoreg_01724\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [0][0/1]\tLoss: 3.433\n","Epoch [1][0/1]\tLoss: 3.418\n","Epoch [2][0/1]\tLoss: 3.404\n","Epoch [3][0/1]\tLoss: 3.411\n","Epoch [4][0/1]\tLoss: 3.400\n","Epoch [5][0/1]\tLoss: 3.404\n","Epoch [6][0/1]\tLoss: 3.394\n","Epoch [7][0/1]\tLoss: 3.370\n","Epoch [8][0/1]\tLoss: 3.369\n","Epoch [9][0/1]\tLoss: 3.357\n","Epoch [10][0/1]\tLoss: 3.335\n","Epoch [11][0/1]\tLoss: 3.349\n","Epoch [12][0/1]\tLoss: 3.301\n","Epoch [13][0/1]\tLoss: 3.271\n","Epoch [14][0/1]\tLoss: 3.244\n","Epoch [15][0/1]\tLoss: 3.230\n","Epoch [16][0/1]\tLoss: 3.206\n","Epoch [17][0/1]\tLoss: 3.190\n","Epoch [18][0/1]\tLoss: 3.190\n","Epoch [19][0/1]\tLoss: 3.146\n","Epoch [20][0/1]\tLoss: 3.145\n","Epoch [21][0/1]\tLoss: 3.127\n","Epoch [22][0/1]\tLoss: 3.116\n","Epoch [23][0/1]\tLoss: 3.093\n","Epoch [24][0/1]\tLoss: 3.075\n","Epoch [25][0/1]\tLoss: 3.082\n","Epoch [26][0/1]\tLoss: 3.084\n","Epoch [27][0/1]\tLoss: 3.081\n","Epoch [28][0/1]\tLoss: 3.054\n","Epoch [29][0/1]\tLoss: 3.049\n","Epoch [30][0/1]\tLoss: 3.040\n","Epoch [31][0/1]\tLoss: 3.035\n","Epoch [32][0/1]\tLoss: 3.015\n","Epoch [33][0/1]\tLoss: 3.007\n","Epoch [34][0/1]\tLoss: 3.001\n","Epoch [35][0/1]\tLoss: 3.010\n","Epoch [36][0/1]\tLoss: 2.994\n","Epoch [37][0/1]\tLoss: 2.996\n","Epoch [38][0/1]\tLoss: 2.995\n","Epoch [39][0/1]\tLoss: 3.004\n","Epoch [40][0/1]\tLoss: 2.952\n","Epoch [41][0/1]\tLoss: 2.975\n","Epoch [42][0/1]\tLoss: 2.984\n","Epoch [43][0/1]\tLoss: 2.979\n","Epoch [44][0/1]\tLoss: 2.961\n","Epoch [45][0/1]\tLoss: 2.990\n","Epoch [46][0/1]\tLoss: 2.958\n","Epoch [47][0/1]\tLoss: 2.982\n","Epoch [48][0/1]\tLoss: 2.979\n","Epoch [49][0/1]\tLoss: 2.997\n","Epoch [50][0/1]\tLoss: 2.994\n","Epoch [51][0/1]\tLoss: 2.978\n","Epoch [52][0/1]\tLoss: 2.980\n","Epoch [53][0/1]\tLoss: 2.980\n","Epoch [54][0/1]\tLoss: 2.969\n","Epoch [55][0/1]\tLoss: 2.961\n","Epoch [56][0/1]\tLoss: 2.965\n","Epoch [57][0/1]\tLoss: 2.963\n","Epoch [58][0/1]\tLoss: 2.941\n","Epoch [59][0/1]\tLoss: 2.971\n","Epoch [60][0/1]\tLoss: 2.969\n","Epoch [61][0/1]\tLoss: 2.949\n","Epoch [62][0/1]\tLoss: 2.960\n","Epoch [63][0/1]\tLoss: 2.967\n","Epoch [64][0/1]\tLoss: 2.959\n","Epoch [65][0/1]\tLoss: 2.954\n","Epoch [66][0/1]\tLoss: 2.958\n","Epoch [67][0/1]\tLoss: 2.960\n","Epoch [68][0/1]\tLoss: 2.953\n","Epoch [69][0/1]\tLoss: 2.948\n","Epoch [70][0/1]\tLoss: 2.927\n","Epoch [71][0/1]\tLoss: 2.946\n","Epoch [72][0/1]\tLoss: 2.956\n","Epoch [73][0/1]\tLoss: 2.953\n","Epoch [74][0/1]\tLoss: 2.924\n","Epoch [75][0/1]\tLoss: 2.927\n","Epoch [76][0/1]\tLoss: 2.942\n","Epoch [77][0/1]\tLoss: 2.927\n","Epoch [78][0/1]\tLoss: 2.944\n","Epoch [79][0/1]\tLoss: 2.927\n","Epoch [80][0/1]\tLoss: 2.908\n","Epoch [81][0/1]\tLoss: 2.918\n","Epoch [82][0/1]\tLoss: 2.932\n","Epoch [83][0/1]\tLoss: 2.917\n","Epoch [84][0/1]\tLoss: 2.910\n","Epoch [85][0/1]\tLoss: 2.915\n","Epoch [86][0/1]\tLoss: 2.910\n","Epoch [87][0/1]\tLoss: 2.897\n","Epoch [88][0/1]\tLoss: 2.918\n","Epoch [89][0/1]\tLoss: 2.885\n","Epoch [90][0/1]\tLoss: 2.890\n","Epoch [91][0/1]\tLoss: 2.895\n","Epoch [92][0/1]\tLoss: 2.859\n","Epoch [93][0/1]\tLoss: 2.866\n","Epoch [94][0/1]\tLoss: 2.899\n","Epoch [95][0/1]\tLoss: 2.865\n","Epoch [96][0/1]\tLoss: 2.872\n","Epoch [97][0/1]\tLoss: 2.851\n","Epoch [98][0/1]\tLoss: 2.839\n","Epoch [99][0/1]\tLoss: 2.857\n","Epoch [100][0/1]\tLoss: 2.847\n","Epoch [101][0/1]\tLoss: 2.852\n","Epoch [102][0/1]\tLoss: 2.833\n","Epoch [103][0/1]\tLoss: 2.839\n","Epoch [104][0/1]\tLoss: 2.815\n","Epoch [105][0/1]\tLoss: 2.819\n","Epoch [106][0/1]\tLoss: 2.803\n","Epoch [107][0/1]\tLoss: 2.808\n","Epoch [108][0/1]\tLoss: 2.796\n","Epoch [109][0/1]\tLoss: 2.794\n","Epoch [110][0/1]\tLoss: 2.753\n","Epoch [111][0/1]\tLoss: 2.755\n","Epoch [112][0/1]\tLoss: 2.731\n","Epoch [113][0/1]\tLoss: 2.725\n","Epoch [114][0/1]\tLoss: 2.720\n","Epoch [115][0/1]\tLoss: 2.722\n","Epoch [116][0/1]\tLoss: 2.694\n","Epoch [117][0/1]\tLoss: 2.688\n","Epoch [118][0/1]\tLoss: 2.659\n","Epoch [119][0/1]\tLoss: 2.677\n","Epoch [120][0/1]\tLoss: 2.642\n","Epoch [121][0/1]\tLoss: 2.626\n","Epoch [122][0/1]\tLoss: 2.627\n","Epoch [123][0/1]\tLoss: 2.591\n","Epoch [124][0/1]\tLoss: 2.556\n","Epoch [125][0/1]\tLoss: 2.600\n","Epoch [126][0/1]\tLoss: 2.572\n","Epoch [127][0/1]\tLoss: 2.523\n","Epoch [128][0/1]\tLoss: 2.523\n","Epoch [129][0/1]\tLoss: 2.511\n","Epoch [130][0/1]\tLoss: 2.482\n","Epoch [131][0/1]\tLoss: 2.465\n","Epoch [132][0/1]\tLoss: 2.465\n","Epoch [133][0/1]\tLoss: 2.448\n","Epoch [134][0/1]\tLoss: 2.432\n","Epoch [135][0/1]\tLoss: 2.438\n","Epoch [136][0/1]\tLoss: 2.410\n","Epoch [137][0/1]\tLoss: 2.414\n","Epoch [138][0/1]\tLoss: 2.385\n","Epoch [139][0/1]\tLoss: 2.388\n","Epoch [140][0/1]\tLoss: 2.347\n","Epoch [141][0/1]\tLoss: 2.313\n","Epoch [142][0/1]\tLoss: 2.342\n","Epoch [143][0/1]\tLoss: 2.306\n","Epoch [144][0/1]\tLoss: 2.306\n","Epoch [145][0/1]\tLoss: 2.309\n","Epoch [146][0/1]\tLoss: 2.288\n","Epoch [147][0/1]\tLoss: 2.251\n","Epoch [148][0/1]\tLoss: 2.234\n","Epoch [149][0/1]\tLoss: 2.226\n","Epoch [150][0/1]\tLoss: 2.265\n","Epoch [151][0/1]\tLoss: 2.232\n","Epoch [152][0/1]\tLoss: 2.204\n","Epoch [153][0/1]\tLoss: 2.199\n","Epoch [154][0/1]\tLoss: 2.171\n","Epoch [155][0/1]\tLoss: 2.141\n","Epoch [156][0/1]\tLoss: 2.162\n","Epoch [157][0/1]\tLoss: 2.162\n","Epoch [158][0/1]\tLoss: 2.122\n","Epoch [159][0/1]\tLoss: 2.093\n","Epoch [160][0/1]\tLoss: 2.109\n","Epoch [161][0/1]\tLoss: 2.114\n","Epoch [162][0/1]\tLoss: 2.095\n","Epoch [163][0/1]\tLoss: 2.036\n","Epoch [164][0/1]\tLoss: 2.063\n","Epoch [165][0/1]\tLoss: 2.001\n","Epoch [166][0/1]\tLoss: 2.022\n","Epoch [167][0/1]\tLoss: 2.019\n","Epoch [168][0/1]\tLoss: 1.958\n","Epoch [169][0/1]\tLoss: 1.984\n","Epoch [170][0/1]\tLoss: 1.966\n","Epoch [171][0/1]\tLoss: 1.973\n","Epoch [172][0/1]\tLoss: 1.909\n","Epoch [173][0/1]\tLoss: 1.920\n","Epoch [174][0/1]\tLoss: 1.918\n","Epoch [175][0/1]\tLoss: 1.913\n","Epoch [176][0/1]\tLoss: 1.924\n","Epoch [177][0/1]\tLoss: 1.888\n","Epoch [178][0/1]\tLoss: 1.878\n","Epoch [179][0/1]\tLoss: 1.879\n","Epoch [180][0/1]\tLoss: 1.873\n","Epoch [181][0/1]\tLoss: 1.827\n","Epoch [182][0/1]\tLoss: 1.838\n","Epoch [183][0/1]\tLoss: 1.843\n","Epoch [184][0/1]\tLoss: 1.846\n","Epoch [185][0/1]\tLoss: 1.802\n","Epoch [186][0/1]\tLoss: 1.764\n","Epoch [187][0/1]\tLoss: 1.807\n","Epoch [188][0/1]\tLoss: 1.763\n","Epoch [189][0/1]\tLoss: 1.765\n","Epoch [190][0/1]\tLoss: 1.716\n","Epoch [191][0/1]\tLoss: 1.760\n","Epoch [192][0/1]\tLoss: 1.739\n","Epoch [193][0/1]\tLoss: 1.730\n","Epoch [194][0/1]\tLoss: 1.744\n","Epoch [195][0/1]\tLoss: 1.697\n","Epoch [196][0/1]\tLoss: 1.741\n","Epoch [197][0/1]\tLoss: 1.675\n","Epoch [198][0/1]\tLoss: 1.695\n","Epoch [199][0/1]\tLoss: 1.691\n","Epoch [200][0/1]\tLoss: 1.677\n","Epoch [201][0/1]\tLoss: 1.657\n","Epoch [202][0/1]\tLoss: 1.624\n","Epoch [203][0/1]\tLoss: 1.646\n","Epoch [204][0/1]\tLoss: 1.687\n","Epoch [205][0/1]\tLoss: 1.626\n","Epoch [206][0/1]\tLoss: 1.614\n","Epoch [207][0/1]\tLoss: 1.575\n","Epoch [208][0/1]\tLoss: 1.611\n","Epoch [209][0/1]\tLoss: 1.629\n","Epoch [210][0/1]\tLoss: 1.667\n","Epoch [211][0/1]\tLoss: 1.580\n","Epoch [212][0/1]\tLoss: 1.609\n","Epoch [213][0/1]\tLoss: 1.548\n","Epoch [214][0/1]\tLoss: 1.593\n","Epoch [215][0/1]\tLoss: 1.529\n","Epoch [216][0/1]\tLoss: 1.541\n","Epoch [217][0/1]\tLoss: 1.497\n","Epoch [218][0/1]\tLoss: 1.514\n","Epoch [219][0/1]\tLoss: 1.546\n","Epoch [220][0/1]\tLoss: 1.493\n","Epoch [221][0/1]\tLoss: 1.550\n","Epoch [222][0/1]\tLoss: 1.482\n","Epoch [223][0/1]\tLoss: 1.503\n","Epoch [224][0/1]\tLoss: 1.465\n","Epoch [225][0/1]\tLoss: 1.458\n","Epoch [226][0/1]\tLoss: 1.520\n","Epoch [227][0/1]\tLoss: 1.438\n","Epoch [228][0/1]\tLoss: 1.504\n","Epoch [229][0/1]\tLoss: 1.435\n","Epoch [230][0/1]\tLoss: 1.421\n","Epoch [231][0/1]\tLoss: 1.413\n","Epoch [232][0/1]\tLoss: 1.455\n","Epoch [233][0/1]\tLoss: 1.407\n","Epoch [234][0/1]\tLoss: 1.378\n","Epoch [235][0/1]\tLoss: 1.407\n","Epoch [236][0/1]\tLoss: 1.403\n","Epoch [237][0/1]\tLoss: 1.375\n","Epoch [238][0/1]\tLoss: 1.365\n","Epoch [239][0/1]\tLoss: 1.372\n","Epoch [240][0/1]\tLoss: 1.321\n","Epoch [241][0/1]\tLoss: 1.379\n","Epoch [242][0/1]\tLoss: 1.363\n","Epoch [243][0/1]\tLoss: 1.346\n","Epoch [244][0/1]\tLoss: 1.344\n","Epoch [245][0/1]\tLoss: 1.324\n","Epoch [246][0/1]\tLoss: 1.349\n","Epoch [247][0/1]\tLoss: 1.310\n","Epoch [248][0/1]\tLoss: 1.302\n","Epoch [249][0/1]\tLoss: 1.323\n","Epoch [250][0/1]\tLoss: 1.300\n","Epoch [251][0/1]\tLoss: 1.292\n","Epoch [252][0/1]\tLoss: 1.268\n","Epoch [253][0/1]\tLoss: 1.307\n","Epoch [254][0/1]\tLoss: 1.277\n","Epoch [255][0/1]\tLoss: 1.283\n","Epoch [256][0/1]\tLoss: 1.231\n","Epoch [257][0/1]\tLoss: 1.281\n","Epoch [258][0/1]\tLoss: 1.238\n","Epoch [259][0/1]\tLoss: 1.260\n","Epoch [260][0/1]\tLoss: 1.225\n","Epoch [261][0/1]\tLoss: 1.257\n","Epoch [262][0/1]\tLoss: 1.230\n","Epoch [263][0/1]\tLoss: 1.200\n","Epoch [264][0/1]\tLoss: 1.242\n","Epoch [265][0/1]\tLoss: 1.172\n","Epoch [266][0/1]\tLoss: 1.205\n","Epoch [267][0/1]\tLoss: 1.187\n","Epoch [268][0/1]\tLoss: 1.168\n","Epoch [269][0/1]\tLoss: 1.165\n","Epoch [270][0/1]\tLoss: 1.207\n","Epoch [271][0/1]\tLoss: 1.185\n","Epoch [272][0/1]\tLoss: 1.159\n","Epoch [273][0/1]\tLoss: 1.159\n","Epoch [274][0/1]\tLoss: 1.140\n","Epoch [275][0/1]\tLoss: 1.138\n","Epoch [276][0/1]\tLoss: 1.157\n","Epoch [277][0/1]\tLoss: 1.130\n","Epoch [278][0/1]\tLoss: 1.146\n","Epoch [279][0/1]\tLoss: 1.120\n","Epoch [280][0/1]\tLoss: 1.119\n","Epoch [281][0/1]\tLoss: 1.104\n","Epoch [282][0/1]\tLoss: 1.107\n","Epoch [283][0/1]\tLoss: 1.087\n","Epoch [284][0/1]\tLoss: 1.093\n","Epoch [285][0/1]\tLoss: 1.070\n","Epoch [286][0/1]\tLoss: 1.094\n","Epoch [287][0/1]\tLoss: 1.060\n","Epoch [288][0/1]\tLoss: 1.057\n","Epoch [289][0/1]\tLoss: 1.086\n","Epoch [290][0/1]\tLoss: 1.078\n","Epoch [291][0/1]\tLoss: 1.087\n","Epoch [292][0/1]\tLoss: 1.061\n","Epoch [293][0/1]\tLoss: 1.090\n","Epoch [294][0/1]\tLoss: 1.101\n","Epoch [295][0/1]\tLoss: 1.081\n","Epoch [296][0/1]\tLoss: 1.093\n","Epoch [297][0/1]\tLoss: 1.086\n","Epoch [298][0/1]\tLoss: 1.041\n","Epoch [299][0/1]\tLoss: 1.069\n","Epoch [300][0/1]\tLoss: 1.002\n","Epoch [301][0/1]\tLoss: 1.041\n","Epoch [302][0/1]\tLoss: 1.058\n","Epoch [303][0/1]\tLoss: 1.002\n","Epoch [304][0/1]\tLoss: 1.002\n","Epoch [305][0/1]\tLoss: 0.989\n","Epoch [306][0/1]\tLoss: 1.003\n","Epoch [307][0/1]\tLoss: 1.002\n","Epoch [308][0/1]\tLoss: 0.947\n","Epoch [309][0/1]\tLoss: 0.999\n","Epoch [310][0/1]\tLoss: 0.968\n","Epoch [311][0/1]\tLoss: 1.023\n","Epoch [312][0/1]\tLoss: 0.988\n","Epoch [313][0/1]\tLoss: 0.973\n","Epoch [314][0/1]\tLoss: 0.957\n","Epoch [315][0/1]\tLoss: 0.946\n","Epoch [316][0/1]\tLoss: 1.012\n","Epoch [317][0/1]\tLoss: 0.957\n","Epoch [318][0/1]\tLoss: 0.959\n","Epoch [319][0/1]\tLoss: 0.949\n","Epoch [320][0/1]\tLoss: 0.967\n","Epoch [321][0/1]\tLoss: 0.976\n","Epoch [322][0/1]\tLoss: 0.945\n","Epoch [323][0/1]\tLoss: 0.992\n","Epoch [324][0/1]\tLoss: 0.945\n","Epoch [325][0/1]\tLoss: 0.949\n","Epoch [326][0/1]\tLoss: 0.944\n","Epoch [327][0/1]\tLoss: 0.885\n","Epoch [328][0/1]\tLoss: 0.914\n","Epoch [329][0/1]\tLoss: 0.906\n","Epoch [330][0/1]\tLoss: 0.887\n","Epoch [331][0/1]\tLoss: 0.930\n","Epoch [332][0/1]\tLoss: 0.913\n","Epoch [333][0/1]\tLoss: 0.899\n","Epoch [334][0/1]\tLoss: 0.921\n","Epoch [335][0/1]\tLoss: 0.888\n","Epoch [336][0/1]\tLoss: 0.885\n","Epoch [337][0/1]\tLoss: 0.872\n","Epoch [338][0/1]\tLoss: 0.889\n","Epoch [339][0/1]\tLoss: 0.880\n","Epoch [340][0/1]\tLoss: 0.838\n","Epoch [341][0/1]\tLoss: 0.901\n","Epoch [342][0/1]\tLoss: 0.849\n","Epoch [343][0/1]\tLoss: 0.874\n","Epoch [344][0/1]\tLoss: 0.875\n","Epoch [345][0/1]\tLoss: 0.828\n","Epoch [346][0/1]\tLoss: 0.871\n","Epoch [347][0/1]\tLoss: 0.853\n","Epoch [348][0/1]\tLoss: 0.840\n","Epoch [349][0/1]\tLoss: 0.839\n","Epoch [350][0/1]\tLoss: 0.834\n","Epoch [351][0/1]\tLoss: 0.804\n","Epoch [352][0/1]\tLoss: 0.826\n","Epoch [353][0/1]\tLoss: 0.829\n","Epoch [354][0/1]\tLoss: 0.852\n","Epoch [355][0/1]\tLoss: 0.795\n","Epoch [356][0/1]\tLoss: 0.810\n","Epoch [357][0/1]\tLoss: 0.775\n","Epoch [358][0/1]\tLoss: 0.748\n","Epoch [359][0/1]\tLoss: 0.799\n","Epoch [360][0/1]\tLoss: 0.837\n","Epoch [361][0/1]\tLoss: 0.733\n","Epoch [362][0/1]\tLoss: 0.832\n","Epoch [363][0/1]\tLoss: 0.762\n","Epoch [364][0/1]\tLoss: 0.778\n","Epoch [365][0/1]\tLoss: 0.804\n","Epoch [366][0/1]\tLoss: 0.738\n","Epoch [367][0/1]\tLoss: 0.784\n","Epoch [368][0/1]\tLoss: 0.785\n","Epoch [369][0/1]\tLoss: 0.737\n","Epoch [370][0/1]\tLoss: 0.738\n","Epoch [371][0/1]\tLoss: 0.744\n","Epoch [372][0/1]\tLoss: 0.720\n","Epoch [373][0/1]\tLoss: 0.679\n","Epoch [374][0/1]\tLoss: 0.725\n","Epoch [375][0/1]\tLoss: 0.710\n","Epoch [376][0/1]\tLoss: 0.689\n","Epoch [377][0/1]\tLoss: 0.688\n","Epoch [378][0/1]\tLoss: 0.707\n","Epoch [379][0/1]\tLoss: 0.682\n","Epoch [380][0/1]\tLoss: 0.663\n","Epoch [381][0/1]\tLoss: 0.660\n","Epoch [382][0/1]\tLoss: 0.684\n","Epoch [383][0/1]\tLoss: 0.647\n","Epoch [384][0/1]\tLoss: 0.653\n","Epoch [385][0/1]\tLoss: 0.677\n","Epoch [386][0/1]\tLoss: 0.645\n","Epoch [387][0/1]\tLoss: 0.731\n","Epoch [388][0/1]\tLoss: 0.707\n","Epoch [389][0/1]\tLoss: 0.800\n","Epoch [390][0/1]\tLoss: 0.641\n","Epoch [391][0/1]\tLoss: 0.730\n","Epoch [392][0/1]\tLoss: 0.659\n","Epoch [393][0/1]\tLoss: 0.620\n","Epoch [394][0/1]\tLoss: 0.684\n","Epoch [395][0/1]\tLoss: 0.652\n","Epoch [396][0/1]\tLoss: 0.619\n","Epoch [397][0/1]\tLoss: 0.640\n","Epoch [398][0/1]\tLoss: 0.604\n","Epoch [399][0/1]\tLoss: 0.625\n","Epoch [400][0/1]\tLoss: 0.598\n","Epoch [401][0/1]\tLoss: 0.579\n","Epoch [402][0/1]\tLoss: 0.593\n","Epoch [403][0/1]\tLoss: 0.568\n","Epoch [404][0/1]\tLoss: 0.579\n","Epoch [405][0/1]\tLoss: 0.562\n","Epoch [406][0/1]\tLoss: 0.568\n","Epoch [407][0/1]\tLoss: 0.604\n","Epoch [408][0/1]\tLoss: 0.581\n","Epoch [409][0/1]\tLoss: 0.582\n","Epoch [410][0/1]\tLoss: 0.580\n","Epoch [411][0/1]\tLoss: 0.575\n","Epoch [412][0/1]\tLoss: 0.529\n","Epoch [413][0/1]\tLoss: 0.601\n","Epoch [414][0/1]\tLoss: 0.543\n","Epoch [415][0/1]\tLoss: 0.566\n","Epoch [416][0/1]\tLoss: 0.546\n","Epoch [417][0/1]\tLoss: 0.566\n","Epoch [418][0/1]\tLoss: 0.525\n","Epoch [419][0/1]\tLoss: 0.527\n","Epoch [420][0/1]\tLoss: 0.578\n","Epoch [421][0/1]\tLoss: 0.513\n","Epoch [422][0/1]\tLoss: 0.528\n","Epoch [423][0/1]\tLoss: 0.543\n","Epoch [424][0/1]\tLoss: 0.551\n","Epoch [425][0/1]\tLoss: 0.508\n","Epoch [426][0/1]\tLoss: 0.551\n","Epoch [427][0/1]\tLoss: 0.526\n","Epoch [428][0/1]\tLoss: 0.548\n","Epoch [429][0/1]\tLoss: 0.524\n","Epoch [430][0/1]\tLoss: 0.466\n","Epoch [431][0/1]\tLoss: 0.558\n","Epoch [432][0/1]\tLoss: 0.502\n","Epoch [433][0/1]\tLoss: 0.468\n","Epoch [434][0/1]\tLoss: 0.480\n","Epoch [435][0/1]\tLoss: 0.514\n","Epoch [436][0/1]\tLoss: 0.467\n","Epoch [437][0/1]\tLoss: 0.469\n","Epoch [438][0/1]\tLoss: 0.475\n","Epoch [439][0/1]\tLoss: 0.474\n","Epoch [440][0/1]\tLoss: 0.463\n","Epoch [441][0/1]\tLoss: 0.469\n","Epoch [442][0/1]\tLoss: 0.466\n","Epoch [443][0/1]\tLoss: 0.439\n","Epoch [444][0/1]\tLoss: 0.478\n","Epoch [445][0/1]\tLoss: 0.436\n","Epoch [446][0/1]\tLoss: 0.420\n","Epoch [447][0/1]\tLoss: 0.456\n","Epoch [448][0/1]\tLoss: 0.447\n","Epoch [449][0/1]\tLoss: 0.405\n","Epoch [450][0/1]\tLoss: 0.425\n","Epoch [451][0/1]\tLoss: 0.447\n","Epoch [452][0/1]\tLoss: 0.412\n","Epoch [453][0/1]\tLoss: 0.419\n","Epoch [454][0/1]\tLoss: 0.431\n","Epoch [455][0/1]\tLoss: 0.391\n","Epoch [456][0/1]\tLoss: 0.389\n","Epoch [457][0/1]\tLoss: 0.423\n","Epoch [458][0/1]\tLoss: 0.396\n","Epoch [459][0/1]\tLoss: 0.415\n","Epoch [460][0/1]\tLoss: 0.386\n","Epoch [461][0/1]\tLoss: 0.379\n","Epoch [462][0/1]\tLoss: 0.395\n","Epoch [463][0/1]\tLoss: 0.387\n","Epoch [464][0/1]\tLoss: 0.401\n","Epoch [465][0/1]\tLoss: 0.345\n","Epoch [466][0/1]\tLoss: 0.392\n","Epoch [467][0/1]\tLoss: 0.357\n","Epoch [468][0/1]\tLoss: 0.377\n","Epoch [469][0/1]\tLoss: 0.352\n","Epoch [470][0/1]\tLoss: 0.361\n","Epoch [471][0/1]\tLoss: 0.354\n","Epoch [472][0/1]\tLoss: 0.347\n","Epoch [473][0/1]\tLoss: 0.363\n","Epoch [474][0/1]\tLoss: 0.311\n","Epoch [475][0/1]\tLoss: 0.347\n","Epoch [476][0/1]\tLoss: 0.343\n","Epoch [477][0/1]\tLoss: 0.338\n","Epoch [478][0/1]\tLoss: 0.335\n","Epoch [479][0/1]\tLoss: 0.320\n","Epoch [480][0/1]\tLoss: 0.316\n","Epoch [481][0/1]\tLoss: 0.312\n","Epoch [482][0/1]\tLoss: 0.314\n","Epoch [483][0/1]\tLoss: 0.306\n","Epoch [484][0/1]\tLoss: 0.307\n","Epoch [485][0/1]\tLoss: 0.296\n","Epoch [486][0/1]\tLoss: 0.306\n","Epoch [487][0/1]\tLoss: 0.313\n","Epoch [488][0/1]\tLoss: 0.286\n","Epoch [489][0/1]\tLoss: 0.291\n","Epoch [490][0/1]\tLoss: 0.290\n","Epoch [491][0/1]\tLoss: 0.275\n","Epoch [492][0/1]\tLoss: 0.291\n","Epoch [493][0/1]\tLoss: 0.268\n","Epoch [494][0/1]\tLoss: 0.270\n","Epoch [495][0/1]\tLoss: 0.269\n","Epoch [496][0/1]\tLoss: 0.265\n","Epoch [497][0/1]\tLoss: 0.274\n","Epoch [498][0/1]\tLoss: 0.278\n","Epoch [499][0/1]\tLoss: 0.267\n","Epoch [500][0/1]\tLoss: 0.253\n","Epoch [501][0/1]\tLoss: 0.267\n","Epoch [502][0/1]\tLoss: 0.250\n","Epoch [503][0/1]\tLoss: 0.247\n","Epoch [504][0/1]\tLoss: 0.249\n","Epoch [505][0/1]\tLoss: 0.249\n","Epoch [506][0/1]\tLoss: 0.258\n","Epoch [507][0/1]\tLoss: 0.248\n","Epoch [508][0/1]\tLoss: 0.257\n","Epoch [509][0/1]\tLoss: 0.245\n","Epoch [510][0/1]\tLoss: 0.248\n","Epoch [511][0/1]\tLoss: 0.257\n","Epoch [512][0/1]\tLoss: 0.235\n","Epoch [513][0/1]\tLoss: 0.257\n","Epoch [514][0/1]\tLoss: 0.227\n","Epoch [515][0/1]\tLoss: 0.237\n","Epoch [516][0/1]\tLoss: 0.231\n","Epoch [517][0/1]\tLoss: 0.229\n","Epoch [518][0/1]\tLoss: 0.232\n","Epoch [519][0/1]\tLoss: 0.226\n","Epoch [520][0/1]\tLoss: 0.227\n","Epoch [521][0/1]\tLoss: 0.232\n","Epoch [522][0/1]\tLoss: 0.238\n","Epoch [523][0/1]\tLoss: 0.211\n","Epoch [524][0/1]\tLoss: 0.234\n","Epoch [525][0/1]\tLoss: 0.223\n","Epoch [526][0/1]\tLoss: 0.231\n","Epoch [527][0/1]\tLoss: 0.220\n","Epoch [528][0/1]\tLoss: 0.221\n","Epoch [529][0/1]\tLoss: 0.211\n","Epoch [530][0/1]\tLoss: 0.223\n","Epoch [531][0/1]\tLoss: 0.217\n","Epoch [532][0/1]\tLoss: 0.216\n","Epoch [533][0/1]\tLoss: 0.215\n","Epoch [534][0/1]\tLoss: 0.237\n","Epoch [535][0/1]\tLoss: 0.219\n","Epoch [536][0/1]\tLoss: 0.234\n","Epoch [537][0/1]\tLoss: 0.207\n","Epoch [538][0/1]\tLoss: 0.221\n","Epoch [539][0/1]\tLoss: 0.214\n","Epoch [540][0/1]\tLoss: 0.203\n","Epoch [541][0/1]\tLoss: 0.202\n","Epoch [542][0/1]\tLoss: 0.215\n","Epoch [543][0/1]\tLoss: 0.203\n","Epoch [544][0/1]\tLoss: 0.192\n","Epoch [545][0/1]\tLoss: 0.222\n","Epoch [546][0/1]\tLoss: 0.188\n","Epoch [547][0/1]\tLoss: 0.198\n","Epoch [548][0/1]\tLoss: 0.184\n","Epoch [549][0/1]\tLoss: 0.204\n","Epoch [550][0/1]\tLoss: 0.193\n","Epoch [551][0/1]\tLoss: 0.182\n","Epoch [552][0/1]\tLoss: 0.189\n","Epoch [553][0/1]\tLoss: 0.194\n","Epoch [554][0/1]\tLoss: 0.183\n","Epoch [555][0/1]\tLoss: 0.195\n","Epoch [556][0/1]\tLoss: 0.191\n","Epoch [557][0/1]\tLoss: 0.179\n","Epoch [558][0/1]\tLoss: 0.175\n","Epoch [559][0/1]\tLoss: 0.183\n","Epoch [560][0/1]\tLoss: 0.173\n","Epoch [561][0/1]\tLoss: 0.174\n","Epoch [562][0/1]\tLoss: 0.169\n","Epoch [563][0/1]\tLoss: 0.162\n","Epoch [564][0/1]\tLoss: 0.177\n","Epoch [565][0/1]\tLoss: 0.164\n","Epoch [566][0/1]\tLoss: 0.175\n","Epoch [567][0/1]\tLoss: 0.159\n","Epoch [568][0/1]\tLoss: 0.174\n","Epoch [569][0/1]\tLoss: 0.163\n","Epoch [570][0/1]\tLoss: 0.162\n","Epoch [571][0/1]\tLoss: 0.160\n","Epoch [572][0/1]\tLoss: 0.165\n","Epoch [573][0/1]\tLoss: 0.168\n","Epoch [574][0/1]\tLoss: 0.155\n","Epoch [575][0/1]\tLoss: 0.160\n","Epoch [576][0/1]\tLoss: 0.156\n","Epoch [577][0/1]\tLoss: 0.160\n","Epoch [578][0/1]\tLoss: 0.154\n","Epoch [579][0/1]\tLoss: 0.149\n","Epoch [580][0/1]\tLoss: 0.165\n","Epoch [581][0/1]\tLoss: 0.157\n","Epoch [582][0/1]\tLoss: 0.155\n","Epoch [583][0/1]\tLoss: 0.155\n","Epoch [584][0/1]\tLoss: 0.155\n","Epoch [585][0/1]\tLoss: 0.150\n","Epoch [586][0/1]\tLoss: 0.146\n","Epoch [587][0/1]\tLoss: 0.159\n","Epoch [588][0/1]\tLoss: 0.145\n","Epoch [589][0/1]\tLoss: 0.147\n","Epoch [590][0/1]\tLoss: 0.149\n","Epoch [591][0/1]\tLoss: 0.141\n","Epoch [592][0/1]\tLoss: 0.139\n","Epoch [593][0/1]\tLoss: 0.143\n","Epoch [594][0/1]\tLoss: 0.140\n","Epoch [595][0/1]\tLoss: 0.134\n","Epoch [596][0/1]\tLoss: 0.145\n","Epoch [597][0/1]\tLoss: 0.137\n","Epoch [598][0/1]\tLoss: 0.141\n","Epoch [599][0/1]\tLoss: 0.152\n","Epoch [600][0/1]\tLoss: 0.140\n","Epoch [601][0/1]\tLoss: 0.153\n","Epoch [602][0/1]\tLoss: 0.151\n","Epoch [603][0/1]\tLoss: 0.143\n","Epoch [604][0/1]\tLoss: 0.141\n","Epoch [605][0/1]\tLoss: 0.154\n","Epoch [606][0/1]\tLoss: 0.132\n","Epoch [607][0/1]\tLoss: 0.139\n","Epoch [608][0/1]\tLoss: 0.148\n","Epoch [609][0/1]\tLoss: 0.143\n","Epoch [610][0/1]\tLoss: 0.144\n","Epoch [611][0/1]\tLoss: 0.136\n","Epoch [612][0/1]\tLoss: 0.151\n","Epoch [613][0/1]\tLoss: 0.137\n","Epoch [614][0/1]\tLoss: 0.138\n","Epoch [615][0/1]\tLoss: 0.129\n","Epoch [616][0/1]\tLoss: 0.134\n","Epoch [617][0/1]\tLoss: 0.135\n","Epoch [618][0/1]\tLoss: 0.130\n","Epoch [619][0/1]\tLoss: 0.128\n","Epoch [620][0/1]\tLoss: 0.136\n","Epoch [621][0/1]\tLoss: 0.132\n","Epoch [622][0/1]\tLoss: 0.123\n","Epoch [623][0/1]\tLoss: 0.124\n","Epoch [624][0/1]\tLoss: 0.128\n","Epoch [625][0/1]\tLoss: 0.118\n","Epoch [626][0/1]\tLoss: 0.133\n","Epoch [627][0/1]\tLoss: 0.125\n","Epoch [628][0/1]\tLoss: 0.118\n","Epoch [629][0/1]\tLoss: 0.130\n","Epoch [630][0/1]\tLoss: 0.119\n","Epoch [631][0/1]\tLoss: 0.127\n","Epoch [632][0/1]\tLoss: 0.118\n","Epoch [633][0/1]\tLoss: 0.114\n","Epoch [634][0/1]\tLoss: 0.119\n","Epoch [635][0/1]\tLoss: 0.118\n","Epoch [636][0/1]\tLoss: 0.115\n","Epoch [637][0/1]\tLoss: 0.133\n","Epoch [638][0/1]\tLoss: 0.112\n","Epoch [639][0/1]\tLoss: 0.115\n","Epoch [640][0/1]\tLoss: 0.117\n","Epoch [641][0/1]\tLoss: 0.111\n","Epoch [642][0/1]\tLoss: 0.126\n","Epoch [643][0/1]\tLoss: 0.134\n","Epoch [644][0/1]\tLoss: 0.112\n","Epoch [645][0/1]\tLoss: 0.138\n","Epoch [646][0/1]\tLoss: 0.124\n","Epoch [647][0/1]\tLoss: 0.109\n","Epoch [648][0/1]\tLoss: 0.107\n","Epoch [649][0/1]\tLoss: 0.134\n","Epoch [650][0/1]\tLoss: 0.118\n","Epoch [651][0/1]\tLoss: 0.114\n","Epoch [652][0/1]\tLoss: 0.114\n","Epoch [653][0/1]\tLoss: 0.108\n","Epoch [654][0/1]\tLoss: 0.112\n","Epoch [655][0/1]\tLoss: 0.110\n","Epoch [656][0/1]\tLoss: 0.108\n","Epoch [657][0/1]\tLoss: 0.110\n","Epoch [658][0/1]\tLoss: 0.109\n","Epoch [659][0/1]\tLoss: 0.113\n","Epoch [660][0/1]\tLoss: 0.099\n","Epoch [661][0/1]\tLoss: 0.103\n","Epoch [662][0/1]\tLoss: 0.106\n","Epoch [663][0/1]\tLoss: 0.109\n","Epoch [664][0/1]\tLoss: 0.108\n","Epoch [665][0/1]\tLoss: 0.103\n","Epoch [666][0/1]\tLoss: 0.108\n","Epoch [667][0/1]\tLoss: 0.097\n","Epoch [668][0/1]\tLoss: 0.110\n","Epoch [669][0/1]\tLoss: 0.111\n","Epoch [670][0/1]\tLoss: 0.110\n","Epoch [671][0/1]\tLoss: 0.093\n","Epoch [672][0/1]\tLoss: 0.106\n","Epoch [673][0/1]\tLoss: 0.100\n","Epoch [674][0/1]\tLoss: 0.099\n","Epoch [675][0/1]\tLoss: 0.098\n","Epoch [676][0/1]\tLoss: 0.100\n","Epoch [677][0/1]\tLoss: 0.091\n","Epoch [678][0/1]\tLoss: 0.094\n","Epoch [679][0/1]\tLoss: 0.095\n","Epoch [680][0/1]\tLoss: 0.098\n","Epoch [681][0/1]\tLoss: 0.095\n","Epoch [682][0/1]\tLoss: 0.092\n","Epoch [683][0/1]\tLoss: 0.091\n","Epoch [684][0/1]\tLoss: 0.097\n","Epoch [685][0/1]\tLoss: 0.097\n","Epoch [686][0/1]\tLoss: 0.091\n","Epoch [687][0/1]\tLoss: 0.097\n","Epoch [688][0/1]\tLoss: 0.094\n","Epoch [689][0/1]\tLoss: 0.088\n","Epoch [690][0/1]\tLoss: 0.086\n","Epoch [691][0/1]\tLoss: 0.088\n","Epoch [692][0/1]\tLoss: 0.089\n","Epoch [693][0/1]\tLoss: 0.096\n","Epoch [694][0/1]\tLoss: 0.090\n","Epoch [695][0/1]\tLoss: 0.094\n","Epoch [696][0/1]\tLoss: 0.093\n","Epoch [697][0/1]\tLoss: 0.087\n","Epoch [698][0/1]\tLoss: 0.086\n","Epoch [699][0/1]\tLoss: 0.081\n"]}],"source":["directory = 'transformers_vanillanoreg_01724'\n","create_directory(directory)\n","\n","# run = neptune_init(directory)\n","\n","parameters = {\n","    'd_model': 2048,\n","    'heads': 32,\n","    'num_layers': 20,\n","    'epochs': 700,\n","}\n","# run['parameters'] = parameters\n","\n","loss_history_vanillanoreg_transformer = []\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n","# device = \"cpu\"\n","\n","with open('WORDMAP_corpus_KBFILKOM.json', 'r') as j:\n","    word_map = json.load(j)\n","    \n","transformer = Transformer(d_model = parameters['d_model'], heads = parameters['heads'], num_layers = parameters['num_layers'], word_map = word_map, max_len=95)\n","transformer = transformer.to(device)\n","adam_optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n","transformer_optimizer = AdamWarmup(model_size = parameters['d_model'], warmup_steps = 4000, optimizer = adam_optimizer)\n","criterion = LossWithLS(len(word_map), 0.2)\n","\n","for epoch in range(parameters['epochs']):\n","    loss_train = train(train_loader, transformer, criterion, epoch)\n","\n","    state = {'epoch': epoch, 'transformer': transformer, 'transformer_optimizer': transformer_optimizer}\n","    # torch.save(state, directory + '/checkpoint_' + str(epoch) +'.pth.tar')\n","    # run['model_checkpoint'].upload(directory + '/checkpoint_' + str(epoch) +'.pth.tar')\n","\n","    loss_history_vanillanoreg_transformer.append(loss_train)\n","    # run['train/loss'].append(loss_train)\n","\n","# with open(directory + '/loss_history_vanillanoreg_transformer.yaml', 'w') as file:\n","#     yaml.dump(loss_history_vanilla_transformer, file)\n","\n","# run.stop()"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[1, 2]], device='cuda:0')\n","tensor([[[[True, True]]]], device='cuda:0')\n","menjadi fakultas yang berdaya saing internasional dan berkontribusi kepada pengembangan teknologi informasi dan ilmu komputer untuk menunjang industri dan masyarakat dengan menyelaraskan pelaksanaan pendidikan penelitian dan pengabdian kepada masyarakat\n"]}],"source":["question = \"visi filkom\" \n","max_len = 100\n","enc_qus = [word_map.get(word, word_map['<unk>']) for word in question.split()]\n","question = torch.LongTensor(enc_qus).to(device).unsqueeze(0)\n","print(question)\n","question_mask = (question!=0).to(device).unsqueeze(1).unsqueeze(1)  \n","print(question_mask)\n","sentence = evaluate(transformer, question, question_mask, int(max_len), word_map)\n","print(sentence)"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[27,  2]], device='cuda:0')\n","tensor([[[[True, True]]]], device='cuda:0')\n","menjadi fakultas yang berdaya saing internasional dan berkontribusi kepada pengembangan teknologi informasi dan ilmu komputer untuk menunjang industri dan masyarakat dengan menyelaraskan pelaksanaan pendidikan penelitian dan pengabdian kepada masyarakat\n"]}],"source":["question = \"misi filkom\" \n","max_len = 100\n","enc_qus = [word_map.get(word, word_map['<unk>']) for word in question.split()]\n","enc_qus += [word_map['<start>']]\n","question = torch.LongTensor(enc_qus).to(device).unsqueeze(0)\n","print(question)\n","question_mask = (question!=0).to(device).unsqueeze(1).unsqueeze(1)  \n","print(question_mask)\n","sentence = evaluate(transformer, question, question_mask, int(max_len), word_map)\n","print(sentence)"]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["menghasilkan lulusan yang kompeten profesional berbudi pekerti luhur berjiwa entrepreneur dan berdaya saing internasional menghasilkan sivitas akademika yang mampu mengembangkan penelitiandan pengabdian yang berorientasi pada pembaruan dan teknologi tepat guna untuk industri dan masyarakat terwujudnya suasana akademik yang kondusif dalam bidang pendidikan penelitian dan pengabdian kepada masyarakat yang berdaya saing unggul terwujudnya tata kelola organisasi yang transparan akuntabel efektif dan efisien meningkatnya kuantitas dan kualitas kerja sama yang berkelanjutan di bidang pendidikan penelitian dan pengabdian kepada masyarakat dalam skala nasional dan internasional\n"]}],"source":["question = \"apa tujuan filkom\" \n","max_len = 100\n","enc_qus = [word_map.get(word, word_map['<unk>']) for word in question.split()]\n","question = torch.LongTensor(enc_qus).to(device).unsqueeze(0)\n","question_mask = (question!=0).to(device).unsqueeze(1).unsqueeze(1)  \n","sentence = evaluate(transformer, question, question_mask, int(max_len), word_map)\n","print(sentence)"]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["menghasilkan lulusan yang kompeten profesional berbudi pekerti luhur berjiwa entrepreneur dan berdaya saing internasional menghasilkan sivitas akademika yang mampu mengembangkan penelitiandan pengabdian yang berorientasi pada pembaruan dan teknologi tepat guna untuk industri dan masyarakat terwujudnya suasana akademik yang kondusif dalam bidang pendidikan penelitian dan pengabdian kepada masyarakat yang berdaya saing unggul terwujudnya tata kelola organisasi yang transparan akuntabel efektif dan efisien meningkatnya kuantitas dan kualitas kerja sama yang berkelanjutan di bidang pendidikan penelitian dan pengabdian kepada masyarakat dalam skala nasional dan internasional\n"]}],"source":["question = \"sasaran pendidikan filkom\" \n","max_len = 100\n","enc_qus = [word_map.get(word, word_map['<unk>']) for word in question.split()]\n","question = torch.LongTensor(enc_qus).to(device).unsqueeze(0)\n","question_mask = (question!=0).to(device).unsqueeze(1).unsqueeze(1)  \n","sentence = evaluate(transformer, question, question_mask, int(max_len), word_map)\n","print(sentence)"]},{"cell_type":"markdown","metadata":{},"source":["## Decoder Only"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["directory = 'experiment_deconly_17624'\n","create_directory(directory)\n","\n","d_model = 2048\n","heads = 16\n","num_layers = 5\n","epochs = 100\n","\n","loss_history_decoder_transformer = []\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","with open('WORDMAP_corpus_KBFILKOM.json', 'r') as j:\n","    word_map = json.load(j)\n","    \n","transformer = TransformerDecoderOnly(d_model = d_model, heads = heads, num_layers = num_layers, word_map = word_map, max_len=95)\n","transformer = transformer.to(device)\n","adam_optimizer = torch.optim.Adam(transformer.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-9)\n","transformer_optimizer = AdamWarmup(model_size = d_model, warmup_steps = 4000, optimizer = adam_optimizer)\n","criterion = LossWithLS(len(word_map), 0.2)\n","\n","for epoch in range(epochs):\n","    loss_train = train(train_loader, transformer, criterion, epoch)\n","\n","    state = {'epoch': epoch, 'transformer': transformer, 'transformer_optimizer': transformer_optimizer}\n","    torch.save(state, directory + '/checkpoint_' + str(epoch) +'.pth.tar')\n","\n","    loss_history_decoder_transformer.append(loss_train)"]},{"cell_type":"markdown","metadata":{},"source":["## LSTM_Transformer"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["directory = 'experiment_2_lstm'\n","\n","d_model = 1024\n","heads = 32\n","num_layers = 10\n","epochs = 100\n","\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","loss_history_lstm_transformer = []\n","\n","with open('WORDMAP_corpus_KBFILKOM.json', 'r') as j:\n","    word_map = json.load(j)\n","    \n","transformer = TransformerLSTM(d_model = d_model, heads = heads, num_layers = num_layers, word_map = word_map, max_len=90)\n","transformer = transformer.to(device)\n","adam_optimizer = torch.optim.Adam(transformer.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-9)\n","transformer_optimizer = AdamWarmup(model_size = d_model, warmup_steps = 4000, optimizer = adam_optimizer)\n","criterion = LossWithLS(len(word_map), 0.2)\n","\n","\n","for epoch in range(epochs):\n","    loss_train = train(train_loader, transformer, criterion, epoch)\n","\n","    state = {'epoch': epoch, 'transformer': transformer, 'transformer_optimizer': transformer_optimizer}\n","    torch.save(state, directory + '/checkpoint_' + str(epoch) +'.pth.tar')\n","\n","    loss_history_lstm_transformer.append(loss_train)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import yaml\n","\n","with open(directory + '/loss_history_lstm_transformer.yaml', 'w') as file:\n","    yaml.dump(loss_history_lstm_transformer, file)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["directory = 'experiment_1'\n","checkpoint = torch.load(directory + '/checkpoint_99.pth.tar')\n","transformer = checkpoint['transformer']\n","\n","question = \"Visi FILKOM\" \n","max_len = 50\n","enc_qus = [word_map.get(word, word_map['<unk>']) for word in question.split()]\n","question = torch.LongTensor(enc_qus).to(device).unsqueeze(0)\n","question_mask = (question!=0).to(device).unsqueeze(1).unsqueeze(1)  \n","sentence = evaluate(transformer, question, question_mask, int(max_len), word_map)\n","print(sentence)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["checkpoint"]},{"cell_type":"markdown","metadata":{},"source":["# Reset Cache"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","\n","torch.cuda.empty_cache()\n","\n","print(torch.cuda.memory_summary(device=None, abbreviated=False))"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30733,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"base","language":"python","name":"base"}},"nbformat":4,"nbformat_minor":4}
