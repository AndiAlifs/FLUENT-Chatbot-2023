{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow.keras import preprocessing, utils\n",
    "import os\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve data train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>No</th>\n",
       "      <th>Contributor</th>\n",
       "      <th>Topik/Tag</th>\n",
       "      <th>Pertanyaan</th>\n",
       "      <th>Jenis</th>\n",
       "      <th>Jawaban</th>\n",
       "      <th>Link Jawaban</th>\n",
       "      <th>Keterangan Tambahan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>Fitra</td>\n",
       "      <td>Informasi Dosen</td>\n",
       "      <td>email Fitra A. Bachtiar</td>\n",
       "      <td>Statis</td>\n",
       "      <td>fitra.bachtiar[at]ub.ac.id</td>\n",
       "      <td>https://filkom.ub.ac.id/sdm-dosen/?search_f7a8...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NIK/NIP Fitra A. Bachtiar</td>\n",
       "      <td>Statis</td>\n",
       "      <td>198406282019031006</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nama lengkap Fitra A. Bachtiar</td>\n",
       "      <td>Statis</td>\n",
       "      <td>Dr.Eng. Fitra A. Bachtiar</td>\n",
       "      <td>https://filkom.ub.ac.id/sdm-dosen/?search_f7a8...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Departemen Fitra A. Bachtiar</td>\n",
       "      <td>Dinamis</td>\n",
       "      <td>Departemen Teknik Informatika</td>\n",
       "      <td>https://filkom.ub.ac.id/sdm-dosen/?search_f7a8...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Program Studi Fitra A. Bachtiar</td>\n",
       "      <td>Dinamis</td>\n",
       "      <td>S2 Ilmu Komputer</td>\n",
       "      <td>https://filkom.ub.ac.id/sdm-dosen/?search_f7a8...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    No Contributor        Topik/Tag                       Pertanyaan    Jenis  \\\n",
       "0  0.0       Fitra  Informasi Dosen          email Fitra A. Bachtiar   Statis   \n",
       "1  NaN         NaN              NaN        NIK/NIP Fitra A. Bachtiar   Statis   \n",
       "2  NaN         NaN              NaN   nama lengkap Fitra A. Bachtiar   Statis   \n",
       "3  NaN         NaN              NaN     Departemen Fitra A. Bachtiar  Dinamis   \n",
       "4  NaN         NaN              NaN  Program Studi Fitra A. Bachtiar  Dinamis   \n",
       "\n",
       "                         Jawaban  \\\n",
       "0     fitra.bachtiar[at]ub.ac.id   \n",
       "1             198406282019031006   \n",
       "2      Dr.Eng. Fitra A. Bachtiar   \n",
       "3  Departemen Teknik Informatika   \n",
       "4               S2 Ilmu Komputer   \n",
       "\n",
       "                                        Link Jawaban  Keterangan Tambahan  \n",
       "0  https://filkom.ub.ac.id/sdm-dosen/?search_f7a8...                  NaN  \n",
       "1                                                NaN                  NaN  \n",
       "2  https://filkom.ub.ac.id/sdm-dosen/?search_f7a8...                  NaN  \n",
       "3  https://filkom.ub.ac.id/sdm-dosen/?search_f7a8...                  NaN  \n",
       "4  https://filkom.ub.ac.id/sdm-dosen/?search_f7a8...                  NaN  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knowledgebase = pd.read_excel('https://raw.githubusercontent.com/AndiAlifs/FLUENT-Chatbot-2023/main/KnowledgeBaseFilkom.xlsx', engine='openpyxl')\n",
    "knowledgebase.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pertanyaan</th>\n",
       "      <th>Jawaban</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>email Fitra A. Bachtiar</td>\n",
       "      <td>fitra.bachtiar[at]ub.ac.id</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NIK/NIP Fitra A. Bachtiar</td>\n",
       "      <td>198406282019031006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nama lengkap Fitra A. Bachtiar</td>\n",
       "      <td>Dr.Eng. Fitra A. Bachtiar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Departemen Fitra A. Bachtiar</td>\n",
       "      <td>Departemen Teknik Informatika</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Program Studi Fitra A. Bachtiar</td>\n",
       "      <td>S2 Ilmu Komputer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1229</th>\n",
       "      <td>Apa Manfaat Konseling FILKOM ?</td>\n",
       "      <td>1. Masalah ditangani oleh ahli yang kompeten d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1230</th>\n",
       "      <td>Berikan informasi mengenai Layanan Konseling</td>\n",
       "      <td>Informasi mengenai Layanan Konseling dapat dia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1231</th>\n",
       "      <td>Siapa Konselor Bimbingan dan Konseling di FILK...</td>\n",
       "      <td>Ada 2 konselor Bimbingan dan Konseling di FILK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1232</th>\n",
       "      <td>Siapa Koordinator Konselor Sebaya ?</td>\n",
       "      <td>Koordinator Konselor Sebaya adalah Muhammad Da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1233</th>\n",
       "      <td>Berikan Rincian Layanan ULTKSP</td>\n",
       "      <td>Rincian Layanan ULTKSP dapat diakses pada taut...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1198 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Pertanyaan  \\\n",
       "0                               email Fitra A. Bachtiar   \n",
       "1                             NIK/NIP Fitra A. Bachtiar   \n",
       "2                        nama lengkap Fitra A. Bachtiar   \n",
       "3                          Departemen Fitra A. Bachtiar   \n",
       "4                       Program Studi Fitra A. Bachtiar   \n",
       "...                                                 ...   \n",
       "1229                     Apa Manfaat Konseling FILKOM ?   \n",
       "1230       Berikan informasi mengenai Layanan Konseling   \n",
       "1231  Siapa Konselor Bimbingan dan Konseling di FILK...   \n",
       "1232                Siapa Koordinator Konselor Sebaya ?   \n",
       "1233                     Berikan Rincian Layanan ULTKSP   \n",
       "\n",
       "                                                Jawaban  \n",
       "0                            fitra.bachtiar[at]ub.ac.id  \n",
       "1                                    198406282019031006  \n",
       "2                             Dr.Eng. Fitra A. Bachtiar  \n",
       "3                         Departemen Teknik Informatika  \n",
       "4                                      S2 Ilmu Komputer  \n",
       "...                                                 ...  \n",
       "1229  1. Masalah ditangani oleh ahli yang kompeten d...  \n",
       "1230  Informasi mengenai Layanan Konseling dapat dia...  \n",
       "1231  Ada 2 konselor Bimbingan dan Konseling di FILK...  \n",
       "1232  Koordinator Konselor Sebaya adalah Muhammad Da...  \n",
       "1233  Rincian Layanan ULTKSP dapat diakses pada taut...  \n",
       "\n",
       "[1198 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_paired = knowledgebase.drop(columns=knowledgebase.columns.drop(['Pertanyaan', 'Jawaban']))\n",
    "qa_paired.dropna(inplace=True)\n",
    "qa_paired"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all the punctuation from qa_paired\n",
    "qa_paired['Pertanyaan'] = qa_paired['Pertanyaan'].str.replace('[^\\w\\s]',' ').str.lower()\n",
    "qa_paired['Jawaban'] = qa_paired['Jawaban'].str.replace('[^\\w\\s]',' ').str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = sorted(set(\" \".join(qa_paired['Pertanyaan'] +\" \" + qa_paired['Jawaban']).split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VOCAB SIZE : 2600\n"
     ]
    }
   ],
   "source": [
    "questions = qa_paired['Pertanyaan']\n",
    "\n",
    "answers = list()\n",
    "for i in range( len(qa_paired['Jawaban']) ) :\n",
    "    answers.append( '<START> ' + \" \".join(qa_paired['Jawaban'].iloc[i].split()) + ' <END>' )\n",
    "\n",
    "tokenizer = preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts( questions + answers )\n",
    "VOCAB_SIZE = len( tokenizer.word_index )+1\n",
    "print( 'VOCAB SIZE : {}'.format( VOCAB_SIZE ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder-Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = []\n",
    "for word in tokenizer.word_index:\n",
    "  vocab.append(word)\n",
    "\n",
    "def tokenize(sentences):\n",
    "  tokens_list = []\n",
    "  vocabulary = []\n",
    "  for sentence in sentences:\n",
    "    sentence = sentence.lower()\n",
    "    sentence = re.sub('[^a-zA-Z]', ' ', sentence)\n",
    "    tokens = sentence.split()\n",
    "    vocabulary += tokens\n",
    "    tokens_list.append(tokens)\n",
    "  return tokens_list, vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1198, 13) 13\n"
     ]
    }
   ],
   "source": [
    "#encoder_input_data\n",
    "tokenized_questions = tokenizer.texts_to_sequences( questions )\n",
    "maxlen_questions = max( [len(x) for x in tokenized_questions ] )\n",
    "padded_questions = preprocessing.sequence.pad_sequences( tokenized_questions, maxlen = maxlen_questions, padding = 'post')\n",
    "encoder_input_data = np.array(padded_questions)\n",
    "print(encoder_input_data.shape, maxlen_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1198, 294) 294\n"
     ]
    }
   ],
   "source": [
    "# decoder_input_data\n",
    "tokenized_answers = tokenizer.texts_to_sequences( answers )\n",
    "maxlen_answers = max( [ len(x) for x in tokenized_answers ] )\n",
    "padded_answers = preprocessing.sequence.pad_sequences( tokenized_answers , maxlen=maxlen_answers , padding='post' )\n",
    "decoder_input_data = np.array( padded_answers )\n",
    "print( decoder_input_data.shape , maxlen_answers )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1198, 294, 2600)\n"
     ]
    }
   ],
   "source": [
    "# decoder_output_data\n",
    "tokenized_answers = tokenizer.texts_to_sequences( answers )\n",
    "for i in range(len(tokenized_answers)) :\n",
    "    tokenized_answers[i] = tokenized_answers[i][1:]\n",
    "padded_answers = preprocessing.sequence.pad_sequences( tokenized_answers , maxlen=maxlen_answers , padding='post' )\n",
    "onehot_answers = utils.to_categorical( padded_answers , VOCAB_SIZE )\n",
    "decoder_output_data = np.array( onehot_answers )\n",
    "print( decoder_output_data.shape )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# define the checkpoint\n",
    "checkpoint = ModelCheckpoint('model_weights.h5', monitor='loss', verbose=1, save_best_only=True, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset model\n",
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 13)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 294)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 13, 500)      1300000     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 294, 500)     1300000     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 500), (None, 2002000     embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, 294, 500), ( 2002000     embedding_1[0][0]                \n",
      "                                                                 lstm[0][1]                       \n",
      "                                                                 lstm[0][2]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 294, 2600)    1302600     lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 7,906,600\n",
      "Trainable params: 7,906,600\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_inputs = tf.keras.layers.Input(shape=( maxlen_questions , ))\n",
    "encoder_embedding = tf.keras.layers.Embedding( VOCAB_SIZE, 500 , mask_zero=True ) (encoder_inputs)\n",
    "encoder_outputs , state_h , state_c = tf.keras.layers.LSTM( 500 , return_state=True )( encoder_embedding )\n",
    "encoder_states = [ state_h , state_c ]\n",
    "\n",
    "decoder_inputs = tf.keras.layers.Input(shape=( maxlen_answers ,  ))\n",
    "decoder_embedding = tf.keras.layers.Embedding( VOCAB_SIZE, 500 , mask_zero=True) (decoder_inputs)\n",
    "decoder_lstm = tf.keras.layers.LSTM( 500 , return_state=True , return_sequences=True )\n",
    "decoder_outputs , _ , _ = decoder_lstm ( decoder_embedding , initial_state=encoder_states )\n",
    "decoder_dense = tf.keras.layers.Dense( VOCAB_SIZE , activation=tf.keras.activations.softmax ) \n",
    "output = decoder_dense ( decoder_outputs )\n",
    "\n",
    "model = tf.keras.models.Model([encoder_inputs, decoder_inputs], output )\n",
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(), loss='categorical_crossentropy')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "24/24 [==============================] - 43s 2s/step - loss: 0.4465\n",
      "\n",
      "Epoch 00001: loss improved from inf to 0.44654, saving model to model_weights.h5\n",
      "Epoch 2/100\n",
      "24/24 [==============================] - 37s 2s/step - loss: 0.3896\n",
      "\n",
      "Epoch 00002: loss improved from 0.44654 to 0.38959, saving model to model_weights.h5\n",
      "Epoch 3/100\n",
      "24/24 [==============================] - 36s 2s/step - loss: 0.3584\n",
      "\n",
      "Epoch 00003: loss improved from 0.38959 to 0.35843, saving model to model_weights.h5\n",
      "Epoch 4/100\n",
      "24/24 [==============================] - 37s 2s/step - loss: 0.3326\n",
      "\n",
      "Epoch 00004: loss improved from 0.35843 to 0.33257, saving model to model_weights.h5\n",
      "Epoch 5/100\n",
      "24/24 [==============================] - 37s 2s/step - loss: 0.3129\n",
      "\n",
      "Epoch 00005: loss improved from 0.33257 to 0.31286, saving model to model_weights.h5\n",
      "Epoch 6/100\n",
      "24/24 [==============================] - 37s 2s/step - loss: 0.2912\n",
      "\n",
      "Epoch 00006: loss improved from 0.31286 to 0.29119, saving model to model_weights.h5\n",
      "Epoch 7/100\n",
      "24/24 [==============================] - 37s 2s/step - loss: 0.2734\n",
      "\n",
      "Epoch 00007: loss improved from 0.29119 to 0.27340, saving model to model_weights.h5\n",
      "Epoch 8/100\n",
      "24/24 [==============================] - 37s 2s/step - loss: 0.2569\n",
      "\n",
      "Epoch 00008: loss improved from 0.27340 to 0.25689, saving model to model_weights.h5\n",
      "Epoch 9/100\n",
      "24/24 [==============================] - 36s 2s/step - loss: 0.2413\n",
      "\n",
      "Epoch 00009: loss improved from 0.25689 to 0.24128, saving model to model_weights.h5\n",
      "Epoch 10/100\n",
      "24/24 [==============================] - 36s 2s/step - loss: 0.2267\n",
      "\n",
      "Epoch 00010: loss improved from 0.24128 to 0.22667, saving model to model_weights.h5\n",
      "Epoch 11/100\n",
      "24/24 [==============================] - 37s 2s/step - loss: 0.2133\n",
      "\n",
      "Epoch 00011: loss improved from 0.22667 to 0.21329, saving model to model_weights.h5\n",
      "Epoch 12/100\n",
      "24/24 [==============================] - 37s 2s/step - loss: 0.2012\n",
      "\n",
      "Epoch 00012: loss improved from 0.21329 to 0.20116, saving model to model_weights.h5\n",
      "Epoch 13/100\n",
      "24/24 [==============================] - 36s 2s/step - loss: 0.1897\n",
      "\n",
      "Epoch 00013: loss improved from 0.20116 to 0.18970, saving model to model_weights.h5\n",
      "Epoch 14/100\n",
      "24/24 [==============================] - 37s 2s/step - loss: 0.1782\n",
      "\n",
      "Epoch 00014: loss improved from 0.18970 to 0.17817, saving model to model_weights.h5\n",
      "Epoch 15/100\n",
      "24/24 [==============================] - 37s 2s/step - loss: 0.1681\n",
      "\n",
      "Epoch 00015: loss improved from 0.17817 to 0.16806, saving model to model_weights.h5\n",
      "Epoch 16/100\n",
      "24/24 [==============================] - 36s 2s/step - loss: 0.1572\n",
      "\n",
      "Epoch 00016: loss improved from 0.16806 to 0.15722, saving model to model_weights.h5\n",
      "Epoch 17/100\n",
      "24/24 [==============================] - 36s 2s/step - loss: 0.1457\n",
      "\n",
      "Epoch 00017: loss improved from 0.15722 to 0.14575, saving model to model_weights.h5\n",
      "Epoch 18/100\n",
      "24/24 [==============================] - 36s 2s/step - loss: 0.1369\n",
      "\n",
      "Epoch 00018: loss improved from 0.14575 to 0.13692, saving model to model_weights.h5\n",
      "Epoch 19/100\n",
      "24/24 [==============================] - 37s 2s/step - loss: 0.1268\n",
      "\n",
      "Epoch 00019: loss improved from 0.13692 to 0.12681, saving model to model_weights.h5\n",
      "Epoch 20/100\n",
      "24/24 [==============================] - 36s 2s/step - loss: 0.1174\n",
      "\n",
      "Epoch 00020: loss improved from 0.12681 to 0.11744, saving model to model_weights.h5\n",
      "Epoch 21/100\n",
      "24/24 [==============================] - 36s 2s/step - loss: 0.1091\n",
      "\n",
      "Epoch 00021: loss improved from 0.11744 to 0.10912, saving model to model_weights.h5\n",
      "Epoch 22/100\n",
      "24/24 [==============================] - 37s 2s/step - loss: 0.1018\n",
      "\n",
      "Epoch 00022: loss improved from 0.10912 to 0.10181, saving model to model_weights.h5\n",
      "Epoch 23/100\n",
      "24/24 [==============================] - 36s 2s/step - loss: 0.0927\n",
      "\n",
      "Epoch 00023: loss improved from 0.10181 to 0.09269, saving model to model_weights.h5\n",
      "Epoch 24/100\n",
      "24/24 [==============================] - 36s 2s/step - loss: 0.0864\n",
      "\n",
      "Epoch 00024: loss improved from 0.09269 to 0.08637, saving model to model_weights.h5\n",
      "Epoch 25/100\n",
      "24/24 [==============================] - 37s 2s/step - loss: 0.0786\n",
      "\n",
      "Epoch 00025: loss improved from 0.08637 to 0.07865, saving model to model_weights.h5\n",
      "Epoch 26/100\n",
      "24/24 [==============================] - 36s 2s/step - loss: 0.0725\n",
      "\n",
      "Epoch 00026: loss improved from 0.07865 to 0.07252, saving model to model_weights.h5\n",
      "Epoch 27/100\n",
      "24/24 [==============================] - 37s 2s/step - loss: 0.0669\n",
      "\n",
      "Epoch 00027: loss improved from 0.07252 to 0.06688, saving model to model_weights.h5\n",
      "Epoch 28/100\n",
      "24/24 [==============================] - 37s 2s/step - loss: 0.0607\n",
      "\n",
      "Epoch 00028: loss improved from 0.06688 to 0.06075, saving model to model_weights.h5\n",
      "Epoch 29/100\n",
      "24/24 [==============================] - 37s 2s/step - loss: 0.0558\n",
      "\n",
      "Epoch 00029: loss improved from 0.06075 to 0.05585, saving model to model_weights.h5\n",
      "Epoch 30/100\n",
      "24/24 [==============================] - 36s 2s/step - loss: 0.0514\n",
      "\n",
      "Epoch 00030: loss improved from 0.05585 to 0.05137, saving model to model_weights.h5\n",
      "Epoch 31/100\n",
      "24/24 [==============================] - 37s 2s/step - loss: 0.0462\n",
      "\n",
      "Epoch 00031: loss improved from 0.05137 to 0.04623, saving model to model_weights.h5\n",
      "Epoch 32/100\n",
      "24/24 [==============================] - 37s 2s/step - loss: 0.0424\n",
      "\n",
      "Epoch 00032: loss improved from 0.04623 to 0.04238, saving model to model_weights.h5\n",
      "Epoch 33/100\n",
      "24/24 [==============================] - 37s 2s/step - loss: 0.0381\n",
      "\n",
      "Epoch 00033: loss improved from 0.04238 to 0.03810, saving model to model_weights.h5\n",
      "Epoch 34/100\n",
      "24/24 [==============================] - 36s 2s/step - loss: 0.0349\n",
      "\n",
      "Epoch 00034: loss improved from 0.03810 to 0.03490, saving model to model_weights.h5\n",
      "Epoch 35/100\n",
      "24/24 [==============================] - 36s 2s/step - loss: 0.0315\n",
      "\n",
      "Epoch 00035: loss improved from 0.03490 to 0.03148, saving model to model_weights.h5\n",
      "Epoch 36/100\n",
      "24/24 [==============================] - 36s 2s/step - loss: 0.0288\n",
      "\n",
      "Epoch 00036: loss improved from 0.03148 to 0.02877, saving model to model_weights.h5\n",
      "Epoch 37/100\n",
      "24/24 [==============================] - 36s 2s/step - loss: 0.0260\n",
      "\n",
      "Epoch 00037: loss improved from 0.02877 to 0.02598, saving model to model_weights.h5\n",
      "Epoch 38/100\n",
      "24/24 [==============================] - 37s 2s/step - loss: 0.0234\n",
      "\n",
      "Epoch 00038: loss improved from 0.02598 to 0.02344, saving model to model_weights.h5\n",
      "Epoch 39/100\n",
      "24/24 [==============================] - 37s 2s/step - loss: 0.0215\n",
      "\n",
      "Epoch 00039: loss improved from 0.02344 to 0.02153, saving model to model_weights.h5\n",
      "Epoch 40/100\n",
      "24/24 [==============================] - 37s 2s/step - loss: 0.0192\n",
      "\n",
      "Epoch 00040: loss improved from 0.02153 to 0.01923, saving model to model_weights.h5\n",
      "Epoch 41/100\n",
      "24/24 [==============================] - 37s 2s/step - loss: 0.0176\n",
      "\n",
      "Epoch 00041: loss improved from 0.01923 to 0.01758, saving model to model_weights.h5\n",
      "Epoch 42/100\n",
      "24/24 [==============================] - 36s 1s/step - loss: 0.0160\n",
      "\n",
      "Epoch 00042: loss improved from 0.01758 to 0.01600, saving model to model_weights.h5\n",
      "Epoch 43/100\n",
      "24/24 [==============================] - 35s 1s/step - loss: 0.0144\n",
      "\n",
      "Epoch 00043: loss improved from 0.01600 to 0.01444, saving model to model_weights.h5\n",
      "Epoch 44/100\n",
      "24/24 [==============================] - 35s 1s/step - loss: 0.0132\n",
      "\n",
      "Epoch 00044: loss improved from 0.01444 to 0.01324, saving model to model_weights.h5\n",
      "Epoch 45/100\n",
      "24/24 [==============================] - 35s 1s/step - loss: 0.0121\n",
      "\n",
      "Epoch 00045: loss improved from 0.01324 to 0.01214, saving model to model_weights.h5\n",
      "Epoch 46/100\n",
      "24/24 [==============================] - 35s 1s/step - loss: 0.0112\n",
      "\n",
      "Epoch 00046: loss improved from 0.01214 to 0.01115, saving model to model_weights.h5\n",
      "Epoch 47/100\n",
      "24/24 [==============================] - 35s 1s/step - loss: 0.0101\n",
      "\n",
      "Epoch 00047: loss improved from 0.01115 to 0.01008, saving model to model_weights.h5\n",
      "Epoch 48/100\n",
      "24/24 [==============================] - 35s 1s/step - loss: 0.0094\n",
      "\n",
      "Epoch 00048: loss improved from 0.01008 to 0.00942, saving model to model_weights.h5\n",
      "Epoch 49/100\n",
      "24/24 [==============================] - 35s 1s/step - loss: 0.0087\n",
      "\n",
      "Epoch 00049: loss improved from 0.00942 to 0.00867, saving model to model_weights.h5\n",
      "Epoch 50/100\n",
      "24/24 [==============================] - 35s 1s/step - loss: 0.0081\n",
      "\n",
      "Epoch 00050: loss improved from 0.00867 to 0.00809, saving model to model_weights.h5\n",
      "Epoch 51/100\n",
      "24/24 [==============================] - 35s 1s/step - loss: 0.0074\n",
      "\n",
      "Epoch 00051: loss improved from 0.00809 to 0.00739, saving model to model_weights.h5\n",
      "Epoch 52/100\n",
      "24/24 [==============================] - 35s 1s/step - loss: 0.0068\n",
      "\n",
      "Epoch 00052: loss improved from 0.00739 to 0.00682, saving model to model_weights.h5\n",
      "Epoch 53/100\n",
      "24/24 [==============================] - 35s 1s/step - loss: 0.0065\n",
      "\n",
      "Epoch 00053: loss improved from 0.00682 to 0.00654, saving model to model_weights.h5\n",
      "Epoch 54/100\n",
      "24/24 [==============================] - 35s 1s/step - loss: 0.0060\n",
      "\n",
      "Epoch 00054: loss improved from 0.00654 to 0.00603, saving model to model_weights.h5\n",
      "Epoch 55/100\n",
      "24/24 [==============================] - 36s 1s/step - loss: 0.0056\n",
      "\n",
      "Epoch 00055: loss improved from 0.00603 to 0.00562, saving model to model_weights.h5\n",
      "Epoch 56/100\n",
      "24/24 [==============================] - 36s 1s/step - loss: 0.0054\n",
      "\n",
      "Epoch 00056: loss improved from 0.00562 to 0.00541, saving model to model_weights.h5\n",
      "Epoch 57/100\n",
      "24/24 [==============================] - 36s 1s/step - loss: 0.0051\n",
      "\n",
      "Epoch 00057: loss improved from 0.00541 to 0.00513, saving model to model_weights.h5\n",
      "Epoch 58/100\n",
      "24/24 [==============================] - 35s 1s/step - loss: 0.0049\n",
      "\n",
      "Epoch 00058: loss improved from 0.00513 to 0.00486, saving model to model_weights.h5\n",
      "Epoch 59/100\n",
      "24/24 [==============================] - 36s 1s/step - loss: 0.0041\n",
      "\n",
      "Epoch 00062: loss improved from 0.00423 to 0.00412, saving model to model_weights.h5\n",
      "Epoch 63/100\n",
      "24/24 [==============================] - 36s 1s/step - loss: 0.0040\n",
      "\n",
      "Epoch 00063: loss improved from 0.00412 to 0.00400, saving model to model_weights.h5\n",
      "Epoch 64/100\n",
      "24/24 [==============================] - 35s 1s/step - loss: 0.0039\n",
      "\n",
      "Epoch 00064: loss improved from 0.00400 to 0.00387, saving model to model_weights.h5\n",
      "Epoch 65/100\n",
      "24/24 [==============================] - 35s 1s/step - loss: 0.0038\n",
      "\n",
      "Epoch 00065: loss improved from 0.00387 to 0.00375, saving model to model_weights.h5\n",
      "Epoch 66/100\n",
      "24/24 [==============================] - 35s 1s/step - loss: 0.0035\n",
      "\n",
      "Epoch 00066: loss improved from 0.00375 to 0.00352, saving model to model_weights.h5\n",
      "Epoch 67/100\n",
      "24/24 [==============================] - 36s 1s/step - loss: 0.0035\n",
      "\n",
      "Epoch 00067: loss improved from 0.00352 to 0.00347, saving model to model_weights.h5\n",
      "Epoch 68/100\n",
      "24/24 [==============================] - 35s 1s/step - loss: 0.0034\n",
      "\n",
      "Epoch 00068: loss improved from 0.00347 to 0.00338, saving model to model_weights.h5\n",
      "Epoch 69/100\n",
      "24/24 [==============================] - 35s 1s/step - loss: 0.0033\n",
      "\n",
      "Epoch 00069: loss improved from 0.00338 to 0.00329, saving model to model_weights.h5\n",
      "Epoch 70/100\n",
      "24/24 [==============================] - 35s 1s/step - loss: 0.0032\n",
      "\n",
      "Epoch 00070: loss improved from 0.00329 to 0.00321, saving model to model_weights.h5\n",
      "Epoch 71/100\n",
      "24/24 [==============================] - 35s 1s/step - loss: 0.0031\n",
      "\n",
      "Epoch 00071: loss improved from 0.00321 to 0.00309, saving model to model_weights.h5\n",
      "Epoch 72/100\n",
      "24/24 [==============================] - 35s 1s/step - loss: 0.0031\n",
      "\n",
      "Epoch 00072: loss improved from 0.00309 to 0.00307, saving model to model_weights.h5\n",
      "Epoch 73/100\n",
      "24/24 [==============================] - 36s 1s/step - loss: 0.0029\n",
      "\n",
      "Epoch 00073: loss improved from 0.00307 to 0.00292, saving model to model_weights.h5\n",
      "Epoch 74/100\n",
      "24/24 [==============================] - 36s 1s/step - loss: 0.0029\n",
      "\n",
      "Epoch 00074: loss improved from 0.00292 to 0.00289, saving model to model_weights.h5\n",
      "Epoch 75/100\n",
      "24/24 [==============================] - 35s 1s/step - loss: 0.0029\n",
      "\n",
      "Epoch 00075: loss improved from 0.00289 to 0.00287, saving model to model_weights.h5\n",
      "Epoch 76/100\n",
      "24/24 [==============================] - 36s 1s/step - loss: 0.0030\n",
      "\n",
      "Epoch 00076: loss did not improve from 0.00287\n",
      "Epoch 77/100\n",
      "24/24 [==============================] - 36s 1s/step - loss: 0.0028\n",
      "\n",
      "Epoch 00077: loss improved from 0.00287 to 0.00278, saving model to model_weights.h5\n",
      "Epoch 78/100\n",
      "24/24 [==============================] - 35s 1s/step - loss: 0.0027\n",
      "\n",
      "Epoch 00078: loss improved from 0.00278 to 0.00268, saving model to model_weights.h5\n",
      "Epoch 79/100\n",
      "24/24 [==============================] - 35s 1s/step - loss: 0.0031\n",
      "\n",
      "Epoch 00079: loss did not improve from 0.00268\n",
      "Epoch 80/100\n",
      "24/24 [==============================] - 35s 1s/step - loss: 0.0026\n",
      "\n",
      "Epoch 00080: loss improved from 0.00268 to 0.00256, saving model to model_weights.h5\n",
      "Epoch 81/100\n",
      "24/24 [==============================] - 35s 1s/step - loss: 0.0026\n",
      "\n",
      "Epoch 00081: loss did not improve from 0.00256\n",
      "Epoch 82/100\n",
      "24/24 [==============================] - 35s 1s/step - loss: 0.0026\n",
      "\n",
      "Epoch 00082: loss improved from 0.00256 to 0.00255, saving model to model_weights.h5\n",
      "Epoch 83/100\n",
      "24/24 [==============================] - 35s 1s/step - loss: 0.0026\n",
      "\n",
      "Epoch 00083: loss did not improve from 0.00255\n",
      "Epoch 84/100\n",
      "24/24 [==============================] - 35s 1s/step - loss: 0.0026\n",
      "\n",
      "Epoch 00084: loss did not improve from 0.00255\n",
      "Epoch 85/100\n",
      "24/24 [==============================] - 36s 1s/step - loss: 0.0025\n",
      "\n",
      "Epoch 00085: loss improved from 0.00255 to 0.00247, saving model to model_weights.h5\n",
      "Epoch 86/100\n",
      "24/24 [==============================] - 35s 1s/step - loss: 0.0025\n",
      "\n",
      "Epoch 00086: loss improved from 0.00247 to 0.00246, saving model to model_weights.h5\n",
      "Epoch 87/100\n",
      "24/24 [==============================] - 36s 1s/step - loss: 0.0025\n",
      "\n",
      "Epoch 00087: loss did not improve from 0.00246\n",
      "Epoch 88/100\n",
      "24/24 [==============================] - 36s 1s/step - loss: 0.0025\n",
      "\n",
      "Epoch 00088: loss did not improve from 0.00246\n",
      "Epoch 89/100\n",
      "24/24 [==============================] - 35s 1s/step - loss: 0.0025\n",
      "\n",
      "Epoch 00089: loss did not improve from 0.00246\n",
      "Epoch 90/100\n",
      "24/24 [==============================] - 35s 1s/step - loss: 0.0023\n",
      "\n",
      "Epoch 00090: loss improved from 0.00246 to 0.00234, saving model to model_weights.h5\n",
      "Epoch 91/100\n",
      "24/24 [==============================] - 36s 1s/step - loss: 0.0024\n",
      "\n",
      "Epoch 00091: loss did not improve from 0.00234\n",
      "Epoch 92/100\n",
      "24/24 [==============================] - 35s 1s/step - loss: 0.0024\n",
      "\n",
      "Epoch 00092: loss did not improve from 0.00234\n",
      "Epoch 93/100\n",
      "24/24 [==============================] - 35s 1s/step - loss: 0.0024\n",
      "\n",
      "Epoch 00093: loss did not improve from 0.00234\n",
      "Epoch 94/100\n",
      "24/24 [==============================] - 36s 1s/step - loss: 0.0024\n",
      "\n",
      "Epoch 00094: loss did not improve from 0.00234\n",
      "Epoch 95/100\n",
      "24/24 [==============================] - 36s 1s/step - loss: 0.0023\n",
      "\n",
      "Epoch 00095: loss improved from 0.00234 to 0.00231, saving model to model_weights.h5\n",
      "Epoch 96/100\n",
      "24/24 [==============================] - 35s 1s/step - loss: 0.0024\n",
      "\n",
      "Epoch 00096: loss did not improve from 0.00231\n",
      "Epoch 97/100\n",
      "24/24 [==============================] - 36s 1s/step - loss: 0.0023\n",
      "\n",
      "Epoch 00097: loss did not improve from 0.00231\n",
      "Epoch 98/100\n",
      "24/24 [==============================] - 36s 1s/step - loss: 0.0023\n",
      "\n",
      "Epoch 00098: loss improved from 0.00231 to 0.00229, saving model to model_weights.h5\n",
      "Epoch 99/100\n",
      "24/24 [==============================] - 36s 1s/step - loss: 0.0023\n",
      "\n",
      "Epoch 00099: loss did not improve from 0.00229\n",
      "Epoch 100/100\n",
      "24/24 [==============================] - 36s 1s/step - loss: 0.0024\n",
      "\n",
      "Epoch 00100: loss did not improve from 0.00229\n"
     ]
    }
   ],
   "source": [
    "history = model.fit([encoder_input_data , decoder_input_data], \n",
    "                    decoder_output_data, \n",
    "                    batch_size=50, \n",
    "                    epochs=100,\n",
    "                    callbacks=[checkpoint] ) \n",
    "# model.save( 'model.h5' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model best weights\n",
    "model.load_weights('model_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualization = pd.DataFrame(history.history)\n",
    "visualization[['loss']].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_inference_models():\n",
    "    \n",
    "    encoder_model = tf.keras.models.Model(encoder_inputs, encoder_states)\n",
    "    \n",
    "    decoder_state_input_h = tf.keras.layers.Input(shape=( 500 ,))\n",
    "    decoder_state_input_c = tf.keras.layers.Input(shape=( 500 ,))\n",
    "    \n",
    "    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "    \n",
    "    decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "        decoder_embedding , initial_state=decoder_states_inputs)\n",
    "    \n",
    "    decoder_states = [state_h, state_c]\n",
    "\n",
    "    decoder_outputs = decoder_dense(decoder_outputs)\n",
    "    \n",
    "    decoder_model = tf.keras.models.Model(\n",
    "        [decoder_inputs] + decoder_states_inputs,\n",
    "        [decoder_outputs] + decoder_states)\n",
    "    \n",
    "    return encoder_model , decoder_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_tokens( sentence : str ):\n",
    "\n",
    "    words = sentence.lower().split()\n",
    "    tokens_list = list()\n",
    "\n",
    "    for word in words:\n",
    "        tokens_list.append( tokenizer.word_index[ word ] ) \n",
    "    return preprocessing.sequence.pad_sequences( [tokens_list] , maxlen=maxlen_questions , padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 294) for input KerasTensor(type_spec=TensorSpec(shape=(None, 294), dtype=tf.float32, name='input_2'), name='input_2', description=\"created by layer 'input_2'\"), but it was called on an input with incompatible shape (None, 1).\n",
      "Pertanyaan :  informasi kalender akademik\n",
      "Jawaban :  https docs google com spreadsheets d 1wiefeh2rbdljzyznv8 lmbv8azejqqqtndkdntwv63g edit gid 213487730 end\n"
     ]
    }
   ],
   "source": [
    "# disable warning tensorflow\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '5'\n",
    "\n",
    "enc_model , dec_model = make_inference_models()\n",
    "\n",
    "quest = input(\"Enter question : \")\n",
    "\n",
    "states_values = enc_model.predict( str_to_tokens(quest) )\n",
    "empty_target_seq = np.zeros( ( 1 , 1 ) )\n",
    "empty_target_seq[0, 0] = tokenizer.word_index['start']\n",
    "stop_condition = False\n",
    "decoded_translation = ''\n",
    "while not stop_condition :\n",
    "    dec_outputs , h , c = dec_model.predict([ empty_target_seq ] + states_values )\n",
    "    sampled_word_index = np.argmax( dec_outputs[0, -1, :] )\n",
    "    sampled_word = None\n",
    "    for word , index in tokenizer.word_index.items() :\n",
    "        if sampled_word_index == index :\n",
    "            decoded_translation += ' {}'.format( word )\n",
    "            sampled_word = word\n",
    "    \n",
    "    if sampled_word == 'end' or len(decoded_translation.split()) > maxlen_answers:\n",
    "        stop_condition = True\n",
    "        \n",
    "    empty_target_seq = np.zeros( ( 1 , 1 ) )  \n",
    "    empty_target_seq[ 0 , 0 ] = sampled_word_index\n",
    "    states_values = [ h , c ] \n",
    "\n",
    "print(\"Pertanyaan : \", quest.strip())\n",
    "print(\"Jawaban : \", decoded_translation.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting rouge\n",
      "  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: six in ./.local/lib/python3.6/site-packages (from rouge) (1.15.0)\n",
      "Installing collected packages: rouge\n",
      "\u001b[33m  WARNING: The script rouge is installed in '/home/acrig/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "Successfully installed rouge-1.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sacrebleu\n",
    "from sacrebleu.metrics import BLEU\n",
    "import bert_score\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from torchmetrics import BLEUScore, SacreBLEUScore\n",
    "from torchmetrics.text.bert import BERTScore\n",
    "import rouge as rouge_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rouge(preds, real):\n",
    "    scorer = rouge_lib.Rouge()\n",
    "    scorer.get_scores(preds,\n",
    "                    real.tolist(),\n",
    "                    avg=True,\n",
    "                    ignore_empty=True)\n",
    "    return scorer\n",
    "    \n",
    "def calculate_bertscore(preds, real):\n",
    "    bertscorer = bert_score.BERTScorer(idf=True,\n",
    "                      lang='en',\n",
    "                      rescale_with_baseline=True,\n",
    "                      use_fast_tokenizer=True,\n",
    "                      device='cuda')\n",
    "\n",
    "    bertscorer.compute_idf([r for rs in preds for r in rs])\n",
    "    prf = bertscorer.score(preds, real.tolist(), batch_size=64)\n",
    "    return {key: scores.mean().item() for key, scores in zip(('p', 'r', 'f'), prf)}\n",
    "    \n",
    "def calculate_bleu(preds, questions, answers):\n",
    "    bleu_score_1 = 0\n",
    "    bleu_score_2 = 0\n",
    "    bleu_score_3 = 0\n",
    "    bleu_score_4 = 0\n",
    "    bleu_score_all = 0\n",
    "\n",
    "    num_of_rows_calculated = 0\n",
    "\n",
    "    for i, (question, real_answer) in enumerate(zip(questions, answers)):\n",
    "        try:\n",
    "            refs = [real_answer.split(' ')]\n",
    "            hyp = preds[i].split(' ')\n",
    "\n",
    "            bleu_score_1 += sentence_bleu(refs, hyp, weights=(1,0,0,0))\n",
    "            bleu_score_2 += sentence_bleu(refs, hyp, weights=(0,1,0,0))\n",
    "            bleu_score_3 += sentence_bleu(refs, hyp, weights=(0,0,1,0))\n",
    "            bleu_score_4 += sentence_bleu(refs, hyp, weights=(0,0,0,1))\n",
    "            bleu_score_all += sentence_bleu(refs, hyp, weights=(.25,.25,.25,.25))\n",
    "\n",
    "            num_of_rows_calculated+=1\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    results = {\"1-gram\": (bleu_score_1/num_of_rows_calculated),\n",
    "                \"2-gram\": (bleu_score_2/num_of_rows_calculated),\n",
    "                \"3-gram\": (bleu_score_3/num_of_rows_calculated),\n",
    "                \"4-gram\": (bleu_score_all/num_of_rows_calculated)}\n",
    "    \n",
    "    return results\n",
    "\n",
    "def calculate_bleu_final(preds, questions, answers):\n",
    "    for i, (question, real_answer) in enumerate(zip(questions, answers)):\n",
    "        print(i)\n",
    "        print(\"Pertanyaan : \", question.strip())\n",
    "        print(\"Jawaban : \", preds[i].strip())\n",
    "        print(\"Jawaban Sebenarnya : \", real_answer.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_questions = questions\n",
    "real_answers = answers\n",
    "pred_answers = []\n",
    "\n",
    "for i, question in enumerate(real_questions):\n",
    "    real_answers[i] = real_answers[i].replace('<START>', '').replace('<END>', '').strip()\n",
    "    states_values = enc_model.predict( str_to_tokens(question) )\n",
    "    empty_target_seq = np.zeros( ( 1 , 1 ) )\n",
    "    empty_target_seq[0, 0] = tokenizer.word_index['start']\n",
    "    stop_condition = False\n",
    "    decoded_translation = ''\n",
    "    while not stop_condition :\n",
    "        dec_outputs , h , c = dec_model.predict([ empty_target_seq ] + states_values )\n",
    "        sampled_word_index = np.argmax( dec_outputs[0, -1, :] )\n",
    "        sampled_word = None\n",
    "        for word , index in tokenizer.word_index.items() :\n",
    "            if sampled_word_index == index :\n",
    "                decoded_translation += ' {}'.format( word )\n",
    "                sampled_word = word\n",
    "        \n",
    "        if sampled_word == 'end' or len(decoded_translation.split()) > maxlen_answers:\n",
    "            stop_condition = True\n",
    "            \n",
    "        empty_target_seq = np.zeros( ( 1 , 1 ) )  \n",
    "        empty_target_seq[ 0 , 0 ] = sampled_word_index\n",
    "        states_values = [ h , c ] \n",
    "\n",
    "    pred_answers.append(decoded_translation.replace('end', '').strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/acrig/.local/lib/python3.6/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/acrig/.local/lib/python3.6/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/acrig/.local/lib/python3.6/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1-gram': 0.881439926191369, '2-gram': 0.857337447733578, '3-gram': 0.8146137552774978, '4-gram': 0.7286819933377803}\n"
     ]
    }
   ],
   "source": [
    "all_score =  calculate_bleu(pred_answers, real_questions, real_answers)\n",
    "print(all_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8205182806350563\n"
     ]
    }
   ],
   "source": [
    "rata2 = (all_score['1-gram'] + all_score['2-gram'] + all_score['3-gram'] + all_score['4-gram'])/4\n",
    "print(rata2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
