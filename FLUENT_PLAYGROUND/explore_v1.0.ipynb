{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Jan 18 12:08:03 2025       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.141.03   Driver Version: 470.141.03   CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM...  On   | 00000000:47:00.0 Off |                    0 |\n",
      "| N/A   41C    P0   128W / 400W |  81222MiB / 81251MiB |     89%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "import torch.utils.data\n",
    "import math\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.query_linear = nn.Linear(d_model, d_model)\n",
    "        self.key_linear = nn.Linear(d_model, d_model)\n",
    "        self.value_linear = nn.Linear(d_model, d_model)\n",
    "        self.output_linear = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "        \n",
    "        # Linear transformation and splitting into heads\n",
    "        Q = self.query_linear(query).view(batch_size, -1, self.num_heads, self.d_model // self.num_heads).transpose(1, 2)\n",
    "        K = self.key_linear(key).view(batch_size, -1, self.num_heads, self.d_model // self.num_heads).transpose(1, 2)\n",
    "        V = self.value_linear(value).view(batch_size, -1, self.num_heads, self.d_model // self.num_heads).transpose(1, 2)\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.d_model ** 0.5)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float(\"-inf\"))\n",
    "        \n",
    "        attention = nn.Softmax(dim=-1)(scores)\n",
    "        context = torch.matmul(attention, V)\n",
    "        \n",
    "        # Concatenate and linear transformation\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        output = self.output_linear(context)\n",
    "        return output\n",
    "\n",
    "# Define the Feed-Forward Neural Network Layer\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.linear_1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear_2(x)\n",
    "        return x\n",
    "    \n",
    "# Define the positional encoding\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=512):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(0), :]\n",
    "\n",
    "# Define the Transformer Model\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, num_encoder_layers, num_decoder_layers, src_vocab_size, tgt_vocab_size, max_len=512, dropout=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_len=max_len)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.encoder_layers = nn.ModuleList([nn.ModuleList([MultiHeadAttention(d_model, num_heads), FeedForward(d_model, d_ff, dropout=dropout)]) for _ in range(num_encoder_layers)]) \n",
    "        self.decoder_layers = nn.ModuleList([nn.ModuleList([MultiHeadAttention(d_model, num_heads), MultiHeadAttention(d_model, num_heads), FeedForward(d_model, d_ff, dropout=dropout)]) for _ in range(num_decoder_layers)])\n",
    "        \n",
    "        self.fc_out = nn.Linear(d_model, tgt_vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
    "        src_embedding = self.embedding(src)\n",
    "        src_embedding = self.positional_encoding(src_embedding)\n",
    "        src_embedding = self.dropout(src_embedding)\n",
    "\n",
    "        for attention, feed_forward in self.encoder_layers:\n",
    "            src_embedding = attention(src_embedding, src_embedding, src_embedding, mask=src_mask)\n",
    "            src_embedding = feed_forward(src_embedding)\n",
    "\n",
    "        tgt_embedding = self.embedding(tgt)\n",
    "        tgt_embedding = self.positional_encoding(tgt_embedding)\n",
    "        tgt_embedding = self.dropout(tgt_embedding)\n",
    "        \n",
    "        for self_attention, encoder_attention, feed_forward in self.decoder_layers:\n",
    "            tgt_embedding = self_attention(tgt_embedding, tgt_embedding, tgt_embedding, mask=tgt_mask)\n",
    "            tgt_embedding = encoder_attention(tgt_embedding, src_embedding, src_embedding, mask=src_mask)\n",
    "            tgt_embedding = feed_forward(tgt_embedding)\n",
    "\n",
    "        output = self.fc_out(tgt_embedding)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        attn_output = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
    "        attn_output = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
    "        x = self.norm2(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import torch.utils.data\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a text corpus from a file and return as a list of lines\n",
    "def load_corpus(file_path):\n",
    "    with open(file_path, 'r',  errors='ignore') as f:\n",
    "        return f.readlines()\n",
    "\n",
    "# Create a dictionary mapping line IDs to their corresponding text\n",
    "def create_line_dict(lines):\n",
    "    line_dict = {}\n",
    "    for line in lines:\n",
    "        parts = line.split(\" +++$+++ \")\n",
    "        line_dict[parts[0]] = parts[-1]\n",
    "    return line_dict\n",
    "\n",
    "# Remove punctuations and convert text to lowercase\n",
    "def clean_text(text):\n",
    "    punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "    return ''.join(char.lower() for char in text if char not in punctuations)\n",
    "\n",
    "# Create question-answer pairs from conversations\n",
    "def create_qa_pairs(conversations, line_dict):\n",
    "    qa_pairs = []\n",
    "    for conversation in conversations:\n",
    "        ids = eval(conversation.split(\" +++$+++ \")[-1])\n",
    "        for i in range(len(ids) - 1):\n",
    "            question = clean_text(line_dict[ids[i]].strip())\n",
    "            answer = clean_text(line_dict[ids[i+1]].strip())\n",
    "            qa_pairs.append([question.split()[:max_sequence_length], answer.split()[:max_sequence_length]])\n",
    "    return qa_pairs\n",
    "\n",
    "# Encode reply text to integer values\n",
    "def encode_reply(words, word_map, max_length=max_sequence_length):\n",
    "    encoded = [word_map['<start>']]\n",
    "    encoded += [word_map.get(word, word_map['<unk>']) for word in words]\n",
    "    encoded.append(word_map['<end>'])\n",
    "    padding_needed = max_length - len(encoded)\n",
    "    encoded.extend([word_map['<pad>']] * padding_needed)\n",
    "    return encoded\n",
    "\n",
    "# Encode question text to integer values\n",
    "def encode_question(words, word_map, max_length=max_sequence_length):\n",
    "    encoded = [word_map.get(word, word_map['<unk>']) for word in words]\n",
    "    padding_needed = max_length - len(encoded)\n",
    "    encoded.extend([word_map['<pad>']] * padding_needed)\n",
    "    return encoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversations = load_corpus(movie_conversations_path)\n",
    "lines = load_corpus(movie_lines_path)\n",
    "\n",
    "# Create line dictionary\n",
    "line_dict = create_line_dict(lines)\n",
    "\n",
    "# Create question-answer pairs\n",
    "qa_pairs = create_qa_pairs(conversations, line_dict)\n",
    "\n",
    "# Count word frequencies and build vocabulary\n",
    "word_frequency = Counter()\n",
    "for pair in qa_pairs:\n",
    "    word_frequency.update(pair[0])\n",
    "    word_frequency.update(pair[1])\n",
    "\n",
    "min_frequency = 5\n",
    "vocab = [word for word, freq in word_frequency.items() if freq > min_frequency]\n",
    "word_map = {word: idx + 1 for idx, word in enumerate(vocab)}\n",
    "word_map.update({'<unk>': len(word_map) + 1, '<start>': len(word_map) + 2, '<end>': len(word_map) + 3, '<pad>': 0})\n",
    "\n",
    "# Save word map\n",
    "with open('WORDMAP_corpus.json', 'w') as json_file:\n",
    "    json.dump(word_map, json_file)\n",
    "\n",
    "\n",
    "    # Loop through each question-answer pair in the original 'pairs' list\n",
    "pairs_encoded = []\n",
    "for pair in qa_pairs:\n",
    "    # Encode the question part of the pair using the 'encode_question' function\n",
    "    qus = encode_question(pair[0], word_map)\n",
    "    \n",
    "    # Encode the answer part of the pair using the 'encode_reply' function\n",
    "    ans = encode_reply(pair[1], word_map)\n",
    "    \n",
    "    # Append the encoded question and answer as a pair to 'pairs_encoded' list\n",
    "    pairs_encoded.append([qus, ans])\n",
    "\n",
    "# Save the encoded pairs to a JSON file for future use\n",
    "with open('pairs_encoded.json', 'w') as p:\n",
    "    json.dump(pairs_encoded, p)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
