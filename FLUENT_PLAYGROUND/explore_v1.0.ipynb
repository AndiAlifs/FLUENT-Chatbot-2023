{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Jan 18 12:08:03 2025       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 470.141.03   Driver Version: 470.141.03   CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM...  On   | 00000000:47:00.0 Off |                    0 |\n",
      "| N/A   41C    P0   128W / 400W |  81222MiB / 81251MiB |     89%      Default |\n",
      "|                               |                      |             Disabled |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "import torch.utils.data\n",
    "import math\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        self.query_linear = nn.Linear(d_model, d_model)\n",
    "        self.key_linear = nn.Linear(d_model, d_model)\n",
    "        self.value_linear = nn.Linear(d_model, d_model)\n",
    "        self.output_linear = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "        \n",
    "        # Linear transformation and splitting into heads\n",
    "        Q = self.query_linear(query).view(batch_size, -1, self.num_heads, self.d_model // self.num_heads).transpose(1, 2)\n",
    "        K = self.key_linear(key).view(batch_size, -1, self.num_heads, self.d_model // self.num_heads).transpose(1, 2)\n",
    "        V = self.value_linear(value).view(batch_size, -1, self.num_heads, self.d_model // self.num_heads).transpose(1, 2)\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.d_model ** 0.5)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float(\"-inf\"))\n",
    "        \n",
    "        attention = nn.Softmax(dim=-1)(scores)\n",
    "        context = torch.matmul(attention, V)\n",
    "        \n",
    "        # Concatenate and linear transformation\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        output = self.output_linear(context)\n",
    "        return output\n",
    "\n",
    "# Define the Feed-Forward Neural Network Layer\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.linear_1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear_2(x)\n",
    "        return x\n",
    "    \n",
    "# Define the positional encoding\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=512):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(0), :]\n",
    "\n",
    "# Define the Transformer Model\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, num_encoder_layers, num_decoder_layers, src_vocab_size, tgt_vocab_size, max_len=512, dropout=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_len=max_len)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.encoder_layers = nn.ModuleList([nn.ModuleList([MultiHeadAttention(d_model, num_heads), FeedForward(d_model, d_ff, dropout=dropout)]) for _ in range(num_encoder_layers)]) \n",
    "        self.decoder_layers = nn.ModuleList([nn.ModuleList([MultiHeadAttention(d_model, num_heads), MultiHeadAttention(d_model, num_heads), FeedForward(d_model, d_ff, dropout=dropout)]) for _ in range(num_decoder_layers)])\n",
    "        \n",
    "        self.fc_out = nn.Linear(d_model, tgt_vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
    "        src_embedding = self.embedding(src)\n",
    "        src_embedding = self.positional_encoding(src_embedding)\n",
    "        src_embedding = self.dropout(src_embedding)\n",
    "\n",
    "        for attention, feed_forward in self.encoder_layers:\n",
    "            src_embedding = attention(src_embedding, src_embedding, src_embedding, mask=src_mask)\n",
    "            src_embedding = feed_forward(src_embedding)\n",
    "\n",
    "        tgt_embedding = self.embedding(tgt)\n",
    "        tgt_embedding = self.positional_encoding(tgt_embedding)\n",
    "        tgt_embedding = self.dropout(tgt_embedding)\n",
    "        \n",
    "        for self_attention, encoder_attention, feed_forward in self.decoder_layers:\n",
    "            tgt_embedding = self_attention(tgt_embedding, tgt_embedding, tgt_embedding, mask=tgt_mask)\n",
    "            tgt_embedding = encoder_attention(tgt_embedding, src_embedding, src_embedding, mask=src_mask)\n",
    "            tgt_embedding = feed_forward(tgt_embedding)\n",
    "\n",
    "        output = self.fc_out(tgt_embedding)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        attn_output = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
    "        attn_output = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
    "        x = self.norm2(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import torch.utils.data\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a text corpus from a file and return as a list of lines\n",
    "def load_corpus(file_path):\n",
    "    with open(file_path, 'r',  errors='ignore') as f:\n",
    "        return f.readlines()\n",
    "\n",
    "# Create a dictionary mapping line IDs to their corresponding text\n",
    "def create_line_dict(lines):\n",
    "    line_dict = {}\n",
    "    for line in lines:\n",
    "        parts = line.split(\" +++$+++ \")\n",
    "        line_dict[parts[0]] = parts[-1]\n",
    "    return line_dict\n",
    "\n",
    "# Remove punctuations and convert text to lowercase\n",
    "def clean_text(text):\n",
    "    punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "    return ''.join(char.lower() for char in text if char not in punctuations)\n",
    "\n",
    "# Create question-answer pairs from conversations\n",
    "def create_qa_pairs(conversations, line_dict):\n",
    "    qa_pairs = []\n",
    "    for conversation in conversations:\n",
    "        ids = eval(conversation.split(\" +++$+++ \")[-1])\n",
    "        for i in range(len(ids) - 1):\n",
    "            question = clean_text(line_dict[ids[i]].strip())\n",
    "            answer = clean_text(line_dict[ids[i+1]].strip())\n",
    "            qa_pairs.append([question.split()[:max_sequence_length], answer.split()[:max_sequence_length]])\n",
    "    return qa_pairs\n",
    "\n",
    "# Encode reply text to integer values\n",
    "def encode_reply(words, word_map, max_length=max_sequence_length):\n",
    "    encoded = [word_map['<start>']]\n",
    "    encoded += [word_map.get(word, word_map['<unk>']) for word in words]\n",
    "    encoded.append(word_map['<end>'])\n",
    "    padding_needed = max_length - len(encoded)\n",
    "    encoded.extend([word_map['<pad>']] * padding_needed)\n",
    "    return encoded\n",
    "\n",
    "# Encode question text to integer values\n",
    "def encode_question(words, word_map, max_length=max_sequence_length):\n",
    "    encoded = [word_map.get(word, word_map['<unk>']) for word in words]\n",
    "    padding_needed = max_length - len(encoded)\n",
    "    encoded.extend([word_map['<pad>']] * padding_needed)\n",
    "    return encoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversations = load_corpus(movie_conversations_path)\n",
    "lines = load_corpus(movie_lines_path)\n",
    "\n",
    "# Create line dictionary\n",
    "line_dict = create_line_dict(lines)\n",
    "\n",
    "# Create question-answer pairs\n",
    "qa_pairs = create_qa_pairs(conversations, line_dict)\n",
    "\n",
    "# Count word frequencies and build vocabulary\n",
    "word_frequency = Counter()\n",
    "for pair in qa_pairs:\n",
    "    word_frequency.update(pair[0])\n",
    "    word_frequency.update(pair[1])\n",
    "\n",
    "min_frequency = 5\n",
    "vocab = [word for word, freq in word_frequency.items() if freq > min_frequency]\n",
    "word_map = {word: idx + 1 for idx, word in enumerate(vocab)}\n",
    "word_map.update({'<unk>': len(word_map) + 1, '<start>': len(word_map) + 2, '<end>': len(word_map) + 3, '<pad>': 0})\n",
    "\n",
    "# Save word map\n",
    "with open('WORDMAP_corpus.json', 'w') as json_file:\n",
    "    json.dump(word_map, json_file)\n",
    "\n",
    "\n",
    "    # Loop through each question-answer pair in the original 'pairs' list\n",
    "pairs_encoded = []\n",
    "for pair in qa_pairs:\n",
    "    # Encode the question part of the pair using the 'encode_question' function\n",
    "    qus = encode_question(pair[0], word_map)\n",
    "    \n",
    "    # Encode the answer part of the pair using the 'encode_reply' function\n",
    "    ans = encode_reply(pair[1], word_map)\n",
    "    \n",
    "    # Append the encoded question and answer as a pair to 'pairs_encoded' list\n",
    "    pairs_encoded.append([qus, ans])\n",
    "\n",
    "# Save the encoded pairs to a JSON file for future use\n",
    "with open('pairs_encoded.json', 'w') as p:\n",
    "    json.dump(pairs_encoded, p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -mpy (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ping-extensions (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -atbuffers (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -mpy (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -nsorflow (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ping-extensions (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -0mpy (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -0ping-extensions (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -1ping-extensions (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -2ping-extensions (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -3ping-extensions (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -4ping-extensions (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -5ping-extensions (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -6ping-extensions (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -7ping-extensions (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -8ping-extensions (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -9ping-extensions (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -atbuffers (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -arl (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -dna (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -equests (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ertifi (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ip (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -latbuffers (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -raitlets (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -umpy (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -yping-extensions (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -nsorflow (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ing-extensions (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting evaluate\n",
      "  Using cached evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting rouge_score\n",
      "  Using cached rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.45.0)\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (1.13.1+cu116)\n",
      "Collecting datasets>=2.0.0 (from evaluate)\n",
      "  Using cached datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.24.1)\n",
      "Collecting dill (from evaluate)\n",
      "  Using cached dill-0.3.9-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.5.3)\n",
      "Collecting requests>=2.19.0 (from evaluate)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.5.0)\n",
      "Collecting multiprocess (from evaluate)\n",
      "  Using cached multiprocess-0.70.17-py310-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.6.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.24.5)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (23.0)\n",
      "Requirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.4.0)\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge_score) (3.9.1)\n",
      "Requirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.15.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2024.7.24)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.4)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.4.0)\n",
      "Collecting pyarrow>=15.0.0 (from datasets>=2.0.0->evaluate)\n",
      "  Using cached pyarrow-19.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting dill (from evaluate)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting multiprocess (from evaluate)\n",
      "  Using cached multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Collecting aiohttp (from datasets>=2.0.0->evaluate)\n",
      "  Using cached aiohttp-3.11.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2022.6.15)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk->rouge_score) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk->rouge_score) (1.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2022.7.1)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets>=2.0.0->evaluate)\n",
      "  Using cached aiohappyeyeballs-2.4.4-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets>=2.0.0->evaluate)\n",
      "  Using cached aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting async-timeout<6.0,>=4.0 (from aiohttp->datasets>=2.0.0->evaluate)\n",
      "  Using cached async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->datasets>=2.0.0->evaluate)\n",
      "  Using cached propcache-0.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.2 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets>=2.0.0->evaluate)\n",
      "  Using cached yarl-1.18.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (69 kB)\n",
      "Using cached evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "Using cached datasets-3.2.0-py3-none-any.whl (480 kB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached aiohttp-3.11.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
      "Using cached pyarrow-19.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (42.1 MB)\n",
      "Using cached aiohappyeyeballs-2.4.4-py3-none-any.whl (14 kB)\n",
      "Using cached aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Using cached propcache-0.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (205 kB)\n",
      "Using cached yarl-1.18.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (319 kB)\n",
      "Building wheels for collected packages: rouge_score\n",
      "  Building wheel for rouge_score (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[22 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"<string>\", line 2, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-install-rhisi7_m/rouge-score_700e4f77ee014521bf41d7fa563afa10/setup.py\", line 34, in <module>\n",
      "  \u001b[31m   \u001b[0m     setuptools.setup(\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/setuptools/__init__.py\", line 117, in setup\n",
      "  \u001b[31m   \u001b[0m     return distutils.core.setup(**attrs)\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/setuptools/_distutils/core.py\", line 170, in setup\n",
      "  \u001b[31m   \u001b[0m     ok = dist.parse_command_line()\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/setuptools/_distutils/dist.py\", line 472, in parse_command_line\n",
      "  \u001b[31m   \u001b[0m     args = self._parse_command_opts(parser, args)\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/setuptools/dist.py\", line 893, in _parse_command_opts\n",
      "  \u001b[31m   \u001b[0m     nargs = _Distribution._parse_command_opts(self, parser, args)\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/setuptools/_distutils/dist.py\", line 531, in _parse_command_opts\n",
      "  \u001b[31m   \u001b[0m     cmd_class = self.get_command_class(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/setuptools/dist.py\", line 730, in get_command_class\n",
      "  \u001b[31m   \u001b[0m     from .command.bdist_wheel import bdist_wheel\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/setuptools/command/bdist_wheel.py\", line 26, in <module>\n",
      "  \u001b[31m   \u001b[0m     from wheel.wheelfile import WheelFile\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/lib/python3.10/site-packages/wheel/wheelfile.py\", line 14, in <module>\n",
      "  \u001b[31m   \u001b[0m     from wheel.cli import WheelError\n",
      "  \u001b[31m   \u001b[0m ImportError: cannot import name 'WheelError' from 'wheel.cli' (unknown location)\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[31m  ERROR: Failed building wheel for rouge_score\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[?25h  Running setup.py clean for rouge_score\n",
      "Failed to build rouge_score\n",
      "\u001b[31mERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (rouge_score)\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install evaluate rouge_score transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    # Simple whitespace tokenizer; consider using nltk.word_tokenize for better results\n",
    "    return text.lower().split()\n",
    "\n",
    "def get_ngrams(tokens, n):\n",
    "    return [' '.join(tokens[i:i+n]) for i in range(len(tokens)-n+1)]\n",
    "\n",
    "def count_ngrams(text, n):\n",
    "    tokens = tokenize(text)\n",
    "    return get_ngrams(tokens, n)\n",
    "\n",
    "def compute_overlap(candidate_ngrams, reference_ngrams):\n",
    "    candidate_counts = {}\n",
    "    for ngram in candidate_ngrams:\n",
    "        candidate_counts[ngram] = candidate_counts.get(ngram, 0) + 1\n",
    "\n",
    "    reference_counts = {}\n",
    "    for ngram in reference_ngrams:\n",
    "        reference_counts[ngram] = reference_counts.get(ngram, 0) + 1\n",
    "\n",
    "    overlap = 0\n",
    "    for ngram in candidate_counts:\n",
    "        if ngram in reference_counts:\n",
    "            overlap += min(candidate_counts[ngram], reference_counts[ngram])\n",
    "    return overlap\n",
    "\n",
    "def compute_rouge_n(candidate, references, n):\n",
    "    candidate_ngrams = count_ngrams(candidate, n)\n",
    "    reference_ngrams = []\n",
    "    for ref in references:\n",
    "        reference_ngrams.extend(count_ngrams(ref, n))\n",
    "\n",
    "    overlap_count = compute_overlap(candidate_ngrams, reference_ngrams)\n",
    "    total_candidate_ngrams = len(candidate_ngrams)\n",
    "    total_reference_ngrams = len(reference_ngrams)\n",
    "\n",
    "    precision = overlap_count / total_candidate_ngrams if total_candidate_ngrams > 0 else 0\n",
    "    recall = overlap_count / total_reference_ngrams if total_reference_ngrams > 0 else 0\n",
    "    f1_score = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1_score\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-1: {'precision': 0.7777777777777778, 'recall': 0.875, 'f1_score': 0.823529411764706}\n",
      "ROUGE-2: {'precision': 0.625, 'recall': 0.7142857142857143, 'f1_score': 0.6666666666666666}\n",
      "ROUGE-3: {'precision': 0.5714285714285714, 'recall': 0.6666666666666666, 'f1_score': 0.6153846153846153}\n"
     ]
    }
   ],
   "source": [
    "candidate_summary = \"The quick brown fox jumps over the lazy dog\"\n",
    "reference_summaries = [\n",
    "    # \"A fast brown fox leaps over a lazy dog\",\n",
    "    \"The quick brown fox jumps over lazy dogs\"\n",
    "]\n",
    "\n",
    "rouge_1 = compute_rouge_n(candidate_summary, reference_summaries, 1)\n",
    "rouge_2 = compute_rouge_n(candidate_summary, reference_summaries, 2)\n",
    "rouge_3 = compute_rouge_n(candidate_summary, reference_summaries, 3)\n",
    "\n",
    "print(\"ROUGE-1:\", rouge_1)\n",
    "print(\"ROUGE-2:\", rouge_2)\n",
    "\n",
    "print(\"ROUGE-3:\", rouge_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/andyalyfsyah/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "METEOR Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.meteor_score import single_meteor_score \n",
    "\n",
    "# Define candidate and reference sentences\n",
    "candidate = \"visi misi fakultas ilmu komputer\"\n",
    "reference = \"visi fakultas ilmu komputer\"\n",
    "\n",
    "# Calculate METEOR score\n",
    "score = single_meteor_score([reference], [candidate])\n",
    "\n",
    "# Print the result\n",
    "print(f\"METEOR Score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Normalize the text by converting to lowercase and stripping whitespace\n",
    "    return text.lower().strip()\n",
    "\n",
    "def get_character_ngrams(text, n):\n",
    "    ngrams = [text[i:i+n] for i in range(len(text)-n+1)]\n",
    "    return ngrams\n",
    "\n",
    "def count_ngrams(text, max_order):\n",
    "    ngram_counts = {}\n",
    "    for n in range(1, max_order+1):\n",
    "        ngrams = get_character_ngrams(text, n)\n",
    "        for ngram in ngrams:\n",
    "            ngram_counts[ngram] = ngram_counts.get(ngram, 0) + 1\n",
    "    return ngram_counts\n",
    "\n",
    "def compute_precision_recall(candidate_counts, reference_counts):\n",
    "    overlap = 0\n",
    "    total_candidate = sum(candidate_counts.values())\n",
    "    total_reference = sum(reference_counts.values())\n",
    "\n",
    "    for ngram in candidate_counts:\n",
    "        if ngram in reference_counts:\n",
    "            overlap += min(candidate_counts[ngram], reference_counts[ngram])\n",
    "\n",
    "    precision = overlap / total_candidate if total_candidate > 0 else 0.0\n",
    "    recall = overlap / total_reference if total_reference > 0 else 0.0\n",
    "\n",
    "    return precision, recall\n",
    "\n",
    "def compute_chrf(candidate, references, max_order=6, beta=2):\n",
    "    candidate = preprocess_text(candidate)\n",
    "    candidate_ngram_counts = count_ngrams(candidate, max_order)\n",
    "\n",
    "    # Aggregate n-gram counts from all references\n",
    "    reference_ngram_counts = {}\n",
    "    for reference in references:\n",
    "        reference = preprocess_text(reference)\n",
    "        reference_counts = count_ngrams(reference, max_order)\n",
    "        for ngram, count in reference_counts.items():\n",
    "            if ngram in reference_ngram_counts:\n",
    "                reference_ngram_counts[ngram] = max(reference_ngram_counts[ngram], count)\n",
    "            else:\n",
    "                reference_ngram_counts[ngram] = count\n",
    "\n",
    "    precision, recall = compute_precision_recall(candidate_ngram_counts, reference_ngram_counts)\n",
    "\n",
    "    beta_squared = beta ** 2\n",
    "    if precision + recall > 0:\n",
    "        chrf_score = (1 + beta_squared) * (precision * recall) / (beta_squared * precision + recall)\n",
    "    else:\n",
    "        chrf_score = 0.0\n",
    "\n",
    "    # Multiply by 100 to get a percentage\n",
    "    return chrf_score * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chrF Score: 100.00\n"
     ]
    }
   ],
   "source": [
    "candidate_translation = \"The quick brown fox jumps over the lazy dog.\"\n",
    "reference_translations = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\"\n",
    "]\n",
    "\n",
    "chrf_score = compute_chrf(candidate_translation, reference_translations)\n",
    "print(f\"chrF Score: {chrf_score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate 1 chrF Score: 77.40\n",
      "Candidate 2 chrF Score: 92.12\n",
      "Candidate 3 chrF Score: 80.55\n",
      "Average chrF Score: 83.35\n"
     ]
    }
   ],
   "source": [
    "# List of candidate translations\n",
    "list_of_candidates = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"An apple a day keeps the doctor away.\",\n",
    "    \"To be or not to be, that is the question.\"\n",
    "]\n",
    "\n",
    "# Corresponding list of reference translations (each candidate can have multiple references)\n",
    "list_of_references = [\n",
    "        \"A brown fox leaps over the lazy dog.\",\n",
    "        \"An apple a day keeps doctor away.\",\n",
    "        \"to be or not this is the question.\",\n",
    "    \n",
    "]\n",
    "\n",
    "# Compute chrF scores for each candidate-reference pair\n",
    "chrf_scores = compute_chrf_list(list_of_candidates, list_of_references)\n",
    "\n",
    "# Display the results\n",
    "for idx, score in enumerate(chrf_scores):\n",
    "    print(f\"Candidate {idx+1} chrF Score: {score:.2f}\")\n",
    "\n",
    "# Compute the average chrF score\n",
    "average_chrf = compute_average_chrf(list_of_candidates, list_of_references)\n",
    "print(f\"Average chrF Score: {average_chrf:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
