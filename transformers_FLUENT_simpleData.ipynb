{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:27.682419Z","iopub.status.busy":"2024-07-02T20:17:27.682115Z","iopub.status.idle":"2024-07-02T20:17:30.597078Z","shell.execute_reply":"2024-07-02T20:17:30.596113Z","shell.execute_reply.started":"2024-07-02T20:17:27.682391Z"},"trusted":true},"outputs":[],"source":["from collections import Counter\n","import json\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset\n","import torch.utils.data\n","import math\n","import torch.nn.functional as F\n","import neptune\n","import pandas as pd"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:30.598900Z","iopub.status.busy":"2024-07-02T20:17:30.598458Z","iopub.status.idle":"2024-07-02T20:17:32.694354Z","shell.execute_reply":"2024-07-02T20:17:32.693406Z","shell.execute_reply.started":"2024-07-02T20:17:30.598873Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Pertanyaan</th>\n","      <th>Jawaban</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>visi filkom</td>\n","      <td>menjadi fakultas yang berdaya saing internasio...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>misi filkom</td>\n","      <td>menyelenggarakan pendidikan di bidang teknolog...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>apa tujuan filkom?</td>\n","      <td>menghasilkan lulusan yang kompeten , profesion...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>sasaran pendidikan filkom</td>\n","      <td>meningkatkan kompetensi dan kualifikasi pendid...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>email fitra a. bachtiar</td>\n","      <td>fitra.bachtiar[at]ub.ac.id</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>bidang penelitian fitra a. bachtiar</td>\n","      <td>affective computing, affective engineering, in...</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>tanggal dibentuk ptiik</td>\n","      <td>27 oktober 2011</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>sasaran pengabdian filkom</td>\n","      <td>1. meningkatkan kualitas dan kuantitas pengabd...</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>sasaran kerjasama filkom</td>\n","      <td>1. mengadakan kerjasama pendidikan, penlitian ...</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>dekan fakultas ilmu komputer filkom</td>\n","      <td>prof. ir. wayan firdaus mahmudy, s.si., mt., p...</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>wakil dekan bidang akademik / wakil dekan 1</td>\n","      <td>dr. eng. ir. herman tolle, st., mt.</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>wakil dekan bidang umum, keuangan, dan sumber ...</td>\n","      <td>agus wahyu widodo, st., m.cs.</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>wakil dekan bidang kemahasiswaan, alumni, dan ...</td>\n","      <td>drs. muh. arif rahman, m.kom.</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>ketua departemen teknik informatika</td>\n","      <td>achmad basuki, s.t., m.mg., ph.d.</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>sekretaris departemen teknik informatika</td>\n","      <td>ir. primantara hari trisnawan, m.sc.</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>ketua program studi magister ilmu komputer</td>\n","      <td>sabriansyah rizqika akbar, s.t., m.eng., ph.d</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>ketua program studi sarjana teknik informatika</td>\n","      <td>adhitya bhawiyuga, s.kom., m.sc.</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>ketua program studi sarjana teknik komputer</td>\n","      <td>barlian henryranu prasetio, s.t., m.t., ph.d</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>ketua departemen sistem informasi</td>\n","      <td>issa arwani, s.kom., m.sc.</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>seketaris departemen sistem informasi</td>\n","      <td>satrio agung wicaksono, s.kom., m.kom</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>ketua program studi sarjana sistem informasi</td>\n","      <td>yusi tyroni mursityo, s.kom., m.s.</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>ketua program studi sarjana pendidikan teknolo...</td>\n","      <td>ir. admaja dwi herlambang, s.pd., m.pd.</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>ketua program studi sarjana teknologi informasi</td>\n","      <td>ir. widhy hayuhardhika nugraha putra, s.kom., ...</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>berikan saya informasi alumni</td>\n","      <td>informasi alumni dapat diakses pada link berik...</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>apa saja layanan kemahasiswaan filkom ub ?</td>\n","      <td>1. pengajuan proposal dan lpj kegiatan kemahas...</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>bagaimana pengajuan proposal dan lpj kegiatan ...</td>\n","      <td>pengajuan proposal kegiatan kemahasiswaan\\npen...</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>berikan informasi dokumen kemahasiswaan ?</td>\n","      <td>informasi dokumen kemahasiswaan dapat dilihat ...</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>bagaimana pengajuan surat tugas dosen pembimbi...</td>\n","      <td>informasi pengajuan surat tugas dosen pembimbi...</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>bagaimana permohonan validasi data skm ?</td>\n","      <td>permohonan validasi data skm dapat dilihat pad...</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>berikan informasi mengenai validasi syarat wisuda</td>\n","      <td>1. unggah dokumen di siam\\n2. mengisi gform pe...</td>\n","    </tr>\n","    <tr>\n","      <th>30</th>\n","      <td>bagaimana mengakses tracer study fakultas ?</td>\n","      <td>tracer study fakultas dapat diakses pada tauta...</td>\n","    </tr>\n","    <tr>\n","      <th>31</th>\n","      <td>bagaimana cara pendaftaran wisuda ulang ?</td>\n","      <td>pendaftaran wisuda ulang dapat diakses pada ta...</td>\n","    </tr>\n","    <tr>\n","      <th>32</th>\n","      <td>berikan informasi mengenai layanan bimbingan d...</td>\n","      <td>layanan bimbingan dan konseling dapat diaksesp...</td>\n","    </tr>\n","    <tr>\n","      <th>33</th>\n","      <td>berikan informasi mengenai layanan ultksp (uni...</td>\n","      <td>layanan ultksp dapat diaksespada tautan beriku...</td>\n","    </tr>\n","    <tr>\n","      <th>34</th>\n","      <td>berikan informasi mengenai tracking layanan ke...</td>\n","      <td>tracking layanan kemahasiswaan dapat diaksespa...</td>\n","    </tr>\n","    <tr>\n","      <th>35</th>\n","      <td>berikan informasi mengenai bimbingan dan konse...</td>\n","      <td>dalam perjalanannya menuntut ilmu, mahasiswa t...</td>\n","    </tr>\n","    <tr>\n","      <th>36</th>\n","      <td>apa tujuan unit konseling ?</td>\n","      <td>1. mewujudkan potensi dirinya secara optimal, ...</td>\n","    </tr>\n","    <tr>\n","      <th>37</th>\n","      <td>apa fungsi bimbingan dan konseling serta penas...</td>\n","      <td>1. penyaluran: bimbingan berfungsi dalam memba...</td>\n","    </tr>\n","    <tr>\n","      <th>38</th>\n","      <td>apa saja program layanan unit konseling ?</td>\n","      <td>1. pelayanan bantuan pemecahan masalah, baik y...</td>\n","    </tr>\n","    <tr>\n","      <th>39</th>\n","      <td>apa manfaat konseling filkom ?</td>\n","      <td>1. masalah ditangani oleh ahli yang kompeten d...</td>\n","    </tr>\n","    <tr>\n","      <th>40</th>\n","      <td>berikan informasi mengenai layanan konseling</td>\n","      <td>informasi mengenai layanan konseling dapat dia...</td>\n","    </tr>\n","    <tr>\n","      <th>41</th>\n","      <td>siapa konselor bimbingan dan konseling di filk...</td>\n","      <td>ada 2 konselor bimbingan dan konseling di filk...</td>\n","    </tr>\n","    <tr>\n","      <th>42</th>\n","      <td>siapa koordinator konselor sebaya ?</td>\n","      <td>koordinator konselor sebaya adalah muhammad da...</td>\n","    </tr>\n","    <tr>\n","      <th>43</th>\n","      <td>berikan rincian layanan ultksp</td>\n","      <td>rincian layanan ultksp dapat diakses pada taut...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                           Pertanyaan  \\\n","0                                         visi filkom   \n","1                                         misi filkom   \n","2                                  apa tujuan filkom?   \n","3                           sasaran pendidikan filkom   \n","4                             email fitra a. bachtiar   \n","5                 bidang penelitian fitra a. bachtiar   \n","6                              tanggal dibentuk ptiik   \n","7                           sasaran pengabdian filkom   \n","8                            sasaran kerjasama filkom   \n","9                 dekan fakultas ilmu komputer filkom   \n","10        wakil dekan bidang akademik / wakil dekan 1   \n","11  wakil dekan bidang umum, keuangan, dan sumber ...   \n","12  wakil dekan bidang kemahasiswaan, alumni, dan ...   \n","13                ketua departemen teknik informatika   \n","14           sekretaris departemen teknik informatika   \n","15         ketua program studi magister ilmu komputer   \n","16     ketua program studi sarjana teknik informatika   \n","17        ketua program studi sarjana teknik komputer   \n","18                  ketua departemen sistem informasi   \n","19              seketaris departemen sistem informasi   \n","20       ketua program studi sarjana sistem informasi   \n","21  ketua program studi sarjana pendidikan teknolo...   \n","22    ketua program studi sarjana teknologi informasi   \n","23                      berikan saya informasi alumni   \n","24         apa saja layanan kemahasiswaan filkom ub ?   \n","25  bagaimana pengajuan proposal dan lpj kegiatan ...   \n","26          berikan informasi dokumen kemahasiswaan ?   \n","27  bagaimana pengajuan surat tugas dosen pembimbi...   \n","28           bagaimana permohonan validasi data skm ?   \n","29  berikan informasi mengenai validasi syarat wisuda   \n","30        bagaimana mengakses tracer study fakultas ?   \n","31          bagaimana cara pendaftaran wisuda ulang ?   \n","32  berikan informasi mengenai layanan bimbingan d...   \n","33  berikan informasi mengenai layanan ultksp (uni...   \n","34  berikan informasi mengenai tracking layanan ke...   \n","35  berikan informasi mengenai bimbingan dan konse...   \n","36                        apa tujuan unit konseling ?   \n","37  apa fungsi bimbingan dan konseling serta penas...   \n","38          apa saja program layanan unit konseling ?   \n","39                     apa manfaat konseling filkom ?   \n","40       berikan informasi mengenai layanan konseling   \n","41  siapa konselor bimbingan dan konseling di filk...   \n","42                siapa koordinator konselor sebaya ?   \n","43                     berikan rincian layanan ultksp   \n","\n","                                              Jawaban  \n","0   menjadi fakultas yang berdaya saing internasio...  \n","1   menyelenggarakan pendidikan di bidang teknolog...  \n","2   menghasilkan lulusan yang kompeten , profesion...  \n","3   meningkatkan kompetensi dan kualifikasi pendid...  \n","4                          fitra.bachtiar[at]ub.ac.id  \n","5   affective computing, affective engineering, in...  \n","6                                     27 oktober 2011  \n","7   1. meningkatkan kualitas dan kuantitas pengabd...  \n","8   1. mengadakan kerjasama pendidikan, penlitian ...  \n","9   prof. ir. wayan firdaus mahmudy, s.si., mt., p...  \n","10                dr. eng. ir. herman tolle, st., mt.  \n","11                      agus wahyu widodo, st., m.cs.  \n","12                      drs. muh. arif rahman, m.kom.  \n","13                  achmad basuki, s.t., m.mg., ph.d.  \n","14               ir. primantara hari trisnawan, m.sc.  \n","15      sabriansyah rizqika akbar, s.t., m.eng., ph.d  \n","16                   adhitya bhawiyuga, s.kom., m.sc.  \n","17       barlian henryranu prasetio, s.t., m.t., ph.d  \n","18                         issa arwani, s.kom., m.sc.  \n","19              satrio agung wicaksono, s.kom., m.kom  \n","20                 yusi tyroni mursityo, s.kom., m.s.  \n","21            ir. admaja dwi herlambang, s.pd., m.pd.  \n","22  ir. widhy hayuhardhika nugraha putra, s.kom., ...  \n","23  informasi alumni dapat diakses pada link berik...  \n","24  1. pengajuan proposal dan lpj kegiatan kemahas...  \n","25  pengajuan proposal kegiatan kemahasiswaan\\npen...  \n","26  informasi dokumen kemahasiswaan dapat dilihat ...  \n","27  informasi pengajuan surat tugas dosen pembimbi...  \n","28  permohonan validasi data skm dapat dilihat pad...  \n","29  1. unggah dokumen di siam\\n2. mengisi gform pe...  \n","30  tracer study fakultas dapat diakses pada tauta...  \n","31  pendaftaran wisuda ulang dapat diakses pada ta...  \n","32  layanan bimbingan dan konseling dapat diaksesp...  \n","33  layanan ultksp dapat diaksespada tautan beriku...  \n","34  tracking layanan kemahasiswaan dapat diaksespa...  \n","35  dalam perjalanannya menuntut ilmu, mahasiswa t...  \n","36  1. mewujudkan potensi dirinya secara optimal, ...  \n","37  1. penyaluran: bimbingan berfungsi dalam memba...  \n","38  1. pelayanan bantuan pemecahan masalah, baik y...  \n","39  1. masalah ditangani oleh ahli yang kompeten d...  \n","40  informasi mengenai layanan konseling dapat dia...  \n","41  ada 2 konselor bimbingan dan konseling di filk...  \n","42  koordinator konselor sebaya adalah muhammad da...  \n","43  rincian layanan ultksp dapat diakses pada taut...  "]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["# knowledgebase_url = 'https://github.com/AndiAlifs/FLUENT-Chatbot-2023/raw/main/KnowledgeBaseFilkom.xlsx'\n","knowledgebase_url = 'https://github.com/AndiAlifs/FLUENT-Chatbot-2023/raw/main/KnowledgeBaseFilkom_simple.xlsx'\n","knowledgebase = pd.read_excel(knowledgebase_url)\n","\n","qa_paired = knowledgebase.drop(columns=knowledgebase.columns.drop(['Pertanyaan', 'Jawaban']))\n","qa_paired.dropna(inplace=True)\n","qa_paired"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:32.697066Z","iopub.status.busy":"2024-07-02T20:17:32.696633Z","iopub.status.idle":"2024-07-02T20:17:32.702322Z","shell.execute_reply":"2024-07-02T20:17:32.701334Z","shell.execute_reply.started":"2024-07-02T20:17:32.697038Z"},"trusted":true},"outputs":[],"source":["def remove_punc(string):\n","    punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n","    no_punct = \"\"\n","    for char in string:\n","        if char not in punctuations:\n","            no_punct = no_punct + char  # space is also a character\n","    return no_punct.lower()"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:32.703938Z","iopub.status.busy":"2024-07-02T20:17:32.703658Z","iopub.status.idle":"2024-07-02T20:17:32.827025Z","shell.execute_reply":"2024-07-02T20:17:32.825935Z","shell.execute_reply.started":"2024-07-02T20:17:32.703915Z"},"trusted":true},"outputs":[],"source":["pairs = []\n","max_len = 90\n","\n","for line in qa_paired.iterrows():\n","    pertanyaan = line[1]['Pertanyaan']\n","    jawaban = line[1]['Jawaban']\n","    qa_pairs = []\n","    first = remove_punc(pertanyaan.strip())      \n","    second = remove_punc(jawaban.strip())\n","    qa_pairs.append(first.split()[:max_len])\n","    qa_pairs.append(second.split()[:max_len])\n","    pairs.append(qa_pairs)"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>question</th>\n","      <th>answer</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>[visi, filkom]</td>\n","      <td>[menjadi, fakultas, yang, berdaya, saing, inte...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>[misi, filkom]</td>\n","      <td>[menyelenggarakan, pendidikan, di, bidang, tek...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>[apa, tujuan, filkom]</td>\n","      <td>[menghasilkan, lulusan, yang, kompeten, profes...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>[sasaran, pendidikan, filkom]</td>\n","      <td>[meningkatkan, kompetensi, dan, kualifikasi, p...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>[email, fitra, a, bachtiar]</td>\n","      <td>[fitrabachtiaratubacid]</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>[bidang, penelitian, fitra, a, bachtiar]</td>\n","      <td>[affective, computing, affective, engineering,...</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>[tanggal, dibentuk, ptiik]</td>\n","      <td>[27, oktober, 2011]</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>[sasaran, pengabdian, filkom]</td>\n","      <td>[1, meningkatkan, kualitas, dan, kuantitas, pe...</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>[sasaran, kerjasama, filkom]</td>\n","      <td>[1, mengadakan, kerjasama, pendidikan, penliti...</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>[dekan, fakultas, ilmu, komputer, filkom]</td>\n","      <td>[prof, ir, wayan, firdaus, mahmudy, ssi, mt, phd]</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>[wakil, dekan, bidang, akademik, wakil, dekan, 1]</td>\n","      <td>[dr, eng, ir, herman, tolle, st, mt]</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>[wakil, dekan, bidang, umum, keuangan, dan, su...</td>\n","      <td>[agus, wahyu, widodo, st, mcs]</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>[wakil, dekan, bidang, kemahasiswaan, alumni, ...</td>\n","      <td>[drs, muh, arif, rahman, mkom]</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>[ketua, departemen, teknik, informatika]</td>\n","      <td>[achmad, basuki, st, mmg, phd]</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>[sekretaris, departemen, teknik, informatika]</td>\n","      <td>[ir, primantara, hari, trisnawan, msc]</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>[ketua, program, studi, magister, ilmu, komputer]</td>\n","      <td>[sabriansyah, rizqika, akbar, st, meng, phd]</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>[ketua, program, studi, sarjana, teknik, infor...</td>\n","      <td>[adhitya, bhawiyuga, skom, msc]</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>[ketua, program, studi, sarjana, teknik, kompu...</td>\n","      <td>[barlian, henryranu, prasetio, st, mt, phd]</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>[ketua, departemen, sistem, informasi]</td>\n","      <td>[issa, arwani, skom, msc]</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>[seketaris, departemen, sistem, informasi]</td>\n","      <td>[satrio, agung, wicaksono, skom, mkom]</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>[ketua, program, studi, sarjana, sistem, infor...</td>\n","      <td>[yusi, tyroni, mursityo, skom, ms]</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>[ketua, program, studi, sarjana, pendidikan, t...</td>\n","      <td>[ir, admaja, dwi, herlambang, spd, mpd]</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>[ketua, program, studi, sarjana, teknologi, in...</td>\n","      <td>[ir, widhy, hayuhardhika, nugraha, putra, skom...</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>[berikan, saya, informasi, alumni]</td>\n","      <td>[informasi, alumni, dapat, diakses, pada, link...</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>[apa, saja, layanan, kemahasiswaan, filkom, ub]</td>\n","      <td>[1, pengajuan, proposal, dan, lpj, kegiatan, k...</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>[bagaimana, pengajuan, proposal, dan, lpj, keg...</td>\n","      <td>[pengajuan, proposal, kegiatan, kemahasiswaan,...</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>[berikan, informasi, dokumen, kemahasiswaan]</td>\n","      <td>[informasi, dokumen, kemahasiswaan, dapat, dil...</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>[bagaimana, pengajuan, surat, tugas, dosen, pe...</td>\n","      <td>[informasi, pengajuan, surat, tugas, dosen, pe...</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>[bagaimana, permohonan, validasi, data, skm]</td>\n","      <td>[permohonan, validasi, data, skm, dapat, dilih...</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>[berikan, informasi, mengenai, validasi, syara...</td>\n","      <td>[1, unggah, dokumen, di, siam, 2, mengisi, gfo...</td>\n","    </tr>\n","    <tr>\n","      <th>30</th>\n","      <td>[bagaimana, mengakses, tracer, study, fakultas]</td>\n","      <td>[tracer, study, fakultas, dapat, diakses, pada...</td>\n","    </tr>\n","    <tr>\n","      <th>31</th>\n","      <td>[bagaimana, cara, pendaftaran, wisuda, ulang]</td>\n","      <td>[pendaftaran, wisuda, ulang, dapat, diakses, p...</td>\n","    </tr>\n","    <tr>\n","      <th>32</th>\n","      <td>[berikan, informasi, mengenai, layanan, bimbin...</td>\n","      <td>[layanan, bimbingan, dan, konseling, dapat, di...</td>\n","    </tr>\n","    <tr>\n","      <th>33</th>\n","      <td>[berikan, informasi, mengenai, layanan, ultksp...</td>\n","      <td>[layanan, ultksp, dapat, diaksespada, tautan, ...</td>\n","    </tr>\n","    <tr>\n","      <th>34</th>\n","      <td>[berikan, informasi, mengenai, tracking, layan...</td>\n","      <td>[tracking, layanan, kemahasiswaan, dapat, diak...</td>\n","    </tr>\n","    <tr>\n","      <th>35</th>\n","      <td>[berikan, informasi, mengenai, bimbingan, dan,...</td>\n","      <td>[dalam, perjalanannya, menuntut, ilmu, mahasis...</td>\n","    </tr>\n","    <tr>\n","      <th>36</th>\n","      <td>[apa, tujuan, unit, konseling]</td>\n","      <td>[1, mewujudkan, potensi, dirinya, secara, opti...</td>\n","    </tr>\n","    <tr>\n","      <th>37</th>\n","      <td>[apa, fungsi, bimbingan, dan, konseling, serta...</td>\n","      <td>[1, penyaluran, bimbingan, berfungsi, dalam, m...</td>\n","    </tr>\n","    <tr>\n","      <th>38</th>\n","      <td>[apa, saja, program, layanan, unit, konseling]</td>\n","      <td>[1, pelayanan, bantuan, pemecahan, masalah, ba...</td>\n","    </tr>\n","    <tr>\n","      <th>39</th>\n","      <td>[apa, manfaat, konseling, filkom]</td>\n","      <td>[1, masalah, ditangani, oleh, ahli, yang, komp...</td>\n","    </tr>\n","    <tr>\n","      <th>40</th>\n","      <td>[berikan, informasi, mengenai, layanan, konsel...</td>\n","      <td>[informasi, mengenai, layanan, konseling, dapa...</td>\n","    </tr>\n","    <tr>\n","      <th>41</th>\n","      <td>[siapa, konselor, bimbingan, dan, konseling, d...</td>\n","      <td>[ada, 2, konselor, bimbingan, dan, konseling, ...</td>\n","    </tr>\n","    <tr>\n","      <th>42</th>\n","      <td>[siapa, koordinator, konselor, sebaya]</td>\n","      <td>[koordinator, konselor, sebaya, adalah, muhamm...</td>\n","    </tr>\n","    <tr>\n","      <th>43</th>\n","      <td>[berikan, rincian, layanan, ultksp]</td>\n","      <td>[rincian, layanan, ultksp, dapat, diakses, pad...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                             question  \\\n","0                                      [visi, filkom]   \n","1                                      [misi, filkom]   \n","2                               [apa, tujuan, filkom]   \n","3                       [sasaran, pendidikan, filkom]   \n","4                         [email, fitra, a, bachtiar]   \n","5            [bidang, penelitian, fitra, a, bachtiar]   \n","6                          [tanggal, dibentuk, ptiik]   \n","7                       [sasaran, pengabdian, filkom]   \n","8                        [sasaran, kerjasama, filkom]   \n","9           [dekan, fakultas, ilmu, komputer, filkom]   \n","10  [wakil, dekan, bidang, akademik, wakil, dekan, 1]   \n","11  [wakil, dekan, bidang, umum, keuangan, dan, su...   \n","12  [wakil, dekan, bidang, kemahasiswaan, alumni, ...   \n","13           [ketua, departemen, teknik, informatika]   \n","14      [sekretaris, departemen, teknik, informatika]   \n","15  [ketua, program, studi, magister, ilmu, komputer]   \n","16  [ketua, program, studi, sarjana, teknik, infor...   \n","17  [ketua, program, studi, sarjana, teknik, kompu...   \n","18             [ketua, departemen, sistem, informasi]   \n","19         [seketaris, departemen, sistem, informasi]   \n","20  [ketua, program, studi, sarjana, sistem, infor...   \n","21  [ketua, program, studi, sarjana, pendidikan, t...   \n","22  [ketua, program, studi, sarjana, teknologi, in...   \n","23                 [berikan, saya, informasi, alumni]   \n","24    [apa, saja, layanan, kemahasiswaan, filkom, ub]   \n","25  [bagaimana, pengajuan, proposal, dan, lpj, keg...   \n","26       [berikan, informasi, dokumen, kemahasiswaan]   \n","27  [bagaimana, pengajuan, surat, tugas, dosen, pe...   \n","28       [bagaimana, permohonan, validasi, data, skm]   \n","29  [berikan, informasi, mengenai, validasi, syara...   \n","30    [bagaimana, mengakses, tracer, study, fakultas]   \n","31      [bagaimana, cara, pendaftaran, wisuda, ulang]   \n","32  [berikan, informasi, mengenai, layanan, bimbin...   \n","33  [berikan, informasi, mengenai, layanan, ultksp...   \n","34  [berikan, informasi, mengenai, tracking, layan...   \n","35  [berikan, informasi, mengenai, bimbingan, dan,...   \n","36                     [apa, tujuan, unit, konseling]   \n","37  [apa, fungsi, bimbingan, dan, konseling, serta...   \n","38     [apa, saja, program, layanan, unit, konseling]   \n","39                  [apa, manfaat, konseling, filkom]   \n","40  [berikan, informasi, mengenai, layanan, konsel...   \n","41  [siapa, konselor, bimbingan, dan, konseling, d...   \n","42             [siapa, koordinator, konselor, sebaya]   \n","43                [berikan, rincian, layanan, ultksp]   \n","\n","                                               answer  \n","0   [menjadi, fakultas, yang, berdaya, saing, inte...  \n","1   [menyelenggarakan, pendidikan, di, bidang, tek...  \n","2   [menghasilkan, lulusan, yang, kompeten, profes...  \n","3   [meningkatkan, kompetensi, dan, kualifikasi, p...  \n","4                             [fitrabachtiaratubacid]  \n","5   [affective, computing, affective, engineering,...  \n","6                                 [27, oktober, 2011]  \n","7   [1, meningkatkan, kualitas, dan, kuantitas, pe...  \n","8   [1, mengadakan, kerjasama, pendidikan, penliti...  \n","9   [prof, ir, wayan, firdaus, mahmudy, ssi, mt, phd]  \n","10               [dr, eng, ir, herman, tolle, st, mt]  \n","11                     [agus, wahyu, widodo, st, mcs]  \n","12                     [drs, muh, arif, rahman, mkom]  \n","13                     [achmad, basuki, st, mmg, phd]  \n","14             [ir, primantara, hari, trisnawan, msc]  \n","15       [sabriansyah, rizqika, akbar, st, meng, phd]  \n","16                    [adhitya, bhawiyuga, skom, msc]  \n","17        [barlian, henryranu, prasetio, st, mt, phd]  \n","18                          [issa, arwani, skom, msc]  \n","19             [satrio, agung, wicaksono, skom, mkom]  \n","20                 [yusi, tyroni, mursityo, skom, ms]  \n","21            [ir, admaja, dwi, herlambang, spd, mpd]  \n","22  [ir, widhy, hayuhardhika, nugraha, putra, skom...  \n","23  [informasi, alumni, dapat, diakses, pada, link...  \n","24  [1, pengajuan, proposal, dan, lpj, kegiatan, k...  \n","25  [pengajuan, proposal, kegiatan, kemahasiswaan,...  \n","26  [informasi, dokumen, kemahasiswaan, dapat, dil...  \n","27  [informasi, pengajuan, surat, tugas, dosen, pe...  \n","28  [permohonan, validasi, data, skm, dapat, dilih...  \n","29  [1, unggah, dokumen, di, siam, 2, mengisi, gfo...  \n","30  [tracer, study, fakultas, dapat, diakses, pada...  \n","31  [pendaftaran, wisuda, ulang, dapat, diakses, p...  \n","32  [layanan, bimbingan, dan, konseling, dapat, di...  \n","33  [layanan, ultksp, dapat, diaksespada, tautan, ...  \n","34  [tracking, layanan, kemahasiswaan, dapat, diak...  \n","35  [dalam, perjalanannya, menuntut, ilmu, mahasis...  \n","36  [1, mewujudkan, potensi, dirinya, secara, opti...  \n","37  [1, penyaluran, bimbingan, berfungsi, dalam, m...  \n","38  [1, pelayanan, bantuan, pemecahan, masalah, ba...  \n","39  [1, masalah, ditangani, oleh, ahli, yang, komp...  \n","40  [informasi, mengenai, layanan, konseling, dapa...  \n","41  [ada, 2, konselor, bimbingan, dan, konseling, ...  \n","42  [koordinator, konselor, sebaya, adalah, muhamm...  \n","43  [rincian, layanan, ultksp, dapat, diakses, pad...  "]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["pairs_df = pd.DataFrame(pairs, columns=['question', 'answer'])\n","pairs_df"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:32.828565Z","iopub.status.busy":"2024-07-02T20:17:32.828255Z","iopub.status.idle":"2024-07-02T20:17:32.838911Z","shell.execute_reply":"2024-07-02T20:17:32.837848Z","shell.execute_reply.started":"2024-07-02T20:17:32.828539Z"},"trusted":true},"outputs":[],"source":["word_freq = Counter()\n","for pair in pairs:\n","    word_freq.update(pair[0])\n","    word_freq.update(pair[1])"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:32.840576Z","iopub.status.busy":"2024-07-02T20:17:32.840244Z","iopub.status.idle":"2024-07-02T20:17:32.850265Z","shell.execute_reply":"2024-07-02T20:17:32.849352Z","shell.execute_reply.started":"2024-07-02T20:17:32.840548Z"},"trusted":true},"outputs":[],"source":["min_word_freq = 0\n","words = [w for w in word_freq.keys() if word_freq[w] > min_word_freq]\n","word_map = {k: v + 1 for v, k in enumerate(words)}\n","word_map['<unk>'] = len(word_map) + 1\n","word_map['<start>'] = len(word_map) + 1\n","word_map['<end>'] = len(word_map) + 1\n","word_map['<pad>'] = 0"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:32.851802Z","iopub.status.busy":"2024-07-02T20:17:32.851465Z","iopub.status.idle":"2024-07-02T20:17:32.860482Z","shell.execute_reply":"2024-07-02T20:17:32.859625Z","shell.execute_reply.started":"2024-07-02T20:17:32.851770Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Total words are: 449\n"]}],"source":["print(\"Total words are: {}\".format(len(word_map)))"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:32.861919Z","iopub.status.busy":"2024-07-02T20:17:32.861579Z","iopub.status.idle":"2024-07-02T20:17:32.872138Z","shell.execute_reply":"2024-07-02T20:17:32.871399Z","shell.execute_reply.started":"2024-07-02T20:17:32.861895Z"},"trusted":true},"outputs":[],"source":["with open('WORDMAP_corpus_KBFILKOM.json', 'w') as j:\n","    json.dump(word_map, j)"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:32.877074Z","iopub.status.busy":"2024-07-02T20:17:32.876749Z","iopub.status.idle":"2024-07-02T20:17:32.884325Z","shell.execute_reply":"2024-07-02T20:17:32.883430Z","shell.execute_reply.started":"2024-07-02T20:17:32.877048Z"},"trusted":true},"outputs":[],"source":["def encode_question(words, word_map):\n","    enc_c = [word_map.get(word, word_map['<unk>']) for word in words] + [word_map['<pad>']] * (max_len - len(words))\n","    return enc_c\n","\n","\n","def encode_question_left(words, word_map):\n","    enc_c = [word_map['<pad>']] * (max_len - len(words)) + [word_map.get(word, word_map['<unk>']) for word in words]\n","    return enc_c\n","\n","def encode_reply(words, word_map):\n","    enc_c = [word_map['<start>']] + [word_map.get(word, word_map['<unk>']) for word in words] + \\\n","    [word_map['<end>']] + [word_map['<pad>']] * (max_len - len(words))\n","    return enc_c\n","\n","def encode_reply_with_maxlen(words, word_map):\n","    enc_c = [word_map['<start>']] + [word_map.get(word, word_map['<unk>']) for word in words] + [word_map['<end>']] + [word_map['<pad>']] * (max_len-2 - len(words))\n","    return enc_c"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:32.885632Z","iopub.status.busy":"2024-07-02T20:17:32.885341Z","iopub.status.idle":"2024-07-02T20:17:33.099456Z","shell.execute_reply":"2024-07-02T20:17:33.098486Z","shell.execute_reply.started":"2024-07-02T20:17:32.885608Z"},"trusted":true},"outputs":[],"source":["pairs_encoded = []\n","for pair in pairs:\n","    qus = encode_question(pair[0], word_map)\n","    # qus = encode_question_left(pair[0], word_map)\n","    ans = encode_reply(pair[1], word_map)\n","    pairs_encoded.append([qus, ans])\n","\n","with open('pairs_encoded_kbfilkom.json', 'w') as p:\n","    json.dump(pairs_encoded, p)"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"data":{"text/plain":["90"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["len( pairs_encoded[1][0])"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:33.318791Z","iopub.status.busy":"2024-07-02T20:17:33.318483Z","iopub.status.idle":"2024-07-02T20:17:33.325793Z","shell.execute_reply":"2024-07-02T20:17:33.325035Z","shell.execute_reply.started":"2024-07-02T20:17:33.318756Z"},"trusted":true},"outputs":[],"source":["class Dataset(Dataset):\n","\n","    def __init__(self):\n","\n","        self.pairs = json.load(open('pairs_encoded_kbfilkom.json'))\n","        self.dataset_size = len(self.pairs)\n","\n","    def __getitem__(self, i):\n","        \n","        question = torch.LongTensor(self.pairs[i][0])\n","        reply = torch.LongTensor(self.pairs[i][1])\n","            \n","        return question, reply\n","\n","    def __len__(self):\n","        return self.dataset_size"]},{"cell_type":"markdown","metadata":{},"source":["## Train Loader"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:33.337842Z","iopub.status.busy":"2024-07-02T20:17:33.337481Z","iopub.status.idle":"2024-07-02T20:17:33.379910Z","shell.execute_reply":"2024-07-02T20:17:33.378865Z","shell.execute_reply.started":"2024-07-02T20:17:33.337813Z"},"trusted":true},"outputs":[],"source":["train_loader = torch.utils.data.DataLoader(Dataset(),\n","                                           batch_size = 100, \n","                                           shuffle=True, \n","                                           pin_memory=True)"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:33.381467Z","iopub.status.busy":"2024-07-02T20:17:33.381114Z","iopub.status.idle":"2024-07-02T20:17:33.388963Z","shell.execute_reply":"2024-07-02T20:17:33.388016Z","shell.execute_reply.started":"2024-07-02T20:17:33.381434Z"},"trusted":true},"outputs":[],"source":["def create_masks(question, reply_input, reply_target):\n","    \n","    def subsequent_mask(size):\n","        mask = torch.triu(torch.ones(size, size)).transpose(0, 1).type(dtype=torch.uint8)\n","        return mask.unsqueeze(0)\n","    \n","    question_mask = question!=0\n","    question_mask = question_mask.to(device)\n","    question_mask = question_mask.unsqueeze(1).unsqueeze(1)         # (batch_size, 1, 1, max_words)\n","     \n","    reply_input_mask = reply_input!=0\n","    reply_input_mask = reply_input_mask.unsqueeze(1)  # (batch_size, 1, max_words)\n","    reply_input_mask = reply_input_mask & subsequent_mask(reply_input.size(-1)).type_as(reply_input_mask.data) \n","    reply_input_mask = reply_input_mask.unsqueeze(1) # (batch_size, 1, max_words, max_words)\n","    reply_target_mask = reply_target!=0              # (batch_size, max_words)\n","    \n","    return question_mask, reply_input_mask, reply_target_mask"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:33.390611Z","iopub.status.busy":"2024-07-02T20:17:33.390255Z","iopub.status.idle":"2024-07-02T20:17:33.397673Z","shell.execute_reply":"2024-07-02T20:17:33.396839Z","shell.execute_reply.started":"2024-07-02T20:17:33.390578Z"},"trusted":true},"outputs":[],"source":["import os\n","\n","def create_directory(path):\n","    if not os.path.exists(path):\n","        os.makedirs(path)\n","        print(f\"Directory created at {path}\")\n","    else:\n","        print(f\"Directory already exists at {path}\")"]},{"cell_type":"markdown","metadata":{},"source":["# Architecture"]},{"cell_type":"markdown","metadata":{},"source":["## Embeddings"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:33.399270Z","iopub.status.busy":"2024-07-02T20:17:33.398914Z","iopub.status.idle":"2024-07-02T20:17:33.421634Z","shell.execute_reply":"2024-07-02T20:17:33.420718Z","shell.execute_reply.started":"2024-07-02T20:17:33.399239Z"},"trusted":true},"outputs":[],"source":["class Embeddings(nn.Module):\n","    \"\"\"\n","    Implements embeddings of the words and adds their positional encodings. \n","    \"\"\"\n","    def __init__(self, vocab_size, d_model, max_len = 50, num_layers = 6):\n","        super(Embeddings, self).__init__()\n","        self.d_model = d_model\n","        self.dropout = nn.Dropout(0.1)\n","        self.embed = nn.Embedding(vocab_size, d_model)\n","        self.pe = self.create_positinal_encoding(max_len, self.d_model)     # (1, max_len, d_model)\n","        self.te = self.create_positinal_encoding(num_layers, self.d_model)  # (1, num_layers, d_model)\n","        self.dropout = nn.Dropout(0.1)\n","        \n","    def create_positinal_encoding(self, max_len, d_model):\n","        pe = torch.zeros(max_len, d_model).to(device)\n","        for pos in range(max_len):   # for each position of the word\n","            for i in range(0, d_model, 2):   # for each dimension of the each position\n","                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/d_model)))\n","                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n","        pe = pe.unsqueeze(0)   # include the batch size\n","        return pe\n","        \n","    def forward(self, embedding, layer_idx):\n","        if layer_idx == 0:\n","            embedding = self.embed(embedding) * math.sqrt(self.d_model)\n","        embedding += self.pe[:, :embedding.size(1)]   # pe will automatically be expanded with the same batch size as encoded_words\n","        # embedding: (batch_size, max_len, d_model), te: (batch_size, 1, d_model)\n","        embedding += self.te[:, layer_idx, :].unsqueeze(1).repeat(1, embedding.size(1), 1)\n","        embedding = self.dropout(embedding)\n","        return embedding\n","\n","class EmbeddingsLSTM(nn.Module):\n","    \"\"\"\n","    Implements embeddingsLSTM of the words and adds their positional encodings. \n","    \"\"\"\n","    def __init__(self, vocab_size, d_model, num_layers = 6):\n","        super(EmbeddingsLSTM, self).__init__()\n","        self.d_model = d_model\n","        self.dropout = nn.Dropout(0.1)\n","        self.embed = nn.Embedding(vocab_size, d_model)\n","        self.lstm = nn.LSTM(d_model, d_model, num_layers=num_layers)\n","        \n","    def forward(self, embedding, layer_idx, hidden, cell_state):\n","        if layer_idx == 0:\n","            embedding = self.embed(embedding) * math.sqrt(self.d_model)\n","        # Pass the embeddings through the LSTM\\\n","        embedding, (hidden, cell_state) = self.lstm(embedding, (hidden, cell_state))\n","        print()\n","        print(embedding.size())\n","        print(embedding)\n","        return embedding, hidden, cell_state\n","\n","class PretrainedEmbedding(nn.Module):\n","    \"\"\"\n","    Implements embeddingsLSTM of the words and adds their positional encodings. \n","    \"\"\"\n","    def __init__(self, vocab_size, d_model, max_len = 50, num_layers = 6):\n","        super(PretrainedEmbedding, self).__init__()\n","        self.d_model = d_model\n","        self.dropout = nn.Dropout(0.1)\n","        self.embed = nn.Embedding(vocab_size, d_model)\n","        self.pe = self.create_positinal_encoding(max_len, self.d_model)     # (1, max_len, d_model)\n","        self.te = self.create_positinal_encoding(num_layers, self.d_model)  # (1, num_layers, d_model)\n","        self.dropout = nn.Dropout(0.1)\n","        \n","    def create_positinal_encoding(self, max_len, d_model):\n","        pe = torch.zeros(max_len, d_model).to(device)\n","        for pos in range(max_len):   # for each position of the word\n","            for i in range(0, d_model, 2):   # for each dimension of the each position\n","                pe[pos, i] = math.sin(pos / (10000 ** ((2 * i)/d_model)))\n","                pe[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1))/d_model)))\n","        pe = pe.unsqueeze(0)   # include the batch size\n","        return pe\n","        \n","    def forward(self, embedding, layer_idx):\n","        if layer_idx == 0:\n","            embedding = self.embed(embedding) * math.sqrt(self.d_model)\n","        embedding += self.pe[:, :embedding.size(1)]   # pe will automatically be expanded with the same batch size as encoded_words\n","        # embedding: (batch_size, max_len, d_model), te: (batch_size, 1, d_model)\n","        embedding += self.te[:, layer_idx, :].unsqueeze(1).repeat(1, embedding.size(1), 1)\n","        embedding = self.dropout(embedding)\n","        print()\n","        print(embedding.size())\n","        print(embedding)\n","        return embedding"]},{"cell_type":"markdown","metadata":{},"source":["## Multi-Head Attention"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:33.423711Z","iopub.status.busy":"2024-07-02T20:17:33.423056Z","iopub.status.idle":"2024-07-02T20:17:33.435766Z","shell.execute_reply":"2024-07-02T20:17:33.435028Z","shell.execute_reply.started":"2024-07-02T20:17:33.423675Z"},"trusted":true},"outputs":[],"source":["class MultiHeadAttention(nn.Module):\n","    \n","    def __init__(self, heads, d_model):\n","        super(MultiHeadAttention, self).__init__()\n","        assert d_model % heads == 0\n","        self.d_k = d_model // heads\n","        self.heads = heads\n","        self.dropout = nn.Dropout(0.1)\n","        self.query = nn.Linear(d_model, d_model)\n","        self.key = nn.Linear(d_model, d_model)\n","        self.value = nn.Linear(d_model, d_model)\n","        self.concat = nn.Linear(d_model, d_model)\n","        \n","    def forward(self, query, key, value, mask):\n","        \"\"\"\n","        query, key, value of shape: (batch_size, max_len, 512)\n","        mask of shape: (batch_size, 1, 1, max_words)\n","        \"\"\"\n","        # (batch_size, max_len, 512)\n","        query = self.query(query)\n","        key = self.key(key)        \n","        value = self.value(value)   \n","        \n","        # (batch_size, max_len, 512) --> (batch_size, max_len, h, d_k) --> (batch_size, h, max_len, d_k)\n","        query = query.view(query.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)   \n","        key = key.view(key.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)  \n","        value = value.view(value.shape[0], -1, self.heads, self.d_k).permute(0, 2, 1, 3)  \n","        \n","        # (batch_size, h, max_len, d_k) matmul (batch_size, h, d_k, max_len) --> (batch_size, h, max_len, max_len)\n","        scores = torch.matmul(query, key.permute(0,1,3,2)) / math.sqrt(query.size(-1))\n","        scores = scores.masked_fill(mask == 0, -1e9)    # (batch_size, h, max_len, max_len)\n","        weights = F.softmax(scores, dim = -1)           # (batch_size, h, max_len, max_len)\n","        weights = self.dropout(weights)\n","        # (batch_size, h, max_len, max_len) matmul (batch_size, h, max_len, d_k) --> (batch_size, h, max_len, d_k)\n","        context = torch.matmul(weights, value)\n","        # (batch_size, h, max_len, d_k) --> (batch_size, max_len, h, d_k) --> (batch_size, max_len, h * d_k)\n","        context = context.permute(0,2,1,3).contiguous().view(context.shape[0], -1, self.heads * self.d_k)\n","        # (batch_size, max_len, h * d_k)\n","        interacted = self.concat(context)\n","        return interacted "]},{"cell_type":"markdown","metadata":{},"source":["## Feed Forward Neural Network"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:33.437331Z","iopub.status.busy":"2024-07-02T20:17:33.437039Z","iopub.status.idle":"2024-07-02T20:17:33.450588Z","shell.execute_reply":"2024-07-02T20:17:33.449684Z","shell.execute_reply.started":"2024-07-02T20:17:33.437306Z"},"trusted":true},"outputs":[],"source":["class FeedForward(nn.Module):\n","    def __init__(self, d_model, middle_dim = 2048):\n","        super(FeedForward, self).__init__()\n","        \n","        self.fc1 = nn.Linear(d_model, middle_dim)\n","        self.fc2 = nn.Linear(middle_dim, d_model)\n","        self.dropout = nn.Dropout(0.1)\n","\n","    def forward(self, x):\n","        out = F.relu(self.fc1(x))\n","        out = self.fc2(self.dropout(out))\n","        return out\n","\n","class FeedForwardLSTM(nn.Module):\n","    def __init__(self, d_model, middle_dim = 2048):\n","        super(FeedForwardLSTM, self).__init__()\n","        \n","        self.fc1 = nn.Linear(d_model, middle_dim)\n","        self.lstm = nn.LSTM(middle_dim, middle_dim)\n","        self.fc2 = nn.Linear(middle_dim, d_model)\n","        self.dropout = nn.Dropout(0.1)\n","\n","    def forward(self, x):\n","        out = F.relu(self.fc1(x))\n","        out, _ = self.lstm(out)\n","        out = self.fc2(self.dropout(out))\n","        return out\n","\n","class FeedForwardLayerNoReg(nn.Module):\n","    def __init__(self, d_model, middle_dim = 2048):\n","        super(FeedForward, self).__init__()\n","        \n","        self.fc1 = nn.Linear(d_model, middle_dim)\n","        self.fc2 = nn.Linear(middle_dim, d_model)\n","\n","    def forward(self, x):\n","        out = F.relu(self.fc1(x))\n","        out = self.fc2(out)\n","        return out"]},{"cell_type":"markdown","metadata":{},"source":["## Encoder"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:33.452046Z","iopub.status.busy":"2024-07-02T20:17:33.451709Z","iopub.status.idle":"2024-07-02T20:17:33.461741Z","shell.execute_reply":"2024-07-02T20:17:33.460847Z","shell.execute_reply.started":"2024-07-02T20:17:33.452019Z"},"trusted":true},"outputs":[],"source":["class EncoderLayer(nn.Module):\n","    def __init__(self, d_model, heads):\n","        super(EncoderLayer, self).__init__()\n","        self.layernorm = nn.LayerNorm(d_model)\n","        self.self_multihead = MultiHeadAttention(heads, d_model)\n","        self.feed_forward = FeedForward(d_model)\n","        self.dropout = nn.Dropout(0.1)\n","\n","    def forward(self, embeddings, mask):\n","        interacted = self.dropout(self.self_multihead(embeddings, embeddings, embeddings, mask))\n","        interacted = self.layernorm(interacted + embeddings)\n","        feed_forward_out = self.dropout(self.feed_forward(interacted))\n","        encoded = self.layernorm(feed_forward_out + interacted)\n","        return encoded\n","\n","class EncoderLayerNoReg(nn.Module):\n","    def __init__(self, d_model, heads):\n","        super(EncoderLayerNoReg, self).__init__()\n","        self.layernorm = nn.LayerNorm(d_model)\n","        self.self_multihead = MultiHeadAttention(heads, d_model)\n","        self.feed_forward = FeedForward(d_model)\n","\n","    def forward(self, embeddings, mask):\n","        interacted = self.self_multihead(embeddings, embeddings, embeddings, mask)\n","        interacted = self.layernorm(interacted + embeddings)\n","        feed_forward_out = self.feed_forward(interacted)\n","        encoded = self.layernorm(feed_forward_out + interacted)\n","        return encoded\n","        \n","        "]},{"cell_type":"markdown","metadata":{},"source":["## Decoder"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:33.463365Z","iopub.status.busy":"2024-07-02T20:17:33.463025Z","iopub.status.idle":"2024-07-02T20:17:33.475859Z","shell.execute_reply":"2024-07-02T20:17:33.475008Z","shell.execute_reply.started":"2024-07-02T20:17:33.463339Z"},"trusted":true},"outputs":[],"source":["class DecoderLayer(nn.Module):\n","    \n","    def __init__(self, d_model, heads):\n","        super(DecoderLayer, self).__init__()\n","        self.layernorm = nn.LayerNorm(d_model)\n","        self.self_multihead = MultiHeadAttention(heads, d_model)\n","        self.src_multihead = MultiHeadAttention(heads, d_model)\n","        self.feed_forward = FeedForward(d_model)\n","        self.dropout = nn.Dropout(0.1)\n","        \n","    def forward(self, embeddings, encoded, src_mask, target_mask):\n","        query = self.dropout(self.self_multihead(embeddings, embeddings, embeddings, target_mask))\n","        query = self.layernorm(query + embeddings)\n","        interacted = self.dropout(self.src_multihead(query, encoded, encoded, src_mask))\n","        interacted = self.layernorm(interacted + query)\n","        feed_forward_out = self.dropout(self.feed_forward(interacted))\n","        decoded = self.layernorm(feed_forward_out + interacted)\n","        return decoded\n","\n","class DecoderLayerNoReg(nn.Module):\n","    def __init__(self, d_model, heads):\n","        super(DecoderLayerNoReg, self).__init__()\n","        self.layernorm = nn.LayerNorm(d_model)\n","        self.self_multihead = MultiHeadAttention(heads, d_model)\n","        self.src_multihead = MultiHeadAttention(heads, d_model)\n","        self.feed_forward = FeedForward(d_model)\n","        \n","    def forward(self, embeddings, encoded, src_mask, target_mask):\n","        query = self.self_multihead(embeddings, embeddings, embeddings, target_mask)\n","        query = self.layernorm(query + embeddings)\n","        interacted = self.src_multihead(query, encoded, encoded, src_mask)\n","        interacted = self.layernorm(interacted + query)\n","        feed_forward_out = self.feed_forward(interacted)\n","        decoded = self.layernorm(feed_forward_out + interacted)\n","        return decoded"]},{"cell_type":"markdown","metadata":{},"source":["## Transformer Architecture"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:33.478079Z","iopub.status.busy":"2024-07-02T20:17:33.477782Z","iopub.status.idle":"2024-07-02T20:17:33.514932Z","shell.execute_reply":"2024-07-02T20:17:33.514023Z","shell.execute_reply.started":"2024-07-02T20:17:33.478055Z"},"trusted":true},"outputs":[],"source":["class Transformer(nn.Module):\n","    \n","    def __init__(self, d_model, heads, num_layers, word_map, max_len = 50):\n","        super(Transformer, self).__init__()\n","        \n","        self.d_model = d_model\n","        self.num_layers = num_layers\n","        self.vocab_size = len(word_map)\n","        self.embed = Embeddings(self.vocab_size, d_model, num_layers = num_layers, max_len = max_len)\n","        self.encoder = EncoderLayer(d_model, heads) \n","        self.decoder = DecoderLayer(d_model, heads)\n","        self.logit = nn.Linear(d_model, self.vocab_size)\n","        \n","    def encode(self, src_embeddings, src_mask):\n","        for i in range(self.num_layers):\n","            src_embeddings = self.embed(src_embeddings, i)\n","            src_embeddings = self.encoder(src_embeddings, src_mask)\n","        return src_embeddings\n","    \n","    def decode(self, tgt_embeddings, target_mask, src_embeddings, src_mask):\n","        for i in range(self.num_layers):\n","            tgt_embeddings = self.embed(tgt_embeddings, i)\n","            tgt_embeddings = self.decoder(tgt_embeddings, src_embeddings, src_mask, target_mask)\n","        return tgt_embeddings\n","        \n","    def forward(self, src_words, src_mask, target_words, target_mask):\n","        encoded = self.encode(src_words, src_mask)\n","        decoded = self.decode(target_words, target_mask, encoded, src_mask)\n","        out = F.log_softmax(self.logit(decoded), dim = 2)\n","        return out\n","\n","class TransformerNoReg(nn.Module):\n","    \n","    def __init__(self, d_model, heads, num_layers, word_map, max_len = 50):\n","        super(TransformerNoReg, self).__init__()\n","        \n","        self.d_model = d_model\n","        self.num_layers = num_layers\n","        self.vocab_size = len(word_map)\n","        self.embed = Embeddings(self.vocab_size, d_model, num_layers = num_layers, max_len = max_len)\n","        self.encoder = EncoderLayerNoReg(d_model, heads) \n","        self.decoder = DecoderLayerNoReg(d_model, heads)\n","        self.logit = nn.Linear(d_model, self.vocab_size)\n","        \n","    def encode(self, src_embeddings, src_mask):\n","        for i in range(self.num_layers):\n","            src_embeddings = self.embed(src_embeddings, i)\n","            src_embeddings = self.encoder(src_embeddings, src_mask)\n","        return src_embeddings\n","    \n","    def decode(self, tgt_embeddings, target_mask, src_embeddings, src_mask):\n","        for i in range(self.num_layers):\n","            tgt_embeddings = self.embed(tgt_embeddings, i)\n","            tgt_embeddings = self.decoder(tgt_embeddings, src_embeddings, src_mask, target_mask)\n","        return tgt_embeddings\n","        \n","    def forward(self, src_words, src_mask, target_words, target_mask):\n","        encoded = self.encode(src_words, src_mask)\n","        decoded = self.decode(target_words, target_mask, encoded, src_mask)\n","        out = F.log_softmax(self.logit(decoded), dim = 2)\n","        return out\n","\n","class TransformerLSTM(nn.Module):\n","    \n","    def __init__(self, d_model, heads, num_layers, word_map, max_len = 50):\n","        super(TransformerLSTM, self).__init__()\n","        \n","        self.d_model = d_model\n","        self.num_layers = num_layers\n","        self.vocab_size = len(word_map)\n","        self.embed = EmbeddingsLSTM(self.vocab_size, d_model, num_layers = num_layers)\n","        self.encoder = EncoderLayer(d_model, heads) \n","        self.decoder = DecoderLayer(d_model, heads)\n","        self.logit = nn.Linear(d_model, self.vocab_size)\n","        self.max_len = max_len\n","        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        \n","    def encode(self, src_embeddings, src_mask):\n","        encoder_hidden = torch.zeros([self.num_layers, self.max_len, self.d_model]).to(self.device) # 1 = number of LSTM layers\n","        cell_state = torch.zeros([self.num_layers, self.max_len, self.d_model]).to(self.device)\n","        for i in range(self.num_layers):\n","            src_embeddings, encoder_hidden, cell_state = self.embed(src_embeddings, i, encoder_hidden, cell_state)\n","            src_embeddings = self.encoder(src_embeddings, src_mask)\n","        return src_embeddings, encoder_hidden, cell_state\n","    \n","    def decode(self, tgt_embeddings, target_mask, src_embeddings, src_mask, encoder_hidden, cell_state):\n","        zeros = torch.zeros((self.num_layers, 1, self.d_model), device=encoder_hidden.device)\n","        encoder_hidden = torch.cat((encoder_hidden, zeros), dim=1)\n","        decoder_input = torch.Tensor([[0]]).long().to(self.device) # 0 = SOS_token\n","        decoder_hidden = encoder_hidden\n","        print(decoder_hidden.size())\n","        for i in range(self.num_layers):\n","            tgt_embeddings, decoder_hidden, cell_state = self.embed(tgt_embeddings, i, decoder_hidden, cell_state)\n","            tgt_embeddings = self.decoder(tgt_embeddings, src_embeddings, src_mask, target_mask)\n","        return tgt_embeddings\n","        \n","    def forward(self, src_words, src_mask, target_words, target_mask):\n","        encoded, encoder_hidden, cell_state = self.encode(src_words, src_mask)\n","        decoded = self.decode(target_words, target_mask, encoded, src_mask, encoder_hidden, cell_state)\n","        out = F.log_softmax(self.logit(decoded), dim = 2)\n","        return out\n","\n","class TransformerPreTrainedEmbedding(nn.Module):\n","    def __init__(self, d_model, heads, num_layers, word_map, max_len = 50):\n","        super(TransformerPreTrainedEmbedding, self).__init__()\n","        \n","        self.d_model = d_model\n","        self.num_layers = num_layers\n","        self.vocab_size = len(word_map)\n","        self.embed = PretrainedEmbedding(self.vocab_size, d_model, num_layers = num_layers, max_len = max_len)\n","        self.encoder = EncoderLayer(d_model, heads) \n","        self.decoder = DecoderLayer(d_model, heads)\n","        self.logit = nn.Linear(d_model, self.vocab_size)\n","        \n","    def encode(self, src_embeddings, src_mask):\n","        for i in range(self.num_layers):\n","            src_embeddings = self.embed(src_embeddings, i)\n","            src_embeddings = self.encoder(src_embeddings, src_mask)\n","        return src_embeddings\n","    \n","    def decode(self, tgt_embeddings, target_mask, src_embeddings, src_mask):\n","        for i in range(self.num_layers):\n","            tgt_embeddings = self.embed(tgt_embeddings, i)\n","            tgt_embeddings = self.decoder(tgt_embeddings, src_embeddings, src_mask, target_mask)\n","        return tgt_embeddings\n","        \n","    def forward(self, src_words, src_mask, target_words, target_mask):\n","        encoded = self.encode(src_words, src_mask)\n","        decoded = self.decode(target_words, target_mask, encoded, src_mask)\n","        out = F.log_softmax(self.logit(decoded), dim = 2)\n","        return out\n","\n","class TransformerDecoderOnly(nn.Module):    \n","    def __init__(self, d_model, heads, num_layers, word_map, max_len = 50):\n","        super(TransformerDecoderOnly, self).__init__()\n","        \n","        self.d_model = d_model\n","        self.num_layers = num_layers\n","        self.vocab_size = len(word_map)\n","        self.embed = Embeddings(self.vocab_size, d_model, num_layers = num_layers, max_len = max_len)\n","        self.encoder = EncoderLayer(d_model, heads) \n","        self.decoder = DecoderLayer(d_model, heads)\n","        self.logit = nn.Linear(d_model, self.vocab_size)\n","    \n","    def decode(self, tgt_embeddings, target_mask, src_embeddings, src_mask):\n","        for i in range(self.num_layers):\n","            tgt_embeddings = self.embed(tgt_embeddings, i)\n","            tgt_embeddings = self.decoder(tgt_embeddings, src_embeddings, src_mask, target_mask)\n","        return tgt_embeddings\n","        \n","    def forward(self, src_words, src_mask, target_words, target_mask):\n","        encoded = self.embed(src_words, 0)\n","        decoded = self.decode(target_words, target_mask, encoded, src_mask)\n","        out = F.log_softmax(self.logit(decoded), dim = 2)\n","        return out"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:33.516425Z","iopub.status.busy":"2024-07-02T20:17:33.516094Z","iopub.status.idle":"2024-07-02T20:17:33.527522Z","shell.execute_reply":"2024-07-02T20:17:33.526734Z","shell.execute_reply.started":"2024-07-02T20:17:33.516394Z"},"trusted":true},"outputs":[],"source":["class AdamWarmup:\n","    \n","    def __init__(self, model_size, warmup_steps, optimizer):\n","        \n","        self.model_size = model_size\n","        self.warmup_steps = warmup_steps\n","        self.optimizer = optimizer\n","        self.current_step = 0\n","        self.lr = 0\n","        \n","    def get_lr(self):\n","        return self.model_size ** (-0.5) * min(self.current_step ** (-0.5), self.current_step * self.warmup_steps ** (-1.5))\n","        \n","    def step(self):\n","        # Increment the number of steps each time we call the step function\n","        self.current_step += 1\n","        lr = self.get_lr()\n","        for param_group in self.optimizer.param_groups:\n","            param_group['lr'] = lr\n","        # update the learning rate\n","        self.lr = lr\n","        self.optimizer.step()       \n","        "]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:33.529363Z","iopub.status.busy":"2024-07-02T20:17:33.528622Z","iopub.status.idle":"2024-07-02T20:17:33.540886Z","shell.execute_reply":"2024-07-02T20:17:33.540153Z","shell.execute_reply.started":"2024-07-02T20:17:33.529331Z"},"trusted":true},"outputs":[],"source":["class LossWithLS(nn.Module):\n","\n","    def __init__(self, size, smooth):\n","        super(LossWithLS, self).__init__()\n","        self.criterion = nn.KLDivLoss(size_average=False, reduce=False)\n","        self.confidence = 1.0 - smooth\n","        self.smooth = smooth\n","        self.size = size\n","        \n","    def forward(self, prediction, target, mask):\n","        \"\"\"\n","        prediction of shape: (batch_size, max_words, vocab_size)\n","        target and mask of shape: (batch_size, max_words)\n","        \"\"\"\n","        prediction = prediction.view(-1, prediction.size(-1))   # (batch_size * max_words, vocab_size)\n","        target = target.contiguous().view(-1)   # (batch_size * max_words)\n","        mask = mask.float()\n","        mask = mask.view(-1)       # (batch_size * max_words)\n","        labels = prediction.data.clone()\n","        labels.fill_(self.smooth / (self.size - 1))\n","        labels.scatter_(1, target.data.unsqueeze(1), self.confidence)\n","        loss = self.criterion(prediction, labels)    # (batch_size * max_words, vocab_size)\n","        loss = (loss.sum(1) * mask).sum() / mask.sum()\n","        return loss\n","        "]},{"cell_type":"markdown","metadata":{},"source":["# Define Neptune Experiment"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:33.542496Z","iopub.status.busy":"2024-07-02T20:17:33.542228Z","iopub.status.idle":"2024-07-02T20:17:33.550662Z","shell.execute_reply":"2024-07-02T20:17:33.549840Z","shell.execute_reply.started":"2024-07-02T20:17:33.542473Z"},"trusted":true},"outputs":[],"source":["project = \"andialifs/fluent-tesis-24\"\n","api_token = \"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJjZTY2YWQ3My04OTBkLTQ2OWUtYTc1Ni1jYjk0MGZhMWFiNGEifQ==\"\n","\n","def neptune_init(name):\n","    run = neptune.init_run(\n","        project=project,\n","        api_token=api_token,\n","        name=name\n","    )\n","    return run"]},{"cell_type":"markdown","metadata":{},"source":["# Function"]},{"cell_type":"markdown","metadata":{},"source":["## Train"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:33.564996Z","iopub.status.busy":"2024-07-02T20:17:33.564656Z","iopub.status.idle":"2024-07-02T20:17:33.574590Z","shell.execute_reply":"2024-07-02T20:17:33.573653Z","shell.execute_reply.started":"2024-07-02T20:17:33.564947Z"},"trusted":true},"outputs":[],"source":["def train(train_loader, transformer, criterion, epoch):\n","    transformer.train()\n","    sum_loss = 0\n","    count = 0\n","\n","    for i, (question, reply) in enumerate(train_loader):\n","        \n","        samples = question.shape[0]\n","\n","        # Move to device\n","        question = question.to(device)\n","        reply = reply.to(device)\n","\n","        # Prepare Target Data\n","        reply_input = reply[:, :-1]\n","        reply_target = reply[:, 1:]\n","\n","        # Create mask and add dimensions\n","        question_mask, reply_input_mask, reply_target_mask = create_masks(question, reply_input, reply_target)\n","\n","        # Get the transformer outputs\n","        out = transformer(question, question_mask, reply_input, reply_input_mask)\n","\n","        # Compute the loss\n","        loss = criterion(out, reply_target, reply_target_mask)\n","        \n","        # Backprop\n","        transformer_optimizer.optimizer.zero_grad()\n","        loss.backward()\n","        transformer_optimizer.step()\n","        \n","        sum_loss += loss.item() * samples\n","        count += samples\n","        \n","        if i % 100 == 0:\n","            print(\"Epoch [{}][{}/{}]\\tLoss: {:.3f}\".format(epoch, i, len(train_loader), sum_loss/count))\n","    \n","    return sum_loss/count"]},{"cell_type":"markdown","metadata":{},"source":["## Evaluate"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:33.576127Z","iopub.status.busy":"2024-07-02T20:17:33.575799Z","iopub.status.idle":"2024-07-02T20:17:33.587390Z","shell.execute_reply":"2024-07-02T20:17:33.586545Z","shell.execute_reply.started":"2024-07-02T20:17:33.576103Z"},"trusted":true},"outputs":[],"source":["def evaluate(transformer, question, question_mask, max_len, word_map):\n","    \"\"\"\n","    Performs Greedy Decoding with a batch size of 1\n","    \"\"\"\n","    rev_word_map = {v: k for k, v in word_map.items()}\n","    transformer.eval()\n","    start_token = word_map['<start>']\n","    encoded = transformer.encode(question, question_mask)\n","    words = torch.LongTensor([[start_token]]).to(device)\n","    \n","    for step in range(max_len - 1):\n","        size = words.shape[1]\n","        target_mask = torch.triu(torch.ones(size, size)).transpose(0, 1).type(dtype=torch.uint8)\n","        target_mask = target_mask.to(device).unsqueeze(0).unsqueeze(0)\n","        decoded = transformer.decode(words, target_mask, encoded, question_mask)\n","        predictions = transformer.logit(decoded[:, -1])\n","        _, next_word = torch.max(predictions, dim = 1)\n","        next_word = next_word.item()\n","        if next_word == word_map['<end>']:\n","            break\n","        words = torch.cat([words, torch.LongTensor([[next_word]]).to(device)], dim = 1)   # (1,step+2)\n","        \n","    # Construct Sentence\n","    if words.dim() == 2:\n","        words = words.squeeze(0)\n","        words = words.tolist()\n","        \n","    sen_idx = [w for w in words if w not in {word_map['<start>']}]\n","    sentence = ' '.join([rev_word_map[sen_idx[k]] for k in range(len(sen_idx))])\n","    \n","    return sentence"]},{"cell_type":"markdown","metadata":{},"source":["# Experiment"]},{"cell_type":"markdown","metadata":{},"source":["## Transformers without reg"]},{"cell_type":"code","execution_count":38,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:17:33.588788Z","iopub.status.busy":"2024-07-02T20:17:33.588528Z","iopub.status.idle":"2024-07-02T20:17:47.395240Z","shell.execute_reply":"2024-07-02T20:17:47.393722Z","shell.execute_reply.started":"2024-07-02T20:17:33.588766Z"},"scrolled":true,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\\Skipping experiment 0 with d_model 512, heads8, num_layers5\n","\n","\\Skipping experiment 1 with d_model 512, heads8, num_layers10\n","\n","\\Skipping experiment 2 with d_model 512, heads16, num_layers5\n","\n","\\Skipping experiment 3 with d_model 512, heads16, num_layers10\n","\n","\\Skipping experiment 4 with d_model 512, heads32, num_layers5\n","\n","\\Skipping experiment 5 with d_model 512, heads32, num_layers10\n","\n","\\Skipping experiment 6 with d_model 1024, heads8, num_layers5\n","\n","\\Skipping experiment 7 with d_model 1024, heads8, num_layers10\n","\n","\\Skipping experiment 8 with d_model 1024, heads16, num_layers5\n","\n","\\Skipping experiment 9 with d_model 1024, heads16, num_layers10\n","\n","\\Skipping experiment 10 with d_model 1024, heads32, num_layers5\n","\n","\\Skipping experiment 11 with d_model 1024, heads32, num_layers10\n","\n","\\Skipping experiment 12 with d_model 2048, heads8, num_layers5\n","\n","\n","Running for experiment 13 with d_model 2048, heads8, num_layers10\n","\n","[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/andialifs/fluent-tesis-24/e/FLUENT24-72\n"]},{"name":"stderr","output_type":"stream","text":["/home/andyalyfsyah/.conda/envs/myenv/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [0][0/12]\tLoss: 5.316\n","Epoch [1][0/12]\tLoss: 5.172\n","Epoch [2][0/12]\tLoss: 4.794\n","Epoch [3][0/12]\tLoss: 4.388\n","Epoch [4][0/12]\tLoss: 4.277\n","Epoch [5][0/12]\tLoss: 4.096\n","Epoch [6][0/12]\tLoss: 4.054\n","Epoch [7][0/12]\tLoss: 4.016\n","Epoch [8][0/12]\tLoss: 4.039\n","Epoch [9][0/12]\tLoss: 3.996\n","Epoch [10][0/12]\tLoss: 3.987\n","Epoch [11][0/12]\tLoss: 3.953\n","Epoch [12][0/12]\tLoss: 3.956\n","Epoch [13][0/12]\tLoss: 3.985\n","Epoch [14][0/12]\tLoss: 3.910\n","Epoch [15][0/12]\tLoss: 3.954\n","Epoch [16][0/12]\tLoss: 3.935\n","Epoch [17][0/12]\tLoss: 3.714\n","Epoch [18][0/12]\tLoss: 3.813\n","Epoch [19][0/12]\tLoss: 3.785\n","Epoch [20][0/12]\tLoss: 3.831\n","Epoch [21][0/12]\tLoss: 3.673\n","Epoch [22][0/12]\tLoss: 3.745\n","Epoch [23][0/12]\tLoss: 3.641\n","Epoch [24][0/12]\tLoss: 3.572\n","Epoch [25][0/12]\tLoss: 3.588\n","Epoch [26][0/12]\tLoss: 3.553\n","Epoch [27][0/12]\tLoss: 3.565\n","Epoch [28][0/12]\tLoss: 3.427\n","Epoch [29][0/12]\tLoss: 3.531\n","Epoch [30][0/12]\tLoss: 3.219\n","Epoch [31][0/12]\tLoss: 3.407\n","Epoch [32][0/12]\tLoss: 3.051\n","Epoch [33][0/12]\tLoss: 3.375\n","Epoch [34][0/12]\tLoss: 3.234\n","Epoch [35][0/12]\tLoss: 3.081\n","Epoch [36][0/12]\tLoss: 2.986\n","Epoch [37][0/12]\tLoss: 2.987\n","Epoch [38][0/12]\tLoss: 2.787\n","Epoch [39][0/12]\tLoss: 3.200\n","Epoch [40][0/12]\tLoss: 3.130\n","Epoch [41][0/12]\tLoss: 3.092\n","Epoch [42][0/12]\tLoss: 2.891\n","Epoch [43][0/12]\tLoss: 2.947\n","Epoch [44][0/12]\tLoss: 2.989\n","Epoch [45][0/12]\tLoss: 2.848\n","Epoch [46][0/12]\tLoss: 2.886\n","Epoch [47][0/12]\tLoss: 2.696\n","Epoch [48][0/12]\tLoss: 2.990\n","Epoch [49][0/12]\tLoss: 2.601\n"]}],"source":["d_model = [512, 1024, 2048, 4096]\n","heads = [8, 16, 32]\n","num_layers = [5, 10]\n","epochs = 100\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","with open('WORDMAP_corpus_KBFILKOM.json', 'r') as j:\n","    word_map = json.load(j)\n","\n","transformer_experiment = pd.DataFrame(columns = ['experiment_id', 'd_model', 'heads', 'num_layers', 'train_loss'])\n","loss_history = {}\n","\n","experiment_id = -1\n","\n","torch.cuda.empty_cache()\n","\n","for d_m in d_model:\n","    for h in heads:\n","        for n_l in num_layers: \n","            parameters = {\n","                'd_model': d_m,\n","                'heads': h,\n","                'num_layers': n_l\n","            }\n","            \n","            experiment_id += 1\n","            \n","            if experiment_id < 13:\n","                print('\\Skipping experiment {} with d_model {}, heads{}, num_layers{}\\n'.format(experiment_id, d_m, h, n_l))\n","                continue\n","            \n","            print('\\nRunning for experiment {} with d_model {}, heads{}, num_layers{}\\n'.format(experiment_id, d_m, h, n_l))\n","            name = \"experiment_2724_noreg_\" + str(experiment_id)\n","\n","            run = neptune_init(name)\n","            run['parameters'] = parameters\n","            run['group_tags'] = \"transformers-vanilla-no-reg\"\n","            \n","            transformer_experiment.loc[experiment_id, 'experiment_id'] = 'experiment_{}'.format(str(experiment_id))\n","            transformer_experiment.loc[experiment_id, 'd_model'] = d_m\n","            transformer_experiment.loc[experiment_id, 'heads'] = h\n","            transformer_experiment.loc[experiment_id, 'num_layers'] = n_l\n","\n","            transformer = Transformer(d_model = d_m, heads = h, num_layers = n_l, word_map = word_map, max_len=95)\n","            transformer = transformer.to(device)\n","            adam_optimizer = torch.optim.Adam(transformer.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-9)\n","            transformer_optimizer = AdamWarmup(model_size = d_m, warmup_steps = 4000, optimizer = adam_optimizer)\n","            criterion = LossWithLS(len(word_map), 0.2)\n","\n","            loss_list_experiment = []\n","            for epoch in range(epochs):\n","                loss_train = train(train_loader, transformer, criterion, epoch)\n","\n","                state = {'epoch': epoch, 'transformer': transformer, 'transformer_optimizer': transformer_optimizer}\n","                # torch.save(state, 'checkpoint_experiment_' + str(epoch) + 'id_'+ str(experiment_id) +'.pth.tar')\n","\n","                loss_list_experiment.append(loss_train)\n","\n","                run['train/loss'].append(loss_train)\n","\n","            transformer_experiment.loc[experiment_id, 'train_loss'] = loss_train\n","            loss_history['experiment_{}'.format(str(experiment_id))] = loss_list_experiment\n","            \n","            # torch.cuda.empty_cache() \n","            run.stop()\n"]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2024-07-02T20:16:32.298394Z","iopub.status.busy":"2024-07-02T20:16:32.297969Z","iopub.status.idle":"2024-07-02T20:16:32.305442Z","shell.execute_reply":"2024-07-02T20:16:32.304012Z","shell.execute_reply.started":"2024-07-02T20:16:32.298351Z"},"trusted":true},"outputs":[],"source":["torch.cuda.empty_cache() "]},{"cell_type":"markdown","metadata":{},"source":["## Transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"trusted":true},"outputs":[],"source":["d_model = [512, 1024, 2048, 4096]\n","heads = [8, 16, 32]\n","num_layers = [5, 10]\n","epochs = 100\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","with open('WORDMAP_corpus_KBFILKOM.json', 'r') as j:\n","    word_map = json.load(j)\n","\n","transformer_experiment = pd.DataFrame(columns = ['experiment_id', 'd_model', 'heads', 'num_layers', 'train_loss'])\n","loss_history = {}\n","\n","experiment_id = -1\n","\n","for d_m in d_model:\n","    for h in heads:\n","        for n_l in num_layers: \n","            experiment_id += 1\n","            print('\\nRunning for experiment {} with d_model {}, heads{}, num_layers{}\\n'.format(experiment_id, d_m, h, n_l))\n","\n","            run = neptune.init_run(\n","                project=project,\n","                api_token=api_token,\n","                name=\"experiment_1724_\" + str(experiment_id)\n","            ) \n","            run['parameters'] = {\n","                'd_model': d_m,\n","                'heads': h,\n","                'num_layers': n_l\n","            }\n","\n","            transformer_experiment.loc[experiment_id, 'experiment_id'] = 'experiment_{}'.format(str(experiment_id))\n","            transformer_experiment.loc[experiment_id, 'd_model'] = d_m\n","            transformer_experiment.loc[experiment_id, 'heads'] = h\n","            transformer_experiment.loc[experiment_id, 'num_layers'] = n_l\n","\n","            transformer = Transformer(d_model = d_m, heads = h, num_layers = n_l, word_map = word_map, max_len=95)\n","            transformer = transformer.to(device)\n","            adam_optimizer = torch.optim.Adam(transformer.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-9)\n","            transformer_optimizer = AdamWarmup(model_size = d_m, warmup_steps = 4000, optimizer = adam_optimizer)\n","            criterion = LossWithLS(len(word_map), 0.2)\n","\n","            loss_list_experiment = []\n","            for epoch in range(epochs):\n","                loss_train = train(train_loader, transformer, criterion, epoch)\n","\n","                state = {'epoch': epoch, 'transformer': transformer, 'transformer_optimizer': transformer_optimizer}\n","                torch.save(state, 'checkpoint_experiment_' + str(epoch) + 'id_'+ str(experiment_id) +'.pth.tar')\n","\n","                loss_list_experiment.append(loss_train)\n","\n","                run['train/loss'].append(loss_train)\n","\n","            transformer_experiment.loc[experiment_id, 'train_loss'] = loss_train\n","            loss_history['experiment_{}'.format(str(experiment_id))] = loss_list_experiment\n","            \n","            run.stop()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import yaml\n","\n","with open('loss_history_transformer_experiment_KBFILKOM.yaml', 'w') as file:\n","    documents = yaml.dump(loss_history, file)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["transformer_experiment.dropna(inplace=True)\n","transformer_experiment"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import yaml\n","\n","with open('history_rnn_150524.yaml', 'r') as file:\n","    history_rnn = yaml.load(file, Loader=yaml.FullLoader)"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true},"outputs":[],"source":["import matplotlib \n","import matplotlib.pyplot as plt\n","\n","loss_history_key = list(loss_history.keys())\n","\n","plt.figure(figsize=(15,10))\n","plt.title(\"Training loss vs. Number of Epochs\")\n","plt.xlabel(\"Number of Epochs\")\n","plt.ylabel(\"Training Loss\")\n","z\n","\n","for key in loss_history_key:\n","    loss_list = loss_history[key]\n","    labels = f'd_model: {transformer_experiment[transformer_experiment[\"experiment_id\"] == key][\"d_model\"].values[0]}, heads: {transformer_experiment[transformer_experiment[\"experiment_id\"] == key][\"heads\"].values[0]}, num_layers: {transformer_experiment[transformer_experiment[\"experiment_id\"] == key][\"num_layers\"].values[0]}'\n","    plt.plot(loss_list, label = labels)\n","\n","    \n","plt.plot(history_rnn['loss'], \n","                label = 'LSTM (Baseline FLUENT 2023)', \n","                linestyle='dashed', \n","                color='black', \n","                linewidth=2.5, \n","                alpha=0.7, \n","                marker='o', \n","                markerfacecolor='black', \n","                markersize=5\n","        )\n","\n","plt.legend()\n","torch.cuda.is_available()\n","plt.grid()"]},{"cell_type":"markdown","metadata":{},"source":["# Training"]},{"cell_type":"markdown","metadata":{},"source":["## Pretrained"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["directory = 'experiment_pretrained_1'\n","\n","d_model = 2048\n","heads = 16\n","num_layers = 5\n","epochs = 100\n","\n","loss_history_pratrained_embed_transformer = []\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","with open('WORDMAP_corpus_KBFILKOM.json', 'r') as j:\n","    word_map = json.load(j)\n","    \n","transformer = TransformerPreTrainedEmbedding(d_model = d_model, heads = heads, num_layers = num_layers, word_map = word_map, max_len=95)\n","transformer = transformer.to(device)\n","adam_optimizer = torch.optim.Adam(transformer.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-9)\n","transformer_optimizer = AdamWarmup(model_size = d_model, warmup_steps = 4000, optimizer = adam_optimizer)\n","criterion = LossWithLS(len(word_map), 0.2)\n","\n","for epoch in range(epochs):\n","    loss_train = train(train_loader, transformer, criterion, epoch)\n","\n","    state = {'epoch': epoch, 'transformer': transformer, 'transformer_optimizer': transformer_optimizer}\n","    torch.save(state, directory + '/checkpoint_' + str(epoch) +'.pth.tar')\n","\n","    loss_history_pratrained_embed_transformer.append(loss_train)"]},{"cell_type":"markdown","metadata":{},"source":["## Vanilla"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Directory already exists at experiment_vanilla_30624\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [0][0/1]\tLoss: 4.559\n","Epoch [1][0/1]\tLoss: 4.565\n","Epoch [2][0/1]\tLoss: 4.554\n","Epoch [3][0/1]\tLoss: 4.549\n","Epoch [4][0/1]\tLoss: 4.536\n","Epoch [5][0/1]\tLoss: 4.527\n","Epoch [6][0/1]\tLoss: 4.515\n","Epoch [7][0/1]\tLoss: 4.498\n","Epoch [8][0/1]\tLoss: 4.487\n","Epoch [9][0/1]\tLoss: 4.474\n","Epoch [10][0/1]\tLoss: 4.449\n","Epoch [11][0/1]\tLoss: 4.426\n","Epoch [12][0/1]\tLoss: 4.397\n","Epoch [13][0/1]\tLoss: 4.377\n","Epoch [14][0/1]\tLoss: 4.341\n","Epoch [15][0/1]\tLoss: 4.315\n","Epoch [16][0/1]\tLoss: 4.290\n","Epoch [17][0/1]\tLoss: 4.267\n","Epoch [18][0/1]\tLoss: 4.235\n","Epoch [19][0/1]\tLoss: 4.216\n","Epoch [20][0/1]\tLoss: 4.193\n","Epoch [21][0/1]\tLoss: 4.166\n","Epoch [22][0/1]\tLoss: 4.154\n","Epoch [23][0/1]\tLoss: 4.130\n","Epoch [24][0/1]\tLoss: 4.112\n","Epoch [25][0/1]\tLoss: 4.104\n","Epoch [26][0/1]\tLoss: 4.090\n","Epoch [27][0/1]\tLoss: 4.088\n","Epoch [28][0/1]\tLoss: 4.070\n","Epoch [29][0/1]\tLoss: 4.049\n","Epoch [30][0/1]\tLoss: 4.037\n","Epoch [31][0/1]\tLoss: 4.027\n","Epoch [32][0/1]\tLoss: 4.012\n","Epoch [33][0/1]\tLoss: 4.006\n","Epoch [34][0/1]\tLoss: 4.005\n","Epoch [35][0/1]\tLoss: 3.998\n","Epoch [36][0/1]\tLoss: 3.997\n","Epoch [37][0/1]\tLoss: 3.999\n","Epoch [38][0/1]\tLoss: 3.985\n","Epoch [39][0/1]\tLoss: 3.977\n","Epoch [40][0/1]\tLoss: 3.967\n","Epoch [41][0/1]\tLoss: 3.971\n","Epoch [42][0/1]\tLoss: 3.965\n","Epoch [43][0/1]\tLoss: 3.970\n","Epoch [44][0/1]\tLoss: 3.955\n","Epoch [45][0/1]\tLoss: 3.959\n","Epoch [46][0/1]\tLoss: 3.946\n","Epoch [47][0/1]\tLoss: 3.942\n","Epoch [48][0/1]\tLoss: 3.943\n","Epoch [49][0/1]\tLoss: 3.937\n","Epoch [50][0/1]\tLoss: 3.946\n","Epoch [51][0/1]\tLoss: 3.938\n","Epoch [52][0/1]\tLoss: 3.923\n","Epoch [53][0/1]\tLoss: 3.927\n","Epoch [54][0/1]\tLoss: 3.919\n","Epoch [55][0/1]\tLoss: 3.922\n","Epoch [56][0/1]\tLoss: 3.929\n","Epoch [57][0/1]\tLoss: 3.916\n","Epoch [58][0/1]\tLoss: 3.911\n","Epoch [59][0/1]\tLoss: 3.904\n","Epoch [60][0/1]\tLoss: 3.906\n","Epoch [61][0/1]\tLoss: 3.911\n","Epoch [62][0/1]\tLoss: 3.894\n","Epoch [63][0/1]\tLoss: 3.901\n","Epoch [64][0/1]\tLoss: 3.890\n","Epoch [65][0/1]\tLoss: 3.892\n","Epoch [66][0/1]\tLoss: 3.885\n","Epoch [67][0/1]\tLoss: 3.884\n","Epoch [68][0/1]\tLoss: 3.872\n","Epoch [69][0/1]\tLoss: 3.883\n","Epoch [70][0/1]\tLoss: 3.856\n","Epoch [71][0/1]\tLoss: 3.871\n","Epoch [72][0/1]\tLoss: 3.861\n","Epoch [73][0/1]\tLoss: 3.852\n","Epoch [74][0/1]\tLoss: 3.846\n","Epoch [75][0/1]\tLoss: 3.839\n","Epoch [76][0/1]\tLoss: 3.841\n","Epoch [77][0/1]\tLoss: 3.821\n","Epoch [78][0/1]\tLoss: 3.825\n","Epoch [79][0/1]\tLoss: 3.816\n","Epoch [80][0/1]\tLoss: 3.787\n","Epoch [81][0/1]\tLoss: 3.805\n","Epoch [82][0/1]\tLoss: 3.794\n","Epoch [83][0/1]\tLoss: 3.764\n","Epoch [84][0/1]\tLoss: 3.759\n","Epoch [85][0/1]\tLoss: 3.756\n","Epoch [86][0/1]\tLoss: 3.741\n","Epoch [87][0/1]\tLoss: 3.729\n","Epoch [88][0/1]\tLoss: 3.715\n","Epoch [89][0/1]\tLoss: 3.692\n","Epoch [90][0/1]\tLoss: 3.683\n","Epoch [91][0/1]\tLoss: 3.674\n","Epoch [92][0/1]\tLoss: 3.658\n","Epoch [93][0/1]\tLoss: 3.637\n","Epoch [94][0/1]\tLoss: 3.637\n","Epoch [95][0/1]\tLoss: 3.614\n","Epoch [96][0/1]\tLoss: 3.584\n","Epoch [97][0/1]\tLoss: 3.576\n","Epoch [98][0/1]\tLoss: 3.550\n","Epoch [99][0/1]\tLoss: 3.543\n","Model saved at epoch: 99 name \n","Epoch [100][0/1]\tLoss: 3.540\n","Epoch [101][0/1]\tLoss: 3.512\n","Epoch [102][0/1]\tLoss: 3.499\n","Epoch [103][0/1]\tLoss: 3.480\n","Epoch [104][0/1]\tLoss: 3.466\n","Epoch [105][0/1]\tLoss: 3.466\n","Epoch [106][0/1]\tLoss: 3.437\n","Epoch [107][0/1]\tLoss: 3.411\n","Epoch [108][0/1]\tLoss: 3.388\n","Epoch [109][0/1]\tLoss: 3.381\n","Epoch [110][0/1]\tLoss: 3.372\n","Epoch [111][0/1]\tLoss: 3.357\n","Epoch [112][0/1]\tLoss: 3.330\n","Epoch [113][0/1]\tLoss: 3.323\n","Epoch [114][0/1]\tLoss: 3.306\n","Epoch [115][0/1]\tLoss: 3.279\n","Epoch [116][0/1]\tLoss: 3.267\n","Epoch [117][0/1]\tLoss: 3.244\n","Epoch [118][0/1]\tLoss: 3.228\n","Epoch [119][0/1]\tLoss: 3.217\n","Epoch [120][0/1]\tLoss: 3.193\n","Epoch [121][0/1]\tLoss: 3.172\n","Epoch [122][0/1]\tLoss: 3.186\n","Epoch [123][0/1]\tLoss: 3.164\n","Epoch [124][0/1]\tLoss: 3.149\n","Epoch [125][0/1]\tLoss: 3.131\n","Epoch [126][0/1]\tLoss: 3.104\n","Epoch [127][0/1]\tLoss: 3.084\n","Epoch [128][0/1]\tLoss: 3.075\n","Epoch [129][0/1]\tLoss: 3.065\n","Epoch [130][0/1]\tLoss: 3.055\n","Epoch [131][0/1]\tLoss: 3.043\n","Epoch [132][0/1]\tLoss: 3.031\n","Epoch [133][0/1]\tLoss: 3.001\n","Epoch [134][0/1]\tLoss: 2.993\n","Epoch [135][0/1]\tLoss: 2.984\n","Epoch [136][0/1]\tLoss: 2.983\n","Epoch [137][0/1]\tLoss: 2.951\n","Epoch [138][0/1]\tLoss: 2.944\n","Epoch [139][0/1]\tLoss: 2.936\n","Epoch [140][0/1]\tLoss: 2.919\n","Epoch [141][0/1]\tLoss: 2.900\n","Epoch [142][0/1]\tLoss: 2.886\n","Epoch [143][0/1]\tLoss: 2.871\n","Epoch [144][0/1]\tLoss: 2.868\n","Epoch [145][0/1]\tLoss: 2.856\n","Epoch [146][0/1]\tLoss: 2.850\n","Epoch [147][0/1]\tLoss: 2.831\n","Epoch [148][0/1]\tLoss: 2.830\n","Epoch [149][0/1]\tLoss: 2.805\n","Epoch [150][0/1]\tLoss: 2.801\n","Epoch [151][0/1]\tLoss: 2.782\n","Epoch [152][0/1]\tLoss: 2.782\n","Epoch [153][0/1]\tLoss: 2.765\n","Epoch [154][0/1]\tLoss: 2.760\n","Epoch [155][0/1]\tLoss: 2.750\n","Epoch [156][0/1]\tLoss: 2.745\n","Epoch [157][0/1]\tLoss: 2.717\n","Epoch [158][0/1]\tLoss: 2.719\n","Epoch [159][0/1]\tLoss: 2.696\n","Epoch [160][0/1]\tLoss: 2.701\n","Epoch [161][0/1]\tLoss: 2.677\n","Epoch [162][0/1]\tLoss: 2.665\n","Epoch [163][0/1]\tLoss: 2.660\n","Epoch [164][0/1]\tLoss: 2.654\n","Epoch [165][0/1]\tLoss: 2.637\n","Epoch [166][0/1]\tLoss: 2.636\n","Epoch [167][0/1]\tLoss: 2.619\n","Epoch [168][0/1]\tLoss: 2.603\n","Epoch [169][0/1]\tLoss: 2.596\n","Epoch [170][0/1]\tLoss: 2.577\n","Epoch [171][0/1]\tLoss: 2.580\n","Epoch [172][0/1]\tLoss: 2.558\n","Epoch [173][0/1]\tLoss: 2.577\n","Epoch [174][0/1]\tLoss: 2.555\n","Epoch [175][0/1]\tLoss: 2.533\n","Epoch [176][0/1]\tLoss: 2.531\n","Epoch [177][0/1]\tLoss: 2.514\n","Epoch [178][0/1]\tLoss: 2.483\n","Epoch [179][0/1]\tLoss: 2.489\n","Epoch [180][0/1]\tLoss: 2.495\n","Epoch [181][0/1]\tLoss: 2.472\n","Epoch [182][0/1]\tLoss: 2.465\n","Epoch [183][0/1]\tLoss: 2.454\n","Epoch [184][0/1]\tLoss: 2.446\n","Epoch [185][0/1]\tLoss: 2.430\n","Epoch [186][0/1]\tLoss: 2.425\n","Epoch [187][0/1]\tLoss: 2.412\n","Epoch [188][0/1]\tLoss: 2.414\n","Epoch [189][0/1]\tLoss: 2.397\n","Epoch [190][0/1]\tLoss: 2.383\n","Epoch [191][0/1]\tLoss: 2.370\n","Epoch [192][0/1]\tLoss: 2.365\n","Epoch [193][0/1]\tLoss: 2.366\n","Epoch [194][0/1]\tLoss: 2.353\n","Epoch [195][0/1]\tLoss: 2.353\n","Epoch [196][0/1]\tLoss: 2.317\n","Epoch [197][0/1]\tLoss: 2.311\n","Epoch [198][0/1]\tLoss: 2.322\n","Epoch [199][0/1]\tLoss: 2.282\n","Model saved at epoch: 199 name \n","Epoch [200][0/1]\tLoss: 2.284\n","Epoch [201][0/1]\tLoss: 2.277\n","Epoch [202][0/1]\tLoss: 2.261\n","Epoch [203][0/1]\tLoss: 2.267\n","Epoch [204][0/1]\tLoss: 2.242\n","Epoch [205][0/1]\tLoss: 2.238\n","Epoch [206][0/1]\tLoss: 2.242\n","Epoch [207][0/1]\tLoss: 2.220\n","Epoch [208][0/1]\tLoss: 2.190\n","Epoch [209][0/1]\tLoss: 2.221\n","Epoch [210][0/1]\tLoss: 2.220\n","Epoch [211][0/1]\tLoss: 2.183\n","Epoch [212][0/1]\tLoss: 2.199\n","Epoch [213][0/1]\tLoss: 2.176\n","Epoch [214][0/1]\tLoss: 2.156\n","Epoch [215][0/1]\tLoss: 2.137\n","Epoch [216][0/1]\tLoss: 2.140\n","Epoch [217][0/1]\tLoss: 2.138\n","Epoch [218][0/1]\tLoss: 2.125\n","Epoch [219][0/1]\tLoss: 2.124\n","Epoch [220][0/1]\tLoss: 2.098\n","Epoch [221][0/1]\tLoss: 2.108\n","Epoch [222][0/1]\tLoss: 2.078\n","Epoch [223][0/1]\tLoss: 2.099\n","Epoch [224][0/1]\tLoss: 2.071\n","Epoch [225][0/1]\tLoss: 2.047\n","Epoch [226][0/1]\tLoss: 2.024\n","Epoch [227][0/1]\tLoss: 2.039\n","Epoch [228][0/1]\tLoss: 2.025\n","Epoch [229][0/1]\tLoss: 2.016\n","Epoch [230][0/1]\tLoss: 2.013\n","Epoch [231][0/1]\tLoss: 1.989\n","Epoch [232][0/1]\tLoss: 1.976\n","Epoch [233][0/1]\tLoss: 1.973\n","Epoch [234][0/1]\tLoss: 1.964\n","Epoch [235][0/1]\tLoss: 1.954\n","Epoch [236][0/1]\tLoss: 1.975\n","Epoch [237][0/1]\tLoss: 1.935\n","Epoch [238][0/1]\tLoss: 1.932\n","Epoch [239][0/1]\tLoss: 1.931\n","Epoch [240][0/1]\tLoss: 1.937\n","Epoch [241][0/1]\tLoss: 1.912\n","Epoch [242][0/1]\tLoss: 1.884\n","Epoch [243][0/1]\tLoss: 1.887\n","Epoch [244][0/1]\tLoss: 1.873\n","Epoch [245][0/1]\tLoss: 1.860\n","Epoch [246][0/1]\tLoss: 1.903\n","Epoch [247][0/1]\tLoss: 1.865\n","Epoch [248][0/1]\tLoss: 1.846\n","Epoch [249][0/1]\tLoss: 1.881\n","Epoch [250][0/1]\tLoss: 1.846\n","Epoch [251][0/1]\tLoss: 1.820\n","Epoch [252][0/1]\tLoss: 1.799\n","Epoch [253][0/1]\tLoss: 1.793\n","Epoch [254][0/1]\tLoss: 1.802\n","Epoch [255][0/1]\tLoss: 1.803\n","Epoch [256][0/1]\tLoss: 1.778\n","Epoch [257][0/1]\tLoss: 1.784\n","Epoch [258][0/1]\tLoss: 1.757\n","Epoch [259][0/1]\tLoss: 1.740\n","Epoch [260][0/1]\tLoss: 1.742\n","Epoch [261][0/1]\tLoss: 1.724\n","Epoch [262][0/1]\tLoss: 1.715\n","Epoch [263][0/1]\tLoss: 1.701\n","Epoch [264][0/1]\tLoss: 1.709\n","Epoch [265][0/1]\tLoss: 1.684\n","Epoch [266][0/1]\tLoss: 1.678\n","Epoch [267][0/1]\tLoss: 1.659\n","Epoch [268][0/1]\tLoss: 1.635\n","Epoch [269][0/1]\tLoss: 1.650\n","Epoch [270][0/1]\tLoss: 1.641\n","Epoch [271][0/1]\tLoss: 1.632\n","Epoch [272][0/1]\tLoss: 1.607\n","Epoch [273][0/1]\tLoss: 1.608\n","Epoch [274][0/1]\tLoss: 1.618\n","Epoch [275][0/1]\tLoss: 1.611\n","Epoch [276][0/1]\tLoss: 1.562\n","Epoch [277][0/1]\tLoss: 1.609\n","Epoch [278][0/1]\tLoss: 1.637\n","Epoch [279][0/1]\tLoss: 1.592\n","Epoch [280][0/1]\tLoss: 1.585\n","Epoch [281][0/1]\tLoss: 1.614\n","Epoch [282][0/1]\tLoss: 1.538\n","Epoch [283][0/1]\tLoss: 1.543\n","Epoch [284][0/1]\tLoss: 1.502\n","Epoch [285][0/1]\tLoss: 1.525\n","Epoch [286][0/1]\tLoss: 1.501\n","Epoch [287][0/1]\tLoss: 1.493\n","Epoch [288][0/1]\tLoss: 1.510\n","Epoch [289][0/1]\tLoss: 1.485\n","Epoch [290][0/1]\tLoss: 1.483\n","Epoch [291][0/1]\tLoss: 1.480\n","Epoch [292][0/1]\tLoss: 1.486\n","Epoch [293][0/1]\tLoss: 1.454\n","Epoch [294][0/1]\tLoss: 1.485\n","Epoch [295][0/1]\tLoss: 1.441\n","Epoch [296][0/1]\tLoss: 1.435\n","Epoch [297][0/1]\tLoss: 1.443\n","Epoch [298][0/1]\tLoss: 1.437\n","Epoch [299][0/1]\tLoss: 1.418\n","Model saved at epoch: 299 name \n","Epoch [300][0/1]\tLoss: 1.405\n","Epoch [301][0/1]\tLoss: 1.392\n","Epoch [302][0/1]\tLoss: 1.407\n","Epoch [303][0/1]\tLoss: 1.409\n","Epoch [304][0/1]\tLoss: 1.360\n","Epoch [305][0/1]\tLoss: 1.385\n","Epoch [306][0/1]\tLoss: 1.337\n","Epoch [307][0/1]\tLoss: 1.353\n","Epoch [308][0/1]\tLoss: 1.339\n","Epoch [309][0/1]\tLoss: 1.302\n","Epoch [310][0/1]\tLoss: 1.333\n","Epoch [311][0/1]\tLoss: 1.325\n","Epoch [312][0/1]\tLoss: 1.285\n","Epoch [313][0/1]\tLoss: 1.334\n","Epoch [314][0/1]\tLoss: 1.281\n","Epoch [315][0/1]\tLoss: 1.305\n","Epoch [316][0/1]\tLoss: 1.292\n","Epoch [317][0/1]\tLoss: 1.274\n","Epoch [318][0/1]\tLoss: 1.271\n","Epoch [319][0/1]\tLoss: 1.256\n","Epoch [320][0/1]\tLoss: 1.252\n","Epoch [321][0/1]\tLoss: 1.277\n","Epoch [322][0/1]\tLoss: 1.220\n","Epoch [323][0/1]\tLoss: 1.270\n","Epoch [324][0/1]\tLoss: 1.256\n","Epoch [325][0/1]\tLoss: 1.245\n","Epoch [326][0/1]\tLoss: 1.229\n","Epoch [327][0/1]\tLoss: 1.193\n","Epoch [328][0/1]\tLoss: 1.215\n","Epoch [329][0/1]\tLoss: 1.184\n","Epoch [330][0/1]\tLoss: 1.219\n","Epoch [331][0/1]\tLoss: 1.221\n","Epoch [332][0/1]\tLoss: 1.170\n","Epoch [333][0/1]\tLoss: 1.215\n","Epoch [334][0/1]\tLoss: 1.136\n","Epoch [335][0/1]\tLoss: 1.152\n","Epoch [336][0/1]\tLoss: 1.135\n","Epoch [337][0/1]\tLoss: 1.120\n","Epoch [338][0/1]\tLoss: 1.109\n","Epoch [339][0/1]\tLoss: 1.129\n","Epoch [340][0/1]\tLoss: 1.097\n","Epoch [341][0/1]\tLoss: 1.112\n","Epoch [342][0/1]\tLoss: 1.076\n","Epoch [343][0/1]\tLoss: 1.074\n","Epoch [344][0/1]\tLoss: 1.099\n","Epoch [345][0/1]\tLoss: 1.064\n","Epoch [346][0/1]\tLoss: 1.073\n","Epoch [347][0/1]\tLoss: 1.070\n","Epoch [348][0/1]\tLoss: 1.079\n","Epoch [349][0/1]\tLoss: 1.124\n","Epoch [350][0/1]\tLoss: 1.070\n","Epoch [351][0/1]\tLoss: 1.062\n","Epoch [352][0/1]\tLoss: 1.076\n","Epoch [353][0/1]\tLoss: 1.099\n","Epoch [354][0/1]\tLoss: 1.016\n","Epoch [355][0/1]\tLoss: 1.046\n","Epoch [356][0/1]\tLoss: 0.989\n","Epoch [357][0/1]\tLoss: 1.027\n","Epoch [358][0/1]\tLoss: 1.004\n","Epoch [359][0/1]\tLoss: 1.018\n","Epoch [360][0/1]\tLoss: 0.984\n","Epoch [361][0/1]\tLoss: 0.978\n","Epoch [362][0/1]\tLoss: 1.010\n","Epoch [363][0/1]\tLoss: 0.957\n","Epoch [364][0/1]\tLoss: 0.951\n","Epoch [365][0/1]\tLoss: 0.945\n","Epoch [366][0/1]\tLoss: 0.957\n","Epoch [367][0/1]\tLoss: 0.924\n","Epoch [368][0/1]\tLoss: 0.936\n","Epoch [369][0/1]\tLoss: 0.940\n","Epoch [370][0/1]\tLoss: 0.923\n","Epoch [371][0/1]\tLoss: 0.899\n","Epoch [372][0/1]\tLoss: 0.917\n","Epoch [373][0/1]\tLoss: 0.908\n","Epoch [374][0/1]\tLoss: 0.900\n","Epoch [375][0/1]\tLoss: 0.880\n","Epoch [376][0/1]\tLoss: 0.875\n","Epoch [377][0/1]\tLoss: 0.868\n","Epoch [378][0/1]\tLoss: 0.873\n","Epoch [379][0/1]\tLoss: 0.840\n","Epoch [380][0/1]\tLoss: 0.852\n","Epoch [381][0/1]\tLoss: 0.848\n","Epoch [382][0/1]\tLoss: 0.854\n","Epoch [383][0/1]\tLoss: 0.847\n","Epoch [384][0/1]\tLoss: 0.868\n","Epoch [385][0/1]\tLoss: 0.835\n","Epoch [386][0/1]\tLoss: 0.856\n","Epoch [387][0/1]\tLoss: 0.821\n","Epoch [388][0/1]\tLoss: 0.830\n","Epoch [389][0/1]\tLoss: 0.832\n","Epoch [390][0/1]\tLoss: 0.814\n","Epoch [391][0/1]\tLoss: 0.813\n","Epoch [392][0/1]\tLoss: 0.827\n","Epoch [393][0/1]\tLoss: 0.785\n","Epoch [394][0/1]\tLoss: 0.820\n","Epoch [395][0/1]\tLoss: 0.799\n","Epoch [396][0/1]\tLoss: 0.770\n","Epoch [397][0/1]\tLoss: 0.775\n","Epoch [398][0/1]\tLoss: 0.752\n","Epoch [399][0/1]\tLoss: 0.770\n","Model saved at epoch: 399 name \n","Epoch [400][0/1]\tLoss: 0.754\n","Epoch [401][0/1]\tLoss: 0.742\n","Epoch [402][0/1]\tLoss: 0.748\n","Epoch [403][0/1]\tLoss: 0.744\n","Epoch [404][0/1]\tLoss: 0.741\n","Epoch [405][0/1]\tLoss: 0.740\n","Epoch [406][0/1]\tLoss: 0.752\n","Epoch [407][0/1]\tLoss: 0.719\n","Epoch [408][0/1]\tLoss: 0.711\n","Epoch [409][0/1]\tLoss: 0.717\n","Epoch [410][0/1]\tLoss: 0.701\n","Epoch [411][0/1]\tLoss: 0.710\n","Epoch [412][0/1]\tLoss: 0.691\n","Epoch [413][0/1]\tLoss: 0.690\n","Epoch [414][0/1]\tLoss: 0.718\n","Epoch [415][0/1]\tLoss: 0.718\n","Epoch [416][0/1]\tLoss: 0.720\n","Epoch [417][0/1]\tLoss: 0.671\n","Epoch [418][0/1]\tLoss: 0.697\n","Epoch [419][0/1]\tLoss: 0.681\n","Epoch [420][0/1]\tLoss: 0.664\n","Epoch [421][0/1]\tLoss: 0.656\n","Epoch [422][0/1]\tLoss: 0.653\n","Epoch [423][0/1]\tLoss: 0.650\n","Epoch [424][0/1]\tLoss: 0.633\n","Epoch [425][0/1]\tLoss: 0.656\n","Epoch [426][0/1]\tLoss: 0.636\n","Epoch [427][0/1]\tLoss: 0.642\n","Epoch [428][0/1]\tLoss: 0.650\n","Epoch [429][0/1]\tLoss: 0.625\n","Epoch [430][0/1]\tLoss: 0.619\n","Epoch [431][0/1]\tLoss: 0.616\n","Epoch [432][0/1]\tLoss: 0.595\n","Epoch [433][0/1]\tLoss: 0.618\n","Epoch [434][0/1]\tLoss: 0.592\n","Epoch [435][0/1]\tLoss: 0.597\n","Epoch [436][0/1]\tLoss: 0.592\n","Epoch [437][0/1]\tLoss: 0.592\n","Epoch [438][0/1]\tLoss: 0.592\n","Epoch [439][0/1]\tLoss: 0.582\n","Epoch [440][0/1]\tLoss: 0.576\n","Epoch [441][0/1]\tLoss: 0.597\n","Epoch [442][0/1]\tLoss: 0.602\n","Epoch [443][0/1]\tLoss: 0.580\n","Epoch [444][0/1]\tLoss: 0.569\n","Epoch [445][0/1]\tLoss: 0.555\n","Epoch [446][0/1]\tLoss: 0.575\n","Epoch [447][0/1]\tLoss: 0.548\n","Epoch [448][0/1]\tLoss: 0.552\n","Epoch [449][0/1]\tLoss: 0.569\n","Epoch [450][0/1]\tLoss: 0.552\n","Epoch [451][0/1]\tLoss: 0.538\n","Epoch [452][0/1]\tLoss: 0.603\n","Epoch [453][0/1]\tLoss: 0.611\n","Epoch [454][0/1]\tLoss: 0.540\n","Epoch [455][0/1]\tLoss: 0.599\n","Epoch [456][0/1]\tLoss: 0.530\n","Epoch [457][0/1]\tLoss: 0.560\n","Epoch [458][0/1]\tLoss: 0.531\n","Epoch [459][0/1]\tLoss: 0.538\n","Epoch [460][0/1]\tLoss: 0.539\n","Epoch [461][0/1]\tLoss: 0.532\n","Epoch [462][0/1]\tLoss: 0.527\n","Epoch [463][0/1]\tLoss: 0.513\n","Epoch [464][0/1]\tLoss: 0.515\n","Epoch [465][0/1]\tLoss: 0.504\n","Epoch [466][0/1]\tLoss: 0.493\n","Epoch [467][0/1]\tLoss: 0.508\n","Epoch [468][0/1]\tLoss: 0.496\n","Epoch [469][0/1]\tLoss: 0.493\n","Epoch [470][0/1]\tLoss: 0.471\n","Epoch [471][0/1]\tLoss: 0.466\n","Epoch [472][0/1]\tLoss: 0.485\n","Epoch [473][0/1]\tLoss: 0.462\n","Epoch [474][0/1]\tLoss: 0.471\n","Epoch [475][0/1]\tLoss: 0.477\n","Epoch [476][0/1]\tLoss: 0.467\n","Epoch [477][0/1]\tLoss: 0.449\n","Epoch [478][0/1]\tLoss: 0.450\n","Epoch [479][0/1]\tLoss: 0.460\n","Epoch [480][0/1]\tLoss: 0.435\n","Epoch [481][0/1]\tLoss: 0.433\n","Epoch [482][0/1]\tLoss: 0.430\n","Epoch [483][0/1]\tLoss: 0.426\n","Epoch [484][0/1]\tLoss: 0.424\n","Epoch [485][0/1]\tLoss: 0.414\n","Epoch [486][0/1]\tLoss: 0.417\n","Epoch [487][0/1]\tLoss: 0.410\n","Epoch [488][0/1]\tLoss: 0.412\n","Epoch [489][0/1]\tLoss: 0.403\n","Epoch [490][0/1]\tLoss: 0.412\n","Epoch [491][0/1]\tLoss: 0.390\n","Epoch [492][0/1]\tLoss: 0.413\n","Epoch [493][0/1]\tLoss: 0.397\n","Epoch [494][0/1]\tLoss: 0.397\n","Epoch [495][0/1]\tLoss: 0.387\n","Epoch [496][0/1]\tLoss: 0.389\n","Epoch [497][0/1]\tLoss: 0.391\n","Epoch [498][0/1]\tLoss: 0.387\n","Epoch [499][0/1]\tLoss: 0.382\n","Model saved at epoch: 499 name \n","Epoch [500][0/1]\tLoss: 0.387\n","Epoch [501][0/1]\tLoss: 0.373\n","Epoch [502][0/1]\tLoss: 0.378\n","Epoch [503][0/1]\tLoss: 0.370\n","Epoch [504][0/1]\tLoss: 0.369\n","Epoch [505][0/1]\tLoss: 0.365\n","Epoch [506][0/1]\tLoss: 0.366\n","Epoch [507][0/1]\tLoss: 0.368\n","Epoch [508][0/1]\tLoss: 0.367\n","Epoch [509][0/1]\tLoss: 0.362\n","Epoch [510][0/1]\tLoss: 0.356\n","Epoch [511][0/1]\tLoss: 0.352\n","Epoch [512][0/1]\tLoss: 0.357\n","Epoch [513][0/1]\tLoss: 0.346\n","Epoch [514][0/1]\tLoss: 0.343\n","Epoch [515][0/1]\tLoss: 0.351\n","Epoch [516][0/1]\tLoss: 0.350\n","Epoch [517][0/1]\tLoss: 0.336\n","Epoch [518][0/1]\tLoss: 0.347\n","Epoch [519][0/1]\tLoss: 0.330\n","Epoch [520][0/1]\tLoss: 0.327\n","Epoch [521][0/1]\tLoss: 0.328\n","Epoch [522][0/1]\tLoss: 0.322\n","Epoch [523][0/1]\tLoss: 0.324\n","Epoch [524][0/1]\tLoss: 0.329\n","Epoch [525][0/1]\tLoss: 0.334\n","Epoch [526][0/1]\tLoss: 0.322\n","Epoch [527][0/1]\tLoss: 0.330\n","Epoch [528][0/1]\tLoss: 0.316\n","Epoch [529][0/1]\tLoss: 0.325\n","Epoch [530][0/1]\tLoss: 0.315\n","Epoch [531][0/1]\tLoss: 0.305\n","Epoch [532][0/1]\tLoss: 0.323\n","Epoch [533][0/1]\tLoss: 0.310\n","Epoch [534][0/1]\tLoss: 0.306\n","Epoch [535][0/1]\tLoss: 0.307\n","Epoch [536][0/1]\tLoss: 0.302\n","Epoch [537][0/1]\tLoss: 0.307\n","Epoch [538][0/1]\tLoss: 0.315\n","Epoch [539][0/1]\tLoss: 0.298\n","Epoch [540][0/1]\tLoss: 0.311\n","Epoch [541][0/1]\tLoss: 0.317\n","Epoch [542][0/1]\tLoss: 0.296\n","Epoch [543][0/1]\tLoss: 0.305\n","Epoch [544][0/1]\tLoss: 0.303\n","Epoch [545][0/1]\tLoss: 0.299\n","Epoch [546][0/1]\tLoss: 0.299\n","Epoch [547][0/1]\tLoss: 0.290\n","Epoch [548][0/1]\tLoss: 0.299\n","Epoch [549][0/1]\tLoss: 0.280\n","Epoch [550][0/1]\tLoss: 0.291\n","Epoch [551][0/1]\tLoss: 0.285\n","Epoch [552][0/1]\tLoss: 0.281\n","Epoch [553][0/1]\tLoss: 0.279\n","Epoch [554][0/1]\tLoss: 0.269\n","Epoch [555][0/1]\tLoss: 0.276\n","Epoch [556][0/1]\tLoss: 0.279\n","Epoch [557][0/1]\tLoss: 0.283\n","Epoch [558][0/1]\tLoss: 0.263\n","Epoch [559][0/1]\tLoss: 0.281\n","Epoch [560][0/1]\tLoss: 0.273\n","Epoch [561][0/1]\tLoss: 0.262\n","Epoch [562][0/1]\tLoss: 0.273\n","Epoch [563][0/1]\tLoss: 0.277\n","Epoch [564][0/1]\tLoss: 0.259\n","Epoch [565][0/1]\tLoss: 0.281\n","Epoch [566][0/1]\tLoss: 0.251\n","Epoch [567][0/1]\tLoss: 0.256\n","Epoch [568][0/1]\tLoss: 0.279\n","Epoch [569][0/1]\tLoss: 0.270\n","Epoch [570][0/1]\tLoss: 0.255\n","Epoch [571][0/1]\tLoss: 0.273\n","Epoch [572][0/1]\tLoss: 0.253\n","Epoch [573][0/1]\tLoss: 0.262\n","Epoch [574][0/1]\tLoss: 0.262\n","Epoch [575][0/1]\tLoss: 0.264\n","Epoch [576][0/1]\tLoss: 0.251\n","Epoch [577][0/1]\tLoss: 0.247\n","Epoch [578][0/1]\tLoss: 0.254\n","Epoch [579][0/1]\tLoss: 0.253\n","Epoch [580][0/1]\tLoss: 0.241\n","Epoch [581][0/1]\tLoss: 0.240\n","Epoch [582][0/1]\tLoss: 0.244\n","Epoch [583][0/1]\tLoss: 0.240\n","Epoch [584][0/1]\tLoss: 0.240\n","Epoch [585][0/1]\tLoss: 0.234\n","Epoch [586][0/1]\tLoss: 0.240\n","Epoch [587][0/1]\tLoss: 0.233\n","Epoch [588][0/1]\tLoss: 0.235\n","Epoch [589][0/1]\tLoss: 0.238\n","Epoch [590][0/1]\tLoss: 0.230\n","Epoch [591][0/1]\tLoss: 0.231\n","Epoch [592][0/1]\tLoss: 0.230\n","Epoch [593][0/1]\tLoss: 0.229\n","Epoch [594][0/1]\tLoss: 0.224\n","Epoch [595][0/1]\tLoss: 0.223\n","Epoch [596][0/1]\tLoss: 0.226\n","Epoch [597][0/1]\tLoss: 0.226\n","Epoch [598][0/1]\tLoss: 0.236\n","Epoch [599][0/1]\tLoss: 0.218\n","Model saved at epoch: 599 name \n","Epoch [600][0/1]\tLoss: 0.233\n","Epoch [601][0/1]\tLoss: 0.216\n","Epoch [602][0/1]\tLoss: 0.224\n","Epoch [603][0/1]\tLoss: 0.220\n","Epoch [604][0/1]\tLoss: 0.220\n","Epoch [605][0/1]\tLoss: 0.206\n","Epoch [606][0/1]\tLoss: 0.230\n","Epoch [607][0/1]\tLoss: 0.234\n","Epoch [608][0/1]\tLoss: 0.231\n","Epoch [609][0/1]\tLoss: 0.218\n","Epoch [610][0/1]\tLoss: 0.212\n","Epoch [611][0/1]\tLoss: 0.224\n","Epoch [612][0/1]\tLoss: 0.211\n","Epoch [613][0/1]\tLoss: 0.221\n","Epoch [614][0/1]\tLoss: 0.235\n","Epoch [615][0/1]\tLoss: 0.210\n","Epoch [616][0/1]\tLoss: 0.219\n","Epoch [617][0/1]\tLoss: 0.209\n","Epoch [618][0/1]\tLoss: 0.208\n","Epoch [619][0/1]\tLoss: 0.212\n","Epoch [620][0/1]\tLoss: 0.212\n","Epoch [621][0/1]\tLoss: 0.204\n","Epoch [622][0/1]\tLoss: 0.201\n","Epoch [623][0/1]\tLoss: 0.200\n","Epoch [624][0/1]\tLoss: 0.202\n","Epoch [625][0/1]\tLoss: 0.197\n","Epoch [626][0/1]\tLoss: 0.201\n","Epoch [627][0/1]\tLoss: 0.189\n","Epoch [628][0/1]\tLoss: 0.189\n","Epoch [629][0/1]\tLoss: 0.190\n","Epoch [630][0/1]\tLoss: 0.194\n","Epoch [631][0/1]\tLoss: 0.190\n","Epoch [632][0/1]\tLoss: 0.192\n","Epoch [633][0/1]\tLoss: 0.185\n","Epoch [634][0/1]\tLoss: 0.200\n","Epoch [635][0/1]\tLoss: 0.185\n","Epoch [636][0/1]\tLoss: 0.181\n","Epoch [637][0/1]\tLoss: 0.185\n","Epoch [638][0/1]\tLoss: 0.177\n","Epoch [639][0/1]\tLoss: 0.193\n","Epoch [640][0/1]\tLoss: 0.186\n","Epoch [641][0/1]\tLoss: 0.182\n","Epoch [642][0/1]\tLoss: 0.198\n","Epoch [643][0/1]\tLoss: 0.187\n","Epoch [644][0/1]\tLoss: 0.197\n","Epoch [645][0/1]\tLoss: 0.206\n","Epoch [646][0/1]\tLoss: 0.200\n","Epoch [647][0/1]\tLoss: 0.201\n","Epoch [648][0/1]\tLoss: 0.203\n","Epoch [649][0/1]\tLoss: 0.216\n","Epoch [650][0/1]\tLoss: 0.215\n","Epoch [651][0/1]\tLoss: 0.203\n","Epoch [652][0/1]\tLoss: 0.208\n","Epoch [653][0/1]\tLoss: 0.191\n","Epoch [654][0/1]\tLoss: 0.194\n","Epoch [655][0/1]\tLoss: 0.198\n","Epoch [656][0/1]\tLoss: 0.187\n","Epoch [657][0/1]\tLoss: 0.188\n","Epoch [658][0/1]\tLoss: 0.187\n","Epoch [659][0/1]\tLoss: 0.182\n","Epoch [660][0/1]\tLoss: 0.179\n","Epoch [661][0/1]\tLoss: 0.180\n","Epoch [662][0/1]\tLoss: 0.178\n","Epoch [663][0/1]\tLoss: 0.175\n","Epoch [664][0/1]\tLoss: 0.186\n","Epoch [665][0/1]\tLoss: 0.166\n","Epoch [666][0/1]\tLoss: 0.185\n","Epoch [667][0/1]\tLoss: 0.171\n","Epoch [668][0/1]\tLoss: 0.170\n","Epoch [669][0/1]\tLoss: 0.174\n","Epoch [670][0/1]\tLoss: 0.171\n","Epoch [671][0/1]\tLoss: 0.161\n","Epoch [672][0/1]\tLoss: 0.158\n","Epoch [673][0/1]\tLoss: 0.168\n","Epoch [674][0/1]\tLoss: 0.159\n","Epoch [675][0/1]\tLoss: 0.156\n","Epoch [676][0/1]\tLoss: 0.166\n","Epoch [677][0/1]\tLoss: 0.163\n","Epoch [678][0/1]\tLoss: 0.155\n","Epoch [679][0/1]\tLoss: 0.160\n","Epoch [680][0/1]\tLoss: 0.152\n","Epoch [681][0/1]\tLoss: 0.151\n","Epoch [682][0/1]\tLoss: 0.144\n","Epoch [683][0/1]\tLoss: 0.148\n","Epoch [684][0/1]\tLoss: 0.154\n","Epoch [685][0/1]\tLoss: 0.143\n","Epoch [686][0/1]\tLoss: 0.157\n","Epoch [687][0/1]\tLoss: 0.150\n","Epoch [688][0/1]\tLoss: 0.155\n","Epoch [689][0/1]\tLoss: 0.158\n","Epoch [690][0/1]\tLoss: 0.149\n","Epoch [691][0/1]\tLoss: 0.158\n","Epoch [692][0/1]\tLoss: 0.142\n","Epoch [693][0/1]\tLoss: 0.168\n","Epoch [694][0/1]\tLoss: 0.175\n","Epoch [695][0/1]\tLoss: 0.202\n","Epoch [696][0/1]\tLoss: 0.288\n","Epoch [697][0/1]\tLoss: 0.706\n","Epoch [698][0/1]\tLoss: 0.707\n","Epoch [699][0/1]\tLoss: 0.586\n","Model saved at epoch: 699 name \n","Epoch [700][0/1]\tLoss: 0.570\n","Epoch [701][0/1]\tLoss: 0.494\n","Epoch [702][0/1]\tLoss: 0.518\n","Epoch [703][0/1]\tLoss: 0.413\n","Epoch [704][0/1]\tLoss: 0.359\n","Epoch [705][0/1]\tLoss: 0.359\n","Epoch [706][0/1]\tLoss: 0.300\n","Epoch [707][0/1]\tLoss: 0.294\n","Epoch [708][0/1]\tLoss: 0.298\n","Epoch [709][0/1]\tLoss: 0.262\n","Epoch [710][0/1]\tLoss: 0.249\n","Epoch [711][0/1]\tLoss: 0.246\n","Epoch [712][0/1]\tLoss: 0.250\n","Epoch [713][0/1]\tLoss: 0.234\n","Epoch [714][0/1]\tLoss: 0.223\n","Epoch [715][0/1]\tLoss: 0.216\n","Epoch [716][0/1]\tLoss: 0.220\n","Epoch [717][0/1]\tLoss: 0.211\n","Epoch [718][0/1]\tLoss: 0.200\n","Epoch [719][0/1]\tLoss: 0.192\n","Epoch [720][0/1]\tLoss: 0.201\n","Epoch [721][0/1]\tLoss: 0.188\n","Epoch [722][0/1]\tLoss: 0.185\n","Epoch [723][0/1]\tLoss: 0.175\n","Epoch [724][0/1]\tLoss: 0.180\n","Epoch [725][0/1]\tLoss: 0.170\n","Epoch [726][0/1]\tLoss: 0.171\n","Epoch [727][0/1]\tLoss: 0.170\n","Epoch [728][0/1]\tLoss: 0.163\n","Epoch [729][0/1]\tLoss: 0.159\n","Epoch [730][0/1]\tLoss: 0.162\n","Epoch [731][0/1]\tLoss: 0.151\n","Epoch [732][0/1]\tLoss: 0.158\n","Epoch [733][0/1]\tLoss: 0.150\n","Epoch [734][0/1]\tLoss: 0.156\n","Epoch [735][0/1]\tLoss: 0.142\n","Epoch [736][0/1]\tLoss: 0.152\n","Epoch [737][0/1]\tLoss: 0.147\n","Epoch [738][0/1]\tLoss: 0.156\n","Epoch [739][0/1]\tLoss: 0.153\n","Epoch [740][0/1]\tLoss: 0.141\n","Epoch [741][0/1]\tLoss: 0.145\n","Epoch [742][0/1]\tLoss: 0.145\n","Epoch [743][0/1]\tLoss: 0.138\n","Epoch [744][0/1]\tLoss: 0.140\n","Epoch [745][0/1]\tLoss: 0.142\n","Epoch [746][0/1]\tLoss: 0.133\n","Epoch [747][0/1]\tLoss: 0.135\n","Epoch [748][0/1]\tLoss: 0.131\n","Epoch [749][0/1]\tLoss: 0.134\n","Epoch [750][0/1]\tLoss: 0.130\n","Epoch [751][0/1]\tLoss: 0.127\n","Epoch [752][0/1]\tLoss: 0.126\n","Epoch [753][0/1]\tLoss: 0.125\n","Epoch [754][0/1]\tLoss: 0.125\n","Epoch [755][0/1]\tLoss: 0.123\n","Epoch [756][0/1]\tLoss: 0.122\n","Epoch [757][0/1]\tLoss: 0.121\n","Epoch [758][0/1]\tLoss: 0.120\n","Epoch [759][0/1]\tLoss: 0.123\n","Epoch [760][0/1]\tLoss: 0.125\n","Epoch [761][0/1]\tLoss: 0.117\n","Epoch [762][0/1]\tLoss: 0.116\n","Epoch [763][0/1]\tLoss: 0.120\n","Epoch [764][0/1]\tLoss: 0.121\n","Epoch [765][0/1]\tLoss: 0.115\n","Epoch [766][0/1]\tLoss: 0.120\n","Epoch [767][0/1]\tLoss: 0.116\n","Epoch [768][0/1]\tLoss: 0.123\n","Epoch [769][0/1]\tLoss: 0.114\n","Epoch [770][0/1]\tLoss: 0.118\n","Epoch [771][0/1]\tLoss: 0.115\n","Epoch [772][0/1]\tLoss: 0.113\n","Epoch [773][0/1]\tLoss: 0.117\n","Epoch [774][0/1]\tLoss: 0.113\n","Epoch [775][0/1]\tLoss: 0.116\n","Epoch [776][0/1]\tLoss: 0.109\n","Epoch [777][0/1]\tLoss: 0.116\n","Epoch [778][0/1]\tLoss: 0.115\n","Epoch [779][0/1]\tLoss: 0.111\n","Epoch [780][0/1]\tLoss: 0.112\n","Epoch [781][0/1]\tLoss: 0.116\n","Epoch [782][0/1]\tLoss: 0.109\n","Epoch [783][0/1]\tLoss: 0.112\n","Epoch [784][0/1]\tLoss: 0.111\n","Epoch [785][0/1]\tLoss: 0.108\n","Epoch [786][0/1]\tLoss: 0.111\n","Epoch [787][0/1]\tLoss: 0.108\n","Epoch [788][0/1]\tLoss: 0.106\n","Epoch [789][0/1]\tLoss: 0.114\n","Epoch [790][0/1]\tLoss: 0.107\n","Epoch [791][0/1]\tLoss: 0.128\n","Epoch [792][0/1]\tLoss: 0.108\n","Epoch [793][0/1]\tLoss: 0.158\n","Epoch [794][0/1]\tLoss: 0.126\n","Epoch [795][0/1]\tLoss: 0.152\n","Epoch [796][0/1]\tLoss: 0.134\n","Epoch [797][0/1]\tLoss: 0.168\n","Epoch [798][0/1]\tLoss: 0.145\n","Epoch [799][0/1]\tLoss: 0.132\n","Model saved at epoch: 799 name \n","Epoch [800][0/1]\tLoss: 0.149\n","Epoch [801][0/1]\tLoss: 0.133\n","Epoch [802][0/1]\tLoss: 0.124\n","Epoch [803][0/1]\tLoss: 0.134\n","Epoch [804][0/1]\tLoss: 0.138\n","Epoch [805][0/1]\tLoss: 0.129\n","Epoch [806][0/1]\tLoss: 0.136\n","Epoch [807][0/1]\tLoss: 0.146\n","Epoch [808][0/1]\tLoss: 0.135\n","Epoch [809][0/1]\tLoss: 0.127\n","Epoch [810][0/1]\tLoss: 0.131\n","Epoch [811][0/1]\tLoss: 0.121\n","Epoch [812][0/1]\tLoss: 0.124\n","Epoch [813][0/1]\tLoss: 0.120\n","Epoch [814][0/1]\tLoss: 0.126\n","Epoch [815][0/1]\tLoss: 0.115\n","Epoch [816][0/1]\tLoss: 0.122\n","Epoch [817][0/1]\tLoss: 0.118\n","Epoch [818][0/1]\tLoss: 0.118\n","Epoch [819][0/1]\tLoss: 0.108\n","Epoch [820][0/1]\tLoss: 0.118\n","Epoch [821][0/1]\tLoss: 0.109\n","Epoch [822][0/1]\tLoss: 0.110\n","Epoch [823][0/1]\tLoss: 0.112\n","Epoch [824][0/1]\tLoss: 0.101\n","Epoch [825][0/1]\tLoss: 0.104\n","Epoch [826][0/1]\tLoss: 0.106\n","Epoch [827][0/1]\tLoss: 0.103\n","Epoch [828][0/1]\tLoss: 0.101\n","Epoch [829][0/1]\tLoss: 0.102\n","Epoch [830][0/1]\tLoss: 0.104\n","Epoch [831][0/1]\tLoss: 0.100\n","Epoch [832][0/1]\tLoss: 0.099\n","Epoch [833][0/1]\tLoss: 0.101\n","Epoch [834][0/1]\tLoss: 0.098\n","Epoch [835][0/1]\tLoss: 0.103\n","Epoch [836][0/1]\tLoss: 0.101\n","Epoch [837][0/1]\tLoss: 0.100\n","Epoch [838][0/1]\tLoss: 0.101\n","Epoch [839][0/1]\tLoss: 0.100\n","Epoch [840][0/1]\tLoss: 0.100\n","Epoch [841][0/1]\tLoss: 0.097\n","Epoch [842][0/1]\tLoss: 0.107\n","Epoch [843][0/1]\tLoss: 0.097\n","Epoch [844][0/1]\tLoss: 0.107\n","Epoch [845][0/1]\tLoss: 0.102\n","Epoch [846][0/1]\tLoss: 0.103\n","Epoch [847][0/1]\tLoss: 0.098\n","Epoch [848][0/1]\tLoss: 0.105\n","Epoch [849][0/1]\tLoss: 0.101\n","Epoch [850][0/1]\tLoss: 0.104\n","Epoch [851][0/1]\tLoss: 0.100\n","Epoch [852][0/1]\tLoss: 0.097\n","Epoch [853][0/1]\tLoss: 0.100\n","Epoch [854][0/1]\tLoss: 0.097\n","Epoch [855][0/1]\tLoss: 0.097\n","Epoch [856][0/1]\tLoss: 0.099\n","Epoch [857][0/1]\tLoss: 0.100\n","Epoch [858][0/1]\tLoss: 0.097\n","Epoch [859][0/1]\tLoss: 0.094\n","Epoch [860][0/1]\tLoss: 0.094\n","Epoch [861][0/1]\tLoss: 0.092\n","Epoch [862][0/1]\tLoss: 0.099\n","Epoch [863][0/1]\tLoss: 0.092\n","Epoch [864][0/1]\tLoss: 0.091\n","Epoch [865][0/1]\tLoss: 0.089\n","Epoch [866][0/1]\tLoss: 0.092\n","Epoch [867][0/1]\tLoss: 0.093\n","Epoch [868][0/1]\tLoss: 0.091\n","Epoch [869][0/1]\tLoss: 0.092\n","Epoch [870][0/1]\tLoss: 0.089\n","Epoch [871][0/1]\tLoss: 0.091\n","Epoch [872][0/1]\tLoss: 0.088\n","Epoch [873][0/1]\tLoss: 0.091\n","Epoch [874][0/1]\tLoss: 0.088\n","Epoch [875][0/1]\tLoss: 0.091\n","Epoch [876][0/1]\tLoss: 0.088\n","Epoch [877][0/1]\tLoss: 0.088\n","Epoch [878][0/1]\tLoss: 0.090\n","Epoch [879][0/1]\tLoss: 0.086\n","Epoch [880][0/1]\tLoss: 0.087\n","Epoch [881][0/1]\tLoss: 0.085\n","Epoch [882][0/1]\tLoss: 0.088\n","Epoch [883][0/1]\tLoss: 0.088\n","Epoch [884][0/1]\tLoss: 0.089\n","Epoch [885][0/1]\tLoss: 0.084\n","Epoch [886][0/1]\tLoss: 0.091\n","Epoch [887][0/1]\tLoss: 0.090\n","Epoch [888][0/1]\tLoss: 0.093\n","Epoch [889][0/1]\tLoss: 0.099\n","Epoch [890][0/1]\tLoss: 0.099\n","Epoch [891][0/1]\tLoss: 0.122\n","Epoch [892][0/1]\tLoss: 0.142\n","Epoch [893][0/1]\tLoss: 0.192\n","Epoch [894][0/1]\tLoss: 0.412\n","Epoch [895][0/1]\tLoss: 0.841\n","Epoch [896][0/1]\tLoss: 1.456\n","Epoch [897][0/1]\tLoss: 0.741\n","Epoch [898][0/1]\tLoss: 0.936\n","Epoch [899][0/1]\tLoss: 1.327\n","Model saved at epoch: 899 name \n","Epoch [900][0/1]\tLoss: 1.021\n","Epoch [901][0/1]\tLoss: 0.628\n","Epoch [902][0/1]\tLoss: 0.579\n","Epoch [903][0/1]\tLoss: 0.500\n","Epoch [904][0/1]\tLoss: 0.523\n","Epoch [905][0/1]\tLoss: 0.367\n","Epoch [906][0/1]\tLoss: 0.344\n","Epoch [907][0/1]\tLoss: 0.311\n","Epoch [908][0/1]\tLoss: 0.290\n","Epoch [909][0/1]\tLoss: 0.276\n","Epoch [910][0/1]\tLoss: 0.271\n","Epoch [911][0/1]\tLoss: 0.249\n","Epoch [912][0/1]\tLoss: 0.237\n","Epoch [913][0/1]\tLoss: 0.223\n","Epoch [914][0/1]\tLoss: 0.221\n","Epoch [915][0/1]\tLoss: 0.208\n","Epoch [916][0/1]\tLoss: 0.207\n","Epoch [917][0/1]\tLoss: 0.203\n","Epoch [918][0/1]\tLoss: 0.190\n","Epoch [919][0/1]\tLoss: 0.180\n","Epoch [920][0/1]\tLoss: 0.176\n","Epoch [921][0/1]\tLoss: 0.189\n","Epoch [922][0/1]\tLoss: 0.172\n","Epoch [923][0/1]\tLoss: 0.165\n","Epoch [924][0/1]\tLoss: 0.162\n","Epoch [925][0/1]\tLoss: 0.157\n","Epoch [926][0/1]\tLoss: 0.152\n","Epoch [927][0/1]\tLoss: 0.152\n","Epoch [928][0/1]\tLoss: 0.143\n","Epoch [929][0/1]\tLoss: 0.145\n","Epoch [930][0/1]\tLoss: 0.138\n","Epoch [931][0/1]\tLoss: 0.137\n","Epoch [932][0/1]\tLoss: 0.133\n","Epoch [933][0/1]\tLoss: 0.129\n","Epoch [934][0/1]\tLoss: 0.125\n","Epoch [935][0/1]\tLoss: 0.125\n","Epoch [936][0/1]\tLoss: 0.124\n","Epoch [937][0/1]\tLoss: 0.122\n","Epoch [938][0/1]\tLoss: 0.117\n","Epoch [939][0/1]\tLoss: 0.114\n","Epoch [940][0/1]\tLoss: 0.110\n","Epoch [941][0/1]\tLoss: 0.108\n","Epoch [942][0/1]\tLoss: 0.109\n","Epoch [943][0/1]\tLoss: 0.109\n","Epoch [944][0/1]\tLoss: 0.107\n","Epoch [945][0/1]\tLoss: 0.108\n","Epoch [946][0/1]\tLoss: 0.104\n","Epoch [947][0/1]\tLoss: 0.105\n","Epoch [948][0/1]\tLoss: 0.100\n","Epoch [949][0/1]\tLoss: 0.100\n","Epoch [950][0/1]\tLoss: 0.101\n","Epoch [951][0/1]\tLoss: 0.101\n","Epoch [952][0/1]\tLoss: 0.100\n","Epoch [953][0/1]\tLoss: 0.096\n","Epoch [954][0/1]\tLoss: 0.099\n","Epoch [955][0/1]\tLoss: 0.094\n","Epoch [956][0/1]\tLoss: 0.093\n","Epoch [957][0/1]\tLoss: 0.094\n","Epoch [958][0/1]\tLoss: 0.092\n","Epoch [959][0/1]\tLoss: 0.093\n","Epoch [960][0/1]\tLoss: 0.091\n","Epoch [961][0/1]\tLoss: 0.091\n","Epoch [962][0/1]\tLoss: 0.091\n","Epoch [963][0/1]\tLoss: 0.088\n","Epoch [964][0/1]\tLoss: 0.089\n","Epoch [965][0/1]\tLoss: 0.089\n","Epoch [966][0/1]\tLoss: 0.089\n","Epoch [967][0/1]\tLoss: 0.085\n","Epoch [968][0/1]\tLoss: 0.087\n","Epoch [969][0/1]\tLoss: 0.086\n","Epoch [970][0/1]\tLoss: 0.086\n","Epoch [971][0/1]\tLoss: 0.087\n","Epoch [972][0/1]\tLoss: 0.087\n","Epoch [973][0/1]\tLoss: 0.083\n","Epoch [974][0/1]\tLoss: 0.085\n","Epoch [975][0/1]\tLoss: 0.083\n","Epoch [976][0/1]\tLoss: 0.087\n","Epoch [977][0/1]\tLoss: 0.081\n","Epoch [978][0/1]\tLoss: 0.082\n","Epoch [979][0/1]\tLoss: 0.083\n","Epoch [980][0/1]\tLoss: 0.080\n","Epoch [981][0/1]\tLoss: 0.083\n","Epoch [982][0/1]\tLoss: 0.082\n","Epoch [983][0/1]\tLoss: 0.081\n","Epoch [984][0/1]\tLoss: 0.079\n","Epoch [985][0/1]\tLoss: 0.078\n","Epoch [986][0/1]\tLoss: 0.081\n","Epoch [987][0/1]\tLoss: 0.081\n","Epoch [988][0/1]\tLoss: 0.084\n","Epoch [989][0/1]\tLoss: 0.081\n","Epoch [990][0/1]\tLoss: 0.090\n","Epoch [991][0/1]\tLoss: 0.083\n","Epoch [992][0/1]\tLoss: 0.094\n","Epoch [993][0/1]\tLoss: 0.094\n","Epoch [994][0/1]\tLoss: 0.080\n","Epoch [995][0/1]\tLoss: 0.079\n","Epoch [996][0/1]\tLoss: 0.090\n","Epoch [997][0/1]\tLoss: 0.076\n","Epoch [998][0/1]\tLoss: 0.088\n","Epoch [999][0/1]\tLoss: 0.077\n","Model saved at epoch: 999 name \n"]}],"source":["directory = 'experiment_vanilla_30624'\n","create_directory(directory)\n","\n","d_model = 4096\n","heads = 16\n","num_layers = 15\n","epochs = 1000\n","\n","loss_history_vanilla_transformer = []\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","with open('WORDMAP_corpus_KBFILKOM.json', 'r') as j:\n","    word_map = json.load(j)\n","    \n","transformer = Transformer(d_model = d_model, heads = heads, num_layers = num_layers, word_map = word_map, max_len=95)\n","transformer = transformer.to(device)\n","adam_optimizer = torch.optim.Adam(transformer.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-9)\n","transformer_optimizer = AdamWarmup(model_size = d_model, warmup_steps = 4000, optimizer = adam_optimizer)\n","criterion = LossWithLS(len(word_map), 0.2)\n","\n","for epoch in range(epochs):\n","    loss_train = train(train_loader, transformer, criterion, epoch)\n","\n","    if (epoch + 1) % 100 == 0:\n","        state = {'epoch': epoch, 'transformer': transformer, 'transformer_optimizer': transformer_optimizer}\n","        file_name = directory + '/checkpoint_' + str(epoch) +'.pth.tar'\n","        torch.save(state, file_name)\n","        print('Model saved at epoch: {} name '.format(epoch, file_name))\n","\n","    # loss_history_vanilla_transformer.append(loss_train)"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["menyelenggarakan pendidikan di bidang teknologi informasi dan ilmu komputer yang berkualitas dan berstandar internasional secara berkelanjutan meningkatkan kemampuan sivitas akademika dalam pengembangan penelitian dan pengabdian yang selaras dengan kebutuhan industri dan masyarakat mengintegrasikan pengembangan pendidikan penelitian dan pengabdian kepada masyarakat yang ditunjang dengan tatakelola organisasi yang transparan akuntabel efektif\n"]}],"source":["directory = 'experiment_vanilla_30624'\n","checkpoint = torch.load(directory + '/checkpoint_999.pth.tar')\n","transformer = checkpoint['transformer']\n","\n","question = \"visi filkom\" \n","max_len = 50\n","enc_qus = [word_map.get(word, word_map['<unk>']) for word in question.split()]\n","question = torch.LongTensor(enc_qus).to(device).unsqueeze(0)\n","question_mask = (question!=0).to(device).unsqueeze(1).unsqueeze(1)  \n","sentence = evaluate(transformer, question, question_mask, int(max_len), word_map)\n","print(sentence)"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["menyelenggarakan pendidikan di bidang teknologi informasi dan ilmu komputer yang berkualitas dan berstandar internasional secara berkelanjutan meningkatkan kemampuan sivitas akademika dalam pengembangan penelitian dan pengabdian yang selaras dengan kebutuhan industri dan masyarakat mengintegrasikan pengembangan pendidikan penelitian dan pengabdian kepada masyarakat yang ditunjang dengan tatakelola organisasi yang transparan akuntabel efektif\n"]}],"source":["transformer = checkpoint['transformer']\n","\n","question = \"misi filkom\" \n","max_len = 50\n","enc_qus = [word_map.get(word, word_map['<unk>']) for word in question.split()]\n","question = torch.LongTensor(enc_qus).to(device).unsqueeze(0)\n","question_mask = (question!=0).to(device).unsqueeze(1).unsqueeze(1)  \n","sentence = evaluate(transformer, question, question_mask, int(max_len), word_map)\n","print(sentence)"]},{"cell_type":"code","execution_count":49,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[169, 170, 194, 14]\n","issa arwani skom msc\n"]}],"source":["question = \"ketua departemen sistem informasi\" \n","max_len = 50\n","enc_qus = [word_map.get(word, word_map['<unk>']) for word in question.split()]\n","print(enc_qus)\n","question = torch.LongTensor(enc_qus).to(device).unsqueeze(0)\n","question_mask = (question!=0).to(device).unsqueeze(1).unsqueeze(1)  \n","sentence = evaluate(transformer, question, question_mask, int(max_len), word_map)\n","print(sentence)"]},{"cell_type":"code","execution_count":50,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[176, 170]\n","menyelenggarakan pendidikan di bidang teknologi informasi dan ilmu komputer yang berkualitas dan berstandar internasional secara berkelanjutan meningkatkan kemampuan sivitas akademika dalam pengembangan penelitian dan pengabdian yang selaras dengan kebutuhan industri dan masyarakat mengintegrasikan pengembangan pendidikan penelitian dan pengabdian kepada masyarakat yang ditunjang dengan tatakelola organisasi yang transparan akuntabel efektif\n"]}],"source":["question = \"sekretaris departemen\" \n","max_len = 50\n","enc_qus = [word_map.get(word, word_map['<unk>']) for word in question.split()]\n","print(enc_qus)\n","question = torch.LongTensor(enc_qus).to(device).unsqueeze(0)\n","question_mask = (question!=0).to(device).unsqueeze(1).unsqueeze(1)  \n","sentence = evaluate(transformer, question, question_mask, int(max_len), word_map)\n","print(sentence)"]},{"cell_type":"code","execution_count":48,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[169, 181, 99, 182, 15, 16]\n","sabriansyah rizqika akbar st meng phd\n"]}],"source":["question = \"ketua program studi magister ilmu komputer\" \n","max_len = 50\n","enc_qus = [word_map.get(word, word_map['<unk>']) for word in question.split()]\n","print(enc_qus)\n","question = torch.LongTensor(enc_qus).to(device).unsqueeze(0)\n","question_mask = (question!=0).to(device).unsqueeze(1).unsqueeze(1)  \n","sentence = evaluate(transformer, question, question_mask, int(max_len), word_map)\n","print(sentence)"]},{"cell_type":"code","execution_count":51,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[1, 2]\n","menyelenggarakan pendidikan di bidang teknologi informasi dan ilmu komputer yang berkualitas dan berstandar internasional secara berkelanjutan meningkatkan kemampuan sivitas akademika dalam pengembangan penelitian dan pengabdian yang selaras dengan kebutuhan industri dan masyarakat mengintegrasikan pengembangan pendidikan penelitian dan pengabdian kepada masyarakat yang ditunjang dengan tatakelola organisasi yang transparan akuntabel efektif\n"]}],"source":["question = \"visi filkom\" \n","max_len = 50\n","enc_qus = [word_map.get(word, word_map['<unk>']) for word in question.split()]\n","print(enc_qus)\n","question = torch.LongTensor(enc_qus).to(device).unsqueeze(0)\n","question_mask = (question!=0).to(device).unsqueeze(1).unsqueeze(1)  \n","sentence = evaluate(transformer, question, question_mask, int(max_len), word_map)\n","print(sentence)"]},{"cell_type":"code","execution_count":52,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[27, 2]\n","menyelenggarakan pendidikan di bidang teknologi informasi dan ilmu komputer yang berkualitas dan berstandar internasional secara berkelanjutan meningkatkan kemampuan sivitas akademika dalam pengembangan penelitian dan pengabdian yang selaras dengan kebutuhan industri dan masyarakat mengintegrasikan pengembangan pendidikan penelitian dan pengabdian kepada masyarakat yang ditunjang dengan tatakelola organisasi yang transparan akuntabel efektif\n"]}],"source":["question = \"misi filkom\" \n","max_len = 50\n","enc_qus = [word_map.get(word, word_map['<unk>']) for word in question.split()]\n","print(enc_qus)\n","question = torch.LongTensor(enc_qus).to(device).unsqueeze(0)\n","question_mask = (question!=0).to(device).unsqueeze(1).unsqueeze(1)  \n","sentence = evaluate(transformer, question, question_mask, int(max_len), word_map)\n","print(sentence)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["## Vanilla without Regularization"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Directory already exists at transformers_vanillanoreg_01724\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n","  warnings.warn(warning.format(ret))\n"]},{"name":"stdout","output_type":"stream","text":["Epoch [0][0/1]\tLoss: 3.433\n","Epoch [1][0/1]\tLoss: 3.418\n","Epoch [2][0/1]\tLoss: 3.404\n","Epoch [3][0/1]\tLoss: 3.411\n","Epoch [4][0/1]\tLoss: 3.400\n","Epoch [5][0/1]\tLoss: 3.404\n","Epoch [6][0/1]\tLoss: 3.394\n","Epoch [7][0/1]\tLoss: 3.370\n","Epoch [8][0/1]\tLoss: 3.369\n","Epoch [9][0/1]\tLoss: 3.357\n","Epoch [10][0/1]\tLoss: 3.335\n","Epoch [11][0/1]\tLoss: 3.349\n","Epoch [12][0/1]\tLoss: 3.301\n","Epoch [13][0/1]\tLoss: 3.271\n","Epoch [14][0/1]\tLoss: 3.244\n","Epoch [15][0/1]\tLoss: 3.230\n","Epoch [16][0/1]\tLoss: 3.206\n","Epoch [17][0/1]\tLoss: 3.190\n","Epoch [18][0/1]\tLoss: 3.190\n","Epoch [19][0/1]\tLoss: 3.146\n","Epoch [20][0/1]\tLoss: 3.145\n","Epoch [21][0/1]\tLoss: 3.127\n","Epoch [22][0/1]\tLoss: 3.116\n","Epoch [23][0/1]\tLoss: 3.093\n","Epoch [24][0/1]\tLoss: 3.075\n","Epoch [25][0/1]\tLoss: 3.082\n","Epoch [26][0/1]\tLoss: 3.084\n","Epoch [27][0/1]\tLoss: 3.081\n","Epoch [28][0/1]\tLoss: 3.054\n","Epoch [29][0/1]\tLoss: 3.049\n","Epoch [30][0/1]\tLoss: 3.040\n","Epoch [31][0/1]\tLoss: 3.035\n","Epoch [32][0/1]\tLoss: 3.015\n","Epoch [33][0/1]\tLoss: 3.007\n","Epoch [34][0/1]\tLoss: 3.001\n","Epoch [35][0/1]\tLoss: 3.010\n","Epoch [36][0/1]\tLoss: 2.994\n","Epoch [37][0/1]\tLoss: 2.996\n","Epoch [38][0/1]\tLoss: 2.995\n","Epoch [39][0/1]\tLoss: 3.004\n","Epoch [40][0/1]\tLoss: 2.952\n","Epoch [41][0/1]\tLoss: 2.975\n","Epoch [42][0/1]\tLoss: 2.984\n","Epoch [43][0/1]\tLoss: 2.979\n","Epoch [44][0/1]\tLoss: 2.961\n","Epoch [45][0/1]\tLoss: 2.990\n","Epoch [46][0/1]\tLoss: 2.958\n","Epoch [47][0/1]\tLoss: 2.982\n","Epoch [48][0/1]\tLoss: 2.979\n","Epoch [49][0/1]\tLoss: 2.997\n","Epoch [50][0/1]\tLoss: 2.994\n","Epoch [51][0/1]\tLoss: 2.978\n","Epoch [52][0/1]\tLoss: 2.980\n","Epoch [53][0/1]\tLoss: 2.980\n","Epoch [54][0/1]\tLoss: 2.969\n","Epoch [55][0/1]\tLoss: 2.961\n","Epoch [56][0/1]\tLoss: 2.965\n","Epoch [57][0/1]\tLoss: 2.963\n","Epoch [58][0/1]\tLoss: 2.941\n","Epoch [59][0/1]\tLoss: 2.971\n","Epoch [60][0/1]\tLoss: 2.969\n","Epoch [61][0/1]\tLoss: 2.949\n","Epoch [62][0/1]\tLoss: 2.960\n","Epoch [63][0/1]\tLoss: 2.967\n","Epoch [64][0/1]\tLoss: 2.959\n","Epoch [65][0/1]\tLoss: 2.954\n","Epoch [66][0/1]\tLoss: 2.958\n","Epoch [67][0/1]\tLoss: 2.960\n","Epoch [68][0/1]\tLoss: 2.953\n","Epoch [69][0/1]\tLoss: 2.948\n","Epoch [70][0/1]\tLoss: 2.927\n","Epoch [71][0/1]\tLoss: 2.946\n","Epoch [72][0/1]\tLoss: 2.956\n","Epoch [73][0/1]\tLoss: 2.953\n","Epoch [74][0/1]\tLoss: 2.924\n","Epoch [75][0/1]\tLoss: 2.927\n","Epoch [76][0/1]\tLoss: 2.942\n","Epoch [77][0/1]\tLoss: 2.927\n","Epoch [78][0/1]\tLoss: 2.944\n","Epoch [79][0/1]\tLoss: 2.927\n","Epoch [80][0/1]\tLoss: 2.908\n","Epoch [81][0/1]\tLoss: 2.918\n","Epoch [82][0/1]\tLoss: 2.932\n","Epoch [83][0/1]\tLoss: 2.917\n","Epoch [84][0/1]\tLoss: 2.910\n","Epoch [85][0/1]\tLoss: 2.915\n","Epoch [86][0/1]\tLoss: 2.910\n","Epoch [87][0/1]\tLoss: 2.897\n","Epoch [88][0/1]\tLoss: 2.918\n","Epoch [89][0/1]\tLoss: 2.885\n","Epoch [90][0/1]\tLoss: 2.890\n","Epoch [91][0/1]\tLoss: 2.895\n","Epoch [92][0/1]\tLoss: 2.859\n","Epoch [93][0/1]\tLoss: 2.866\n","Epoch [94][0/1]\tLoss: 2.899\n","Epoch [95][0/1]\tLoss: 2.865\n","Epoch [96][0/1]\tLoss: 2.872\n","Epoch [97][0/1]\tLoss: 2.851\n","Epoch [98][0/1]\tLoss: 2.839\n","Epoch [99][0/1]\tLoss: 2.857\n","Epoch [100][0/1]\tLoss: 2.847\n","Epoch [101][0/1]\tLoss: 2.852\n","Epoch [102][0/1]\tLoss: 2.833\n","Epoch [103][0/1]\tLoss: 2.839\n","Epoch [104][0/1]\tLoss: 2.815\n","Epoch [105][0/1]\tLoss: 2.819\n","Epoch [106][0/1]\tLoss: 2.803\n","Epoch [107][0/1]\tLoss: 2.808\n","Epoch [108][0/1]\tLoss: 2.796\n","Epoch [109][0/1]\tLoss: 2.794\n","Epoch [110][0/1]\tLoss: 2.753\n","Epoch [111][0/1]\tLoss: 2.755\n","Epoch [112][0/1]\tLoss: 2.731\n","Epoch [113][0/1]\tLoss: 2.725\n","Epoch [114][0/1]\tLoss: 2.720\n","Epoch [115][0/1]\tLoss: 2.722\n","Epoch [116][0/1]\tLoss: 2.694\n","Epoch [117][0/1]\tLoss: 2.688\n","Epoch [118][0/1]\tLoss: 2.659\n","Epoch [119][0/1]\tLoss: 2.677\n","Epoch [120][0/1]\tLoss: 2.642\n","Epoch [121][0/1]\tLoss: 2.626\n","Epoch [122][0/1]\tLoss: 2.627\n","Epoch [123][0/1]\tLoss: 2.591\n","Epoch [124][0/1]\tLoss: 2.556\n","Epoch [125][0/1]\tLoss: 2.600\n","Epoch [126][0/1]\tLoss: 2.572\n","Epoch [127][0/1]\tLoss: 2.523\n","Epoch [128][0/1]\tLoss: 2.523\n","Epoch [129][0/1]\tLoss: 2.511\n","Epoch [130][0/1]\tLoss: 2.482\n","Epoch [131][0/1]\tLoss: 2.465\n","Epoch [132][0/1]\tLoss: 2.465\n","Epoch [133][0/1]\tLoss: 2.448\n","Epoch [134][0/1]\tLoss: 2.432\n","Epoch [135][0/1]\tLoss: 2.438\n","Epoch [136][0/1]\tLoss: 2.410\n","Epoch [137][0/1]\tLoss: 2.414\n","Epoch [138][0/1]\tLoss: 2.385\n","Epoch [139][0/1]\tLoss: 2.388\n","Epoch [140][0/1]\tLoss: 2.347\n","Epoch [141][0/1]\tLoss: 2.313\n","Epoch [142][0/1]\tLoss: 2.342\n","Epoch [143][0/1]\tLoss: 2.306\n","Epoch [144][0/1]\tLoss: 2.306\n","Epoch [145][0/1]\tLoss: 2.309\n","Epoch [146][0/1]\tLoss: 2.288\n","Epoch [147][0/1]\tLoss: 2.251\n","Epoch [148][0/1]\tLoss: 2.234\n","Epoch [149][0/1]\tLoss: 2.226\n","Epoch [150][0/1]\tLoss: 2.265\n","Epoch [151][0/1]\tLoss: 2.232\n","Epoch [152][0/1]\tLoss: 2.204\n","Epoch [153][0/1]\tLoss: 2.199\n","Epoch [154][0/1]\tLoss: 2.171\n","Epoch [155][0/1]\tLoss: 2.141\n","Epoch [156][0/1]\tLoss: 2.162\n","Epoch [157][0/1]\tLoss: 2.162\n","Epoch [158][0/1]\tLoss: 2.122\n","Epoch [159][0/1]\tLoss: 2.093\n","Epoch [160][0/1]\tLoss: 2.109\n","Epoch [161][0/1]\tLoss: 2.114\n","Epoch [162][0/1]\tLoss: 2.095\n","Epoch [163][0/1]\tLoss: 2.036\n","Epoch [164][0/1]\tLoss: 2.063\n","Epoch [165][0/1]\tLoss: 2.001\n","Epoch [166][0/1]\tLoss: 2.022\n","Epoch [167][0/1]\tLoss: 2.019\n","Epoch [168][0/1]\tLoss: 1.958\n","Epoch [169][0/1]\tLoss: 1.984\n","Epoch [170][0/1]\tLoss: 1.966\n","Epoch [171][0/1]\tLoss: 1.973\n","Epoch [172][0/1]\tLoss: 1.909\n","Epoch [173][0/1]\tLoss: 1.920\n","Epoch [174][0/1]\tLoss: 1.918\n","Epoch [175][0/1]\tLoss: 1.913\n","Epoch [176][0/1]\tLoss: 1.924\n","Epoch [177][0/1]\tLoss: 1.888\n","Epoch [178][0/1]\tLoss: 1.878\n","Epoch [179][0/1]\tLoss: 1.879\n","Epoch [180][0/1]\tLoss: 1.873\n","Epoch [181][0/1]\tLoss: 1.827\n","Epoch [182][0/1]\tLoss: 1.838\n","Epoch [183][0/1]\tLoss: 1.843\n","Epoch [184][0/1]\tLoss: 1.846\n","Epoch [185][0/1]\tLoss: 1.802\n","Epoch [186][0/1]\tLoss: 1.764\n","Epoch [187][0/1]\tLoss: 1.807\n","Epoch [188][0/1]\tLoss: 1.763\n","Epoch [189][0/1]\tLoss: 1.765\n","Epoch [190][0/1]\tLoss: 1.716\n","Epoch [191][0/1]\tLoss: 1.760\n","Epoch [192][0/1]\tLoss: 1.739\n","Epoch [193][0/1]\tLoss: 1.730\n","Epoch [194][0/1]\tLoss: 1.744\n","Epoch [195][0/1]\tLoss: 1.697\n","Epoch [196][0/1]\tLoss: 1.741\n","Epoch [197][0/1]\tLoss: 1.675\n","Epoch [198][0/1]\tLoss: 1.695\n","Epoch [199][0/1]\tLoss: 1.691\n","Epoch [200][0/1]\tLoss: 1.677\n","Epoch [201][0/1]\tLoss: 1.657\n","Epoch [202][0/1]\tLoss: 1.624\n","Epoch [203][0/1]\tLoss: 1.646\n","Epoch [204][0/1]\tLoss: 1.687\n","Epoch [205][0/1]\tLoss: 1.626\n","Epoch [206][0/1]\tLoss: 1.614\n","Epoch [207][0/1]\tLoss: 1.575\n","Epoch [208][0/1]\tLoss: 1.611\n","Epoch [209][0/1]\tLoss: 1.629\n","Epoch [210][0/1]\tLoss: 1.667\n","Epoch [211][0/1]\tLoss: 1.580\n","Epoch [212][0/1]\tLoss: 1.609\n","Epoch [213][0/1]\tLoss: 1.548\n","Epoch [214][0/1]\tLoss: 1.593\n","Epoch [215][0/1]\tLoss: 1.529\n","Epoch [216][0/1]\tLoss: 1.541\n","Epoch [217][0/1]\tLoss: 1.497\n","Epoch [218][0/1]\tLoss: 1.514\n","Epoch [219][0/1]\tLoss: 1.546\n","Epoch [220][0/1]\tLoss: 1.493\n","Epoch [221][0/1]\tLoss: 1.550\n","Epoch [222][0/1]\tLoss: 1.482\n","Epoch [223][0/1]\tLoss: 1.503\n","Epoch [224][0/1]\tLoss: 1.465\n","Epoch [225][0/1]\tLoss: 1.458\n","Epoch [226][0/1]\tLoss: 1.520\n","Epoch [227][0/1]\tLoss: 1.438\n","Epoch [228][0/1]\tLoss: 1.504\n","Epoch [229][0/1]\tLoss: 1.435\n","Epoch [230][0/1]\tLoss: 1.421\n","Epoch [231][0/1]\tLoss: 1.413\n","Epoch [232][0/1]\tLoss: 1.455\n","Epoch [233][0/1]\tLoss: 1.407\n","Epoch [234][0/1]\tLoss: 1.378\n","Epoch [235][0/1]\tLoss: 1.407\n","Epoch [236][0/1]\tLoss: 1.403\n","Epoch [237][0/1]\tLoss: 1.375\n","Epoch [238][0/1]\tLoss: 1.365\n","Epoch [239][0/1]\tLoss: 1.372\n","Epoch [240][0/1]\tLoss: 1.321\n","Epoch [241][0/1]\tLoss: 1.379\n","Epoch [242][0/1]\tLoss: 1.363\n","Epoch [243][0/1]\tLoss: 1.346\n","Epoch [244][0/1]\tLoss: 1.344\n","Epoch [245][0/1]\tLoss: 1.324\n","Epoch [246][0/1]\tLoss: 1.349\n","Epoch [247][0/1]\tLoss: 1.310\n","Epoch [248][0/1]\tLoss: 1.302\n","Epoch [249][0/1]\tLoss: 1.323\n","Epoch [250][0/1]\tLoss: 1.300\n","Epoch [251][0/1]\tLoss: 1.292\n","Epoch [252][0/1]\tLoss: 1.268\n","Epoch [253][0/1]\tLoss: 1.307\n","Epoch [254][0/1]\tLoss: 1.277\n","Epoch [255][0/1]\tLoss: 1.283\n","Epoch [256][0/1]\tLoss: 1.231\n","Epoch [257][0/1]\tLoss: 1.281\n","Epoch [258][0/1]\tLoss: 1.238\n","Epoch [259][0/1]\tLoss: 1.260\n","Epoch [260][0/1]\tLoss: 1.225\n","Epoch [261][0/1]\tLoss: 1.257\n","Epoch [262][0/1]\tLoss: 1.230\n","Epoch [263][0/1]\tLoss: 1.200\n","Epoch [264][0/1]\tLoss: 1.242\n","Epoch [265][0/1]\tLoss: 1.172\n","Epoch [266][0/1]\tLoss: 1.205\n","Epoch [267][0/1]\tLoss: 1.187\n","Epoch [268][0/1]\tLoss: 1.168\n","Epoch [269][0/1]\tLoss: 1.165\n","Epoch [270][0/1]\tLoss: 1.207\n","Epoch [271][0/1]\tLoss: 1.185\n","Epoch [272][0/1]\tLoss: 1.159\n","Epoch [273][0/1]\tLoss: 1.159\n","Epoch [274][0/1]\tLoss: 1.140\n","Epoch [275][0/1]\tLoss: 1.138\n","Epoch [276][0/1]\tLoss: 1.157\n","Epoch [277][0/1]\tLoss: 1.130\n","Epoch [278][0/1]\tLoss: 1.146\n","Epoch [279][0/1]\tLoss: 1.120\n","Epoch [280][0/1]\tLoss: 1.119\n","Epoch [281][0/1]\tLoss: 1.104\n","Epoch [282][0/1]\tLoss: 1.107\n","Epoch [283][0/1]\tLoss: 1.087\n","Epoch [284][0/1]\tLoss: 1.093\n","Epoch [285][0/1]\tLoss: 1.070\n","Epoch [286][0/1]\tLoss: 1.094\n","Epoch [287][0/1]\tLoss: 1.060\n","Epoch [288][0/1]\tLoss: 1.057\n","Epoch [289][0/1]\tLoss: 1.086\n","Epoch [290][0/1]\tLoss: 1.078\n","Epoch [291][0/1]\tLoss: 1.087\n","Epoch [292][0/1]\tLoss: 1.061\n","Epoch [293][0/1]\tLoss: 1.090\n","Epoch [294][0/1]\tLoss: 1.101\n","Epoch [295][0/1]\tLoss: 1.081\n","Epoch [296][0/1]\tLoss: 1.093\n","Epoch [297][0/1]\tLoss: 1.086\n","Epoch [298][0/1]\tLoss: 1.041\n","Epoch [299][0/1]\tLoss: 1.069\n","Epoch [300][0/1]\tLoss: 1.002\n","Epoch [301][0/1]\tLoss: 1.041\n","Epoch [302][0/1]\tLoss: 1.058\n","Epoch [303][0/1]\tLoss: 1.002\n","Epoch [304][0/1]\tLoss: 1.002\n","Epoch [305][0/1]\tLoss: 0.989\n","Epoch [306][0/1]\tLoss: 1.003\n","Epoch [307][0/1]\tLoss: 1.002\n","Epoch [308][0/1]\tLoss: 0.947\n","Epoch [309][0/1]\tLoss: 0.999\n","Epoch [310][0/1]\tLoss: 0.968\n","Epoch [311][0/1]\tLoss: 1.023\n","Epoch [312][0/1]\tLoss: 0.988\n","Epoch [313][0/1]\tLoss: 0.973\n","Epoch [314][0/1]\tLoss: 0.957\n","Epoch [315][0/1]\tLoss: 0.946\n","Epoch [316][0/1]\tLoss: 1.012\n","Epoch [317][0/1]\tLoss: 0.957\n","Epoch [318][0/1]\tLoss: 0.959\n","Epoch [319][0/1]\tLoss: 0.949\n","Epoch [320][0/1]\tLoss: 0.967\n","Epoch [321][0/1]\tLoss: 0.976\n","Epoch [322][0/1]\tLoss: 0.945\n","Epoch [323][0/1]\tLoss: 0.992\n","Epoch [324][0/1]\tLoss: 0.945\n","Epoch [325][0/1]\tLoss: 0.949\n","Epoch [326][0/1]\tLoss: 0.944\n","Epoch [327][0/1]\tLoss: 0.885\n","Epoch [328][0/1]\tLoss: 0.914\n","Epoch [329][0/1]\tLoss: 0.906\n","Epoch [330][0/1]\tLoss: 0.887\n","Epoch [331][0/1]\tLoss: 0.930\n","Epoch [332][0/1]\tLoss: 0.913\n","Epoch [333][0/1]\tLoss: 0.899\n","Epoch [334][0/1]\tLoss: 0.921\n","Epoch [335][0/1]\tLoss: 0.888\n","Epoch [336][0/1]\tLoss: 0.885\n","Epoch [337][0/1]\tLoss: 0.872\n","Epoch [338][0/1]\tLoss: 0.889\n","Epoch [339][0/1]\tLoss: 0.880\n","Epoch [340][0/1]\tLoss: 0.838\n","Epoch [341][0/1]\tLoss: 0.901\n","Epoch [342][0/1]\tLoss: 0.849\n","Epoch [343][0/1]\tLoss: 0.874\n","Epoch [344][0/1]\tLoss: 0.875\n","Epoch [345][0/1]\tLoss: 0.828\n","Epoch [346][0/1]\tLoss: 0.871\n","Epoch [347][0/1]\tLoss: 0.853\n","Epoch [348][0/1]\tLoss: 0.840\n","Epoch [349][0/1]\tLoss: 0.839\n","Epoch [350][0/1]\tLoss: 0.834\n","Epoch [351][0/1]\tLoss: 0.804\n","Epoch [352][0/1]\tLoss: 0.826\n","Epoch [353][0/1]\tLoss: 0.829\n","Epoch [354][0/1]\tLoss: 0.852\n","Epoch [355][0/1]\tLoss: 0.795\n","Epoch [356][0/1]\tLoss: 0.810\n","Epoch [357][0/1]\tLoss: 0.775\n","Epoch [358][0/1]\tLoss: 0.748\n","Epoch [359][0/1]\tLoss: 0.799\n","Epoch [360][0/1]\tLoss: 0.837\n","Epoch [361][0/1]\tLoss: 0.733\n","Epoch [362][0/1]\tLoss: 0.832\n","Epoch [363][0/1]\tLoss: 0.762\n","Epoch [364][0/1]\tLoss: 0.778\n","Epoch [365][0/1]\tLoss: 0.804\n","Epoch [366][0/1]\tLoss: 0.738\n","Epoch [367][0/1]\tLoss: 0.784\n","Epoch [368][0/1]\tLoss: 0.785\n","Epoch [369][0/1]\tLoss: 0.737\n","Epoch [370][0/1]\tLoss: 0.738\n","Epoch [371][0/1]\tLoss: 0.744\n","Epoch [372][0/1]\tLoss: 0.720\n","Epoch [373][0/1]\tLoss: 0.679\n","Epoch [374][0/1]\tLoss: 0.725\n","Epoch [375][0/1]\tLoss: 0.710\n","Epoch [376][0/1]\tLoss: 0.689\n","Epoch [377][0/1]\tLoss: 0.688\n","Epoch [378][0/1]\tLoss: 0.707\n","Epoch [379][0/1]\tLoss: 0.682\n","Epoch [380][0/1]\tLoss: 0.663\n","Epoch [381][0/1]\tLoss: 0.660\n","Epoch [382][0/1]\tLoss: 0.684\n","Epoch [383][0/1]\tLoss: 0.647\n","Epoch [384][0/1]\tLoss: 0.653\n","Epoch [385][0/1]\tLoss: 0.677\n","Epoch [386][0/1]\tLoss: 0.645\n","Epoch [387][0/1]\tLoss: 0.731\n","Epoch [388][0/1]\tLoss: 0.707\n","Epoch [389][0/1]\tLoss: 0.800\n","Epoch [390][0/1]\tLoss: 0.641\n","Epoch [391][0/1]\tLoss: 0.730\n","Epoch [392][0/1]\tLoss: 0.659\n","Epoch [393][0/1]\tLoss: 0.620\n","Epoch [394][0/1]\tLoss: 0.684\n","Epoch [395][0/1]\tLoss: 0.652\n","Epoch [396][0/1]\tLoss: 0.619\n","Epoch [397][0/1]\tLoss: 0.640\n","Epoch [398][0/1]\tLoss: 0.604\n","Epoch [399][0/1]\tLoss: 0.625\n","Epoch [400][0/1]\tLoss: 0.598\n","Epoch [401][0/1]\tLoss: 0.579\n","Epoch [402][0/1]\tLoss: 0.593\n","Epoch [403][0/1]\tLoss: 0.568\n","Epoch [404][0/1]\tLoss: 0.579\n","Epoch [405][0/1]\tLoss: 0.562\n","Epoch [406][0/1]\tLoss: 0.568\n","Epoch [407][0/1]\tLoss: 0.604\n","Epoch [408][0/1]\tLoss: 0.581\n","Epoch [409][0/1]\tLoss: 0.582\n","Epoch [410][0/1]\tLoss: 0.580\n","Epoch [411][0/1]\tLoss: 0.575\n","Epoch [412][0/1]\tLoss: 0.529\n","Epoch [413][0/1]\tLoss: 0.601\n","Epoch [414][0/1]\tLoss: 0.543\n","Epoch [415][0/1]\tLoss: 0.566\n","Epoch [416][0/1]\tLoss: 0.546\n","Epoch [417][0/1]\tLoss: 0.566\n","Epoch [418][0/1]\tLoss: 0.525\n","Epoch [419][0/1]\tLoss: 0.527\n","Epoch [420][0/1]\tLoss: 0.578\n","Epoch [421][0/1]\tLoss: 0.513\n","Epoch [422][0/1]\tLoss: 0.528\n","Epoch [423][0/1]\tLoss: 0.543\n","Epoch [424][0/1]\tLoss: 0.551\n","Epoch [425][0/1]\tLoss: 0.508\n","Epoch [426][0/1]\tLoss: 0.551\n","Epoch [427][0/1]\tLoss: 0.526\n","Epoch [428][0/1]\tLoss: 0.548\n","Epoch [429][0/1]\tLoss: 0.524\n","Epoch [430][0/1]\tLoss: 0.466\n","Epoch [431][0/1]\tLoss: 0.558\n","Epoch [432][0/1]\tLoss: 0.502\n","Epoch [433][0/1]\tLoss: 0.468\n","Epoch [434][0/1]\tLoss: 0.480\n","Epoch [435][0/1]\tLoss: 0.514\n","Epoch [436][0/1]\tLoss: 0.467\n","Epoch [437][0/1]\tLoss: 0.469\n","Epoch [438][0/1]\tLoss: 0.475\n","Epoch [439][0/1]\tLoss: 0.474\n","Epoch [440][0/1]\tLoss: 0.463\n","Epoch [441][0/1]\tLoss: 0.469\n","Epoch [442][0/1]\tLoss: 0.466\n","Epoch [443][0/1]\tLoss: 0.439\n","Epoch [444][0/1]\tLoss: 0.478\n","Epoch [445][0/1]\tLoss: 0.436\n","Epoch [446][0/1]\tLoss: 0.420\n","Epoch [447][0/1]\tLoss: 0.456\n","Epoch [448][0/1]\tLoss: 0.447\n","Epoch [449][0/1]\tLoss: 0.405\n","Epoch [450][0/1]\tLoss: 0.425\n","Epoch [451][0/1]\tLoss: 0.447\n","Epoch [452][0/1]\tLoss: 0.412\n","Epoch [453][0/1]\tLoss: 0.419\n","Epoch [454][0/1]\tLoss: 0.431\n","Epoch [455][0/1]\tLoss: 0.391\n","Epoch [456][0/1]\tLoss: 0.389\n","Epoch [457][0/1]\tLoss: 0.423\n","Epoch [458][0/1]\tLoss: 0.396\n","Epoch [459][0/1]\tLoss: 0.415\n","Epoch [460][0/1]\tLoss: 0.386\n","Epoch [461][0/1]\tLoss: 0.379\n","Epoch [462][0/1]\tLoss: 0.395\n","Epoch [463][0/1]\tLoss: 0.387\n","Epoch [464][0/1]\tLoss: 0.401\n","Epoch [465][0/1]\tLoss: 0.345\n","Epoch [466][0/1]\tLoss: 0.392\n","Epoch [467][0/1]\tLoss: 0.357\n","Epoch [468][0/1]\tLoss: 0.377\n","Epoch [469][0/1]\tLoss: 0.352\n","Epoch [470][0/1]\tLoss: 0.361\n","Epoch [471][0/1]\tLoss: 0.354\n","Epoch [472][0/1]\tLoss: 0.347\n","Epoch [473][0/1]\tLoss: 0.363\n","Epoch [474][0/1]\tLoss: 0.311\n","Epoch [475][0/1]\tLoss: 0.347\n","Epoch [476][0/1]\tLoss: 0.343\n","Epoch [477][0/1]\tLoss: 0.338\n","Epoch [478][0/1]\tLoss: 0.335\n","Epoch [479][0/1]\tLoss: 0.320\n","Epoch [480][0/1]\tLoss: 0.316\n","Epoch [481][0/1]\tLoss: 0.312\n","Epoch [482][0/1]\tLoss: 0.314\n","Epoch [483][0/1]\tLoss: 0.306\n","Epoch [484][0/1]\tLoss: 0.307\n","Epoch [485][0/1]\tLoss: 0.296\n","Epoch [486][0/1]\tLoss: 0.306\n","Epoch [487][0/1]\tLoss: 0.313\n","Epoch [488][0/1]\tLoss: 0.286\n","Epoch [489][0/1]\tLoss: 0.291\n","Epoch [490][0/1]\tLoss: 0.290\n","Epoch [491][0/1]\tLoss: 0.275\n","Epoch [492][0/1]\tLoss: 0.291\n","Epoch [493][0/1]\tLoss: 0.268\n","Epoch [494][0/1]\tLoss: 0.270\n","Epoch [495][0/1]\tLoss: 0.269\n","Epoch [496][0/1]\tLoss: 0.265\n","Epoch [497][0/1]\tLoss: 0.274\n","Epoch [498][0/1]\tLoss: 0.278\n","Epoch [499][0/1]\tLoss: 0.267\n","Epoch [500][0/1]\tLoss: 0.253\n","Epoch [501][0/1]\tLoss: 0.267\n","Epoch [502][0/1]\tLoss: 0.250\n","Epoch [503][0/1]\tLoss: 0.247\n","Epoch [504][0/1]\tLoss: 0.249\n","Epoch [505][0/1]\tLoss: 0.249\n","Epoch [506][0/1]\tLoss: 0.258\n","Epoch [507][0/1]\tLoss: 0.248\n","Epoch [508][0/1]\tLoss: 0.257\n","Epoch [509][0/1]\tLoss: 0.245\n","Epoch [510][0/1]\tLoss: 0.248\n","Epoch [511][0/1]\tLoss: 0.257\n","Epoch [512][0/1]\tLoss: 0.235\n","Epoch [513][0/1]\tLoss: 0.257\n","Epoch [514][0/1]\tLoss: 0.227\n","Epoch [515][0/1]\tLoss: 0.237\n","Epoch [516][0/1]\tLoss: 0.231\n","Epoch [517][0/1]\tLoss: 0.229\n","Epoch [518][0/1]\tLoss: 0.232\n","Epoch [519][0/1]\tLoss: 0.226\n","Epoch [520][0/1]\tLoss: 0.227\n","Epoch [521][0/1]\tLoss: 0.232\n","Epoch [522][0/1]\tLoss: 0.238\n","Epoch [523][0/1]\tLoss: 0.211\n","Epoch [524][0/1]\tLoss: 0.234\n","Epoch [525][0/1]\tLoss: 0.223\n","Epoch [526][0/1]\tLoss: 0.231\n","Epoch [527][0/1]\tLoss: 0.220\n","Epoch [528][0/1]\tLoss: 0.221\n","Epoch [529][0/1]\tLoss: 0.211\n","Epoch [530][0/1]\tLoss: 0.223\n","Epoch [531][0/1]\tLoss: 0.217\n","Epoch [532][0/1]\tLoss: 0.216\n","Epoch [533][0/1]\tLoss: 0.215\n","Epoch [534][0/1]\tLoss: 0.237\n","Epoch [535][0/1]\tLoss: 0.219\n","Epoch [536][0/1]\tLoss: 0.234\n","Epoch [537][0/1]\tLoss: 0.207\n","Epoch [538][0/1]\tLoss: 0.221\n","Epoch [539][0/1]\tLoss: 0.214\n","Epoch [540][0/1]\tLoss: 0.203\n","Epoch [541][0/1]\tLoss: 0.202\n","Epoch [542][0/1]\tLoss: 0.215\n","Epoch [543][0/1]\tLoss: 0.203\n","Epoch [544][0/1]\tLoss: 0.192\n","Epoch [545][0/1]\tLoss: 0.222\n","Epoch [546][0/1]\tLoss: 0.188\n","Epoch [547][0/1]\tLoss: 0.198\n","Epoch [548][0/1]\tLoss: 0.184\n","Epoch [549][0/1]\tLoss: 0.204\n","Epoch [550][0/1]\tLoss: 0.193\n","Epoch [551][0/1]\tLoss: 0.182\n","Epoch [552][0/1]\tLoss: 0.189\n","Epoch [553][0/1]\tLoss: 0.194\n","Epoch [554][0/1]\tLoss: 0.183\n","Epoch [555][0/1]\tLoss: 0.195\n","Epoch [556][0/1]\tLoss: 0.191\n","Epoch [557][0/1]\tLoss: 0.179\n","Epoch [558][0/1]\tLoss: 0.175\n","Epoch [559][0/1]\tLoss: 0.183\n","Epoch [560][0/1]\tLoss: 0.173\n","Epoch [561][0/1]\tLoss: 0.174\n","Epoch [562][0/1]\tLoss: 0.169\n","Epoch [563][0/1]\tLoss: 0.162\n","Epoch [564][0/1]\tLoss: 0.177\n","Epoch [565][0/1]\tLoss: 0.164\n","Epoch [566][0/1]\tLoss: 0.175\n","Epoch [567][0/1]\tLoss: 0.159\n","Epoch [568][0/1]\tLoss: 0.174\n","Epoch [569][0/1]\tLoss: 0.163\n","Epoch [570][0/1]\tLoss: 0.162\n","Epoch [571][0/1]\tLoss: 0.160\n","Epoch [572][0/1]\tLoss: 0.165\n","Epoch [573][0/1]\tLoss: 0.168\n","Epoch [574][0/1]\tLoss: 0.155\n","Epoch [575][0/1]\tLoss: 0.160\n","Epoch [576][0/1]\tLoss: 0.156\n","Epoch [577][0/1]\tLoss: 0.160\n","Epoch [578][0/1]\tLoss: 0.154\n","Epoch [579][0/1]\tLoss: 0.149\n","Epoch [580][0/1]\tLoss: 0.165\n","Epoch [581][0/1]\tLoss: 0.157\n","Epoch [582][0/1]\tLoss: 0.155\n","Epoch [583][0/1]\tLoss: 0.155\n","Epoch [584][0/1]\tLoss: 0.155\n","Epoch [585][0/1]\tLoss: 0.150\n","Epoch [586][0/1]\tLoss: 0.146\n","Epoch [587][0/1]\tLoss: 0.159\n","Epoch [588][0/1]\tLoss: 0.145\n","Epoch [589][0/1]\tLoss: 0.147\n","Epoch [590][0/1]\tLoss: 0.149\n","Epoch [591][0/1]\tLoss: 0.141\n","Epoch [592][0/1]\tLoss: 0.139\n","Epoch [593][0/1]\tLoss: 0.143\n","Epoch [594][0/1]\tLoss: 0.140\n","Epoch [595][0/1]\tLoss: 0.134\n","Epoch [596][0/1]\tLoss: 0.145\n","Epoch [597][0/1]\tLoss: 0.137\n","Epoch [598][0/1]\tLoss: 0.141\n","Epoch [599][0/1]\tLoss: 0.152\n","Epoch [600][0/1]\tLoss: 0.140\n","Epoch [601][0/1]\tLoss: 0.153\n","Epoch [602][0/1]\tLoss: 0.151\n","Epoch [603][0/1]\tLoss: 0.143\n","Epoch [604][0/1]\tLoss: 0.141\n","Epoch [605][0/1]\tLoss: 0.154\n","Epoch [606][0/1]\tLoss: 0.132\n","Epoch [607][0/1]\tLoss: 0.139\n","Epoch [608][0/1]\tLoss: 0.148\n","Epoch [609][0/1]\tLoss: 0.143\n","Epoch [610][0/1]\tLoss: 0.144\n","Epoch [611][0/1]\tLoss: 0.136\n","Epoch [612][0/1]\tLoss: 0.151\n","Epoch [613][0/1]\tLoss: 0.137\n","Epoch [614][0/1]\tLoss: 0.138\n","Epoch [615][0/1]\tLoss: 0.129\n","Epoch [616][0/1]\tLoss: 0.134\n","Epoch [617][0/1]\tLoss: 0.135\n","Epoch [618][0/1]\tLoss: 0.130\n","Epoch [619][0/1]\tLoss: 0.128\n","Epoch [620][0/1]\tLoss: 0.136\n","Epoch [621][0/1]\tLoss: 0.132\n","Epoch [622][0/1]\tLoss: 0.123\n","Epoch [623][0/1]\tLoss: 0.124\n","Epoch [624][0/1]\tLoss: 0.128\n","Epoch [625][0/1]\tLoss: 0.118\n","Epoch [626][0/1]\tLoss: 0.133\n","Epoch [627][0/1]\tLoss: 0.125\n","Epoch [628][0/1]\tLoss: 0.118\n","Epoch [629][0/1]\tLoss: 0.130\n","Epoch [630][0/1]\tLoss: 0.119\n","Epoch [631][0/1]\tLoss: 0.127\n","Epoch [632][0/1]\tLoss: 0.118\n","Epoch [633][0/1]\tLoss: 0.114\n","Epoch [634][0/1]\tLoss: 0.119\n","Epoch [635][0/1]\tLoss: 0.118\n","Epoch [636][0/1]\tLoss: 0.115\n","Epoch [637][0/1]\tLoss: 0.133\n","Epoch [638][0/1]\tLoss: 0.112\n","Epoch [639][0/1]\tLoss: 0.115\n","Epoch [640][0/1]\tLoss: 0.117\n","Epoch [641][0/1]\tLoss: 0.111\n","Epoch [642][0/1]\tLoss: 0.126\n","Epoch [643][0/1]\tLoss: 0.134\n","Epoch [644][0/1]\tLoss: 0.112\n","Epoch [645][0/1]\tLoss: 0.138\n","Epoch [646][0/1]\tLoss: 0.124\n","Epoch [647][0/1]\tLoss: 0.109\n","Epoch [648][0/1]\tLoss: 0.107\n","Epoch [649][0/1]\tLoss: 0.134\n","Epoch [650][0/1]\tLoss: 0.118\n","Epoch [651][0/1]\tLoss: 0.114\n","Epoch [652][0/1]\tLoss: 0.114\n","Epoch [653][0/1]\tLoss: 0.108\n","Epoch [654][0/1]\tLoss: 0.112\n","Epoch [655][0/1]\tLoss: 0.110\n","Epoch [656][0/1]\tLoss: 0.108\n","Epoch [657][0/1]\tLoss: 0.110\n","Epoch [658][0/1]\tLoss: 0.109\n","Epoch [659][0/1]\tLoss: 0.113\n","Epoch [660][0/1]\tLoss: 0.099\n","Epoch [661][0/1]\tLoss: 0.103\n","Epoch [662][0/1]\tLoss: 0.106\n","Epoch [663][0/1]\tLoss: 0.109\n","Epoch [664][0/1]\tLoss: 0.108\n","Epoch [665][0/1]\tLoss: 0.103\n","Epoch [666][0/1]\tLoss: 0.108\n","Epoch [667][0/1]\tLoss: 0.097\n","Epoch [668][0/1]\tLoss: 0.110\n","Epoch [669][0/1]\tLoss: 0.111\n","Epoch [670][0/1]\tLoss: 0.110\n","Epoch [671][0/1]\tLoss: 0.093\n","Epoch [672][0/1]\tLoss: 0.106\n","Epoch [673][0/1]\tLoss: 0.100\n","Epoch [674][0/1]\tLoss: 0.099\n","Epoch [675][0/1]\tLoss: 0.098\n","Epoch [676][0/1]\tLoss: 0.100\n","Epoch [677][0/1]\tLoss: 0.091\n","Epoch [678][0/1]\tLoss: 0.094\n","Epoch [679][0/1]\tLoss: 0.095\n","Epoch [680][0/1]\tLoss: 0.098\n","Epoch [681][0/1]\tLoss: 0.095\n","Epoch [682][0/1]\tLoss: 0.092\n","Epoch [683][0/1]\tLoss: 0.091\n","Epoch [684][0/1]\tLoss: 0.097\n","Epoch [685][0/1]\tLoss: 0.097\n","Epoch [686][0/1]\tLoss: 0.091\n","Epoch [687][0/1]\tLoss: 0.097\n","Epoch [688][0/1]\tLoss: 0.094\n","Epoch [689][0/1]\tLoss: 0.088\n","Epoch [690][0/1]\tLoss: 0.086\n","Epoch [691][0/1]\tLoss: 0.088\n","Epoch [692][0/1]\tLoss: 0.089\n","Epoch [693][0/1]\tLoss: 0.096\n","Epoch [694][0/1]\tLoss: 0.090\n","Epoch [695][0/1]\tLoss: 0.094\n","Epoch [696][0/1]\tLoss: 0.093\n","Epoch [697][0/1]\tLoss: 0.087\n","Epoch [698][0/1]\tLoss: 0.086\n","Epoch [699][0/1]\tLoss: 0.081\n"]}],"source":["directory = 'transformers_vanillanoreg_01724'\n","create_directory(directory)\n","\n","# run = neptune_init(directory)\n","\n","parameters = {\n","    'd_model': 2048,\n","    'heads': 32,\n","    'num_layers': 20,\n","    'epochs': 700,\n","}\n","# run['parameters'] = parameters\n","\n","loss_history_vanillanoreg_transformer = []\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n","# device = \"cpu\"\n","\n","with open('WORDMAP_corpus_KBFILKOM.json', 'r') as j:\n","    word_map = json.load(j)\n","    \n","transformer = Transformer(d_model = parameters['d_model'], heads = parameters['heads'], num_layers = parameters['num_layers'], word_map = word_map, max_len=95)\n","transformer = transformer.to(device)\n","adam_optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n","transformer_optimizer = AdamWarmup(model_size = parameters['d_model'], warmup_steps = 4000, optimizer = adam_optimizer)\n","criterion = LossWithLS(len(word_map), 0.2)\n","\n","for epoch in range(parameters['epochs']):\n","    loss_train = train(train_loader, transformer, criterion, epoch)\n","\n","    state = {'epoch': epoch, 'transformer': transformer, 'transformer_optimizer': transformer_optimizer}\n","    # torch.save(state, directory + '/checkpoint_' + str(epoch) +'.pth.tar')\n","    # run['model_checkpoint'].upload(directory + '/checkpoint_' + str(epoch) +'.pth.tar')\n","\n","    loss_history_vanillanoreg_transformer.append(loss_train)\n","    # run['train/loss'].append(loss_train)\n","\n","# with open(directory + '/loss_history_vanillanoreg_transformer.yaml', 'w') as file:\n","#     yaml.dump(loss_history_vanilla_transformer, file)\n","\n","# run.stop()"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[1, 2]], device='cuda:0')\n","tensor([[[[True, True]]]], device='cuda:0')\n","menjadi fakultas yang berdaya saing internasional dan berkontribusi kepada pengembangan teknologi informasi dan ilmu komputer untuk menunjang industri dan masyarakat dengan menyelaraskan pelaksanaan pendidikan penelitian dan pengabdian kepada masyarakat\n"]}],"source":["question = \"visi filkom\" \n","max_len = 100\n","enc_qus = [word_map.get(word, word_map['<unk>']) for word in question.split()]\n","question = torch.LongTensor(enc_qus).to(device).unsqueeze(0)\n","print(question)\n","question_mask = (question!=0).to(device).unsqueeze(1).unsqueeze(1)  \n","print(question_mask)\n","sentence = evaluate(transformer, question, question_mask, int(max_len), word_map)\n","print(sentence)"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[27,  2]], device='cuda:0')\n","tensor([[[[True, True]]]], device='cuda:0')\n","menjadi fakultas yang berdaya saing internasional dan berkontribusi kepada pengembangan teknologi informasi dan ilmu komputer untuk menunjang industri dan masyarakat dengan menyelaraskan pelaksanaan pendidikan penelitian dan pengabdian kepada masyarakat\n"]}],"source":["question = \"misi filkom\" \n","max_len = 100\n","enc_qus = [word_map.get(word, word_map['<unk>']) for word in question.split()]\n","enc_qus += [word_map['<start>']]\n","question = torch.LongTensor(enc_qus).to(device).unsqueeze(0)\n","print(question)\n","question_mask = (question!=0).to(device).unsqueeze(1).unsqueeze(1)  \n","print(question_mask)\n","sentence = evaluate(transformer, question, question_mask, int(max_len), word_map)\n","print(sentence)"]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["menghasilkan lulusan yang kompeten profesional berbudi pekerti luhur berjiwa entrepreneur dan berdaya saing internasional menghasilkan sivitas akademika yang mampu mengembangkan penelitiandan pengabdian yang berorientasi pada pembaruan dan teknologi tepat guna untuk industri dan masyarakat terwujudnya suasana akademik yang kondusif dalam bidang pendidikan penelitian dan pengabdian kepada masyarakat yang berdaya saing unggul terwujudnya tata kelola organisasi yang transparan akuntabel efektif dan efisien meningkatnya kuantitas dan kualitas kerja sama yang berkelanjutan di bidang pendidikan penelitian dan pengabdian kepada masyarakat dalam skala nasional dan internasional\n"]}],"source":["question = \"apa tujuan filkom\" \n","max_len = 100\n","enc_qus = [word_map.get(word, word_map['<unk>']) for word in question.split()]\n","question = torch.LongTensor(enc_qus).to(device).unsqueeze(0)\n","question_mask = (question!=0).to(device).unsqueeze(1).unsqueeze(1)  \n","sentence = evaluate(transformer, question, question_mask, int(max_len), word_map)\n","print(sentence)"]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["menghasilkan lulusan yang kompeten profesional berbudi pekerti luhur berjiwa entrepreneur dan berdaya saing internasional menghasilkan sivitas akademika yang mampu mengembangkan penelitiandan pengabdian yang berorientasi pada pembaruan dan teknologi tepat guna untuk industri dan masyarakat terwujudnya suasana akademik yang kondusif dalam bidang pendidikan penelitian dan pengabdian kepada masyarakat yang berdaya saing unggul terwujudnya tata kelola organisasi yang transparan akuntabel efektif dan efisien meningkatnya kuantitas dan kualitas kerja sama yang berkelanjutan di bidang pendidikan penelitian dan pengabdian kepada masyarakat dalam skala nasional dan internasional\n"]}],"source":["question = \"sasaran pendidikan filkom\" \n","max_len = 100\n","enc_qus = [word_map.get(word, word_map['<unk>']) for word in question.split()]\n","question = torch.LongTensor(enc_qus).to(device).unsqueeze(0)\n","question_mask = (question!=0).to(device).unsqueeze(1).unsqueeze(1)  \n","sentence = evaluate(transformer, question, question_mask, int(max_len), word_map)\n","print(sentence)"]},{"cell_type":"markdown","metadata":{},"source":["## Decoder Only"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["directory = 'experiment_deconly_17624'\n","create_directory(directory)\n","\n","d_model = 2048\n","heads = 16\n","num_layers = 5\n","epochs = 100\n","\n","loss_history_decoder_transformer = []\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","with open('WORDMAP_corpus_KBFILKOM.json', 'r') as j:\n","    word_map = json.load(j)\n","    \n","transformer = TransformerDecoderOnly(d_model = d_model, heads = heads, num_layers = num_layers, word_map = word_map, max_len=95)\n","transformer = transformer.to(device)\n","adam_optimizer = torch.optim.Adam(transformer.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-9)\n","transformer_optimizer = AdamWarmup(model_size = d_model, warmup_steps = 4000, optimizer = adam_optimizer)\n","criterion = LossWithLS(len(word_map), 0.2)\n","\n","for epoch in range(epochs):\n","    loss_train = train(train_loader, transformer, criterion, epoch)\n","\n","    state = {'epoch': epoch, 'transformer': transformer, 'transformer_optimizer': transformer_optimizer}\n","    torch.save(state, directory + '/checkpoint_' + str(epoch) +'.pth.tar')\n","\n","    loss_history_decoder_transformer.append(loss_train)"]},{"cell_type":"markdown","metadata":{},"source":["## LSTM_Transformer"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["directory = 'experiment_2_lstm'\n","\n","d_model = 1024\n","heads = 32\n","num_layers = 10\n","epochs = 100\n","\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","loss_history_lstm_transformer = []\n","\n","with open('WORDMAP_corpus_KBFILKOM.json', 'r') as j:\n","    word_map = json.load(j)\n","    \n","transformer = TransformerLSTM(d_model = d_model, heads = heads, num_layers = num_layers, word_map = word_map, max_len=90)\n","transformer = transformer.to(device)\n","adam_optimizer = torch.optim.Adam(transformer.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-9)\n","transformer_optimizer = AdamWarmup(model_size = d_model, warmup_steps = 4000, optimizer = adam_optimizer)\n","criterion = LossWithLS(len(word_map), 0.2)\n","\n","\n","for epoch in range(epochs):\n","    loss_train = train(train_loader, transformer, criterion, epoch)\n","\n","    state = {'epoch': epoch, 'transformer': transformer, 'transformer_optimizer': transformer_optimizer}\n","    torch.save(state, directory + '/checkpoint_' + str(epoch) +'.pth.tar')\n","\n","    loss_history_lstm_transformer.append(loss_train)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import yaml\n","\n","with open(directory + '/loss_history_lstm_transformer.yaml', 'w') as file:\n","    yaml.dump(loss_history_lstm_transformer, file)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["directory = 'experiment_1'\n","checkpoint = torch.load(directory + '/checkpoint_99.pth.tar')\n","transformer = checkpoint['transformer']\n","\n","question = \"Visi FILKOM\" \n","max_len = 50\n","enc_qus = [word_map.get(word, word_map['<unk>']) for word in question.split()]\n","question = torch.LongTensor(enc_qus).to(device).unsqueeze(0)\n","question_mask = (question!=0).to(device).unsqueeze(1).unsqueeze(1)  \n","sentence = evaluate(transformer, question, question_mask, int(max_len), word_map)\n","print(sentence)"]},{"cell_type":"markdown","metadata":{},"source":["# Reset Cache"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","\n","torch.cuda.empty_cache()\n","\n","print(torch.cuda.memory_summary(device=None, abbreviated=False))"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30733,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"base","language":"python","name":"base"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"}},"nbformat":4,"nbformat_minor":4}
