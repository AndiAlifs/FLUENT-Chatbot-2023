{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pJAWnBFlkE2w"
   },
   "source": [
    "# LSTM Bot\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "In this project, you will build a chatbot that can converse with you at the command line. The chatbot will use a Sequence to Sequence text generation architecture with an LSTM as it's memory unit. You will also learn to use pretrained word embeddings to improve the performance of the model. At the conclusion of the project, you will be able to show your chatbot to potential employers.\n",
    "\n",
    "Additionally, you have the option to use pretrained word embeddings in your model. We have loaded Brown Embeddings from Gensim in the starter code below. You can compare the performance of your model with pre-trained embeddings against a model without the embeddings.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "A sequence to sequence model (Seq2Seq) has two components:\n",
    "- An Encoder consisting of an embedding layer and LSTM unit.\n",
    "- A Decoder consisting of an embedding layer, LSTM unit, and linear output unit.\n",
    "\n",
    "The Seq2Seq model works by accepting an input into the Encoder, passing the hidden state from the Encoder to the Decoder, which the Decoder uses to output a series of token predictions.\n",
    "\n",
    "## Dependencies\n",
    "\n",
    "- Pytorch\n",
    "- Numpy\n",
    "- Pandas\n",
    "- NLTK\n",
    "- Gzip\n",
    "- Gensim\n",
    "\n",
    "\n",
    "Please choose a dataset from the Torchtext website. We recommend looking at the Squad dataset first. Here is a link to the website where you can view your options:\n",
    "\n",
    "- https://pytorch.org/text/stable/datasets.html\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -mpy (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ping-extensions (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -mpy (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ping-extensions (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -0mpy (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -0ping-extensions (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -1ping-extensions (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -2ping-extensions (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -3ping-extensions (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -4ping-extensions (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -5ping-extensions (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -6ping-extensions (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -7ping-extensions (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -8ping-extensions (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -9ping-extensions (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -arl (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ertifi (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -latbuffers (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -raitlets (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -umpy (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -yping-extensions (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk) (2024.7.24)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from nltk) (4.64.0)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -mpy (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ping-extensions (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -mpy (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ping-extensions (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -0mpy (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -0ping-extensions (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -1ping-extensions (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -2ping-extensions (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -3ping-extensions (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -4ping-extensions (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -5ping-extensions (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -6ping-extensions (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -7ping-extensions (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -8ping-extensions (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -9ping-extensions (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -arl (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ertifi (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -latbuffers (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -raitlets (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -umpy (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -yping-extensions (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -mpy (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ping-extensions (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution - (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -mpy (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ping-extensions (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -0mpy (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -0ping-extensions (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -1ping-extensions (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -2ping-extensions (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -3ping-extensions (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -4ping-extensions (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -5ping-extensions (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -6ping-extensions (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -7ping-extensions (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -8ping-extensions (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -9ping-extensions (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -arl (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ertifi (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -latbuffers (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -raitlets (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -umpy (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -yping-extensions (/opt/conda/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import string\n",
    "import torch\n",
    "import neptune\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = nltk.stem.snowball.SnowballStemmer('english')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def getDict(dataPipe):\n",
    "\n",
    "    data_dict = {\n",
    "        'Question': [],\n",
    "        'Answer': []\n",
    "    }\n",
    "    \n",
    "    for _, question, answers, _ in dataPipe:\n",
    "        data_dict['Question'].append(question)\n",
    "        data_dict['Answer'].append(answers[0])\n",
    "        \n",
    "    return data_dict\n",
    "\n",
    "\n",
    "def loadDF(path):\n",
    "    # load data\n",
    "    train_data, val_data = torchtext.datasets.SQuAD2|(path)\n",
    "    \n",
    "    # convert dataPipe to dictionary \n",
    "    train_dict, val_dict = getDict(train_data), getDict(val_data)\n",
    "    \n",
    "    # convert Dictionaries to Pandas DataFrame\n",
    "    train_df = pd.DataFrame(train_dict)    \n",
    "    validation_df = pd.DataFrame(val_dict)    \n",
    "    \n",
    "    return train_df.append(validation_df)\n",
    "\n",
    "\n",
    "def prepare_text(sentence):\n",
    "    # clean text and tokenize it \n",
    "    sentence = ''.join([s.lower() for s in sentence if s not in string.punctuation])\n",
    "    sentence = ' '.join(stemmer.stem(w) for w in sentence.split())\n",
    "    tokens = nltk.tokenize.RegexpTokenizer(r'\\w+').tokenize(sentence)\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def toTensor(vocab, sentence):\n",
    "    # convert list of words \"sentence\" to a torch tensor of indices\n",
    "    indices = [vocab.word2index[word] for word in sentence.split(' ')]\n",
    "    indices.append(vocab.word2index[''])\n",
    "    return torch.Tensor(indices).long().to(device).view(-1, 1)\n",
    "\n",
    "\n",
    "def getPairs(df):\n",
    "    # convert df to list of pairs\n",
    "    temp1 = df['Question'].apply(lambda x: \" \".join(x) ).to_list()\n",
    "    temp2 = df['Answer'].apply(lambda x: \" \".join(x) ).to_list()\n",
    "    return [list(i) for i in zip(temp1, temp2)]\n",
    "\n",
    "\n",
    "def getMaxLen(pairs):\n",
    "    max_src = 0 \n",
    "    max_trg = 0\n",
    "    \n",
    "    for p in pairs:\n",
    "        max_src = len(p[0].split()) if len(p[0].split()) > max_src else max_src\n",
    "        max_trg = len(p[1].split()) if len(p[1].split()) > max_trg else max_trg\n",
    "        \n",
    "    return max_src, max_trg\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arsitektur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(self.input_size, self.hidden_size)\n",
    "        self.input = nn.Linear(self.input_size, self.hidden_size)\n",
    "        self.lstm = nn.LSTM(self.hidden_size, self.hidden_size)\n",
    "\n",
    "    def forward(self, x, hidden, cell_state):\n",
    "        x = self.embedding(x)\n",
    "        x = x.view(1, 1, -1)\n",
    "        x, (hidden, cell_state) = self.lstm(x, (hidden, cell_state))\n",
    "        return x, hidden, cell_state\n",
    "        \n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_size, self.hidden_size)\n",
    "        self.lstm = nn.LSTM(self.hidden_size, self.hidden_size)\n",
    "        self.fc = nn.Linear(self.hidden_size, self.output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim= 1)\n",
    "\n",
    "    def forward(self, x, hidden, cell_state):\n",
    "        x = self.embedding(x)\n",
    "        x = x.view(1, 1, -1)\n",
    "        x, (hidden, cell_state) = self.lstm(x, (hidden, cell_state))\n",
    "        x = self.softmax(self.fc(x[0]))\n",
    "        return x, hidden, cell_state\n",
    "    \n",
    "    \n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.encoder = Encoder(self.input_size, self.hidden_size)\n",
    "        self.decoder = Decoder(self.hidden_size, self.output_size)\n",
    "        \n",
    "    def forward(self, src, trg, src_len, trg_len, teacher_force=1):\n",
    "        \n",
    "        output = {\n",
    "            'decoder_output':[]\n",
    "        }\n",
    "        \n",
    "        encoder_hidden = torch.zeros([1, 1, self.hidden_size]).to(device) # 1 = number of LSTM layers\n",
    "        cell_state = torch.zeros([1, 1, self.hidden_size]).to(device)  \n",
    "        \n",
    "        for i in range(src_len):\n",
    "            encoder_output, encoder_hidden, cell_state = self.encoder(src[i], encoder_hidden, cell_state)\n",
    "\n",
    "        decoder_input = torch.Tensor([[0]]).long().to(device) # 0 = SOS_token\n",
    "        decoder_hidden = encoder_hidden\n",
    "        \n",
    "        for i in range(trg_len):\n",
    "            decoder_output, decoder_hidden, cell_state = self.decoder(decoder_input, decoder_hidden, cell_state)\n",
    "            output['decoder_output'].append(decoder_output)\n",
    "            \n",
    "            if self.training: # Model not in eval mode\n",
    "                decoder_input = target_tensor[i] if random.random() > teacher_force else decoder_output.argmax(1) # teacher forcing\n",
    "            else:\n",
    "                _, top_index = decoder_output.data.topk(1)\n",
    "                decoder_input = top_index.squeeze().detach()\n",
    "                \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = \"andialifs/siet-24\"\n",
    "api_token = \"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJjZTY2YWQ3My04OTBkLTQ2OWUtYTc1Ni1jYjk0MGZhMWFiNGEifQ==\"\n",
    "\n",
    "def neptune_init(name):\n",
    "    run = neptune.init_run(\n",
    "        project=project,\n",
    "        api_token=api_token,\n",
    "        name=name\n",
    "    )\n",
    "    return run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bleu(preds, questions, answers):\n",
    "    bleu_score_1 = 0\n",
    "    bleu_score_2 = 0\n",
    "    bleu_score_3 = 0\n",
    "    bleu_score_4 = 0\n",
    "    cum_bleu_score_1 = 0\n",
    "    cum_bleu_score_2 = 0\n",
    "    cum_bleu_score_3 = 0\n",
    "    cum_bleu_score_4 = 0\n",
    "\n",
    "    num_of_rows_calculated = 0\n",
    "    smoothing_function = SmoothingFunction().method1\n",
    "\n",
    "    for i, (question, real_answer) in enumerate(zip(questions, answers)):\n",
    "        # print(f\"Question: {question}\")\n",
    "        # print(f\"Real Answer: {real_answer}\")\n",
    "        # print(f\"Predicted Answer: {preds[i]}\")\n",
    "        refs = [real_answer.split(' ')]\n",
    "        hyp = preds[i].split(' ')\n",
    "\n",
    "        bleu_score_1 += sentence_bleu(refs, hyp, weights=(1,0,0,0), smoothing_function=smoothing_function)\n",
    "        bleu_score_2 += sentence_bleu(refs, hyp, weights=(0,1,0,0), smoothing_function=smoothing_function)\n",
    "        bleu_score_3 += sentence_bleu(refs, hyp, weights=(0,0,1,0), smoothing_function=smoothing_function)\n",
    "        bleu_score_4 += sentence_bleu(refs, hyp, weights=(0,0,0,1), smoothing_function=smoothing_function)\n",
    "        cum_bleu_score_1 += sentence_bleu(refs, hyp, weights=(1,0,0,0), smoothing_function=smoothing_function)\n",
    "        cum_bleu_score_2 += sentence_bleu(refs, hyp, weights=(0.5,0.5,0,0), smoothing_function=smoothing_function)\n",
    "        cum_bleu_score_3 += sentence_bleu(refs, hyp, weights=(0.33,0.33,0.33,0), smoothing_function=smoothing_function)\n",
    "        cum_bleu_score_4 += sentence_bleu(refs, hyp, weights=(0.25,0.25,0.25,0.25), smoothing_function=smoothing_function)\n",
    "\n",
    "        num_of_rows_calculated+=1\n",
    "\n",
    "    results = {\"1-gram\": (bleu_score_1/num_of_rows_calculated),\n",
    "                \"2-gram\": (bleu_score_2/num_of_rows_calculated),\n",
    "                \"3-gram\": (bleu_score_3/num_of_rows_calculated),\n",
    "                \"4-gram\": (bleu_score_4/num_of_rows_calculated),\n",
    "                \"cumulative-1-gram\": (cum_bleu_score_1/num_of_rows_calculated),\n",
    "                \"cumulative-2-gram\": (cum_bleu_score_2/num_of_rows_calculated),\n",
    "                \"cumulative-3-gram\": (cum_bleu_score_3/num_of_rows_calculated),\n",
    "                \"cumulative-4-gram\": (cum_bleu_score_4/num_of_rows_calculated)}\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "class Vocab:\n",
    "    def __init__(self):\n",
    "        self.word2index = {\"\": SOS_token, \"\": EOS_token}\n",
    "        self.index2word = {SOS_token: \"\", EOS_token: \"\"}\n",
    "        self.words_count = len(self.word2index)\n",
    "\n",
    "    def add_words(self, sentence):\n",
    "        for word in sentence.split(\" \"):\n",
    "            if word not in self.word2index:\n",
    "                self.word2index[word] = self.words_count\n",
    "                self.index2word[self.words_count] = word\n",
    "                self.words_count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "hidden_size = 512 # encoder and decoder hidden size\n",
    "batch_size = 1\n",
    "epochs = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# knowledgebase = pd.read_excel('https://raw.githubusercontent.com/AndiAlifs/FLUENT-Chatbot-2023/main/KnowledgeBaseFilkom.xlsx', engine='openpyxl')\n",
    "knowledgebase = pd.read_excel('https://raw.githubusercontent.com/AndiAlifs/FLUENT-Chatbot-2023/main/KnowledgeBaseFilkom_simple.xlsx', engine='openpyxl')\n",
    "knowledgebase.head()\n",
    "\n",
    "qa_paired = knowledgebase.drop(columns=knowledgebase.columns.drop(['Pertanyaan', 'Jawaban']))\n",
    "qa_paired.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "knowledgebase = pd.read_excel('https://raw.githubusercontent.com/AndiAlifs/FLUENT-Chatbot-2023/main/KnowledgeBaseFilkom_eval.xlsx', engine='openpyxl')\n",
    "knowledgebase.head()\n",
    "\n",
    "qa_paired_eval = knowledgebase.drop(columns=knowledgebase.columns.drop(['Pertanyaan', 'Jawaban']))\n",
    "qa_paired_eval.dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_df = loadDF('data')\n",
    "# I will take only the first 5,000 Q&A to avoid CUDA out of memory error due to the large dataset\n",
    "# data_df = data_df.iloc[:5000, :]\n",
    "data_df = pd.DataFrame(columns=['Question', 'Answer'])\n",
    "data_df['Question'] = qa_paired['Pertanyaan']\n",
    "data_df['Answer'] = qa_paired['Jawaban']\n",
    "\n",
    "data_df_eval = pd.DataFrame(columns=['Question', 'Answer'])\n",
    "data_df_eval['Question'] = qa_paired_eval['Pertanyaan']\n",
    "data_df_eval['Answer'] = qa_paired_eval['Jawaban']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df['Question'] = data_df['Question'].apply(prepare_text)\n",
    "data_df['Answer'] = data_df['Answer'].apply(prepare_text)\n",
    "data_df_eval['Question'] = data_df_eval['Question'].apply(prepare_text)\n",
    "data_df_eval['Answer'] = data_df_eval['Answer'].apply(prepare_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = getPairs(data_df)\n",
    "pairs_eval = getPairs(data_df_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(115, 12)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_src, max_trg = getMaxLen(pairs)\n",
    "max_trg, max_src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(115, 12)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_src_eval, max_trg_eval = getMaxLen(pairs_eval)\n",
    "max_trg_eval, max_src_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_vocab = Vocab()\n",
    "A_vocab = Vocab()\n",
    "\n",
    "# build vocabularies for questions \"source\" and answers \"target\"\n",
    "for pair in pairs:\n",
    "    Q_vocab.add_words(pair[0])\n",
    "    A_vocab.add_words(pair[1])\n",
    "\n",
    "for pair in pairs_eval:\n",
    "    Q_vocab.add_words(pair[0])\n",
    "    A_vocab.add_words(pair[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_data = [toTensor(Q_vocab, pair[0]) for pair in pairs]\n",
    "target_data = [toTensor(A_vocab, pair[1]) for pair in pairs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_eval = [pair[0] for pair in pairs_eval]\n",
    "answer_eval = [pair[1] for pair in pairs_eval]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Loss: 60.733802795410156\n",
      "Epoch: 2 Loss: 57.1324348449707\n",
      "Epoch: 3 Loss: 51.2673454284668\n",
      "Epoch: 4 Loss: 52.0035400390625\n",
      "Epoch: 5 Loss: 45.639198303222656\n",
      "Epoch: 6 Loss: 41.76565933227539\n",
      "Epoch: 7 Loss: 37.24412536621094\n",
      "Epoch: 8 Loss: 33.496158599853516\n",
      "Epoch: 9 Loss: 30.259422302246094\n",
      "Epoch: 10 Loss: 28.26426887512207\n",
      "BLEU Score: 0.0012412968213411754\n",
      "Epoch: 11 Loss: 24.945079803466797\n",
      "Epoch: 12 Loss: 23.381858825683594\n",
      "Epoch: 13 Loss: 22.78647804260254\n",
      "Epoch: 14 Loss: 19.299184799194336\n",
      "Epoch: 15 Loss: 19.583852767944336\n",
      "Epoch: 16 Loss: 16.876436233520508\n",
      "Epoch: 17 Loss: 16.496397018432617\n",
      "Epoch: 18 Loss: 14.794354438781738\n",
      "Epoch: 19 Loss: 13.754579544067383\n",
      "Epoch: 20 Loss: 13.908315658569336\n",
      "BLEU Score: 0.0041507483550962355\n",
      "Epoch: 21 Loss: 14.94377326965332\n",
      "Epoch: 22 Loss: 11.954492568969727\n",
      "Epoch: 23 Loss: 11.81442642211914\n",
      "Epoch: 24 Loss: 10.630568504333496\n",
      "Epoch: 25 Loss: 10.544401168823242\n",
      "Epoch: 26 Loss: 9.944260597229004\n",
      "Epoch: 27 Loss: 9.14657211303711\n",
      "Epoch: 28 Loss: 8.86144733428955\n",
      "Epoch: 29 Loss: 8.047239303588867\n",
      "Epoch: 30 Loss: 7.425749778747559\n",
      "BLEU Score: 0.006567933245473751\n",
      "Epoch: 31 Loss: 7.214859962463379\n",
      "Epoch: 32 Loss: 6.8017706871032715\n",
      "Epoch: 33 Loss: 6.250018119812012\n",
      "Epoch: 34 Loss: 5.907726764678955\n",
      "Epoch: 35 Loss: 5.8687520027160645\n",
      "Epoch: 36 Loss: 5.327624797821045\n",
      "Epoch: 37 Loss: 5.097014427185059\n",
      "Epoch: 38 Loss: 5.032478332519531\n",
      "Epoch: 39 Loss: 4.792829990386963\n",
      "Epoch: 40 Loss: 4.224206447601318\n",
      "BLEU Score: 0.004164852961637372\n",
      "Epoch: 41 Loss: 3.923640489578247\n",
      "Epoch: 42 Loss: 3.793046474456787\n",
      "Epoch: 43 Loss: 3.9111006259918213\n",
      "Epoch: 44 Loss: 3.508988380432129\n",
      "Epoch: 45 Loss: 3.4759936332702637\n",
      "Epoch: 46 Loss: 3.1748452186584473\n",
      "Epoch: 47 Loss: 2.9807286262512207\n",
      "Epoch: 48 Loss: 2.9329030513763428\n",
      "Epoch: 49 Loss: 2.752507448196411\n",
      "Epoch: 50 Loss: 2.556048631668091\n",
      "BLEU Score: 0.004164852961637372\n",
      "Epoch: 51 Loss: 2.456068754196167\n",
      "Epoch: 52 Loss: 2.421264171600342\n",
      "Epoch: 53 Loss: 2.2693045139312744\n",
      "Epoch: 54 Loss: 2.1153736114501953\n",
      "Epoch: 55 Loss: 2.0116546154022217\n",
      "Epoch: 56 Loss: 2.0026137828826904\n",
      "Epoch: 57 Loss: 1.917117953300476\n",
      "Epoch: 58 Loss: 1.809157371520996\n",
      "Epoch: 59 Loss: 1.7230095863342285\n",
      "Epoch: 60 Loss: 1.659283995628357\n",
      "BLEU Score: 0.004162661636064762\n",
      "Epoch: 61 Loss: 1.692938208580017\n",
      "Epoch: 62 Loss: 1.5826120376586914\n",
      "Epoch: 63 Loss: 1.4652540683746338\n",
      "Epoch: 64 Loss: 1.4046045541763306\n",
      "Epoch: 65 Loss: 1.3560771942138672\n",
      "Epoch: 66 Loss: 1.2899413108825684\n",
      "Epoch: 67 Loss: 1.2443068027496338\n",
      "Epoch: 68 Loss: 1.2420474290847778\n",
      "Epoch: 69 Loss: 1.2342197895050049\n",
      "Epoch: 70 Loss: 1.1471309661865234\n",
      "BLEU Score: 0.004162661636064762\n",
      "Epoch: 71 Loss: 1.0927622318267822\n",
      "Epoch: 72 Loss: 1.0523431301116943\n",
      "Epoch: 73 Loss: 1.012465238571167\n",
      "Epoch: 74 Loss: 0.9900928735733032\n",
      "Epoch: 75 Loss: 0.9621420502662659\n",
      "Epoch: 76 Loss: 0.922619104385376\n",
      "Epoch: 77 Loss: 0.8922547698020935\n",
      "Epoch: 78 Loss: 0.8606406450271606\n",
      "Epoch: 79 Loss: 0.8402946591377258\n",
      "Epoch: 80 Loss: 0.8139438033103943\n",
      "BLEU Score: 0.004162661636064762\n",
      "Epoch: 81 Loss: 0.7855461835861206\n",
      "Epoch: 82 Loss: 0.7528294324874878\n",
      "Epoch: 83 Loss: 0.7414808869361877\n",
      "Epoch: 84 Loss: 0.7632997035980225\n",
      "Epoch: 85 Loss: 0.7140549421310425\n",
      "Epoch: 86 Loss: 0.692283570766449\n",
      "Epoch: 87 Loss: 0.6589022874832153\n",
      "Epoch: 88 Loss: 0.6418945789337158\n",
      "Epoch: 89 Loss: 0.6252519488334656\n",
      "Epoch: 90 Loss: 0.6013020873069763\n",
      "BLEU Score: 0.004118647685365378\n",
      "Epoch: 91 Loss: 0.5917256474494934\n",
      "Epoch: 92 Loss: 0.573661208152771\n",
      "Epoch: 93 Loss: 0.5580818057060242\n",
      "Epoch: 94 Loss: 0.547481119632721\n",
      "Epoch: 95 Loss: 0.5301063060760498\n",
      "Epoch: 96 Loss: 0.5217018127441406\n",
      "Epoch: 97 Loss: 0.504743754863739\n",
      "Epoch: 98 Loss: 0.49977654218673706\n",
      "Epoch: 99 Loss: 0.4864763617515564\n",
      "Epoch: 100 Loss: 0.4738084077835083\n",
      "BLEU Score: 0.004112691044881114\n",
      "Epoch: 101 Loss: 0.46084415912628174\n",
      "Epoch: 102 Loss: 0.44247061014175415\n",
      "Epoch: 103 Loss: 0.43931499123573303\n",
      "Epoch: 104 Loss: 0.4292985796928406\n",
      "Epoch: 105 Loss: 0.4222644865512848\n",
      "Epoch: 106 Loss: 0.40982601046562195\n",
      "Epoch: 107 Loss: 0.4021623432636261\n",
      "Epoch: 108 Loss: 0.3909211754798889\n",
      "Epoch: 109 Loss: 0.38873276114463806\n",
      "Epoch: 110 Loss: 0.37977883219718933\n",
      "BLEU Score: 0.004118647685365378\n",
      "Epoch: 111 Loss: 0.3727509379386902\n",
      "Epoch: 112 Loss: 0.3591359257698059\n",
      "Epoch: 113 Loss: 0.3489854335784912\n",
      "Epoch: 114 Loss: 0.3387003540992737\n",
      "Epoch: 115 Loss: 0.33602991700172424\n",
      "Epoch: 116 Loss: 0.3270573914051056\n",
      "Epoch: 117 Loss: 0.32124751806259155\n",
      "Epoch: 118 Loss: 0.3116604685783386\n",
      "Epoch: 119 Loss: 0.30729082226753235\n",
      "Epoch: 120 Loss: 0.303289532661438\n",
      "BLEU Score: 0.004112691044881114\n",
      "Epoch: 121 Loss: 0.2947908341884613\n",
      "Epoch: 122 Loss: 0.2874656617641449\n",
      "Epoch: 123 Loss: 0.27995261549949646\n",
      "Epoch: 124 Loss: 0.27579477429389954\n",
      "Epoch: 125 Loss: 0.2678520083427429\n",
      "Epoch: 126 Loss: 0.265154629945755\n",
      "Epoch: 127 Loss: 0.2612350583076477\n",
      "Epoch: 128 Loss: 0.25977036356925964\n",
      "Epoch: 129 Loss: 0.2528490722179413\n",
      "Epoch: 130 Loss: 0.2475123554468155\n",
      "BLEU Score: 0.0017096107610447336\n",
      "Epoch: 131 Loss: 0.24173133075237274\n",
      "Epoch: 132 Loss: 0.2373216450214386\n",
      "Epoch: 133 Loss: 0.23320327699184418\n",
      "Epoch: 134 Loss: 0.2286003977060318\n",
      "Epoch: 135 Loss: 0.2183576226234436\n",
      "Epoch: 136 Loss: 0.21362636983394623\n",
      "Epoch: 137 Loss: 0.20957759022712708\n",
      "Epoch: 138 Loss: 0.2060825377702713\n",
      "Epoch: 139 Loss: 0.19982047379016876\n",
      "Epoch: 140 Loss: 0.1987381875514984\n",
      "BLEU Score: 0.0017096107610447336\n",
      "Epoch: 141 Loss: 0.19433213770389557\n",
      "Epoch: 142 Loss: 0.19009646773338318\n",
      "Epoch: 143 Loss: 0.18658648431301117\n",
      "Epoch: 144 Loss: 0.18467962741851807\n",
      "Epoch: 145 Loss: 0.18489794433116913\n",
      "Epoch: 146 Loss: 0.178479865193367\n",
      "Epoch: 147 Loss: 0.1737722009420395\n",
      "Epoch: 148 Loss: 0.17042171955108643\n",
      "Epoch: 149 Loss: 0.16619952023029327\n",
      "Epoch: 150 Loss: 0.1613718867301941\n",
      "BLEU Score: 0.0017096107610447336\n",
      "Epoch: 151 Loss: 0.15951506793498993\n",
      "Epoch: 152 Loss: 0.1556652933359146\n",
      "Epoch: 153 Loss: 0.15242993831634521\n",
      "Epoch: 154 Loss: 0.15251034498214722\n",
      "Epoch: 155 Loss: 0.14769989252090454\n",
      "Epoch: 156 Loss: 0.14585775136947632\n",
      "Epoch: 157 Loss: 0.142610102891922\n",
      "Epoch: 158 Loss: 0.14041852951049805\n",
      "Epoch: 159 Loss: 0.13936780393123627\n",
      "Epoch: 160 Loss: 0.13392120599746704\n",
      "BLEU Score: 0.0017096107610447336\n",
      "Epoch: 161 Loss: 0.1356741487979889\n",
      "Epoch: 162 Loss: 0.1317361742258072\n",
      "Epoch: 163 Loss: 0.1292099505662918\n",
      "Epoch: 164 Loss: 0.12670908868312836\n",
      "Epoch: 165 Loss: 0.12458821386098862\n",
      "Epoch: 166 Loss: 0.12146302312612534\n",
      "Epoch: 167 Loss: 0.12156268209218979\n",
      "Epoch: 168 Loss: 0.11650178581476212\n",
      "Epoch: 169 Loss: 0.11722077429294586\n",
      "Epoch: 170 Loss: 0.11376935988664627\n",
      "BLEU Score: 0.0017096107610447336\n",
      "Epoch: 171 Loss: 0.11386550962924957\n",
      "Epoch: 172 Loss: 0.10999921709299088\n",
      "Epoch: 173 Loss: 0.10955283045768738\n",
      "Epoch: 174 Loss: 0.1068621277809143\n",
      "Epoch: 175 Loss: 0.1062338575720787\n",
      "Epoch: 176 Loss: 0.10319244116544724\n",
      "Epoch: 177 Loss: 0.1054777130484581\n",
      "Epoch: 178 Loss: 0.10032682865858078\n",
      "Epoch: 179 Loss: 0.09937990456819534\n",
      "Epoch: 180 Loss: 0.09784020483493805\n",
      "BLEU Score: 0.0017096107610447336\n",
      "Epoch: 181 Loss: 0.0961851254105568\n",
      "Epoch: 182 Loss: 0.09453436732292175\n",
      "Epoch: 183 Loss: 0.09186039865016937\n",
      "Epoch: 184 Loss: 0.09220090508460999\n",
      "Epoch: 185 Loss: 0.09000237286090851\n",
      "Epoch: 186 Loss: 0.08910326659679413\n",
      "Epoch: 187 Loss: 0.08621566742658615\n",
      "Epoch: 188 Loss: 0.08692849427461624\n",
      "Epoch: 189 Loss: 0.08232605457305908\n",
      "Epoch: 190 Loss: 0.08510998636484146\n",
      "BLEU Score: 0.0017096107610447336\n",
      "Epoch: 191 Loss: 0.08032643049955368\n",
      "Epoch: 192 Loss: 0.0795547291636467\n",
      "Epoch: 193 Loss: 0.07763218134641647\n",
      "Epoch: 194 Loss: 0.0764441266655922\n",
      "Epoch: 195 Loss: 0.07792706787586212\n",
      "Epoch: 196 Loss: 0.0750773623585701\n",
      "Epoch: 197 Loss: 0.0791405439376831\n",
      "Epoch: 198 Loss: 0.07440343499183655\n",
      "Epoch: 199 Loss: 0.07276659458875656\n",
      "Epoch: 200 Loss: 0.07101670652627945\n",
      "BLEU Score: 0.0017096107610447336\n",
      "Epoch: 201 Loss: 0.0696382224559784\n",
      "Epoch: 202 Loss: 0.06670863181352615\n",
      "Epoch: 203 Loss: 0.0698101744055748\n",
      "Epoch: 204 Loss: 0.06710699200630188\n",
      "Epoch: 205 Loss: 0.06814783066511154\n",
      "Epoch: 206 Loss: 0.06615986675024033\n",
      "Epoch: 207 Loss: 0.06785368919372559\n",
      "Epoch: 208 Loss: 0.06571855396032333\n",
      "Epoch: 209 Loss: 0.06605914980173111\n",
      "Epoch: 210 Loss: 0.06470265984535217\n",
      "BLEU Score: 0.0017096107610447336\n",
      "Epoch: 211 Loss: 0.06662068516016006\n",
      "Epoch: 212 Loss: 0.06377905607223511\n",
      "Epoch: 213 Loss: 0.06426180154085159\n",
      "Epoch: 214 Loss: 0.06066911667585373\n",
      "Epoch: 215 Loss: 0.06023070216178894\n",
      "Epoch: 216 Loss: 0.06333506107330322\n",
      "Epoch: 217 Loss: 0.06349830329418182\n",
      "Epoch: 218 Loss: 0.060002584010362625\n",
      "Epoch: 219 Loss: 0.059267621487379074\n",
      "Epoch: 220 Loss: 0.056276269257068634\n",
      "BLEU Score: 0.0017096107610447336\n",
      "Epoch: 221 Loss: 0.0555046945810318\n",
      "Epoch: 222 Loss: 0.053444478660821915\n",
      "Epoch: 223 Loss: 0.05345640331506729\n",
      "Epoch: 224 Loss: 0.05145328491926193\n",
      "Epoch: 225 Loss: 0.05215044692158699\n",
      "Epoch: 226 Loss: 0.050404492765665054\n",
      "Epoch: 227 Loss: 0.04991047456860542\n",
      "Epoch: 228 Loss: 0.04793516919016838\n",
      "Epoch: 229 Loss: 0.04820666462182999\n",
      "Epoch: 230 Loss: 0.045905329287052155\n",
      "BLEU Score: 0.0017096107610447336\n",
      "Epoch: 231 Loss: 0.04563029855489731\n",
      "Epoch: 232 Loss: 0.0442066490650177\n",
      "Epoch: 233 Loss: 0.04433407634496689\n",
      "Epoch: 234 Loss: 0.04345645755529404\n",
      "Epoch: 235 Loss: 0.04302462935447693\n",
      "Epoch: 236 Loss: 0.04178173467516899\n",
      "Epoch: 237 Loss: 0.04173911362886429\n",
      "Epoch: 238 Loss: 0.04102904722094536\n",
      "Epoch: 239 Loss: 0.0403137281537056\n",
      "Epoch: 240 Loss: 0.03929635509848595\n",
      "BLEU Score: 0.0017096107610447336\n",
      "Epoch: 241 Loss: 0.038597472012043\n",
      "Epoch: 242 Loss: 0.038313306868076324\n",
      "Epoch: 243 Loss: 0.037090592086315155\n",
      "Epoch: 244 Loss: 0.03683818504214287\n",
      "Epoch: 245 Loss: 0.03594980388879776\n",
      "Epoch: 246 Loss: 0.03493499383330345\n",
      "Epoch: 247 Loss: 0.03442602977156639\n",
      "Epoch: 248 Loss: 0.034020304679870605\n",
      "Epoch: 249 Loss: 0.03317385911941528\n",
      "Epoch: 250 Loss: 0.03308218717575073\n",
      "BLEU Score: 0.0017096107610447336\n",
      "Epoch: 251 Loss: 0.03200352191925049\n",
      "Epoch: 252 Loss: 0.031445879489183426\n",
      "Epoch: 253 Loss: 0.0309760682284832\n",
      "Epoch: 254 Loss: 0.030387338250875473\n",
      "Epoch: 255 Loss: 0.03125175088644028\n",
      "Epoch: 256 Loss: 0.03056609258055687\n",
      "Epoch: 257 Loss: 0.030716894194483757\n",
      "Epoch: 258 Loss: 0.029332419857382774\n",
      "Epoch: 259 Loss: 0.028603823855519295\n",
      "Epoch: 260 Loss: 0.027814028784632683\n",
      "BLEU Score: 0.0017096107610447336\n",
      "Epoch: 261 Loss: 0.027101954445242882\n",
      "Epoch: 262 Loss: 0.026426181197166443\n",
      "Epoch: 263 Loss: 0.025979183614253998\n",
      "Epoch: 264 Loss: 0.025402778759598732\n",
      "Epoch: 265 Loss: 0.02499907836318016\n",
      "Epoch: 266 Loss: 0.024403231218457222\n",
      "Epoch: 267 Loss: 0.023846400901675224\n",
      "Epoch: 268 Loss: 0.02336491271853447\n",
      "Epoch: 269 Loss: 0.022936534136533737\n",
      "Epoch: 270 Loss: 0.022428082302212715\n",
      "BLEU Score: 0.0017096107610447336\n",
      "Epoch: 271 Loss: 0.021982107311487198\n",
      "Epoch: 272 Loss: 0.021597109735012054\n",
      "Epoch: 273 Loss: 0.021225284785032272\n",
      "Epoch: 274 Loss: 0.020831109955906868\n",
      "Epoch: 275 Loss: 0.020677465945482254\n",
      "Epoch: 276 Loss: 0.02019214630126953\n",
      "Epoch: 277 Loss: 0.019904185086488724\n",
      "Epoch: 278 Loss: 0.019424939528107643\n",
      "Epoch: 279 Loss: 0.01899309642612934\n",
      "Epoch: 280 Loss: 0.018575062975287437\n",
      "BLEU Score: 0.0017096107610447336\n",
      "Epoch: 281 Loss: 0.018404923379421234\n",
      "Epoch: 282 Loss: 0.018224084749817848\n",
      "Epoch: 283 Loss: 0.017989089712500572\n",
      "Epoch: 284 Loss: 0.01762334816157818\n",
      "Epoch: 285 Loss: 0.01729830726981163\n",
      "Epoch: 286 Loss: 0.016945360228419304\n",
      "Epoch: 287 Loss: 0.016703123226761818\n",
      "Epoch: 288 Loss: 0.016467148438096046\n",
      "Epoch: 289 Loss: 0.01632852293550968\n",
      "Epoch: 290 Loss: 0.01593697816133499\n",
      "BLEU Score: 0.0017096107610447336\n",
      "Epoch: 291 Loss: 0.015725750476121902\n",
      "Epoch: 292 Loss: 0.015371346846222878\n",
      "Epoch: 293 Loss: 0.015114343725144863\n",
      "Epoch: 294 Loss: 0.014861850999295712\n",
      "Epoch: 295 Loss: 0.014669734053313732\n",
      "Epoch: 296 Loss: 0.01435048971325159\n",
      "Epoch: 297 Loss: 0.01406089123338461\n",
      "Epoch: 298 Loss: 0.013750119134783745\n",
      "Epoch: 299 Loss: 0.013545180670917034\n",
      "Epoch: 300 Loss: 0.013399346731603146\n",
      "BLEU Score: 0.0017096053292917008\n",
      "Epoch: 301 Loss: 0.0145198293030262\n",
      "Epoch: 302 Loss: 0.0358564630150795\n",
      "Epoch: 303 Loss: 0.034726861864328384\n",
      "Epoch: 304 Loss: 2.039945363998413\n",
      "Epoch: 305 Loss: 0.2615062892436981\n",
      "Epoch: 306 Loss: 0.08277882635593414\n",
      "Epoch: 307 Loss: 0.07475385814905167\n",
      "Epoch: 308 Loss: 0.057313449680805206\n",
      "Epoch: 309 Loss: 0.05187559127807617\n",
      "Epoch: 310 Loss: 0.04680658131837845\n",
      "BLEU Score: 0.004118647685365378\n",
      "Epoch: 311 Loss: 0.042841799557209015\n",
      "Epoch: 312 Loss: 0.04011311009526253\n",
      "Epoch: 313 Loss: 0.03786104544997215\n",
      "Epoch: 314 Loss: 0.03555277734994888\n",
      "Epoch: 315 Loss: 0.03378422185778618\n",
      "Epoch: 316 Loss: 0.03225858509540558\n",
      "Epoch: 317 Loss: 0.030866369605064392\n",
      "Epoch: 318 Loss: 0.029620712623000145\n",
      "Epoch: 319 Loss: 0.028527595102787018\n",
      "Epoch: 320 Loss: 0.027424976229667664\n",
      "BLEU Score: 0.0017096107610447336\n",
      "Epoch: 321 Loss: 0.0264789666980505\n",
      "Epoch: 322 Loss: 0.025614440441131592\n",
      "Epoch: 323 Loss: 0.024742407724261284\n",
      "Epoch: 324 Loss: 0.024048926308751106\n",
      "Epoch: 325 Loss: 0.02340214140713215\n",
      "Epoch: 326 Loss: 0.02275008335709572\n",
      "Epoch: 327 Loss: 0.022171830758452415\n",
      "Epoch: 328 Loss: 0.02166019007563591\n",
      "Epoch: 329 Loss: 0.021068356931209564\n",
      "Epoch: 330 Loss: 0.020597495138645172\n",
      "BLEU Score: 0.0017096107610447336\n",
      "Epoch: 331 Loss: 0.020166553556919098\n",
      "Epoch: 332 Loss: 0.019732292741537094\n",
      "Epoch: 333 Loss: 0.019341763108968735\n",
      "Epoch: 334 Loss: 0.01897537335753441\n",
      "Epoch: 335 Loss: 0.018547819927334785\n",
      "Epoch: 336 Loss: 0.018163222819566727\n",
      "Epoch: 337 Loss: 0.01779535599052906\n",
      "Epoch: 338 Loss: 0.017402660101652145\n",
      "Epoch: 339 Loss: 0.017046555876731873\n",
      "Epoch: 340 Loss: 0.016731521114706993\n",
      "BLEU Score: 0.0017096107610447336\n",
      "Epoch: 341 Loss: 0.01639579050242901\n",
      "Epoch: 342 Loss: 0.016070282086730003\n",
      "Epoch: 343 Loss: 0.01577809453010559\n",
      "Epoch: 344 Loss: 0.015512432903051376\n",
      "Epoch: 345 Loss: 0.015227899886667728\n",
      "Epoch: 346 Loss: 0.014945736154913902\n",
      "Epoch: 347 Loss: 0.014670480974018574\n",
      "Epoch: 348 Loss: 0.014400206506252289\n",
      "Epoch: 349 Loss: 0.014117461629211903\n",
      "Epoch: 350 Loss: 0.013846898451447487\n",
      "BLEU Score: 0.0017096107610447336\n",
      "Epoch: 351 Loss: 0.013607867062091827\n",
      "Epoch: 352 Loss: 0.013367768377065659\n",
      "Epoch: 353 Loss: 0.013116245158016682\n",
      "Epoch: 354 Loss: 0.012876808643341064\n",
      "Epoch: 355 Loss: 0.012639922089874744\n",
      "Epoch: 356 Loss: 0.01242013368755579\n",
      "Epoch: 357 Loss: 0.012175383977591991\n",
      "Epoch: 358 Loss: 0.011953828856348991\n",
      "Epoch: 359 Loss: 0.01175282709300518\n",
      "Epoch: 360 Loss: 0.011545499786734581\n",
      "BLEU Score: 0.0017096107610447336\n",
      "Epoch: 361 Loss: 0.01133603136986494\n",
      "Epoch: 362 Loss: 0.011142618954181671\n",
      "Epoch: 363 Loss: 0.010945877991616726\n",
      "Epoch: 364 Loss: 0.010755551047623158\n",
      "Epoch: 365 Loss: 0.01057224627584219\n",
      "Epoch: 366 Loss: 0.010390468873083591\n",
      "Epoch: 367 Loss: 0.010210968554019928\n",
      "Epoch: 368 Loss: 0.010039662942290306\n",
      "Epoch: 369 Loss: 0.009870504960417747\n",
      "Epoch: 370 Loss: 0.009706332348287106\n",
      "BLEU Score: 0.0017096107610447336\n",
      "Epoch: 371 Loss: 0.009544307366013527\n",
      "Epoch: 372 Loss: 0.009386438876390457\n",
      "Epoch: 373 Loss: 0.009231427684426308\n",
      "Epoch: 374 Loss: 0.009081052616238594\n",
      "Epoch: 375 Loss: 0.008931271731853485\n",
      "Epoch: 376 Loss: 0.008786248043179512\n",
      "Epoch: 377 Loss: 0.008642890490591526\n",
      "Epoch: 378 Loss: 0.008502503857016563\n",
      "Epoch: 379 Loss: 0.008364737033843994\n",
      "Epoch: 380 Loss: 0.00822815578430891\n",
      "BLEU Score: 0.0017096107610447336\n",
      "Epoch: 381 Loss: 0.008094312623143196\n",
      "Epoch: 382 Loss: 0.007963323034346104\n",
      "Epoch: 383 Loss: 0.007834358140826225\n",
      "Epoch: 384 Loss: 0.0077083660289645195\n",
      "Epoch: 385 Loss: 0.007584040518850088\n",
      "Epoch: 386 Loss: 0.00746114132925868\n",
      "Epoch: 387 Loss: 0.007340265437960625\n",
      "Epoch: 388 Loss: 0.007221650332212448\n",
      "Epoch: 389 Loss: 0.007103986106812954\n",
      "Epoch: 390 Loss: 0.0069872732274234295\n",
      "BLEU Score: 0.0017096107610447336\n",
      "Epoch: 391 Loss: 0.006873536389321089\n",
      "Epoch: 392 Loss: 0.006760630290955305\n",
      "Epoch: 393 Loss: 0.0066504632122814655\n",
      "Epoch: 394 Loss: 0.006541484501212835\n",
      "Epoch: 395 Loss: 0.00643500592559576\n",
      "Epoch: 396 Loss: 0.006330192554742098\n",
      "Epoch: 397 Loss: 0.00622668769210577\n",
      "Epoch: 398 Loss: 0.006124731153249741\n",
      "Epoch: 399 Loss: 0.006024796981364489\n",
      "Epoch: 400 Loss: 0.005925338249653578\n",
      "BLEU Score: 0.0017096107610447336\n",
      "Epoch: 401 Loss: 0.005827903747558594\n",
      "Epoch: 402 Loss: 0.005732014775276184\n",
      "Epoch: 403 Loss: 0.0056371986865997314\n",
      "Epoch: 404 Loss: 0.005544405896216631\n",
      "Epoch: 405 Loss: 0.0054525649175047874\n",
      "Epoch: 406 Loss: 0.005362628493458033\n",
      "Epoch: 407 Loss: 0.0052741216495633125\n",
      "Epoch: 408 Loss: 0.005187399219721556\n",
      "Epoch: 409 Loss: 0.005101867485791445\n",
      "Epoch: 410 Loss: 0.005016454495489597\n",
      "BLEU Score: 0.0017096107610447336\n",
      "Epoch: 411 Loss: 0.004933898337185383\n",
      "Epoch: 412 Loss: 0.004851342644542456\n",
      "Epoch: 413 Loss: 0.004771764390170574\n",
      "Epoch: 414 Loss: 0.0046915882267057896\n",
      "Epoch: 415 Loss: 0.004613914526998997\n",
      "Epoch: 416 Loss: 0.0045355260372161865\n",
      "Epoch: 417 Loss: 0.004456305876374245\n",
      "Epoch: 418 Loss: 0.004381249658763409\n",
      "Epoch: 419 Loss: 0.004303219262510538\n",
      "Epoch: 420 Loss: 0.0042299493215978146\n",
      "BLEU Score: 0.0017096107610447336\n",
      "Epoch: 421 Loss: 0.004156322684139013\n",
      "Epoch: 422 Loss: 0.004085910506546497\n",
      "Epoch: 423 Loss: 0.004014426376670599\n",
      "Epoch: 424 Loss: 0.003947347868233919\n",
      "Epoch: 425 Loss: 0.0038782446645200253\n",
      "Epoch: 426 Loss: 0.003813785267993808\n",
      "Epoch: 427 Loss: 0.0037457540165632963\n",
      "Epoch: 428 Loss: 0.0036834392230957747\n",
      "Epoch: 429 Loss: 0.0036182659678161144\n",
      "Epoch: 430 Loss: 0.0035579747054725885\n",
      "BLEU Score: 0.0017096107610447336\n",
      "Epoch: 431 Loss: 0.0034953022841364145\n",
      "Epoch: 432 Loss: 0.0034367977641522884\n",
      "Epoch: 433 Loss: 0.0033757928758859634\n",
      "Epoch: 434 Loss: 0.0033193128183484077\n",
      "Epoch: 435 Loss: 0.0032599749974906445\n",
      "Epoch: 436 Loss: 0.0032055203337222338\n",
      "Epoch: 437 Loss: 0.003147849813103676\n",
      "Epoch: 438 Loss: 0.0030948242638260126\n",
      "Epoch: 439 Loss: 0.0030390587635338306\n",
      "Epoch: 440 Loss: 0.0029884157702326775\n",
      "BLEU Score: 0.0017096107610447336\n",
      "Epoch: 441 Loss: 0.0029343185015022755\n",
      "Epoch: 442 Loss: 0.00288474652916193\n",
      "Epoch: 443 Loss: 0.00283231632784009\n",
      "Epoch: 444 Loss: 0.002783816773444414\n",
      "Epoch: 445 Loss: 0.0027337693609297276\n",
      "Epoch: 446 Loss: 0.0026865797117352486\n",
      "Epoch: 447 Loss: 0.002637722762301564\n",
      "Epoch: 448 Loss: 0.0025920819025486708\n",
      "Epoch: 449 Loss: 0.0025451311375945807\n",
      "Epoch: 450 Loss: 0.002500919857993722\n",
      "BLEU Score: 0.0017096107610447336\n",
      "Epoch: 451 Loss: 0.002455041278153658\n",
      "Epoch: 452 Loss: 0.002412497065961361\n",
      "Epoch: 453 Loss: 0.002368286484852433\n",
      "Epoch: 454 Loss: 0.0023268144577741623\n",
      "Epoch: 455 Loss: 0.0022840327583253384\n",
      "Epoch: 456 Loss: 0.002244706032797694\n",
      "Epoch: 457 Loss: 0.002202758565545082\n",
      "Epoch: 458 Loss: 0.0021647417452186346\n",
      "Epoch: 459 Loss: 0.002124104415997863\n",
      "Epoch: 460 Loss: 0.0020868023857474327\n",
      "BLEU Score: 0.0017096107610447336\n",
      "Epoch: 461 Loss: 0.002047952264547348\n",
      "Epoch: 462 Loss: 0.0020124372094869614\n",
      "Epoch: 463 Loss: 0.00197477824985981\n",
      "Epoch: 464 Loss: 0.001939978334121406\n",
      "Epoch: 465 Loss: 0.0019038681639358401\n",
      "Epoch: 466 Loss: 0.0018701405497267842\n",
      "Epoch: 467 Loss: 0.0018346256110817194\n",
      "Epoch: 468 Loss: 0.0018024472519755363\n",
      "Epoch: 469 Loss: 0.0017683622427284718\n",
      "Epoch: 470 Loss: 0.0017361834179610014\n",
      "BLEU Score: 0.0017096107610447336\n",
      "Epoch: 471 Loss: 0.0017041242681443691\n",
      "Epoch: 472 Loss: 0.0016736136749386787\n",
      "Epoch: 473 Loss: 0.00164167326875031\n",
      "Epoch: 474 Loss: 0.00161235430277884\n",
      "Epoch: 475 Loss: 0.0015817247331142426\n",
      "Epoch: 476 Loss: 0.0015533589757978916\n",
      "Epoch: 477 Loss: 0.0015239212661981583\n",
      "Epoch: 478 Loss: 0.0014960318803787231\n",
      "Epoch: 479 Loss: 0.0014674278208985925\n",
      "Epoch: 480 Loss: 0.0014407302951440215\n",
      "BLEU Score: 0.0017096107610447336\n",
      "Epoch: 481 Loss: 0.0014136756071820855\n",
      "Epoch: 482 Loss: 0.0013876931043341756\n",
      "Epoch: 483 Loss: 0.0013612339971587062\n",
      "Epoch: 484 Loss: 0.0013362047029659152\n",
      "Epoch: 485 Loss: 0.0013102220837026834\n",
      "Epoch: 486 Loss: 0.0012866230681538582\n",
      "Epoch: 487 Loss: 0.0012619512854143977\n",
      "Epoch: 488 Loss: 0.00123882875777781\n",
      "Epoch: 489 Loss: 0.0012147529050707817\n",
      "Epoch: 490 Loss: 0.0011923452839255333\n",
      "BLEU Score: 0.0017096107610447336\n",
      "Epoch: 491 Loss: 0.0011688652448356152\n",
      "Epoch: 492 Loss: 0.001147053437307477\n",
      "Epoch: 493 Loss: 0.0011244076304137707\n",
      "Epoch: 494 Loss: 0.0011039068922400475\n",
      "Epoch: 495 Loss: 0.001081499271094799\n",
      "Epoch: 496 Loss: 0.0010614751372486353\n",
      "Epoch: 497 Loss: 0.0010401400504633784\n",
      "Epoch: 498 Loss: 0.0010209500323981047\n",
      "Epoch: 499 Loss: 0.0010010452242568135\n",
      "Epoch: 500 Loss: 0.0009818552061915398\n",
      "BLEU Score: 0.0017096107610447336\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "epoch = 500\n",
    "learning_rate = 0.0001\n",
    "\n",
    "model = Seq2Seq(Q_vocab.words_count, hidden_size, A_vocab.words_count)\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "bleu_score = pd.DataFrame(columns=['epoch', '1-gram', '2-gram', '3-gram', '4-gram', 'cumulative-1-gram', 'cumulative-2-gram', 'cumulative-3-gram', 'cumulative-4-gram'])\n",
    "\n",
    "for i in range(epoch):\n",
    "    for j in range(len(source_data)):\n",
    "        src = source_data[j]\n",
    "        trg = target_data[j]\n",
    "        \n",
    "        src_len = src.size(0)\n",
    "        trg_len = trg.size(0)\n",
    "        \n",
    "        output = model(src, trg, src_len, trg_len)\n",
    "        \n",
    "        loss = 0\n",
    "        for k in range(trg_len):\n",
    "            loss += criterion(output['decoder_output'][k], trg[k])\n",
    "            \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    print(f'Epoch: {i+1} Loss: {loss.item()}')\n",
    "    if (i+1) % 10 == 0:\n",
    "        preds = []\n",
    "        for j in range(len(question_eval)):\n",
    "            src = toTensor(Q_vocab, question_eval[j])\n",
    "            src_len = src.size(0)\n",
    "            trg_len = 30\n",
    "\n",
    "            output = model(src, src, src_len, trg_len)\n",
    "            pred = [A_vocab.index2word[i.item()] for i in output['decoder_output'][0].argmax(1)]\n",
    "            pred = ' '.join(pred)\n",
    "            preds.append(pred)\n",
    "\n",
    "        results = calculate_bleu(preds, question_eval, answer_eval)\n",
    "        bleu_score = pd.concat([bleu_score, pd.DataFrame([{'epoch': i+1, **results}])])\n",
    "        print(f'BLEU Score: {results[\"cumulative-4-gram\"]}')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>1-gram</th>\n",
       "      <th>2-gram</th>\n",
       "      <th>3-gram</th>\n",
       "      <th>4-gram</th>\n",
       "      <th>cumulative-1-gram</th>\n",
       "      <th>cumulative-2-gram</th>\n",
       "      <th>cumulative-3-gram</th>\n",
       "      <th>cumulative-4-gram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>0.006980</td>\n",
       "      <td>0.000698</td>\n",
       "      <td>0.000698</td>\n",
       "      <td>0.000698</td>\n",
       "      <td>0.006980</td>\n",
       "      <td>0.002207</td>\n",
       "      <td>0.001527</td>\n",
       "      <td>0.001241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>0.023341</td>\n",
       "      <td>0.002334</td>\n",
       "      <td>0.002334</td>\n",
       "      <td>0.002334</td>\n",
       "      <td>0.023341</td>\n",
       "      <td>0.007381</td>\n",
       "      <td>0.005107</td>\n",
       "      <td>0.004151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30</td>\n",
       "      <td>0.036934</td>\n",
       "      <td>0.003693</td>\n",
       "      <td>0.003693</td>\n",
       "      <td>0.003693</td>\n",
       "      <td>0.036934</td>\n",
       "      <td>0.011680</td>\n",
       "      <td>0.008080</td>\n",
       "      <td>0.006568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40</td>\n",
       "      <td>0.023421</td>\n",
       "      <td>0.002342</td>\n",
       "      <td>0.002342</td>\n",
       "      <td>0.002342</td>\n",
       "      <td>0.023421</td>\n",
       "      <td>0.007406</td>\n",
       "      <td>0.005124</td>\n",
       "      <td>0.004165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>0.023421</td>\n",
       "      <td>0.002342</td>\n",
       "      <td>0.002342</td>\n",
       "      <td>0.002342</td>\n",
       "      <td>0.023421</td>\n",
       "      <td>0.007406</td>\n",
       "      <td>0.005124</td>\n",
       "      <td>0.004165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60</td>\n",
       "      <td>0.023408</td>\n",
       "      <td>0.002341</td>\n",
       "      <td>0.002341</td>\n",
       "      <td>0.002341</td>\n",
       "      <td>0.023408</td>\n",
       "      <td>0.007402</td>\n",
       "      <td>0.005121</td>\n",
       "      <td>0.004163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70</td>\n",
       "      <td>0.023408</td>\n",
       "      <td>0.002341</td>\n",
       "      <td>0.002341</td>\n",
       "      <td>0.002341</td>\n",
       "      <td>0.023408</td>\n",
       "      <td>0.007402</td>\n",
       "      <td>0.005121</td>\n",
       "      <td>0.004163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>80</td>\n",
       "      <td>0.023408</td>\n",
       "      <td>0.002341</td>\n",
       "      <td>0.002341</td>\n",
       "      <td>0.002341</td>\n",
       "      <td>0.023408</td>\n",
       "      <td>0.007402</td>\n",
       "      <td>0.005121</td>\n",
       "      <td>0.004163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>90</td>\n",
       "      <td>0.023161</td>\n",
       "      <td>0.002316</td>\n",
       "      <td>0.002316</td>\n",
       "      <td>0.002316</td>\n",
       "      <td>0.023161</td>\n",
       "      <td>0.007324</td>\n",
       "      <td>0.005067</td>\n",
       "      <td>0.004119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>0.023127</td>\n",
       "      <td>0.002313</td>\n",
       "      <td>0.002313</td>\n",
       "      <td>0.002313</td>\n",
       "      <td>0.023127</td>\n",
       "      <td>0.007314</td>\n",
       "      <td>0.005060</td>\n",
       "      <td>0.004113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>110</td>\n",
       "      <td>0.023161</td>\n",
       "      <td>0.002316</td>\n",
       "      <td>0.002316</td>\n",
       "      <td>0.002316</td>\n",
       "      <td>0.023161</td>\n",
       "      <td>0.007324</td>\n",
       "      <td>0.005067</td>\n",
       "      <td>0.004119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>120</td>\n",
       "      <td>0.023127</td>\n",
       "      <td>0.002313</td>\n",
       "      <td>0.002313</td>\n",
       "      <td>0.002313</td>\n",
       "      <td>0.023127</td>\n",
       "      <td>0.007314</td>\n",
       "      <td>0.005060</td>\n",
       "      <td>0.004113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>130</td>\n",
       "      <td>0.009614</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.009614</td>\n",
       "      <td>0.003040</td>\n",
       "      <td>0.002103</td>\n",
       "      <td>0.001710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>140</td>\n",
       "      <td>0.009614</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.009614</td>\n",
       "      <td>0.003040</td>\n",
       "      <td>0.002103</td>\n",
       "      <td>0.001710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>150</td>\n",
       "      <td>0.009614</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.009614</td>\n",
       "      <td>0.003040</td>\n",
       "      <td>0.002103</td>\n",
       "      <td>0.001710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>160</td>\n",
       "      <td>0.009614</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.009614</td>\n",
       "      <td>0.003040</td>\n",
       "      <td>0.002103</td>\n",
       "      <td>0.001710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>170</td>\n",
       "      <td>0.009614</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.009614</td>\n",
       "      <td>0.003040</td>\n",
       "      <td>0.002103</td>\n",
       "      <td>0.001710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>180</td>\n",
       "      <td>0.009614</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.009614</td>\n",
       "      <td>0.003040</td>\n",
       "      <td>0.002103</td>\n",
       "      <td>0.001710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>190</td>\n",
       "      <td>0.009614</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.009614</td>\n",
       "      <td>0.003040</td>\n",
       "      <td>0.002103</td>\n",
       "      <td>0.001710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200</td>\n",
       "      <td>0.009614</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.009614</td>\n",
       "      <td>0.003040</td>\n",
       "      <td>0.002103</td>\n",
       "      <td>0.001710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>210</td>\n",
       "      <td>0.009614</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.009614</td>\n",
       "      <td>0.003040</td>\n",
       "      <td>0.002103</td>\n",
       "      <td>0.001710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>220</td>\n",
       "      <td>0.009614</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.009614</td>\n",
       "      <td>0.003040</td>\n",
       "      <td>0.002103</td>\n",
       "      <td>0.001710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>230</td>\n",
       "      <td>0.009614</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.009614</td>\n",
       "      <td>0.003040</td>\n",
       "      <td>0.002103</td>\n",
       "      <td>0.001710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>240</td>\n",
       "      <td>0.009614</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.009614</td>\n",
       "      <td>0.003040</td>\n",
       "      <td>0.002103</td>\n",
       "      <td>0.001710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>250</td>\n",
       "      <td>0.009614</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.009614</td>\n",
       "      <td>0.003040</td>\n",
       "      <td>0.002103</td>\n",
       "      <td>0.001710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>260</td>\n",
       "      <td>0.009614</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.009614</td>\n",
       "      <td>0.003040</td>\n",
       "      <td>0.002103</td>\n",
       "      <td>0.001710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>270</td>\n",
       "      <td>0.009614</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.009614</td>\n",
       "      <td>0.003040</td>\n",
       "      <td>0.002103</td>\n",
       "      <td>0.001710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>280</td>\n",
       "      <td>0.009614</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.009614</td>\n",
       "      <td>0.003040</td>\n",
       "      <td>0.002103</td>\n",
       "      <td>0.001710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>290</td>\n",
       "      <td>0.009614</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.009614</td>\n",
       "      <td>0.003040</td>\n",
       "      <td>0.002103</td>\n",
       "      <td>0.001710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>300</td>\n",
       "      <td>0.009614</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.009614</td>\n",
       "      <td>0.003040</td>\n",
       "      <td>0.002103</td>\n",
       "      <td>0.001710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>310</td>\n",
       "      <td>0.023161</td>\n",
       "      <td>0.002316</td>\n",
       "      <td>0.002316</td>\n",
       "      <td>0.002316</td>\n",
       "      <td>0.023161</td>\n",
       "      <td>0.007324</td>\n",
       "      <td>0.005067</td>\n",
       "      <td>0.004119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>320</td>\n",
       "      <td>0.009614</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.009614</td>\n",
       "      <td>0.003040</td>\n",
       "      <td>0.002103</td>\n",
       "      <td>0.001710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>330</td>\n",
       "      <td>0.009614</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.009614</td>\n",
       "      <td>0.003040</td>\n",
       "      <td>0.002103</td>\n",
       "      <td>0.001710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>340</td>\n",
       "      <td>0.009614</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.009614</td>\n",
       "      <td>0.003040</td>\n",
       "      <td>0.002103</td>\n",
       "      <td>0.001710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>350</td>\n",
       "      <td>0.009614</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.009614</td>\n",
       "      <td>0.003040</td>\n",
       "      <td>0.002103</td>\n",
       "      <td>0.001710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>360</td>\n",
       "      <td>0.009614</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.009614</td>\n",
       "      <td>0.003040</td>\n",
       "      <td>0.002103</td>\n",
       "      <td>0.001710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>370</td>\n",
       "      <td>0.009614</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.009614</td>\n",
       "      <td>0.003040</td>\n",
       "      <td>0.002103</td>\n",
       "      <td>0.001710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>380</td>\n",
       "      <td>0.009614</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.009614</td>\n",
       "      <td>0.003040</td>\n",
       "      <td>0.002103</td>\n",
       "      <td>0.001710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>390</td>\n",
       "      <td>0.009614</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.009614</td>\n",
       "      <td>0.003040</td>\n",
       "      <td>0.002103</td>\n",
       "      <td>0.001710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>400</td>\n",
       "      <td>0.009614</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.009614</td>\n",
       "      <td>0.003040</td>\n",
       "      <td>0.002103</td>\n",
       "      <td>0.001710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>410</td>\n",
       "      <td>0.009614</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.009614</td>\n",
       "      <td>0.003040</td>\n",
       "      <td>0.002103</td>\n",
       "      <td>0.001710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>420</td>\n",
       "      <td>0.009614</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.009614</td>\n",
       "      <td>0.003040</td>\n",
       "      <td>0.002103</td>\n",
       "      <td>0.001710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>430</td>\n",
       "      <td>0.009614</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.009614</td>\n",
       "      <td>0.003040</td>\n",
       "      <td>0.002103</td>\n",
       "      <td>0.001710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>440</td>\n",
       "      <td>0.009614</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.009614</td>\n",
       "      <td>0.003040</td>\n",
       "      <td>0.002103</td>\n",
       "      <td>0.001710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>450</td>\n",
       "      <td>0.009614</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.009614</td>\n",
       "      <td>0.003040</td>\n",
       "      <td>0.002103</td>\n",
       "      <td>0.001710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>460</td>\n",
       "      <td>0.009614</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.009614</td>\n",
       "      <td>0.003040</td>\n",
       "      <td>0.002103</td>\n",
       "      <td>0.001710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>470</td>\n",
       "      <td>0.009614</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.009614</td>\n",
       "      <td>0.003040</td>\n",
       "      <td>0.002103</td>\n",
       "      <td>0.001710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>480</td>\n",
       "      <td>0.009614</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.009614</td>\n",
       "      <td>0.003040</td>\n",
       "      <td>0.002103</td>\n",
       "      <td>0.001710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>490</td>\n",
       "      <td>0.009614</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.009614</td>\n",
       "      <td>0.003040</td>\n",
       "      <td>0.002103</td>\n",
       "      <td>0.001710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>500</td>\n",
       "      <td>0.009614</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>0.009614</td>\n",
       "      <td>0.003040</td>\n",
       "      <td>0.002103</td>\n",
       "      <td>0.001710</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  epoch    1-gram    2-gram    3-gram    4-gram  cumulative-1-gram  \\\n",
       "0    10  0.006980  0.000698  0.000698  0.000698           0.006980   \n",
       "0    20  0.023341  0.002334  0.002334  0.002334           0.023341   \n",
       "0    30  0.036934  0.003693  0.003693  0.003693           0.036934   \n",
       "0    40  0.023421  0.002342  0.002342  0.002342           0.023421   \n",
       "0    50  0.023421  0.002342  0.002342  0.002342           0.023421   \n",
       "0    60  0.023408  0.002341  0.002341  0.002341           0.023408   \n",
       "0    70  0.023408  0.002341  0.002341  0.002341           0.023408   \n",
       "0    80  0.023408  0.002341  0.002341  0.002341           0.023408   \n",
       "0    90  0.023161  0.002316  0.002316  0.002316           0.023161   \n",
       "0   100  0.023127  0.002313  0.002313  0.002313           0.023127   \n",
       "0   110  0.023161  0.002316  0.002316  0.002316           0.023161   \n",
       "0   120  0.023127  0.002313  0.002313  0.002313           0.023127   \n",
       "0   130  0.009614  0.000961  0.000961  0.000961           0.009614   \n",
       "0   140  0.009614  0.000961  0.000961  0.000961           0.009614   \n",
       "0   150  0.009614  0.000961  0.000961  0.000961           0.009614   \n",
       "0   160  0.009614  0.000961  0.000961  0.000961           0.009614   \n",
       "0   170  0.009614  0.000961  0.000961  0.000961           0.009614   \n",
       "0   180  0.009614  0.000961  0.000961  0.000961           0.009614   \n",
       "0   190  0.009614  0.000961  0.000961  0.000961           0.009614   \n",
       "0   200  0.009614  0.000961  0.000961  0.000961           0.009614   \n",
       "0   210  0.009614  0.000961  0.000961  0.000961           0.009614   \n",
       "0   220  0.009614  0.000961  0.000961  0.000961           0.009614   \n",
       "0   230  0.009614  0.000961  0.000961  0.000961           0.009614   \n",
       "0   240  0.009614  0.000961  0.000961  0.000961           0.009614   \n",
       "0   250  0.009614  0.000961  0.000961  0.000961           0.009614   \n",
       "0   260  0.009614  0.000961  0.000961  0.000961           0.009614   \n",
       "0   270  0.009614  0.000961  0.000961  0.000961           0.009614   \n",
       "0   280  0.009614  0.000961  0.000961  0.000961           0.009614   \n",
       "0   290  0.009614  0.000961  0.000961  0.000961           0.009614   \n",
       "0   300  0.009614  0.000961  0.000961  0.000961           0.009614   \n",
       "0   310  0.023161  0.002316  0.002316  0.002316           0.023161   \n",
       "0   320  0.009614  0.000961  0.000961  0.000961           0.009614   \n",
       "0   330  0.009614  0.000961  0.000961  0.000961           0.009614   \n",
       "0   340  0.009614  0.000961  0.000961  0.000961           0.009614   \n",
       "0   350  0.009614  0.000961  0.000961  0.000961           0.009614   \n",
       "0   360  0.009614  0.000961  0.000961  0.000961           0.009614   \n",
       "0   370  0.009614  0.000961  0.000961  0.000961           0.009614   \n",
       "0   380  0.009614  0.000961  0.000961  0.000961           0.009614   \n",
       "0   390  0.009614  0.000961  0.000961  0.000961           0.009614   \n",
       "0   400  0.009614  0.000961  0.000961  0.000961           0.009614   \n",
       "0   410  0.009614  0.000961  0.000961  0.000961           0.009614   \n",
       "0   420  0.009614  0.000961  0.000961  0.000961           0.009614   \n",
       "0   430  0.009614  0.000961  0.000961  0.000961           0.009614   \n",
       "0   440  0.009614  0.000961  0.000961  0.000961           0.009614   \n",
       "0   450  0.009614  0.000961  0.000961  0.000961           0.009614   \n",
       "0   460  0.009614  0.000961  0.000961  0.000961           0.009614   \n",
       "0   470  0.009614  0.000961  0.000961  0.000961           0.009614   \n",
       "0   480  0.009614  0.000961  0.000961  0.000961           0.009614   \n",
       "0   490  0.009614  0.000961  0.000961  0.000961           0.009614   \n",
       "0   500  0.009614  0.000961  0.000961  0.000961           0.009614   \n",
       "\n",
       "   cumulative-2-gram  cumulative-3-gram  cumulative-4-gram  \n",
       "0           0.002207           0.001527           0.001241  \n",
       "0           0.007381           0.005107           0.004151  \n",
       "0           0.011680           0.008080           0.006568  \n",
       "0           0.007406           0.005124           0.004165  \n",
       "0           0.007406           0.005124           0.004165  \n",
       "0           0.007402           0.005121           0.004163  \n",
       "0           0.007402           0.005121           0.004163  \n",
       "0           0.007402           0.005121           0.004163  \n",
       "0           0.007324           0.005067           0.004119  \n",
       "0           0.007314           0.005060           0.004113  \n",
       "0           0.007324           0.005067           0.004119  \n",
       "0           0.007314           0.005060           0.004113  \n",
       "0           0.003040           0.002103           0.001710  \n",
       "0           0.003040           0.002103           0.001710  \n",
       "0           0.003040           0.002103           0.001710  \n",
       "0           0.003040           0.002103           0.001710  \n",
       "0           0.003040           0.002103           0.001710  \n",
       "0           0.003040           0.002103           0.001710  \n",
       "0           0.003040           0.002103           0.001710  \n",
       "0           0.003040           0.002103           0.001710  \n",
       "0           0.003040           0.002103           0.001710  \n",
       "0           0.003040           0.002103           0.001710  \n",
       "0           0.003040           0.002103           0.001710  \n",
       "0           0.003040           0.002103           0.001710  \n",
       "0           0.003040           0.002103           0.001710  \n",
       "0           0.003040           0.002103           0.001710  \n",
       "0           0.003040           0.002103           0.001710  \n",
       "0           0.003040           0.002103           0.001710  \n",
       "0           0.003040           0.002103           0.001710  \n",
       "0           0.003040           0.002103           0.001710  \n",
       "0           0.007324           0.005067           0.004119  \n",
       "0           0.003040           0.002103           0.001710  \n",
       "0           0.003040           0.002103           0.001710  \n",
       "0           0.003040           0.002103           0.001710  \n",
       "0           0.003040           0.002103           0.001710  \n",
       "0           0.003040           0.002103           0.001710  \n",
       "0           0.003040           0.002103           0.001710  \n",
       "0           0.003040           0.002103           0.001710  \n",
       "0           0.003040           0.002103           0.001710  \n",
       "0           0.003040           0.002103           0.001710  \n",
       "0           0.003040           0.002103           0.001710  \n",
       "0           0.003040           0.002103           0.001710  \n",
       "0           0.003040           0.002103           0.001710  \n",
       "0           0.003040           0.002103           0.001710  \n",
       "0           0.003040           0.002103           0.001710  \n",
       "0           0.003040           0.002103           0.001710  \n",
       "0           0.003040           0.002103           0.001710  \n",
       "0           0.003040           0.002103           0.001710  \n",
       "0           0.003040           0.002103           0.001710  \n",
       "0           0.003040           0.002103           0.001710  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f53d1665d20>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(bleu_score['epoch'], bleu_score['1-gram'], label='1-gram')\n",
    "plt.plot(bleu_score['epoch'], bleu_score['2-gram'], label='2-gram')\n",
    "plt.plot(bleu_score['epoch'], bleu_score['3-gram'], label='3-gram')\n",
    "plt.plot(bleu_score['epoch'], bleu_score['4-gram'], label='4-gram')\n",
    "plt.plot(bleu_score['epoch'], bleu_score['cumulative-1-gram'], label='cumulative-1-gram')\n",
    "plt.plot(bleu_score['epoch'], bleu_score['cumulative-2-gram'], label='cumulative-2-gram')\n",
    "plt.plot(bleu_score['epoch'], bleu_score['cumulative-3-gram'], label='cumulative-3-gram')\n",
    "plt.plot(bleu_score['epoch'], bleu_score['cumulative-4-gram'], label='cumulative-4-gram')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('BLEU Score')\n",
    "plt.title('BLEU Score on LSTM Same Dataset')\n",
    "plt.legend()\n",
    "# add number for each 100 epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from sklearn.model_selection import KFold\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# def train(source_data, target_data, model, epochs, batch_size, print_every, learning_rate):\n",
    "#     model.to(device)\n",
    "#     total_training_loss = 0\n",
    "#     total_valid_loss = 0\n",
    "#     loss = 0\n",
    "    \n",
    "#     optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "#     criterion = nn.NLLLoss()\n",
    "\n",
    "#     # use cross validation\n",
    "#     kf = KFold(n_splits=epochs, shuffle=True)\n",
    "\n",
    "#     # run = neptune_init(\"LSTM_FLUENT_Baseline_2\")\n",
    "\n",
    "#     for e, (train_index, test_index) in enumerate(kf.split(source_data), 1):\n",
    "#     model.to(device)\n",
    "#     model.train()\n",
    "#     for i in range(0, len(train_index)):\n",
    "\n",
    "#         src = source_data[i]\n",
    "#         trg = target_data[i]\n",
    "\n",
    "#         output = model(src, trg, src.size(0), trg.size(0))\n",
    "\n",
    "#         current_loss = 0\n",
    "#         for (s, t) in zip(output[\"decoder_output\"], trg): \n",
    "#             current_loss += criterion(s, t)\n",
    "\n",
    "#         loss += current_loss\n",
    "#         total_training_loss += (current_loss.item() / trg.size(0)) # add the iteration loss\n",
    "\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         optimizer.zero_grad()\n",
    "#         loss = 0\n",
    "\n",
    "\n",
    "#     # validation set \n",
    "#     model.eval()\n",
    "#     for i in range(0, len(test_index)):\n",
    "#         src = source_data[i]\n",
    "#         trg = target_data[i]\n",
    "\n",
    "#         output = model(src, trg, src.size(0), trg.size(0))\n",
    "\n",
    "#         current_loss = 0\n",
    "#         for (s, t) in zip(output[\"decoder_output\"], trg): \n",
    "#             current_loss += criterion(s, t)\n",
    "\n",
    "#         total_valid_loss += (current_loss.item() / trg.size(0)) # add the iteration loss\n",
    "\n",
    "\n",
    "#     if e % print_every == 0:\n",
    "#         training_loss_average = total_training_loss / (len(train_index)*print_every)\n",
    "#         validation_loss_average = total_valid_loss / (len(test_index)*print_every)\n",
    "#         print(\"{}/{} Epoch  -  Training Loss = {:.4f}  -  Validation Loss = {:.4f}\".format(e, epochs, training_loss_average, validation_loss_average))\n",
    "#         # run['train/loss'].append(training_loss_average)\n",
    "#         total_training_loss = 0\n",
    "#         total_valid_loss = 0 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[neptune] [warning] NeptuneWarning: By default, these monitoring options are disabled in interactive sessions: 'capture_stdout', 'capture_stderr', 'capture_traceback', 'capture_hardware_metrics'. You can set them to 'True' when initializing the run and the monitoring will continue until you call run.stop() or the kernel stops. NOTE: To track the source files, pass their paths to the 'source_code' argument. For help, see: https://docs.neptune.ai/logging/source_code/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/andialifs/siet-24/e/SIET-172\n",
      "1/150 Epoch  -  Training Loss = 7.7315  -  Validation Loss = 7.1495\n",
      "2/150 Epoch  -  Training Loss = 6.7575  -  Validation Loss = 6.5846\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m seq2seq \u001b[38;5;241m=\u001b[39m Seq2Seq(Q_vocab\u001b[38;5;241m.\u001b[39mwords_count, hidden_size, A_vocab\u001b[38;5;241m.\u001b[39mwords_count)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msource_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtarget_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtarget_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mseq2seq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m      \u001b[49m\u001b[43mprint_every\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m      \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m      \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m      \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(source_data, target_data, model, epochs, batch_size, print_every, learning_rate)\u001b[0m\n\u001b[1;32m     36\u001b[0m total_training_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (current_loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m/\u001b[39m trg\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)) \u001b[38;5;66;03m# add the iteration loss\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m batch_size \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m i \u001b[38;5;241m==\u001b[39m (\u001b[38;5;28mlen\u001b[39m(train_index)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 39\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     41\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "seq2seq = Seq2Seq(Q_vocab.words_count, hidden_size, A_vocab.words_count)\n",
    "\n",
    "train(source_data = source_data,\n",
    "    target_data = target_data,\n",
    "    model = seq2seq,\n",
    "    print_every = 1,\n",
    "    epochs = epochs,\n",
    "    learning_rate = learning_rate,\n",
    "    batch_size = batch_size\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('encoder.embedding.weight',\n",
       "              tensor([[ 1.2180,  2.3957,  0.0216,  ...,  0.1178, -1.6404,  1.4385],\n",
       "                      [ 0.2498,  0.5579, -0.2960,  ..., -0.7027, -0.3493, -1.5135],\n",
       "                      [ 1.9521,  0.6869,  0.4072,  ...,  0.7167, -2.0514,  1.1361],\n",
       "                      ...,\n",
       "                      [-1.2900, -1.6707, -1.3739,  ...,  0.2082,  0.3778, -2.0578],\n",
       "                      [-0.5127, -0.9172, -1.3340,  ..., -0.6937,  0.9553,  1.1674],\n",
       "                      [ 1.7684, -1.3130, -1.8440,  ..., -0.4877,  2.1086,  0.8863]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.input.weight',\n",
       "              tensor([[-0.0131,  0.0542,  0.0858,  ...,  0.0804, -0.0135,  0.0493],\n",
       "                      [-0.0762,  0.0377, -0.0451,  ..., -0.0508,  0.0025,  0.0876],\n",
       "                      [-0.0358,  0.0494,  0.0072,  ...,  0.0135,  0.0161,  0.0795],\n",
       "                      ...,\n",
       "                      [-0.0878, -0.0809,  0.0564,  ...,  0.0885,  0.0911, -0.0466],\n",
       "                      [ 0.0433, -0.0624, -0.0078,  ..., -0.0234,  0.0247,  0.0458],\n",
       "                      [-0.0899,  0.0678, -0.0421,  ...,  0.0504,  0.0256,  0.0015]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.input.bias',\n",
       "              tensor([-0.0709, -0.0325,  0.0617,  0.0264, -0.0915, -0.0422, -0.0641,  0.0623,\n",
       "                       0.0635,  0.0908,  0.0282,  0.0171, -0.0819,  0.0042, -0.0686, -0.0610,\n",
       "                      -0.0609, -0.0145, -0.0534,  0.0006,  0.0181,  0.0468, -0.0694, -0.0282,\n",
       "                       0.0519, -0.0480, -0.0228, -0.0594,  0.0415,  0.0740, -0.0247,  0.0372,\n",
       "                       0.0487, -0.0354,  0.0820,  0.0926,  0.0066,  0.0161,  0.0667,  0.0194,\n",
       "                       0.0283,  0.0042,  0.0697,  0.0097,  0.0422, -0.0568, -0.0063, -0.0338,\n",
       "                      -0.0891, -0.0755, -0.0499,  0.0723,  0.0880, -0.0349, -0.0719,  0.0165,\n",
       "                       0.0483, -0.0299, -0.0302,  0.0294, -0.0124,  0.0799,  0.0673, -0.0383,\n",
       "                       0.0778, -0.0637, -0.0014,  0.0717,  0.0696, -0.0309, -0.0295,  0.0843,\n",
       "                       0.0456, -0.0605,  0.0237, -0.0560,  0.0938, -0.0577,  0.0305,  0.0656,\n",
       "                       0.0896,  0.0564, -0.0543,  0.0565,  0.0464,  0.0332,  0.0460, -0.0268,\n",
       "                      -0.0657, -0.0550,  0.0120, -0.0553,  0.0095,  0.0282, -0.0860, -0.0559,\n",
       "                       0.0368, -0.0499, -0.0285, -0.0738, -0.0677, -0.0795, -0.0513,  0.0779,\n",
       "                       0.0475,  0.0161,  0.0092,  0.0897, -0.0771,  0.0849, -0.0422,  0.0422,\n",
       "                       0.0644,  0.0770,  0.0574,  0.0293, -0.0236, -0.0111, -0.0510,  0.0842,\n",
       "                      -0.0713,  0.0327, -0.0196, -0.0815,  0.0052,  0.0359, -0.0486,  0.0864,\n",
       "                      -0.0926,  0.0008,  0.0896, -0.0220, -0.0828, -0.0128, -0.0267, -0.0003,\n",
       "                       0.0262, -0.0429,  0.0187,  0.0778,  0.0488,  0.0276,  0.0182,  0.0565,\n",
       "                       0.0194, -0.0522,  0.0215, -0.0480,  0.0117,  0.0848,  0.0236,  0.0517,\n",
       "                       0.0030,  0.0661, -0.0079,  0.0473, -0.0739,  0.0798, -0.0402, -0.0665,\n",
       "                       0.0153,  0.0590, -0.0048, -0.0664,  0.0572, -0.0189,  0.0318,  0.0700,\n",
       "                      -0.0860, -0.0581,  0.0440,  0.0457,  0.0188,  0.0085, -0.0139, -0.0220,\n",
       "                       0.0179,  0.0448, -0.0132,  0.0818, -0.0141,  0.0235, -0.0550, -0.0845,\n",
       "                      -0.0599,  0.0436, -0.0358,  0.0532, -0.0031,  0.0596,  0.0745,  0.0102,\n",
       "                       0.0247, -0.0733, -0.0001,  0.0898,  0.0094,  0.0900, -0.0852, -0.0488,\n",
       "                      -0.0597, -0.0162, -0.0452, -0.0689,  0.0902, -0.0188, -0.0462, -0.0200,\n",
       "                      -0.0333,  0.0366, -0.0354,  0.0718,  0.0076,  0.0049, -0.0221,  0.0687,\n",
       "                      -0.0150, -0.0731,  0.0490, -0.0683, -0.0296, -0.0852, -0.0326,  0.0302,\n",
       "                       0.0100,  0.0087,  0.0733, -0.0695,  0.0629,  0.0480, -0.0806,  0.0272,\n",
       "                      -0.0657, -0.0829,  0.0447, -0.0535,  0.0894,  0.0256, -0.0564, -0.0487,\n",
       "                       0.0743,  0.0009, -0.0461,  0.0547,  0.0318,  0.0805, -0.0250,  0.0437,\n",
       "                      -0.0803,  0.0441,  0.0102,  0.0192, -0.0014,  0.0837, -0.0519, -0.0867,\n",
       "                      -0.0793, -0.0779,  0.0266,  0.0333,  0.0764, -0.0478,  0.0149,  0.0787,\n",
       "                      -0.0178, -0.0302,  0.0937,  0.0435, -0.0122,  0.0315,  0.0807,  0.0921,\n",
       "                      -0.0814,  0.0295,  0.0506,  0.0574, -0.0807,  0.0069, -0.0473, -0.0368,\n",
       "                      -0.0284, -0.0518, -0.0407,  0.0410,  0.0188, -0.0453, -0.0056, -0.0576,\n",
       "                       0.0035, -0.0824,  0.0164, -0.0459,  0.0760, -0.0015,  0.0442,  0.0209,\n",
       "                       0.0331,  0.0759,  0.0202, -0.0382, -0.0633,  0.0397, -0.0543, -0.0805,\n",
       "                      -0.0054,  0.0163, -0.0706, -0.0144,  0.0444,  0.0176,  0.0668,  0.0482,\n",
       "                       0.0319, -0.0090, -0.0627, -0.0332,  0.0010, -0.0518, -0.0065,  0.0781,\n",
       "                       0.0870, -0.0414, -0.0689, -0.0363, -0.0434,  0.0361, -0.0360,  0.0872,\n",
       "                      -0.0126,  0.0672,  0.0454, -0.0182,  0.0115, -0.0030,  0.0633, -0.0007,\n",
       "                       0.0499,  0.0462, -0.0303, -0.0726, -0.0922,  0.0353, -0.0549,  0.0775,\n",
       "                      -0.0463, -0.0900,  0.0148, -0.0898, -0.0643, -0.0675,  0.0473, -0.0696,\n",
       "                       0.0858, -0.0149,  0.0659,  0.0631, -0.0543, -0.0623,  0.0757,  0.0379,\n",
       "                      -0.0407, -0.0355,  0.0276,  0.0413,  0.0778,  0.0707,  0.0473,  0.0443,\n",
       "                      -0.0495,  0.0553,  0.0060,  0.0189, -0.0496, -0.0325, -0.0925, -0.0479,\n",
       "                       0.0797,  0.0090,  0.0033,  0.0641, -0.0401,  0.0182,  0.0047,  0.0410,\n",
       "                       0.0878, -0.0222,  0.0400, -0.0018,  0.0459,  0.0102,  0.0891,  0.0419,\n",
       "                       0.0711, -0.0590,  0.0920,  0.0306, -0.0040, -0.0254, -0.0677,  0.0504,\n",
       "                      -0.0266,  0.0204, -0.0593, -0.0830, -0.0706, -0.0729, -0.0696, -0.0269,\n",
       "                       0.0819, -0.0424, -0.0260,  0.0309, -0.0436, -0.0781, -0.0010, -0.0787,\n",
       "                       0.0821,  0.0187,  0.0098,  0.0418, -0.0288, -0.0240, -0.0709, -0.0875,\n",
       "                      -0.0618, -0.0439, -0.0686,  0.0518, -0.0445,  0.0593,  0.0435, -0.0271,\n",
       "                      -0.0251,  0.0586,  0.0096, -0.0584,  0.0819,  0.0555, -0.0495, -0.0453,\n",
       "                       0.0401, -0.0418, -0.0049, -0.0720, -0.0410,  0.0147, -0.0294, -0.0573,\n",
       "                       0.0921,  0.0940,  0.0363,  0.0041, -0.0361,  0.0869,  0.0732,  0.0863,\n",
       "                      -0.0039, -0.0463,  0.0245,  0.0371,  0.0148,  0.0420, -0.0655,  0.0250,\n",
       "                      -0.0255, -0.0678,  0.0306,  0.0922,  0.0443, -0.0566,  0.0027, -0.0778,\n",
       "                      -0.0743,  0.0129, -0.0652, -0.0441,  0.0896, -0.0453,  0.0768,  0.0631,\n",
       "                       0.0229,  0.0909,  0.0406, -0.0207, -0.0230,  0.0327, -0.0407, -0.0544,\n",
       "                      -0.0716,  0.0685, -0.0246, -0.0638, -0.0101,  0.0544,  0.0526,  0.0906,\n",
       "                       0.0625, -0.0269, -0.0841,  0.0524,  0.0370, -0.0239, -0.0656, -0.0160,\n",
       "                       0.0408,  0.0662,  0.0681,  0.0678,  0.0167,  0.0020, -0.0182,  0.0320],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.lstm.weight_ih_l0',\n",
       "              tensor([[-0.0352, -0.0335,  0.0096,  ...,  0.0356,  0.0088, -0.0357],\n",
       "                      [ 0.0111,  0.0362,  0.0180,  ...,  0.0218,  0.0174, -0.0027],\n",
       "                      [ 0.0313,  0.0114,  0.0033,  ..., -0.0420, -0.0170, -0.0269],\n",
       "                      ...,\n",
       "                      [-0.0013,  0.0066,  0.0307,  ..., -0.0003,  0.0370, -0.0033],\n",
       "                      [ 0.0241,  0.0226, -0.0423,  ...,  0.0008, -0.0125,  0.0002],\n",
       "                      [-0.0011, -0.0024, -0.0196,  ..., -0.0368, -0.0140,  0.0260]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.lstm.weight_hh_l0',\n",
       "              tensor([[ 0.0418, -0.0153, -0.0235,  ...,  0.0159, -0.0242, -0.0278],\n",
       "                      [-0.0014, -0.0364, -0.0474,  ...,  0.0205, -0.0039, -0.0012],\n",
       "                      [-0.0060, -0.0037,  0.0090,  ...,  0.0284, -0.0002,  0.0342],\n",
       "                      ...,\n",
       "                      [ 0.0392,  0.0233, -0.0083,  ..., -0.0075,  0.0370, -0.0290],\n",
       "                      [-0.0049, -0.0007,  0.0376,  ..., -0.0231,  0.0081, -0.0194],\n",
       "                      [ 0.0514, -0.0426, -0.0068,  ..., -0.0300, -0.0281,  0.0098]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.lstm.bias_ih_l0',\n",
       "              tensor([-0.0174, -0.0064,  0.0056,  ...,  0.0289, -0.0062,  0.0492],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.lstm.bias_hh_l0',\n",
       "              tensor([ 0.0560,  0.0253, -0.0168,  ...,  0.0220,  0.0104, -0.0187],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.embedding.weight',\n",
       "              tensor([[-0.6096, -1.2196,  0.9751,  ...,  1.1257,  0.3082, -0.2171],\n",
       "                      [-1.3684,  0.1686, -2.1931,  ..., -0.3869,  0.2784, -0.8981],\n",
       "                      [-0.6802, -0.0551,  0.1512,  ...,  1.4191, -0.6822, -0.6124],\n",
       "                      ...,\n",
       "                      [ 2.6520, -0.6958,  0.4100,  ..., -1.3355, -0.1546, -2.4715],\n",
       "                      [-2.3463,  1.8148, -0.3896,  ...,  0.5627, -0.7672,  0.9573],\n",
       "                      [-0.8420,  0.7582,  0.3657,  ..., -1.4526,  0.0720,  0.4343]],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.lstm.weight_ih_l0',\n",
       "              tensor([[-0.0054, -0.0409,  0.0321,  ..., -0.0156,  0.0254, -0.0108],\n",
       "                      [-0.0185,  0.0286, -0.0046,  ...,  0.0151, -0.0140, -0.0215],\n",
       "                      [ 0.0032, -0.0310, -0.0203,  ..., -0.0337, -0.0279,  0.0219],\n",
       "                      ...,\n",
       "                      [-0.0398, -0.0293,  0.0231,  ...,  0.0118, -0.0421, -0.0083],\n",
       "                      [-0.0102, -0.0323, -0.0176,  ...,  0.0111, -0.0370,  0.0364],\n",
       "                      [ 0.0005,  0.0301,  0.0072,  ..., -0.0035, -0.0336, -0.0142]],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.lstm.weight_hh_l0',\n",
       "              tensor([[-0.0042, -0.0162,  0.0347,  ...,  0.0004, -0.0135, -0.0095],\n",
       "                      [-0.0445,  0.0228,  0.0040,  ...,  0.0358,  0.0006, -0.0158],\n",
       "                      [-0.0007,  0.0104, -0.0072,  ..., -0.0037, -0.0395,  0.0375],\n",
       "                      ...,\n",
       "                      [ 0.0082,  0.0117,  0.0351,  ..., -0.0134, -0.0172,  0.0410],\n",
       "                      [ 0.0334, -0.0355,  0.0240,  ...,  0.0165,  0.0365,  0.0269],\n",
       "                      [ 0.0197,  0.0258, -0.0378,  ..., -0.0235,  0.0254,  0.0325]],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.lstm.bias_ih_l0',\n",
       "              tensor([ 0.0427, -0.0357, -0.0236,  ..., -0.0158, -0.0097, -0.0309],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.lstm.bias_hh_l0',\n",
       "              tensor([-0.0304,  0.0301, -0.0364,  ...,  0.0218,  0.0206, -0.0031],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.fc.weight',\n",
       "              tensor([[ 0.0322,  0.0143,  0.0283,  ..., -0.0486, -0.0158,  0.0180],\n",
       "                      [-0.0053, -0.0308, -0.0403,  ...,  0.0447,  0.0400,  0.0167],\n",
       "                      [ 0.0132,  0.0093, -0.0334,  ..., -0.0379, -0.0093, -0.0069],\n",
       "                      ...,\n",
       "                      [ 0.0296,  0.0235,  0.0173,  ..., -0.0438,  0.0261, -0.0191],\n",
       "                      [ 0.0258,  0.0347,  0.0174,  ...,  0.0073, -0.0160,  0.0366],\n",
       "                      [ 0.0078, -0.0267, -0.0456,  ...,  0.0254, -0.0222,  0.0142]],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.fc.bias',\n",
       "              tensor([-2.8130e-02,  1.7193e-02,  4.4401e-02, -2.9391e-02, -6.6351e-03,\n",
       "                       1.3662e-02,  2.5079e-02, -9.6440e-03,  2.8124e-02, -1.9848e-02,\n",
       "                       1.9859e-02,  3.9184e-02,  3.0414e-02, -3.6658e-02, -4.0415e-02,\n",
       "                       3.2933e-03,  2.2340e-02, -1.3531e-02, -2.1030e-02, -3.7940e-02,\n",
       "                       1.8454e-02,  2.2957e-02, -3.7482e-02, -2.9624e-03, -1.4459e-02,\n",
       "                      -1.6717e-02,  2.0686e-02, -2.5884e-02,  4.4874e-02, -2.6208e-04,\n",
       "                       1.2799e-02,  2.1471e-02, -3.2737e-02,  2.6866e-02, -4.3824e-02,\n",
       "                       8.0892e-03, -9.8809e-03, -3.3927e-02,  2.1758e-03, -8.1212e-03,\n",
       "                      -4.6754e-02,  1.1799e-02, -2.6034e-02,  2.8042e-02,  2.4670e-02,\n",
       "                       2.4255e-02, -2.9906e-03, -3.7665e-02, -1.8212e-02, -3.7943e-02,\n",
       "                       1.3274e-02,  2.6902e-02, -3.2292e-02, -2.0930e-02, -2.2249e-02,\n",
       "                       4.1836e-02,  2.3907e-03,  1.7878e-02, -2.8765e-02,  2.8189e-02,\n",
       "                      -1.2494e-02, -9.9378e-03, -3.0700e-02, -1.3583e-02,  1.2189e-02,\n",
       "                       1.4704e-02,  4.0434e-02, -1.5522e-02,  3.9636e-02,  2.7076e-05,\n",
       "                      -1.0835e-02,  2.6501e-02, -3.7657e-03,  3.1249e-02,  8.2389e-03,\n",
       "                      -3.1763e-02, -1.4631e-02,  1.1576e-02, -3.1023e-02,  2.1664e-02,\n",
       "                       1.6765e-03,  2.7775e-02,  3.3972e-03,  2.8635e-02, -5.1428e-03,\n",
       "                      -3.5192e-02,  2.7201e-02, -5.6316e-04,  3.9477e-02, -2.0392e-02,\n",
       "                       1.1801e-02,  3.5450e-02,  1.1873e-02, -1.4043e-02,  1.2189e-02,\n",
       "                       2.1547e-02, -7.5478e-04, -1.6859e-03, -5.8048e-03, -3.2739e-02,\n",
       "                      -9.2297e-03,  3.3275e-03,  1.4471e-02, -1.7525e-02, -9.5893e-03,\n",
       "                       1.6662e-02, -1.2943e-02, -3.9392e-02,  2.6953e-02, -1.8848e-02,\n",
       "                       2.1255e-03,  2.7529e-02, -3.1976e-02,  3.1518e-02, -8.9153e-03,\n",
       "                       7.7814e-03,  1.3905e-02,  2.8405e-03, -1.1217e-02,  3.9706e-02,\n",
       "                      -2.7196e-02,  1.6735e-02,  3.2056e-02, -4.6311e-02, -4.1562e-02,\n",
       "                      -1.6470e-02,  3.6438e-02,  3.4269e-02, -6.9866e-03,  3.5074e-02,\n",
       "                       3.7991e-02,  9.5606e-03, -2.5648e-02, -2.8729e-02,  3.1395e-02,\n",
       "                      -2.3105e-02, -3.1725e-02, -2.7820e-02,  1.5876e-02, -2.1975e-02,\n",
       "                      -1.6283e-02, -1.8263e-02, -3.9160e-02,  2.9879e-02, -1.9490e-02,\n",
       "                       2.8795e-03, -1.9790e-02,  2.3734e-02, -2.1282e-02, -1.2488e-03,\n",
       "                      -1.0631e-02,  1.9828e-02,  1.6238e-02,  2.7726e-02,  1.2362e-02,\n",
       "                      -3.9007e-03, -2.6248e-02, -1.3753e-02, -2.3808e-02, -3.2311e-02,\n",
       "                      -4.0654e-02,  2.2591e-02,  3.9684e-02, -2.1646e-02, -1.6578e-02,\n",
       "                       2.7331e-02,  4.7552e-03,  3.4040e-02, -2.5267e-02, -4.3059e-02,\n",
       "                       2.7025e-03, -4.0013e-02, -3.3830e-05,  3.1598e-02,  2.9540e-04,\n",
       "                      -4.2636e-02, -4.3943e-02, -2.0339e-02, -4.1849e-02,  1.3891e-02,\n",
       "                      -1.1960e-02,  9.1430e-03, -9.2294e-03,  4.3816e-02,  8.5754e-03,\n",
       "                       1.1518e-02,  9.1226e-04, -2.4850e-02,  4.3363e-02, -3.1199e-02,\n",
       "                       4.0045e-02, -1.9933e-02, -1.2300e-02,  3.7990e-02, -3.8568e-02,\n",
       "                      -1.9774e-02,  1.8149e-02, -7.8691e-03, -3.5580e-03, -1.7075e-02,\n",
       "                       3.8176e-02,  1.2887e-03,  4.4317e-03, -3.5742e-02, -2.9651e-02,\n",
       "                       3.3828e-02,  3.1651e-02, -2.1511e-02, -8.1215e-03, -7.6455e-03,\n",
       "                       1.1143e-02,  2.6238e-03, -1.9441e-02, -1.1252e-02, -3.0610e-02,\n",
       "                      -1.4128e-02, -1.0842e-02,  1.0078e-02,  9.5963e-03, -3.1433e-02,\n",
       "                      -3.1725e-02, -4.7060e-02,  1.5382e-02,  1.1934e-02,  2.2682e-02,\n",
       "                      -1.2704e-02, -1.8583e-02,  3.8807e-02, -3.5829e-02, -2.5872e-02,\n",
       "                       1.2993e-02, -7.1223e-04, -3.3833e-02, -2.4219e-03, -1.8509e-02,\n",
       "                      -1.6171e-02, -1.7285e-02, -3.6405e-02, -2.5297e-02, -1.9096e-02,\n",
       "                      -2.2685e-02, -4.3045e-02, -1.2953e-02, -4.4107e-02, -4.2195e-02,\n",
       "                       3.6478e-02, -9.4919e-04, -3.6854e-03,  1.7985e-02,  4.4236e-03,\n",
       "                      -2.9499e-02,  3.4674e-02,  5.5601e-03,  3.6097e-02, -2.8175e-02,\n",
       "                       1.9038e-02, -2.2614e-02,  1.0866e-02,  2.6148e-02,  7.7025e-04,\n",
       "                      -3.2395e-03,  1.0145e-02,  1.5676e-02, -2.4237e-02,  1.1749e-02,\n",
       "                       5.1370e-03, -4.0529e-02,  1.2442e-02,  8.3175e-03,  9.0491e-03,\n",
       "                       2.3912e-02,  3.1050e-02,  2.1201e-02, -1.4476e-02, -3.0624e-02,\n",
       "                      -3.4385e-04, -2.4272e-03, -1.7598e-02, -1.2873e-02, -3.4109e-02,\n",
       "                       3.2379e-02, -1.4921e-02,  1.7241e-02,  3.3278e-02,  2.2662e-02,\n",
       "                       1.0487e-02, -1.0599e-02, -1.8666e-02,  3.4696e-03, -1.1754e-02,\n",
       "                      -4.7594e-05, -2.6030e-02, -2.8899e-02, -3.2908e-02, -3.5283e-02,\n",
       "                      -3.8941e-02,  3.2947e-02,  3.8372e-02, -4.0695e-02, -4.3631e-02,\n",
       "                      -3.6197e-02, -5.0872e-03, -5.8145e-03,  2.1076e-03,  1.1397e-02,\n",
       "                       3.5512e-02,  3.2698e-03, -1.2665e-02, -2.2345e-02,  2.2947e-02,\n",
       "                       3.5044e-02,  3.2520e-02,  2.2872e-02, -4.6385e-02,  1.2303e-02,\n",
       "                      -8.0368e-03,  1.0019e-02,  3.8141e-02, -1.2654e-02,  3.4893e-02,\n",
       "                       1.9145e-02, -3.2566e-02, -2.3438e-03,  1.1816e-02, -2.4361e-02,\n",
       "                      -2.2274e-02,  2.4198e-02,  1.1210e-02, -3.0425e-02,  2.6105e-02,\n",
       "                       1.7937e-02,  3.4842e-02,  2.5272e-02,  2.6531e-02, -8.5064e-03,\n",
       "                      -1.0903e-02, -1.8854e-02,  3.3607e-02, -1.7816e-02, -3.4816e-02,\n",
       "                      -3.9379e-03, -1.0483e-02, -1.8544e-02,  4.3051e-02,  2.6052e-02,\n",
       "                       1.3205e-02, -5.1943e-03, -4.7454e-02, -4.1338e-02,  1.8066e-04,\n",
       "                       2.4933e-02, -1.8029e-03, -3.1014e-02,  2.4151e-02,  1.8115e-02,\n",
       "                       3.7290e-02, -1.9815e-03,  8.7743e-03, -4.0289e-02, -1.9851e-02,\n",
       "                      -1.4999e-02,  2.3812e-02,  8.4512e-03, -4.6075e-02,  1.5932e-02,\n",
       "                       9.6121e-03,  2.1670e-02,  2.1077e-02, -3.8535e-02, -2.7130e-02,\n",
       "                       3.3412e-02,  3.1717e-02, -3.1823e-02, -2.5744e-02, -2.0918e-02,\n",
       "                      -2.8891e-02,  1.8161e-02, -6.4445e-03, -1.3055e-02, -5.6069e-03,\n",
       "                      -1.2495e-02,  8.8435e-03, -3.0994e-02, -3.7167e-03, -1.4174e-03,\n",
       "                       4.4229e-03,  3.8165e-02, -1.6842e-02,  1.8669e-03, -1.6539e-02,\n",
       "                      -3.9302e-02, -6.2427e-03, -2.5711e-02, -8.4783e-03, -4.4117e-02,\n",
       "                       2.1086e-02,  1.4661e-03, -1.4775e-02, -1.2559e-02,  6.3977e-03,\n",
       "                       4.1438e-03,  2.0030e-02, -2.5800e-02, -4.0849e-02,  2.1507e-02,\n",
       "                      -2.6476e-02, -2.6885e-02, -9.6328e-03,  1.2416e-02, -1.6894e-02,\n",
       "                      -4.3210e-02, -2.8012e-02,  3.8446e-03,  7.3664e-03, -4.2147e-02,\n",
       "                       2.1758e-02,  5.9470e-03, -7.3164e-03,  3.1395e-02, -1.5757e-02,\n",
       "                      -1.7167e-02, -5.8507e-03, -4.6385e-02, -3.0042e-02, -6.9387e-03,\n",
       "                      -2.5826e-02, -5.7418e-03, -2.7314e-02], device='cuda:0'))])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'seq2seq' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [35]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      3\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseq2seq.pt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 5\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(\u001b[43mseq2seq\u001b[49m, model_path)\n\u001b[1;32m      7\u001b[0m seq2seq \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(model_path, map_location\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      8\u001b[0m seq2seq\u001b[38;5;241m.\u001b[39meval()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'seq2seq' is not defined"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "\n",
    "# model_path = 'seq2seq.pt'\n",
    "\n",
    "# torch.save(seq2seq, model_path)\n",
    "\n",
    "# seq2seq = torch.load(model_path, map_location=torch.device('cuda'))\n",
    "# seq2seq.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_eval = question_eval[:5]\n",
    "answer_eval = answer_eval[:5]\n",
    "\n",
    "preds = []\n",
    "\n",
    "for question in question_eval:\n",
    "    question_tensor = toTensor(Q_vocab, question)\n",
    "    src_len = question_tensor.size(0)\n",
    "    trg_len = 20\n",
    "\n",
    "    output = model(question_tensor, question_tensor, src_len, trg_len)\n",
    "    pred = [A_vocab.index2word[i.item()] for i in output['decoder_output'][0].argmax(1)]\n",
    "    pred = ' '.join(pred)\n",
    "    preds.append(pred)\n",
    "\n",
    "results = calculate_bleu(preds, question_eval, answer_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "< menjadi fakulta yang berdaya sa internasion dan berkontribusi kepada pengembangan teknolog informasi dan ilmu komput untuk menunjang industri dan masyarakat dengan menyelaraskan pelaksanaan pendidikan penelitian dan pengabdian kepada masyarakat menjadi \n"
     ]
    }
   ],
   "source": [
    "src = \"visi filkom\"\n",
    "output = model(toTensor(Q_vocab, src), toTensor(A_vocab, \" \"), len(src.split()), 30)\n",
    "response = \"\"\n",
    "for o in output[\"decoder_output\"]:\n",
    "    response += A_vocab.index2word[o.argmax().item()] + \" \"\n",
    "\n",
    "print(\"<\", response)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "(Starter Code) LSTM Bot",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
