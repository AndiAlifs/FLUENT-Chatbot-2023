{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('/home/andyalyfsyah/FLUENT-Chatbot-2023/FLUENT_REFACTORED_24'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class FLUENTSOTA(nn.Module):\n",
    "    def __init__(self, enc_model, dec_model, enc_tokenizer, dec_tokenizer, max_length=200, dec_size=1024):\n",
    "        super(FLUENTSOTA, self).__init__()\n",
    "        self.enc_model = enc_model\n",
    "        self.dec_model = dec_model\n",
    "        self.enc_tokenizer = enc_tokenizer\n",
    "        self.dec_tokenizer = dec_tokenizer\n",
    "        self.enc_mapper = nn.Linear(1024, dec_size)\n",
    "        self.enc_mapper2 = nn.Linear(dec_size, dec_size)\n",
    "        self.prefix_nn = nn.Linear(dec_size, dec_size)\n",
    "        self.prefix_nn2 = nn.Linear(dec_size, dec_size)\n",
    "        self.prefix_nn3 = nn.Linear(dec_size, dec_size)\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def encoding(self, sentence):\n",
    "        tokens = self.enc_tokenizer(sentence, return_tensors='pt', padding=True, truncation=True, max_length=self.max_length)\n",
    "        tokens = tokens.to(device)\n",
    "        with torch.no_grad():\n",
    "            output = self.enc_model(**tokens)\n",
    "        enc_logits = output.last_hidden_state.sum(dim=1)\n",
    "        enc_logits = self.enc_mapper(enc_logits).to(device)\n",
    "        enc_logits = self.enc_mapper2(enc_logits).to(device)\n",
    "        return enc_logits\n",
    "\n",
    "    def get_embedding(self, sentence):\n",
    "        tokens = self.dec_tokenizer(sentence, return_tensors='pt', padding=True, truncation=True, max_length=self.max_length)\n",
    "        tokens = tokens.to(device)\n",
    "        wte = self.dec_model.get_input_embeddings()\n",
    "        return wte(tokens['input_ids'])\n",
    "\n",
    "    def dec_tokenizer(self, sentence):\n",
    "        tokens = self.dec_tokenizer(sentence, return_tensors='pt', padding=True, truncation=True, max_length=self.max_length)\n",
    "        return tokens\n",
    "\n",
    "    def decoding_train(self, enc_logits, target, target_with_pre):\n",
    "        embed = self.get_embedding(target)\n",
    "        pref_with_embed = torch.cat((enc_logits, embed), dim=1)\n",
    "        output = self.dec_model(inputs_embeds=pref_with_embed, labels=target_with_pre)\n",
    "        return output\n",
    "\n",
    "    def generate(self, quest):\n",
    "        enc_logits = self.encoding(quest)\n",
    "        prefix_se = '[BOS]'\n",
    "        prefix_dec_embed = self.get_embedding(prefix_se)\n",
    "        # prefixs = self.add_prefix(enc_logits)\n",
    "        \n",
    "        pref_with_embed = prefix_dec_embed\n",
    "\n",
    "        output = self.dec_model.generate(   inputs_embeds=pref_with_embed, \n",
    "                                            max_length=self.max_length, \n",
    "                                            pad_token_id=self.dec_model.config.eos_token_id)\n",
    "        returned_output = []\n",
    "        for i in range(len(output[0])):\n",
    "            if output[0][i] != self.dec_model.config.eos_token_id:\n",
    "                returned_output.append(output[0][i])\n",
    "            else:\n",
    "                break\n",
    "        return torch.tensor(returned_output).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initiliazing encoder model and tokenizer : indobenchmark/indobert-large-p1\n",
      "initiliazing decoder model and tokenizer : indonesian-nlp/gpt2-medium-indonesian\n",
      "finished initiliazing encoder model and tokenizer\n",
      "Encoder Trainable Parameters : 33.82627479857128%\n",
      "Decoder Trainable Parameters : 31.94935846747895%\n",
      "All parameters: 695,220,224\n",
      "Trainable parameters: 231,980,032 (33.37%)\n",
      "Untrainable parameters: 463,240,192 (66.63%)\n",
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/andialifs/fluent-tesis-playground-24/e/FLUEN-75\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, GPT2LMHeadModel, GPT2Tokenizer\n",
    "from sklearn.decomposition import PCA\n",
    "from torch import nn\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from data import qa_paired, qa_paired_eval\n",
    "from evaluation_tool import calculate_bleu, count_bleu_score, compute_average_chrf, generate_predictions\n",
    "from neptune_fluent import Neptune_Fluent \n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "encoder_id = 'indobenchmark/indobert-large-p1'\n",
    "print(\"initiliazing encoder model and tokenizer : {}\".format(encoder_id))\n",
    "enc_tokenizer = AutoTokenizer.from_pretrained(encoder_id, clean_up_tokenization_spaces=True)\n",
    "enc_model = AutoModel.from_pretrained(encoder_id)\n",
    "\n",
    "decoder_id = 'indonesian-nlp/gpt2-medium-indonesian'\n",
    "print(\"initiliazing decoder model and tokenizer : {}\".format(decoder_id))\n",
    "dec_model = GPT2LMHeadModel.from_pretrained(decoder_id)\n",
    "dec_tokenizer = GPT2Tokenizer.from_pretrained(decoder_id, clean_up_tokenization_spaces=True)\n",
    "\n",
    "dec_tokenizer.add_tokens(['[PRE1]'])\n",
    "dec_tokenizer.add_tokens(['[PRE2]'])\n",
    "dec_tokenizer.add_tokens(['[PRE3]'])\n",
    "dec_tokenizer.add_special_tokens({'pad_token': '[PAD]',\n",
    "                                    'bos_token': '[BOS]',\n",
    "                                    'eos_token': '[EOS]',\n",
    "                                    'sep_token': '[SEP]',})\n",
    "dec_model.config.pad_token_id = dec_tokenizer.pad_token_id\n",
    "dec_model.config.bos_token_id = dec_tokenizer.bos_token_id\n",
    "dec_model.config.eos_token_id = dec_tokenizer.eos_token_id\n",
    "dec_model.config.sep_token_id = dec_tokenizer.sep_token_id\n",
    "dec_model.resize_token_embeddings(len(dec_tokenizer))\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "enc_model = enc_model.to(device)\n",
    "dec_model = dec_model.to(device)\n",
    "print(\"finished initiliazing encoder model and tokenizer\")\n",
    "\n",
    "for param in enc_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in dec_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in dec_model.transformer.h[:-15].parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for param in enc_model.encoder.layer[:-15].parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "print(\"Encoder Trainable Parameters : {}%\".format(sum(p.numel() for p in enc_model.parameters() if p.requires_grad)/sum(p.numel() for p in enc_model.parameters())*100))\n",
    "print(\"Decoder Trainable Parameters : {}%\".format(sum(p.numel() for p in dec_model.parameters() if p.requires_grad)/sum(p.numel() for p in dec_model.parameters())*100))\n",
    "\n",
    "model = FLUENTSOTA(enc_model, dec_model, enc_tokenizer, dec_tokenizer)\n",
    "model.to(device)\n",
    "\n",
    "all_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "untrainable_params = all_params - trainable_params\n",
    "print(f'All parameters: {all_params:,}')\n",
    "print(f'Trainable parameters: {trainable_params:,} ({trainable_params/all_params*100:.2f}%)')\n",
    "print(f'Untrainable parameters: {untrainable_params:,} ({untrainable_params/all_params*100:.2f}%)')\n",
    "\n",
    "bleu_score_eval = pd.DataFrame(columns=['Epoch', '1-gram', '2-gram', '3-gram', '4-gram', 'cumulative-1-gram', 'cumulative-2-gram', 'cumulative-3-gram', 'cumulative-4-gram'])\n",
    "bleu_score_train = pd.DataFrame(columns=['Epoch', '1-gram', '2-gram', '3-gram', '4-gram', 'cumulative-1-gram', 'cumulative-2-gram', 'cumulative-3-gram', 'cumulative-4-gram'])\n",
    "\n",
    "questions = qa_paired['Pertanyaan'].apply(lambda x: x.lower().replace('[BOS]', '').replace('[EOS]', '')).to_list()\n",
    "answers = qa_paired['Jawaban'].apply(lambda x: x.replace('[BOS]', '').replace('[EOS]', '').lower().strip()).to_list()\n",
    "questions_eval = qa_paired_eval['Pertanyaan'].apply(lambda x: x.lower().replace('[BOS]', '').replace('[EOS]', '')).to_list()\n",
    "answers_eval = qa_paired_eval['Jawaban'].apply(lambda x: x.replace('[BOS]', '').replace('[EOS]', '').lower().strip()).to_list()\n",
    "\n",
    "epochs = 500\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "run = Neptune_Fluent.mulai(encoder_id, decoder_id, num_pre_token=0)\n",
    "bleu_result_eval = {\"cumulative-4-gram\":0}\n",
    "bleu_result_train = {\"cumulative-4-gram\":0}\n",
    "chrf_result_eval = 0\n",
    "chrf_result_train = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Tensors must have same number of dimensions: got 2 and 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m tokenized_jawaban_withpre \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(tokenized_jawaban_withpre[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     17\u001b[0m enc_logits \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencoding(pertanyaan)\n\u001b[0;32m---> 18\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoding_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43menc_logits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjawaban\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_with_pre\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenized_jawaban_withpre\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m loss \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m     21\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36mFLUENTSOTA.decoding_train\u001b[0;34m(self, enc_logits, target, target_with_pre)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecoding_train\u001b[39m(\u001b[38;5;28mself\u001b[39m, enc_logits, target, target_with_pre):\n\u001b[1;32m     41\u001b[0m     embed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_embedding(target)\n\u001b[0;32m---> 42\u001b[0m     pref_with_embed \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43menc_logits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdec_model(inputs_embeds\u001b[38;5;241m=\u001b[39mpref_with_embed, labels\u001b[38;5;241m=\u001b[39mtarget_with_pre)\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Tensors must have same number of dimensions: got 2 and 3"
     ]
    }
   ],
   "source": [
    "print(\"start training\")\n",
    "for ep in range(epochs):\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    total_loss = 0\n",
    "    for instance in qa_paired.iterrows():\n",
    "        # print(\"---------------\")\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        pertanyaan = instance[1]['Pertanyaan']\n",
    "        jawaban = instance[1]['Jawaban']\n",
    "        jawaban_withpre = '[PRE1]' + jawaban\n",
    "\n",
    "        tokenized_jawaban_withpre = model.dec_tokenizer(jawaban_withpre)\n",
    "        tokenized_jawaban_withpre = torch.tensor(tokenized_jawaban_withpre['input_ids']).unsqueeze(0)\n",
    "\n",
    "        enc_logits = model.encoding(pertanyaan)\n",
    "        output = model.decoding_train(enc_logits, target=jawaban, target_with_pre=tokenized_jawaban_withpre)\n",
    "\n",
    "        loss = output.loss\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    run[\"train/loss\"].append(total_loss)\n",
    "    print(f'Epoch {ep+1}/{epochs} - Loss: {total_loss:.4f}')\n",
    "    if (ep+1) % 10 == 0:\n",
    "        print(f'\\n-----------------------------------------')\n",
    "        test_question = qa_paired['Pertanyaan'].iloc[0]\n",
    "        outputs = model.generate(test_question)\n",
    "        decoded_output = model.dec_tokenizer.decode(outputs[0])\n",
    "        print(f'Q >>> {test_question}')\n",
    "        print(f'A <<< {decoded_output}')\n",
    "        test_question = qa_paired['Pertanyaan'].iloc[1]\n",
    "        outputs = model.generate(test_question)\n",
    "        decoded_output = model.dec_tokenizer.decode(outputs[0])\n",
    "        print(f'Q >>> {test_question}')\n",
    "        print(f'A <<< {decoded_output}')\n",
    "        test_question = qa_paired['Pertanyaan'].iloc[4]\n",
    "        outputs = model.generate(test_question)\n",
    "        decoded_output = model.dec_tokenizer.decode(outputs[0])\n",
    "        print(f'Q >>> {test_question}')\n",
    "        print(f'A <<< {decoded_output}')\n",
    "        print(f'-----------------------------------------\\n')\n",
    "\n",
    "    if (ep+1) % 10 == 0:\n",
    "        preds_eval = generate_predictions(model, questions_eval)\n",
    "\n",
    "        bleu_result_eval = calculate_bleu(preds_eval, questions_eval, answers_eval)\n",
    "        bleu_score_eval = pd.concat([bleu_score_eval, pd.DataFrame({'Epoch': ep+1, **bleu_result_eval}, index=[len(bleu_score_eval)])], ignore_index=True)\n",
    "        print(f'BLEU Score Eval: {bleu_result_eval[\"cumulative-4-gram\"]:.4f}\\n')\n",
    "\n",
    "        chrf_result_eval = compute_average_chrf(preds_eval, answers_eval)\n",
    "        print(f'CHRF Score Eval: {chrf_result_eval:.4f}\\n')\n",
    "\n",
    "        preds_train = generate_predictions(model, questions)\n",
    "        bleu_result_train = calculate_bleu(preds_train, questions, answers)\n",
    "        bleu_score_train = pd.concat([bleu_score_train, pd.DataFrame({'Epoch': ep+1, **bleu_result_train}, index=[len(bleu_score_train)])], ignore_index=True)\n",
    "        print(f'BLEU Score Train: {bleu_result_train[\"cumulative-4-gram\"]:.4f}\\n')\n",
    "\n",
    "        chrf_result_train = compute_average_chrf(preds_train, answers)\n",
    "        print(f'CHRF Score Train: {chrf_result_train:.4f}\\n')\n",
    "\n",
    "    run[\"eval/chrf\"].append(chrf_result_eval)\n",
    "    run[\"train/chrf\"].append(chrf_result_train)\n",
    "    run[\"eval/bleu\"].append(bleu_result_eval[\"cumulative-4-gram\"])\n",
    "    run[\"train/bleu\"].append(bleu_result_train[\"cumulative-4-gram\"])\n",
    "\n",
    "run.stop()\n",
    "print(\"finished training\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_jawaban = model.get_embedding(jawaban)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 33, 1024])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_jawaban.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 34, 1024])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((enc_logits.unsqueeze(dim=0), encoded_jawaban), dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 1024])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_logits.unsqueeze(dim=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "torch.save(model.state_dict(), 'model_state_dict_best_2225.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q >>> apa visi filkom\n",
      "A <<< menjadi fakultas yang berdaya saing internasional dan berkontribusi kepada pengembangan teknologi informasi dan ilmu komputer untuk menunjang industri dan masyarakat dengan menyelaraskan pelaksanaan pendidikan, penelitian, dan pengabdian kepada masyarakat\n"
     ]
    }
   ],
   "source": [
    "test_question = \"apa visi filkom\"\n",
    "outputs = model.generate(test_question)\n",
    "decoded_output = model.dec_tokenizer.decode(outputs[0])\n",
    "print(f'Q >>> {test_question}')\n",
    "print(f'A <<< {decoded_output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q >>> apa EMAIL pak fitra bachtiar\n",
      "A <<< anglia.ris@gmail.com.brilliantadita@yahoo.com.mynameirawan.web.idokter.com (Telusuri Lebih terperinci BAB I PENDAHULUAN. A. Latar Belakang Masalah. Perkembangan teknologi informasi yang semakin pesat, menuntut perusahaan untuk\n",
      "BAB I PENDAHULUAN A. Latar Belakang Masalah Perkembangan teknologi informasi yang semakin pesat, menuntut perusahaan untuk terus meningkatkan kualitas dan kuantitas produk yang ditawarkan kepada konsumen. Kualitas produk yang ditawarkan perusahaan dapat dilihat Lebih terperinci BAB I PENDAHULUAN. 1.1 Latar Belakang Masalah. Perkembangan teknologi informasi yang semakin pesat, menuntut perusahaan untuk\n",
      "BAB I PENDAHULUAN 1.1 Latar Belakang Masalah Perkembangan teknologi informasi yang semakin pesat, menuntut perusahaan untuk terus meningkatkan kualitas dan kuantitas produk yang ditawarkan kepada konsumen. Kualitas produk yang ditawarkan perusahaan dapat Lebih terperinci BAB I PENDAHULUAN. 1.1 Latar Belakang Masalah. Perkembangan teknologi informasi yang semakin pesat, menuntut perusahaan untuk\n",
      "BAB I PENDAHULUAN 1.1 Latar Belakang Masalah Perkembangan teknologi informasi yang semakin pesat, menuntut perusahaan untuk terus meningkatkan kualitas dan kuantitas produk yang ditawarkan kepada konsumen. Kualitas produk yang ditawarkan perusahaan dapat Lebih terperinci BAB I PENDAHULUAN. 1.1 Latar Belakang Masalah. Perkembangan teknologi informasi yang semakin pesat, menuntut perusahaan untuk\n",
      "BAB I PENDAHULUAN 1.1 Latar Belakang Masalah Perkembangan teknologi informasi yang semakin pesat, menuntut perusahaan untuk terus meningkatkan kualitas dan kuantitas produk yang ditawarkan kepada konsumen. Kualitas produk yang ditawarkan perusahaan dapat Lebih terperinci BAB I PENDAHULUAN. 1.1 Latar Belakang. Perkembangan teknologi informasi yang semakin pesat, menuntut perusahaan untuk\n",
      "BAB I PENDAHULUAN 1.1 Latar Belakang Perkembangan teknologi informasi yang semakin pesat, menuntut perusahaan untuk terus meningkatkan kualitas dan kuantitas produk yang ditawarkan kepada konsumen. Kualitas produk yang ditawarkan perusahaan dapat Lebih terperinci BAB I PENDAHULUAN. 1.1 Latar Belakang. Perkembangan teknologi informasi yang semakin pesat, menuntut perusahaan untuk\n",
      "BAB I PENDAHULUAN 1.1 Latar Belakang Perkembangan teknologi informasi yang semakin pesat, menuntut perusahaan untuk terus meningkatkan kualitas dan kuantitas produk yang ditawarkan kepada konsumen. Kualitas produk yang ditawarkan perusahaan dapat Lebih terperinci BAB I PENDAHULUAN. 1.1 Latar Belakang. Perkembangan teknologi informasi yang semakin pesat, menuntut perusahaan untuk\n",
      "BAB I PENDAHULUAN 1.1 Latar Belakang Perkembangan teknologi informasi yang semakin pesat, menuntut perusahaan untuk terus meningkatkan kualitas dan kuantitas produk yang ditawarkan kepada konsumen. Kualitas produk yang ditawarkan perusahaan dapat Lebih terperinci BAB I PENDAHULUAN. 1.1 Latar Belakang. Perkembangan teknologi informasi yang semakin pesat, menuntut perusahaan untuk\n",
      "BAB I PENDAHULUAN 1.1 Latar Belakang Perkembangan teknologi informasi yang semakin pesat, menuntut perusahaan untuk terus meningkatkan kualitas dan kuantitas produk yang ditawarkan kepada konsumen. Kualitas produk yang ditawarkan perusahaan dapat Lebih terperinci BAB I PENDAHULUAN. 1. 1. 1. Latar Belakang. Perkembangan teknologi informasi yang semakin pesat, menuntut perusahaan untuk terus meningkatkan kualitas\n",
      "BAB I PENDAHULUAN 1. Latar Belakang Perkembangan teknologi informasi yang semakin pesat, menuntut perusahaan untuk terus meningkatkan kualitas dan kuantitas produk yang ditawarkan kepada konsumen. Kualitas produk yang ditawarkan perusahaan dapat dilihat dari segi harga, kualitas, kecepatan, dan ketepatan waktu. Kualitas produk yang ditawarkan perusahaan dapat dilihat Lebih terperinci BAB I. 1. 1. 1. 1. 1. 1. Latar Belakang. 1. 1. 1. 1. 1. 1. Latar Belakang. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"
     ]
    }
   ],
   "source": [
    "test_question = \"apa EMAIL pak fitra bachtiar\"\n",
    "outputs = model.generate(test_question)\n",
    "decoded_output = model.dec_tokenizer.decode(outputs[0])\n",
    "print(f'Q >>> {test_question}')\n",
    "print(f'A <<< {decoded_output}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
