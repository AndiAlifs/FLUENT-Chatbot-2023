{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pJAWnBFlkE2w"
   },
   "source": [
    "# LSTM Bot\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "In this project, you will build a chatbot that can converse with you at the command line. The chatbot will use a Sequence to Sequence text generation architecture with an LSTM as it's memory unit. You will also learn to use pretrained word embeddings to improve the performance of the model. At the conclusion of the project, you will be able to show your chatbot to potential employers.\n",
    "\n",
    "Additionally, you have the option to use pretrained word embeddings in your model. We have loaded Brown Embeddings from Gensim in the starter code below. You can compare the performance of your model with pre-trained embeddings against a model without the embeddings.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "A sequence to sequence model (Seq2Seq) has two components:\n",
    "- An Encoder consisting of an embedding layer and LSTM unit.\n",
    "- A Decoder consisting of an embedding layer, LSTM unit, and linear output unit.\n",
    "\n",
    "The Seq2Seq model works by accepting an input into the Encoder, passing the hidden state from the Encoder to the Decoder, which the Decoder uses to output a series of token predictions.\n",
    "\n",
    "## Dependencies\n",
    "\n",
    "- Pytorch\n",
    "- Numpy\n",
    "- Pandas\n",
    "- NLTK\n",
    "- Gzip\n",
    "- Gensim\n",
    "\n",
    "\n",
    "Please choose a dataset from the Torchtext website. We recommend looking at the Squad dataset first. Here is a link to the website where you can view your options:\n",
    "\n",
    "- https://pytorch.org/text/stable/datasets.html\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -tplotlib (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -aitlets (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -yping-extensions (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -yjwt (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -wagger-spec-validator (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -uture (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -tplotlib (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -sonschema (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -sonschema-specifications (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -sonref (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -sgpack (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ravado (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ravado-core (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -raitlets (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -penpyxl (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -pds-py (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -oto3 (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -onotonic (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ltk (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ix (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -implejson (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution - (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -equests (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -eptune (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -eferencing (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ebsocket-client (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -cwidth (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -cikit-learn (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -atplotlib (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -tplotlib (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -aitlets (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -yping-extensions (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -yjwt (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -wagger-spec-validator (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -uture (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -tplotlib (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -sonschema (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -sonschema-specifications (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -sonref (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -sgpack (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ravado (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ravado-core (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -raitlets (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -penpyxl (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -pds-py (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -oto3 (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -onotonic (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ltk (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ix (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -implejson (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution - (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -equests (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -eptune (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -eferencing (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ebsocket-client (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -cwidth (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -cikit-learn (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -atplotlib (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "Requirement already satisfied: nltk in ./.local/lib/python3.10/site-packages (3.9.1)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk) (2024.7.24)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk) (8.1.7)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -tplotlib (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -aitlets (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -yping-extensions (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -yjwt (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -wagger-spec-validator (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -uture (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -tplotlib (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -sonschema (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -sonschema-specifications (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -sonref (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -sgpack (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ravado (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ravado-core (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -raitlets (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -penpyxl (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -pds-py (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -oto3 (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -onotonic (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ltk (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ix (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -implejson (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution - (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -equests (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -eptune (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -eferencing (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ebsocket-client (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -cwidth (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -cikit-learn (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -atplotlib (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -tplotlib (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -aitlets (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -yping-extensions (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -yjwt (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -wagger-spec-validator (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -uture (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -tplotlib (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -sonschema (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -sonschema-specifications (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -sonref (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -sgpack (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ravado (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ravado-core (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -raitlets (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -penpyxl (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -pds-py (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -oto3 (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -onotonic (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ltk (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ix (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -implejson (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution - (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -equests (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -eptune (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -eferencing (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ebsocket-client (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -cwidth (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -cikit-learn (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -atplotlib (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -tplotlib (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -aitlets (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -yping-extensions (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -yjwt (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -wagger-spec-validator (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -uture (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -umpy (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -tplotlib (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -sonschema (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -sonschema-specifications (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -sonref (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -sgpack (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ravado (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ravado-core (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -raitlets (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -penpyxl (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -pds-py (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -oto3 (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -onotonic (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ltk (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ix (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -implejson (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution - (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -equests (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -eptune (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -eferencing (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -ebsocket-client (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -cwidth (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -cikit-learn (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -atplotlib (/opt/conda/lib/python3.10/site-packages)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import string\n",
    "import torch\n",
    "import neptune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = nltk.stem.snowball.SnowballStemmer('english')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def getDict(dataPipe):\n",
    "\n",
    "    data_dict = {\n",
    "        'Question': [],\n",
    "        'Answer': []\n",
    "    }\n",
    "    \n",
    "    for _, question, answers, _ in dataPipe:\n",
    "        data_dict['Question'].append(question)\n",
    "        data_dict['Answer'].append(answers[0])\n",
    "        \n",
    "    return data_dict\n",
    "\n",
    "\n",
    "def loadDF(path):\n",
    "    # load data\n",
    "    train_data, val_data = torchtext.datasets.SQuAD2|(path)\n",
    "    \n",
    "    # convert dataPipe to dictionary \n",
    "    train_dict, val_dict = getDict(train_data), getDict(val_data)\n",
    "    \n",
    "    # convert Dictionaries to Pandas DataFrame\n",
    "    train_df = pd.DataFrame(train_dict)    \n",
    "    validation_df = pd.DataFrame(val_dict)    \n",
    "    \n",
    "    return train_df.append(validation_df)\n",
    "\n",
    "\n",
    "def prepare_text(sentence):\n",
    "    # clean text and tokenize it \n",
    "    sentence = ''.join([s.lower() for s in sentence if s not in string.punctuation])\n",
    "    sentence = ' '.join(stemmer.stem(w) for w in sentence.split())\n",
    "    tokens = nltk.tokenize.RegexpTokenizer(r'\\w+').tokenize(sentence)\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def toTensor(vocab, sentence):\n",
    "    # convert list of words \"sentence\" to a torch tensor of indices\n",
    "    indices = [vocab.word2index[word] for word in sentence.split(' ')]\n",
    "    indices.append(vocab.word2index[''])\n",
    "    return torch.Tensor(indices).long().to(device).view(-1, 1)\n",
    "\n",
    "\n",
    "def getPairs(df):\n",
    "    # convert df to list of pairs\n",
    "    temp1 = df['Question'].apply(lambda x: \" \".join(x) ).to_list()\n",
    "    temp2 = df['Answer'].apply(lambda x: \" \".join(x) ).to_list()\n",
    "    return [list(i) for i in zip(temp1, temp2)]\n",
    "\n",
    "\n",
    "def getMaxLen(pairs):\n",
    "    max_src = 0 \n",
    "    max_trg = 0\n",
    "    \n",
    "    for p in pairs:\n",
    "        max_src = len(p[0].split()) if len(p[0].split()) > max_src else max_src\n",
    "        max_trg = len(p[1].split()) if len(p[1].split()) > max_trg else max_trg\n",
    "        \n",
    "    return max_src, max_trg\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arsitektur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(self.input_size, self.hidden_size)\n",
    "        self.input = nn.Linear(self.input_size, self.hidden_size)\n",
    "        self.lstm = nn.LSTM(self.hidden_size, self.hidden_size)\n",
    "\n",
    "    def forward(self, x, hidden, cell_state):\n",
    "        x = self.embedding(x)\n",
    "        x = x.view(1, 1, -1)\n",
    "        x, (hidden, cell_state) = self.lstm(x, (hidden, cell_state))\n",
    "        return x, hidden, cell_state\n",
    "        \n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_size, self.hidden_size)\n",
    "        self.lstm = nn.LSTM(self.hidden_size, self.hidden_size)\n",
    "        self.fc = nn.Linear(self.hidden_size, self.output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim= 1)\n",
    "\n",
    "    def forward(self, x, hidden, cell_state):\n",
    "        x = self.embedding(x)\n",
    "        x = x.view(1, 1, -1)\n",
    "        x, (hidden, cell_state) = self.lstm(x, (hidden, cell_state))\n",
    "        x = self.softmax(self.fc(x[0]))\n",
    "        return x, hidden, cell_state\n",
    "    \n",
    "    \n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.encoder = Encoder(self.input_size, self.hidden_size)\n",
    "        self.decoder = Decoder(self.hidden_size, self.output_size)\n",
    "        \n",
    "    def forward(self, src, trg, src_len, trg_len, teacher_force=1):\n",
    "        \n",
    "        output = {\n",
    "            'decoder_output':[]\n",
    "        }\n",
    "        \n",
    "        encoder_hidden = torch.zeros([1, 1, self.hidden_size]).to(device) # 1 = number of LSTM layers\n",
    "        cell_state = torch.zeros([1, 1, self.hidden_size]).to(device)  \n",
    "        \n",
    "        for i in range(src_len):\n",
    "            encoder_output, encoder_hidden, cell_state = self.encoder(src[i], encoder_hidden, cell_state)\n",
    "\n",
    "        decoder_input = torch.Tensor([[0]]).long().to(device) # 0 = SOS_token\n",
    "        decoder_hidden = encoder_hidden\n",
    "        \n",
    "        for i in range(trg_len):\n",
    "            decoder_output, decoder_hidden, cell_state = self.decoder(decoder_input, decoder_hidden, cell_state)\n",
    "            output['decoder_output'].append(decoder_output)\n",
    "            \n",
    "            if self.training: # Model not in eval mode\n",
    "                decoder_input = target_tensor[i] if random.random() > teacher_force else decoder_output.argmax(1) # teacher forcing\n",
    "            else:\n",
    "                _, top_index = decoder_output.data.topk(1)\n",
    "                decoder_input = top_index.squeeze().detach()\n",
    "                \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = \"andialifs/siet-24\"\n",
    "api_token = \"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJjZTY2YWQ3My04OTBkLTQ2OWUtYTc1Ni1jYjk0MGZhMWFiNGEifQ==\"\n",
    "\n",
    "def neptune_init(name):\n",
    "    run = neptune.init_run(\n",
    "        project=project,\n",
    "        api_token=api_token,\n",
    "        name=name\n",
    "    )\n",
    "    return run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def train(source_data, target_data, model, epochs, batch_size, print_every, learning_rate):\n",
    "    model.to(device)\n",
    "    total_training_loss = 0\n",
    "    total_valid_loss = 0\n",
    "    loss = 0\n",
    "    \n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    # use cross validation\n",
    "    kf = KFold(n_splits=epochs, shuffle=True)\n",
    "\n",
    "    run = neptune_init(\"LSTM_FLUENT_Baseline_2\")\n",
    "\n",
    "    for e, (train_index, test_index) in enumerate(kf.split(source_data), 1):\n",
    "        model.to(device)\n",
    "        model.train()\n",
    "        for i in range(0, len(train_index)):\n",
    "\n",
    "            src = source_data[i]\n",
    "            trg = target_data[i]\n",
    "\n",
    "            output = model(src, trg, src.size(0), trg.size(0))\n",
    "\n",
    "            current_loss = 0\n",
    "            for (s, t) in zip(output[\"decoder_output\"], trg): \n",
    "                current_loss += criterion(s, t)\n",
    "\n",
    "            loss += current_loss\n",
    "            total_training_loss += (current_loss.item() / trg.size(0)) # add the iteration loss\n",
    "\n",
    "            if i % batch_size == 0 or i == (len(train_index)-1):\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                loss = 0\n",
    "\n",
    "\n",
    "        # validation set \n",
    "        model.eval()\n",
    "        for i in range(0, len(test_index)):\n",
    "            src = source_data[i]\n",
    "            trg = target_data[i]\n",
    "\n",
    "            output = model(src, trg, src.size(0), trg.size(0))\n",
    "\n",
    "            current_loss = 0\n",
    "            for (s, t) in zip(output[\"decoder_output\"], trg): \n",
    "                current_loss += criterion(s, t)\n",
    "\n",
    "            total_valid_loss += (current_loss.item() / trg.size(0)) # add the iteration loss\n",
    "\n",
    "\n",
    "        if e % print_every == 0:\n",
    "            training_loss_average = total_training_loss / (len(train_index)*print_every)\n",
    "            validation_loss_average = total_valid_loss / (len(test_index)*print_every)\n",
    "            print(\"{}/{} Epoch  -  Training Loss = {:.4f}  -  Validation Loss = {:.4f}\".format(e, epochs, training_loss_average, validation_loss_average))\n",
    "            run['train/loss'].append(training_loss_average)\n",
    "            total_training_loss = 0\n",
    "            total_valid_loss = 0 \n",
    "    \n",
    "    run.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "class Vocab:\n",
    "    def __init__(self):\n",
    "        self.word2index = {\"\": SOS_token, \"\": EOS_token}\n",
    "        self.index2word = {SOS_token: \"\", EOS_token: \"\"}\n",
    "        self.words_count = len(self.word2index)\n",
    "\n",
    "    def add_words(self, sentence):\n",
    "        for word in sentence.split(\" \"):\n",
    "            if word not in self.word2index:\n",
    "                self.word2index[word] = self.words_count\n",
    "                self.index2word[self.words_count] = word\n",
    "                self.words_count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "hidden_size = 512 # encoder and decoder hidden size\n",
    "batch_size = 50\n",
    "epochs = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "knowledgebase = pd.read_excel('https://raw.githubusercontent.com/AndiAlifs/FLUENT-Chatbot-2023/main/KnowledgeBaseFilkom.xlsx', engine='openpyxl')\n",
    "# knowledgebase = pd.read_excel('https://raw.githubusercontent.com/AndiAlifs/FLUENT-Chatbot-2023/main/KnowledgeBaseFilkom_simple.xlsx', engine='openpyxl')\n",
    "knowledgebase.head()\n",
    "\n",
    "qa_paired = knowledgebase.drop(columns=knowledgebase.columns.drop(['Pertanyaan', 'Jawaban']))\n",
    "qa_paired.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_df = loadDF('data')\n",
    "# I will take only the first 5,000 Q&A to avoid CUDA out of memory error due to the large dataset\n",
    "# data_df = data_df.iloc[:5000, :]\n",
    "data_df = pd.DataFrame(columns=['Question', 'Answer'])\n",
    "data_df['Question'] = qa_paired['Pertanyaan']\n",
    "data_df['Answer'] = qa_paired['Jawaban']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">  email Fitra A. Bachtiar \n",
      "<   fitra.bachtiar[at]ub.ac.id \n",
      "\n",
      ">  NIK/NIP Fitra A. Bachtiar \n",
      "<  198406282019031006 \n",
      "\n",
      ">  nama lengkap Fitra A. Bachtiar \n",
      "<  Dr.Eng. Fitra A. Bachtiar \n",
      "\n",
      ">  Departemen Fitra A. Bachtiar \n",
      "<  Departemen Teknik Informatika \n",
      "\n",
      ">  Program Studi Fitra A. Bachtiar \n",
      "<  S2 Ilmu Komputer \n",
      "\n",
      ">  bidang penelitian Fitra A. Bachtiar \n",
      "<  Affective Computing, Affective Engineering, Intelligent System, Data Mining, Educational Data Mining \n",
      "\n",
      ">  nama awal Fakultas Ilmu Komputer (FILKOM) \n",
      "<  Program Teknologi Informasi dan Ilmu Komputer (PTIIK) \n",
      "\n",
      ">  rujukan surat keputusan SK Dikti dibentuk PTIIK \n",
      "<   SK Dikti No.163/KEP/DIKTI/2007  \n",
      "\n",
      ">  surat keputusan SK Rektor bentuk PTIIK \n",
      "<  Surat Keputusan Rektor Universitas Brawijaya Nomor: 516/SK/2011 \n",
      "\n",
      ">  tanggal dibentuk PTIIK \n",
      "<  27 Oktober 2011 \n",
      "\n",
      ">  program studi pembentuk PTIIK \n",
      "<  Teknik Perangkat Lunak dari Fakultas Teknik dan Ilmu Komputer dari Fakultas MIPA \n",
      "\n",
      ">  tanggal perubahan PTIIK menjadi FILKOM \n",
      "<  10 Desember 2014 \n",
      "\n",
      ">  surat keputusan SK Dikti FILKOM \n",
      "<  Surat Dikti No. 8073/EI/OT/2014 tentang Organisasi dan Tata Kerja Universitas Brawijaya \n",
      "\n",
      ">  visi FILKOM \n",
      "<  Menjadi Fakultas yang berdaya saing internasional dan berkontribusi kepada pengembangan Teknologi Informasi dan Ilmu Komputer untuk menunjang industri dan masyarakat dengan menyelaraskan pelaksanaan Pendidikan, Penelitian, dan Pengabdian kepada Masyarakat \n",
      "\n",
      ">  misi FILKOM \n",
      "<  Menyelenggarakan pendidikan di bidang Teknologi Informasi dan Ilmu Komputer yang berkualitas dan berstandar internasional secara berkelanjutan.\n",
      "Meningkatkan kemampuan sivitas akademika dalam pengembangan penelitian dan pengabdian yang selaras dengan kebutuhan industri dan masyarakat.\n",
      "Mengintegrasikan pengembangan pendidikan, penelitian, dan pengabdian kepada masyarakat yang ditunjang dengan tatakelola organisasi yang transparan, akuntabel, efektif, dan efisien.\n",
      "Mewujudkan kerja sama yang berkelanjutan di bidang pendidikan, penelitian, dan pengabdian kepada masyarakat dalam skala nasional dan internasional. \n",
      "\n",
      ">  apa tujuan filkom? \n",
      "<  Menghasilkan lulusan yang kompeten , profesional, berbudi pekerti luhur, berjiwa entrepreneur dan berdaya saing internasional.\n",
      "Menghasilkan sivitas akademika yang mampu mengembangkan penelitiandan pengabdian yang berorientasi pada pembaruan dan teknologi tepat guna untuk industri dan masyarakat.\n",
      "Terwujudnya suasana akademik yang kondusif dalam bidang pendidikan, penelitian, dan pengabdian kepada masyarakat yang berdaya saing unggul.\n",
      "Terwujudnya tata kelola organisasi yang transparan, akuntabel, efektif, dan efisien.\n",
      "Meningkatnya kuantitas dan kualitas kerja sama yang berkelanjutan di bidang pendidikan, penelitian, dan pengabdian kepada masyarakat dalam skala nasional dan internasional. \n",
      "\n",
      ">  sasaran pendidikan FILKOM \n",
      "<  1. Meningkatkan kompetensi dan kualifikasi pendidikan Dosen\n",
      "2. Meningkatkan sarana dan prasarana pembelajaran\n",
      "3. Mengembangkan kurikulum mengkuti perkembangan dan kebutuhan pemangku kepentingan\n",
      "4. Meningkatkan mutu lulusan yang berkualitas\n",
      "5. Mempercepat masa studi\n",
      "6. Meningkatkan kompetensi lulusan tersertifikasi bidang TIK\n",
      "7. Meningkatkan prestasi mahasiswa\n",
      "8. Meningkatkan mutu kelembagaan \n",
      "\n",
      ">  sasaran penelitian FILKOM \n",
      "<  1. Meningkatkan jumlah publikasi ilmiah\n",
      "2. Meningkatkan sarana dan prasarana penelitian\n",
      "3. Mengembangkan Grup Riset atau kelompok kajian \n",
      "\n",
      ">  sasaran pengabdian FILKOM \n",
      "<  1. Meningkatkan kualitas dan kuantitas pengabdian masyarakat\n",
      "2. Meningkatkan pemberdayaan masyarakat\n",
      "3. Menyediakan dana, sarana dan prasarana pengabdian masyarakat \n",
      "\n",
      ">  sasaran kerjasama FILKOM \n",
      "<  1. Mengadakan kerjasama pendidikan, penlitian dan pengabdian baik di tingkat Nasional maupun internasional\n",
      "2. Peningkatan kerjasama dengan industri dalam pengembangan produk \n",
      "\n",
      ">  Dekan Fakultas Ilmu Komputer FILKOM \n",
      "<  Prof. Ir. Wayan Firdaus Mahmudy, S.Si., MT., Ph.D. \n",
      "\n",
      ">  Wakil Dekan Bidang Akademik / Wakil Dekan 1 \n",
      "<  Dr. Eng. Ir. Herman Tolle, ST., MT. \n",
      "\n",
      ">  Wakil Dekan Bidang Umum, Keuangan, dan Sumber Daya / Wakil Dekan 2 \n",
      "<  Agus Wahyu Widodo, ST., M.Cs. \n",
      "\n",
      ">  Wakil Dekan Bidang Kemahasiswaan, Alumni, dan Kewirausahaan Mahasiswa / Wakil Dekan 3 \n",
      "<  Drs. Muh. Arif Rahman, M.Kom. \n",
      "\n",
      ">  Ketua Departemen Teknik Informatika \n",
      "<  Achmad Basuki, S.T., M.MG., Ph.D. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 25): # first 5 Q&A\n",
    "    print(\"> \", data_df.iloc[i,0], \"\\n< \", data_df.iloc[i,1], \"\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df['Question'] = data_df['Question'].apply(prepare_text)\n",
    "data_df['Answer'] = data_df['Answer'].apply(prepare_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = getPairs(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(290, 13)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_src, max_trg = getMaxLen(pairs)\n",
    "max_trg, max_src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_vocab = Vocab()\n",
    "A_vocab = Vocab()\n",
    "\n",
    "# build vocabularies for questions \"source\" and answers \"target\"\n",
    "for pair in pairs:\n",
    "    Q_vocab.add_words(pair[0])\n",
    "    A_vocab.add_words(pair[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_data = [toTensor(Q_vocab, pair[0]) for pair in pairs]\n",
    "target_data = [toTensor(A_vocab, pair[1]) for pair in pairs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[neptune] [warning] NeptuneWarning: By default, these monitoring options are disabled in interactive sessions: 'capture_stdout', 'capture_stderr', 'capture_traceback', 'capture_hardware_metrics'. You can set them to 'True' when initializing the run and the monitoring will continue until you call run.stop() or the kernel stops. NOTE: To track the source files, pass their paths to the 'source_code' argument. For help, see: https://docs.neptune.ai/logging/source_code/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/andialifs/siet-24/e/SIET-172\n",
      "1/150 Epoch  -  Training Loss = 7.7315  -  Validation Loss = 7.1495\n",
      "2/150 Epoch  -  Training Loss = 6.7575  -  Validation Loss = 6.5846\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m seq2seq \u001b[38;5;241m=\u001b[39m Seq2Seq(Q_vocab\u001b[38;5;241m.\u001b[39mwords_count, hidden_size, A_vocab\u001b[38;5;241m.\u001b[39mwords_count)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msource_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m      \u001b[49m\u001b[43mtarget_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtarget_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mseq2seq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m      \u001b[49m\u001b[43mprint_every\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m      \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m      \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m      \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(source_data, target_data, model, epochs, batch_size, print_every, learning_rate)\u001b[0m\n\u001b[1;32m     36\u001b[0m total_training_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (current_loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m/\u001b[39m trg\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)) \u001b[38;5;66;03m# add the iteration loss\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m batch_size \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m i \u001b[38;5;241m==\u001b[39m (\u001b[38;5;28mlen\u001b[39m(train_index)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 39\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     41\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "seq2seq = Seq2Seq(Q_vocab.words_count, hidden_size, A_vocab.words_count)\n",
    "\n",
    "train(source_data = source_data,\n",
    "    target_data = target_data,\n",
    "    model = seq2seq,\n",
    "    print_every = 1,\n",
    "    epochs = epochs,\n",
    "    learning_rate = learning_rate,\n",
    "    batch_size = batch_size\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('encoder.embedding.weight',\n",
       "              tensor([[ 0.0196,  0.4361,  0.3396,  ..., -0.0272,  0.8641,  0.1193],\n",
       "                      [ 0.3998, -0.6889, -1.2088,  ...,  0.9454,  0.3381,  1.3806],\n",
       "                      [ 0.7484,  0.0165,  0.6383,  ..., -0.4718,  0.7882,  1.5721],\n",
       "                      ...,\n",
       "                      [-0.3652,  0.0491,  0.3054,  ..., -0.4372,  0.0495,  1.3361],\n",
       "                      [ 0.6694,  0.3212, -0.2113,  ..., -0.1426, -0.2712, -1.2462],\n",
       "                      [-1.5181,  0.7646,  1.0461,  ..., -0.5790, -0.3576,  1.1399]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.input.weight',\n",
       "              tensor([[-0.0104,  0.0050, -0.0154,  ...,  0.0258,  0.0024, -0.0348],\n",
       "                      [ 0.0046, -0.0059,  0.0146,  ...,  0.0359, -0.0085,  0.0135],\n",
       "                      [ 0.0015, -0.0158, -0.0250,  ..., -0.0215, -0.0038,  0.0253],\n",
       "                      ...,\n",
       "                      [ 0.0076,  0.0101,  0.0143,  ...,  0.0325, -0.0179, -0.0264],\n",
       "                      [ 0.0253, -0.0061,  0.0270,  ..., -0.0034, -0.0039,  0.0128],\n",
       "                      [-0.0290,  0.0363,  0.0033,  ...,  0.0053, -0.0213,  0.0202]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.input.bias',\n",
       "              tensor([-1.9426e-02, -3.1911e-02,  2.6398e-02,  4.5225e-03, -3.2702e-02,\n",
       "                       9.4538e-04, -2.7503e-02, -2.8288e-02,  2.0007e-02, -1.2840e-02,\n",
       "                       1.6093e-02, -3.6140e-02, -3.3364e-02, -1.5191e-02,  5.3381e-03,\n",
       "                      -1.0808e-02,  6.1046e-03,  2.9323e-02,  8.5395e-03, -2.0316e-02,\n",
       "                       1.3708e-02, -6.1322e-03, -3.1430e-02, -1.8971e-02,  3.3543e-02,\n",
       "                      -1.2635e-03, -3.3223e-02,  1.9892e-02, -1.5848e-02,  1.0156e-02,\n",
       "                      -1.9655e-02,  3.0784e-02,  2.7377e-02, -3.1777e-02,  1.6061e-03,\n",
       "                       1.5626e-02, -5.2077e-03, -3.0174e-02, -3.0236e-02, -3.5815e-02,\n",
       "                       1.4817e-02, -1.4322e-02,  1.3247e-02,  3.3635e-02, -1.7817e-02,\n",
       "                       2.4370e-02,  2.2963e-02,  1.9137e-02,  9.5446e-04,  2.0518e-02,\n",
       "                       2.8352e-02, -1.7720e-02,  2.8645e-02,  3.4057e-02, -3.3916e-02,\n",
       "                      -2.9594e-02,  2.8372e-02,  8.8096e-03, -1.2227e-02, -1.8641e-02,\n",
       "                      -3.1430e-02,  3.0520e-02,  4.2873e-03, -2.0402e-03, -4.3228e-03,\n",
       "                      -1.6186e-02,  2.1514e-02, -2.2622e-02, -2.0063e-02,  4.8404e-03,\n",
       "                      -2.0226e-02,  1.2779e-02, -5.0731e-04,  1.1530e-02, -2.6075e-02,\n",
       "                       1.9619e-02,  4.3145e-03,  2.7963e-03, -1.1705e-02, -6.1701e-03,\n",
       "                       8.1524e-03,  2.1609e-02, -1.4504e-02,  2.8973e-02, -1.1075e-02,\n",
       "                       7.3968e-04, -2.3310e-02, -1.1575e-02,  2.5322e-02, -3.2879e-02,\n",
       "                      -8.2731e-03, -1.5125e-02,  1.2467e-02, -2.9411e-02,  4.8418e-03,\n",
       "                      -3.2118e-02, -1.0901e-02,  3.6443e-02, -9.2000e-03, -3.0305e-02,\n",
       "                      -2.0899e-02,  1.3369e-02, -2.2028e-02,  4.6461e-03, -3.0504e-03,\n",
       "                      -6.3632e-03, -2.8476e-02,  5.2024e-03,  2.5435e-02, -2.5981e-02,\n",
       "                       1.8362e-02, -1.6694e-02,  1.9533e-02,  2.7183e-02, -2.1336e-02,\n",
       "                       1.9090e-02,  2.3409e-03,  3.3894e-02,  1.5973e-02, -7.0499e-03,\n",
       "                      -2.5814e-02,  2.5575e-02,  7.9871e-03, -1.5405e-03,  2.0761e-02,\n",
       "                      -1.2957e-02,  2.1140e-02, -9.8451e-03, -1.9675e-02,  1.3705e-02,\n",
       "                      -3.2381e-02,  2.4737e-02,  2.9534e-03, -3.3172e-02,  2.8576e-02,\n",
       "                      -3.4077e-02,  3.3722e-02,  3.2139e-02,  1.7809e-02, -2.5322e-02,\n",
       "                      -1.4288e-02, -1.3441e-02, -1.3450e-02,  3.3298e-02,  1.9926e-03,\n",
       "                       3.0398e-02,  2.9054e-02, -2.8741e-02, -2.3237e-02, -6.6101e-03,\n",
       "                       1.7534e-02,  1.2855e-02,  1.3373e-02,  3.6554e-02,  1.2647e-02,\n",
       "                       1.5712e-02, -2.5265e-02, -3.4274e-02, -1.2589e-02,  7.9127e-04,\n",
       "                       3.1043e-02,  1.2578e-02, -3.2065e-02, -1.2315e-02,  1.5726e-02,\n",
       "                       1.6526e-02,  3.6004e-02,  3.3197e-02, -1.4852e-02, -2.9291e-02,\n",
       "                      -3.4532e-02,  1.3255e-02, -9.5221e-03,  2.8748e-02,  6.3630e-06,\n",
       "                       1.7608e-02, -2.3890e-02,  6.6393e-03, -3.0302e-02,  2.2550e-02,\n",
       "                       5.0044e-03, -3.3031e-02,  1.2095e-02,  4.6443e-03,  3.4606e-02,\n",
       "                      -1.2965e-02,  6.1205e-03, -1.3019e-02, -1.1515e-02, -1.1036e-02,\n",
       "                      -3.4300e-02, -2.6532e-02,  2.1165e-02,  1.5798e-02,  3.4134e-02,\n",
       "                       3.4228e-02,  2.3946e-02, -2.7769e-02,  2.4483e-02,  4.2697e-03,\n",
       "                      -3.1849e-03, -3.2213e-02, -2.2597e-02,  1.7596e-02,  5.8812e-03,\n",
       "                      -2.3174e-02,  2.0131e-02,  3.4850e-04, -1.5050e-02,  3.5430e-03,\n",
       "                       1.8956e-02, -6.6575e-03,  5.4897e-03, -2.3512e-02, -2.6739e-02,\n",
       "                       3.5426e-02, -2.0633e-02, -2.8724e-02,  2.9025e-02, -2.6728e-03,\n",
       "                      -8.0228e-03, -2.2561e-02,  3.1406e-03,  4.1117e-03, -2.2773e-02,\n",
       "                       2.5990e-02, -2.0552e-02, -8.8679e-03,  1.0780e-02,  1.8024e-02,\n",
       "                       3.5389e-02,  1.0138e-03,  2.1353e-02,  3.4107e-02,  1.4209e-02,\n",
       "                       2.5018e-02,  1.0597e-02,  2.0660e-02, -1.0398e-02, -2.4252e-03,\n",
       "                       1.4723e-02, -2.0792e-02,  1.4481e-02,  2.3316e-02,  3.1197e-02,\n",
       "                      -1.5852e-02, -3.5872e-02,  2.8527e-02, -3.4844e-02,  9.8076e-03,\n",
       "                      -2.4715e-02, -3.2953e-02, -3.3057e-02, -6.6329e-03,  1.1055e-02,\n",
       "                       2.0977e-03, -2.6995e-03, -9.3039e-03, -3.3006e-02,  1.7041e-02,\n",
       "                       1.7382e-02,  1.8420e-02,  1.7430e-02, -2.6035e-02,  3.6597e-02,\n",
       "                       2.2759e-02,  1.8437e-02, -1.0397e-02,  2.5783e-02,  3.2151e-02,\n",
       "                      -1.5585e-02,  4.6118e-03,  1.8035e-02, -5.3344e-03, -1.5399e-02,\n",
       "                      -1.4548e-04,  3.5981e-02, -2.2922e-02,  5.6790e-03, -2.1180e-02,\n",
       "                      -1.0119e-04,  1.7589e-02,  1.3041e-02,  1.9427e-02,  9.5295e-03,\n",
       "                      -4.4913e-03, -1.3698e-02,  5.3472e-03,  3.1559e-02, -1.6098e-02,\n",
       "                       3.2526e-02,  3.1403e-02, -2.6245e-04, -1.3956e-02, -1.4364e-03,\n",
       "                      -5.1015e-03, -3.6089e-02, -7.8857e-03,  1.8655e-02,  1.9299e-02,\n",
       "                       3.6628e-02,  1.7192e-02,  1.5887e-02, -1.1491e-03,  1.7871e-02,\n",
       "                       2.6329e-02, -1.7758e-02, -2.2181e-02, -2.8404e-02,  3.3145e-02,\n",
       "                      -2.4520e-02, -1.4941e-02,  6.9346e-03, -2.2019e-02, -4.6363e-03,\n",
       "                       3.1804e-02, -1.7937e-02,  2.4580e-02,  1.6863e-02,  9.8933e-03,\n",
       "                      -3.3939e-02,  1.0853e-02,  1.8851e-02, -3.3575e-04, -1.0123e-02,\n",
       "                      -2.1983e-02,  2.6681e-02, -3.3094e-02,  3.0561e-02, -5.8442e-03,\n",
       "                      -9.9756e-03,  2.8813e-02, -3.9793e-03,  2.0210e-02,  1.1654e-02,\n",
       "                      -1.8594e-02, -2.5427e-02,  1.3010e-02,  3.6632e-02, -1.6689e-02,\n",
       "                      -1.7892e-02, -3.5758e-02, -4.3227e-03,  9.5793e-03,  7.5483e-03,\n",
       "                       1.0683e-02, -1.9470e-03,  3.2131e-02, -1.9450e-02, -2.9622e-02,\n",
       "                       3.3649e-02,  2.7291e-02, -1.8766e-02, -3.3387e-02, -2.0578e-02,\n",
       "                       3.4966e-02,  3.6674e-02,  4.0683e-03, -3.3438e-02, -3.1159e-02,\n",
       "                       3.5177e-03,  3.1986e-02, -2.1814e-02, -1.1737e-02, -5.0729e-03,\n",
       "                      -5.2522e-03,  2.1606e-02,  8.3914e-03, -2.2264e-02,  2.6420e-02,\n",
       "                       2.8790e-02,  3.4741e-02,  3.5963e-03,  8.3018e-03,  1.9126e-03,\n",
       "                      -1.9402e-02, -2.4729e-03, -1.1265e-02, -1.6241e-02,  2.3319e-02,\n",
       "                       8.6410e-03,  2.1836e-02,  2.5864e-02, -7.7369e-03,  3.1078e-02,\n",
       "                       2.0233e-02, -3.2932e-02,  2.9554e-02,  3.2877e-02, -1.6339e-02,\n",
       "                      -5.0497e-03,  5.0420e-03,  7.3302e-03, -3.1159e-02, -2.7368e-02,\n",
       "                       4.3035e-03, -1.0547e-02,  2.2376e-02,  2.8859e-02,  2.3809e-02,\n",
       "                      -1.5344e-02, -2.2112e-02, -1.0897e-02,  2.8546e-02, -2.4785e-03,\n",
       "                       1.8948e-02, -2.9936e-02, -3.3546e-02,  3.1443e-02, -1.5408e-02,\n",
       "                       1.6055e-02,  1.5487e-02, -4.4605e-03,  2.8829e-02,  2.1334e-02,\n",
       "                      -9.2591e-03, -1.3735e-02,  5.4993e-03,  1.1417e-02, -1.6778e-02,\n",
       "                       7.0403e-03, -3.5289e-02, -9.6393e-03,  3.2675e-02,  3.2735e-02,\n",
       "                      -2.6900e-02, -8.8926e-03, -6.4439e-03, -4.8510e-03,  1.3220e-02,\n",
       "                      -3.2171e-02,  3.5047e-02, -4.0016e-03,  1.5638e-02, -2.3960e-03,\n",
       "                      -9.8192e-03, -2.4287e-02, -9.3769e-03,  2.3908e-02,  1.0693e-02,\n",
       "                       5.8919e-03, -3.4432e-02, -3.6725e-02, -1.7798e-02,  2.5222e-02,\n",
       "                       1.2824e-02, -1.2713e-02,  1.3422e-02, -8.8311e-03, -2.5263e-02,\n",
       "                       1.8233e-02,  6.9610e-03, -2.6463e-02,  3.4618e-02, -9.8391e-03,\n",
       "                       7.5469e-03,  8.2949e-03, -2.1385e-02, -2.7173e-02,  3.3178e-02,\n",
       "                      -3.5163e-02, -2.8676e-02, -3.1683e-02,  2.8317e-02,  2.3600e-04,\n",
       "                       2.2162e-02,  3.4446e-02,  3.4605e-02, -1.3513e-02,  1.2074e-02,\n",
       "                       3.2290e-02, -2.8742e-02,  2.0580e-02, -3.2784e-02, -2.6694e-02,\n",
       "                       1.6432e-03, -2.4949e-02, -1.0090e-02,  1.0235e-02, -2.0964e-02,\n",
       "                       3.0317e-02,  7.6084e-03, -1.8725e-02, -3.1682e-03,  1.4226e-02,\n",
       "                       2.8675e-02, -1.5870e-02,  3.2447e-03, -2.0997e-02, -2.2918e-02,\n",
       "                      -1.5874e-02,  1.5835e-02,  5.8580e-03, -3.4539e-02, -2.8897e-02,\n",
       "                      -3.4501e-02,  1.6972e-02,  1.7700e-02,  3.4441e-02, -3.5300e-02,\n",
       "                       6.4591e-03, -1.7350e-02,  1.4099e-02, -3.5104e-02, -1.6096e-02,\n",
       "                      -2.7521e-02, -1.9509e-02, -1.9508e-02,  2.1382e-03, -3.5054e-02,\n",
       "                      -2.4296e-02, -9.3818e-03], device='cuda:0')),\n",
       "             ('encoder.lstm.weight_ih_l0',\n",
       "              tensor([[-0.0065, -0.0337,  0.0168,  ...,  0.0300, -0.0238, -0.0043],\n",
       "                      [ 0.0352, -0.0220,  0.0071,  ...,  0.0292,  0.0290, -0.0070],\n",
       "                      [-0.0095, -0.0327,  0.0033,  ...,  0.0363,  0.0084, -0.0144],\n",
       "                      ...,\n",
       "                      [ 0.0360, -0.0358, -0.0181,  ..., -0.0239,  0.0107,  0.0177],\n",
       "                      [-0.0421,  0.0385, -0.0036,  ...,  0.0088, -0.0234, -0.0312],\n",
       "                      [-0.0055,  0.0094,  0.0316,  ..., -0.0320, -0.0210, -0.0322]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.lstm.weight_hh_l0',\n",
       "              tensor([[-0.0129,  0.0015, -0.0361,  ..., -0.0234, -0.0041,  0.0024],\n",
       "                      [-0.0163,  0.0200,  0.0126,  ...,  0.0363,  0.0106, -0.0013],\n",
       "                      [ 0.0022, -0.0362, -0.0224,  ..., -0.0348, -0.0104,  0.0336],\n",
       "                      ...,\n",
       "                      [-0.0397,  0.0347, -0.0179,  ..., -0.0037, -0.0395, -0.0214],\n",
       "                      [-0.0257,  0.0051,  0.0227,  ...,  0.0282, -0.0076, -0.0124],\n",
       "                      [ 0.0434,  0.0042, -0.0332,  ...,  0.0107,  0.0306,  0.0247]],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.lstm.bias_ih_l0',\n",
       "              tensor([-0.0290,  0.0445,  0.0141,  ..., -0.0241,  0.0174,  0.0104],\n",
       "                     device='cuda:0')),\n",
       "             ('encoder.lstm.bias_hh_l0',\n",
       "              tensor([-0.0296, -0.0320,  0.0225,  ..., -0.0349,  0.0438,  0.0308],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.embedding.weight',\n",
       "              tensor([[-2.4538e+00, -7.1961e-03, -1.3891e+00,  ..., -6.9713e-04,\n",
       "                       -5.6213e-01, -1.4063e+00],\n",
       "                      [-1.3106e-01, -7.7045e-01,  2.1188e-01,  ..., -1.9623e-01,\n",
       "                        7.7535e-01, -8.4543e-01],\n",
       "                      [-4.0823e-01, -1.7112e-01,  1.9904e+00,  ..., -1.0776e+00,\n",
       "                        5.3241e-02,  2.0155e-01],\n",
       "                      ...,\n",
       "                      [ 1.0750e+00, -1.8260e+00, -6.2936e-01,  ..., -2.8547e-01,\n",
       "                       -3.5101e-01, -9.2587e-01],\n",
       "                      [ 7.9541e-01, -1.0971e+00,  9.2813e-01,  ..., -1.1863e+00,\n",
       "                       -3.0741e-01,  1.5229e+00],\n",
       "                      [ 8.4470e-01, -1.0609e+00,  8.8515e-01,  ...,  5.3716e-01,\n",
       "                        1.3097e+00,  2.5867e-01]], device='cuda:0')),\n",
       "             ('decoder.lstm.weight_ih_l0',\n",
       "              tensor([[-0.0412,  0.0117,  0.0147,  ..., -0.0179,  0.0007,  0.0205],\n",
       "                      [ 0.0167, -0.0125,  0.0325,  ...,  0.0415,  0.0044, -0.0112],\n",
       "                      [ 0.0215, -0.0385,  0.0406,  ...,  0.0209,  0.0273,  0.0046],\n",
       "                      ...,\n",
       "                      [-0.0483, -0.0001,  0.0243,  ...,  0.0094, -0.0251, -0.0189],\n",
       "                      [ 0.0323,  0.0149,  0.0095,  ..., -0.0252, -0.0153, -0.0180],\n",
       "                      [-0.0198,  0.0031, -0.0130,  ..., -0.0249,  0.0057, -0.0245]],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.lstm.weight_hh_l0',\n",
       "              tensor([[-0.0011, -0.0233, -0.0313,  ..., -0.0283,  0.0147,  0.0205],\n",
       "                      [-0.0211,  0.0054,  0.0333,  ...,  0.0274, -0.0288,  0.0188],\n",
       "                      [ 0.0143,  0.0280, -0.0421,  ...,  0.0469, -0.0065,  0.0402],\n",
       "                      ...,\n",
       "                      [-0.0082, -0.0487, -0.0195,  ...,  0.0255,  0.0166, -0.0021],\n",
       "                      [-0.0209, -0.0407,  0.0122,  ...,  0.0197, -0.0203, -0.0095],\n",
       "                      [ 0.0312,  0.0162, -0.0456,  ..., -0.0115,  0.0347,  0.0379]],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.lstm.bias_ih_l0',\n",
       "              tensor([-0.0375,  0.0360, -0.0080,  ..., -0.0202, -0.0142,  0.0170],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.lstm.bias_hh_l0',\n",
       "              tensor([-0.0285,  0.0352, -0.0034,  ...,  0.0361, -0.0007,  0.0437],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.fc.weight',\n",
       "              tensor([[-0.0032, -0.0216,  0.0205,  ..., -0.0238, -0.0496, -0.0483],\n",
       "                      [ 0.0358,  0.1132, -0.0088,  ..., -0.0417,  0.0068, -0.0292],\n",
       "                      [-0.0176,  0.0309, -0.0289,  ...,  0.0255, -0.0051,  0.0137],\n",
       "                      ...,\n",
       "                      [-0.0174,  0.0354, -0.0189,  ..., -0.0356,  0.0024, -0.0452],\n",
       "                      [-0.0131, -0.0282,  0.0217,  ...,  0.0322,  0.0224,  0.0194],\n",
       "                      [ 0.0059,  0.0352,  0.0275,  ..., -0.0040, -0.0301, -0.0267]],\n",
       "                     device='cuda:0')),\n",
       "             ('decoder.fc.bias',\n",
       "              tensor([ 0.0045,  0.2439, -0.0326,  ...,  0.0243, -0.0538,  0.0032],\n",
       "                     device='cuda:0'))])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq2seq.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(740, 512)\n",
       "    (input): Linear(in_features=740, out_features=512, bias=True)\n",
       "    (lstm): LSTM(512, 512)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(2655, 512)\n",
       "    (lstm): LSTM(512, 512)\n",
       "    (fc): Linear(in_features=512, out_features=2655, bias=True)\n",
       "    (softmax): LogSoftmax(dim=1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "model_path = 'seq2seq.pt'\n",
    "\n",
    "torch.save(seq2seq, model_path)\n",
    "\n",
    "seq2seq = torch.load(model_path, map_location=torch.device('cuda'))\n",
    "seq2seq.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type 'exit' to finish the chat.\n",
      " ------------------------------ \n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'seq2seq' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m src\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexit\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mseq2seq\u001b[49m(toTensor(Q_vocab, src), toTensor(A_vocab, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;28mlen\u001b[39m(src\u001b[38;5;241m.\u001b[39msplit()), \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      7\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder_output\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'seq2seq' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Type 'exit' to finish the chat.\\n\", \"-\"*30, '\\n')\n",
    "while (True):\n",
    "    src = input(\"> \")\n",
    "    if src.strip() == \"exit\":\n",
    "        break\n",
    "    output = seq2seq(toTensor(Q_vocab, src), toTensor(A_vocab, \" \"), len(src.split()), 1)\n",
    "    response = \"\"\n",
    "    for o in output[\"decoder_output\"]:\n",
    "        response += A_vocab.index2word[o.argmax().item()] + \" \"\n",
    "    \n",
    "    print(\"<\", response)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "(Starter Code) LSTM Bot",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "kernel-alip",
   "language": "python",
   "name": "kernel-alip"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
