{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pJAWnBFlkE2w"
   },
   "source": [
    "# LSTM Bot\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "In this project, you will build a chatbot that can converse with you at the command line. The chatbot will use a Sequence to Sequence text generation architecture with an LSTM as it's memory unit. You will also learn to use pretrained word embeddings to improve the performance of the model. At the conclusion of the project, you will be able to show your chatbot to potential employers.\n",
    "\n",
    "Additionally, you have the option to use pretrained word embeddings in your model. We have loaded Brown Embeddings from Gensim in the starter code below. You can compare the performance of your model with pre-trained embeddings against a model without the embeddings.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "A sequence to sequence model (Seq2Seq) has two components:\n",
    "- An Encoder consisting of an embedding layer and LSTM unit.\n",
    "- A Decoder consisting of an embedding layer, LSTM unit, and linear output unit.\n",
    "\n",
    "The Seq2Seq model works by accepting an input into the Encoder, passing the hidden state from the Encoder to the Decoder, which the Decoder uses to output a series of token predictions.\n",
    "\n",
    "## Dependencies\n",
    "\n",
    "- Pytorch\n",
    "- Numpy\n",
    "- Pandas\n",
    "- NLTK\n",
    "- Gzip\n",
    "- Gensim\n",
    "\n",
    "\n",
    "Please choose a dataset from the Torchtext website. We recommend looking at the Squad dataset first. Here is a link to the website where you can view your options:\n",
    "\n",
    "- https://pytorch.org/text/stable/datasets.html\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import string\n",
    "import torch\n",
    "import neptune\n",
    "\n",
    "stemmer = nltk.stem.snowball.SnowballStemmer('english')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def getDict(dataPipe):\n",
    "\n",
    "    data_dict = {\n",
    "        'Question': [],\n",
    "        'Answer': []\n",
    "    }\n",
    "    \n",
    "    for _, question, answers, _ in dataPipe:\n",
    "        data_dict['Question'].append(question)\n",
    "        data_dict['Answer'].append(answers[0])\n",
    "        \n",
    "    return data_dict\n",
    "\n",
    "\n",
    "def loadDF(path):\n",
    "    # load data\n",
    "    train_data, val_data = torchtext.datasets.SQuAD2|(path)\n",
    "    \n",
    "    # convert dataPipe to dictionary \n",
    "    train_dict, val_dict = getDict(train_data), getDict(val_data)\n",
    "    \n",
    "    # convert Dictionaries to Pandas DataFrame\n",
    "    train_df = pd.DataFrame(train_dict)    \n",
    "    validation_df = pd.DataFrame(val_dict)    \n",
    "    \n",
    "    return train_df.append(validation_df)\n",
    "\n",
    "\n",
    "def prepare_text(sentence):\n",
    "    # clean text and tokenize it \n",
    "    sentence = ''.join([s.lower() for s in sentence if s not in string.punctuation])\n",
    "    sentence = ' '.join(stemmer.stem(w) for w in sentence.split())\n",
    "    tokens = nltk.tokenize.RegexpTokenizer(r'\\w+').tokenize(sentence)\n",
    "\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def toTensor(vocab, sentence):\n",
    "    # convert list of words \"sentence\" to a torch tensor of indices\n",
    "    indices = [vocab.word2index[word] for word in sentence.split(' ')]\n",
    "    indices.append(vocab.word2index[''])\n",
    "    return torch.Tensor(indices).long().to(device).view(-1, 1)\n",
    "\n",
    "\n",
    "def getPairs(df):\n",
    "    # convert df to list of pairs\n",
    "    temp1 = df['Question'].apply(lambda x: \" \".join(x) ).to_list()\n",
    "    temp2 = df['Answer'].apply(lambda x: \" \".join(x) ).to_list()\n",
    "    return [list(i) for i in zip(temp1, temp2)]\n",
    "\n",
    "\n",
    "def getMaxLen(pairs):\n",
    "    max_src = 0 \n",
    "    max_trg = 0\n",
    "    \n",
    "    for p in pairs:\n",
    "        max_src = len(p[0].split()) if len(p[0].split()) > max_src else max_src\n",
    "        max_trg = len(p[1].split()) if len(p[1].split()) > max_trg else max_trg\n",
    "        \n",
    "    return max_src, max_trg\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arsitektur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(self.input_size, self.hidden_size)\n",
    "        self.input = nn.Linear(self.input_size, self.hidden_size)\n",
    "        self.lstm = nn.LSTM(self.hidden_size, self.hidden_size)\n",
    "\n",
    "    def forward(self, x, hidden, cell_state):\n",
    "        x = self.embedding(x)\n",
    "        x = x.view(1, 1, -1)\n",
    "        x, (hidden, cell_state) = self.lstm(x, (hidden, cell_state))\n",
    "        return x, hidden, cell_state\n",
    "        \n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_size, self.hidden_size)\n",
    "        self.lstm = nn.LSTM(self.hidden_size, self.hidden_size)\n",
    "        self.fc = nn.Linear(self.hidden_size, self.output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim= 1)\n",
    "\n",
    "    def forward(self, x, hidden, cell_state):\n",
    "        x = self.embedding(x)\n",
    "        x = x.view(1, 1, -1)\n",
    "        x, (hidden, cell_state) = self.lstm(x, (hidden, cell_state))\n",
    "        x = self.softmax(self.fc(x[0]))\n",
    "        return x, hidden, cell_state\n",
    "    \n",
    "    \n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.encoder = Encoder(self.input_size, self.hidden_size)\n",
    "        self.decoder = Decoder(self.hidden_size, self.output_size)\n",
    "        \n",
    "    def forward(self, src, trg, src_len, trg_len, teacher_force=1):\n",
    "        \n",
    "        output = {\n",
    "            'decoder_output':[]\n",
    "        }\n",
    "        \n",
    "        encoder_hidden = torch.zeros([1, 1, self.hidden_size]).to(device) # 1 = number of LSTM layers\n",
    "        cell_state = torch.zeros([1, 1, self.hidden_size]).to(device)  \n",
    "        \n",
    "        for i in range(src_len):\n",
    "            encoder_output, encoder_hidden, cell_state = self.encoder(src[i], encoder_hidden, cell_state)\n",
    "\n",
    "        decoder_input = torch.Tensor([[0]]).long().to(device) # 0 = SOS_token\n",
    "        decoder_hidden = encoder_hidden\n",
    "        \n",
    "        for i in range(trg_len):\n",
    "            decoder_output, decoder_hidden, cell_state = self.decoder(decoder_input, decoder_hidden, cell_state)\n",
    "            output['decoder_output'].append(decoder_output)\n",
    "            \n",
    "            if self.training: # Model not in eval mode\n",
    "                decoder_input = target_tensor[i] if random.random() > teacher_force else decoder_output.argmax(1) # teacher forcing\n",
    "            else:\n",
    "                _, top_index = decoder_output.data.topk(1)\n",
    "                decoder_input = top_index.squeeze().detach()\n",
    "                \n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = \"andialifs/fluent-tesis-24\"\n",
    "api_token = \"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJjZTY2YWQ3My04OTBkLTQ2OWUtYTc1Ni1jYjk0MGZhMWFiNGEifQ==\"\n",
    "\n",
    "def neptune_init(name):\n",
    "    run = neptune.init_run(\n",
    "        project=project,\n",
    "        api_token=api_token,\n",
    "        name=name\n",
    "    )\n",
    "    return run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def train(source_data, target_data, model, epochs, batch_size, print_every, learning_rate):\n",
    "    model.to(device)\n",
    "    total_training_loss = 0\n",
    "    total_valid_loss = 0\n",
    "    loss = 0\n",
    "    \n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    # use cross validation\n",
    "    kf = KFold(n_splits=epochs, shuffle=True)\n",
    "\n",
    "    run = neptune_init(\"LSTM_FLUENT_Baseline\")\n",
    "\n",
    "    for e, (train_index, test_index) in enumerate(kf.split(source_data), 1):\n",
    "        model.to(device)\n",
    "        model.train()\n",
    "        for i in range(0, len(train_index)):\n",
    "\n",
    "            src = source_data[i]\n",
    "            trg = target_data[i]\n",
    "\n",
    "            output = model(src, trg, src.size(0), trg.size(0))\n",
    "\n",
    "            current_loss = 0\n",
    "            for (s, t) in zip(output[\"decoder_output\"], trg): \n",
    "                current_loss += criterion(s, t)\n",
    "\n",
    "            loss += current_loss\n",
    "            total_training_loss += (current_loss.item() / trg.size(0)) # add the iteration loss\n",
    "\n",
    "            if i % batch_size == 0 or i == (len(train_index)-1):\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                loss = 0\n",
    "\n",
    "\n",
    "        # validation set \n",
    "        model.eval()\n",
    "        for i in range(0, len(test_index)):\n",
    "            src = source_data[i]\n",
    "            trg = target_data[i]\n",
    "\n",
    "            output = model(src, trg, src.size(0), trg.size(0))\n",
    "\n",
    "            current_loss = 0\n",
    "            for (s, t) in zip(output[\"decoder_output\"], trg): \n",
    "                current_loss += criterion(s, t)\n",
    "\n",
    "            total_valid_loss += (current_loss.item() / trg.size(0)) # add the iteration loss\n",
    "\n",
    "\n",
    "        if e % print_every == 0:\n",
    "            training_loss_average = total_training_loss / (len(train_index)*print_every)\n",
    "            validation_loss_average = total_valid_loss / (len(test_index)*print_every)\n",
    "            print(\"{}/{} Epoch  -  Training Loss = {:.4f}  -  Validation Loss = {:.4f}\".format(e, epochs, training_loss_average, validation_loss_average))\n",
    "            run['train/loss'].append(training_loss_average)\n",
    "            total_training_loss = 0\n",
    "            total_valid_loss = 0 \n",
    "\n",
    "def train_wo_valid(source_data, target_data, model, epochs, batch_size, print_every, learning_rate):\n",
    "    model.to(device)\n",
    "    total_training_loss = 0\n",
    "    total_valid_loss = 0\n",
    "    loss = 0\n",
    "    \n",
    "    optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # use cross validation\n",
    "    kf = KFold(n_splits=epochs, shuffle=True)\n",
    "\n",
    "    for e, (train_index, test_index) in enumerate(kf.split(source_data), 1):\n",
    "        model.train()\n",
    "        for i in range(0, len(train_index)):\n",
    "\n",
    "            src = source_data[i]\n",
    "            trg = target_data[i]\n",
    "\n",
    "            output = model(src, trg, src.size(0), trg.size(0))\n",
    "\n",
    "            current_loss = 0\n",
    "            for (s, t) in zip(output[\"decoder_output\"], trg): \n",
    "                current_loss += criterion(s, t)\n",
    "\n",
    "            loss += current_loss\n",
    "            total_training_loss += (current_loss.item() / trg.size(0)) # add the iteration loss\n",
    "\n",
    "            if i % batch_size == 0 or i == (len(train_index)-1):\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                loss = 0\n",
    "\n",
    "        if e % print_every == 0:\n",
    "            training_loss_average = total_training_loss / (len(train_index)*print_every)\n",
    "            validation_loss_average = total_valid_loss / (len(test_index)*print_every)\n",
    "            print(\"{}/{} Epoch  -  Training Loss = {:.4f}  -  Validation Loss = {:.4f}\".format(e, epochs, training_loss_average, validation_loss_average))\n",
    "            total_training_loss = 0\n",
    "            total_valid_loss = 0 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "class Vocab:\n",
    "    def __init__(self):\n",
    "        self.word2index = {\"\": SOS_token, \"\": EOS_token}\n",
    "        self.index2word = {SOS_token: \"\", EOS_token: \"\"}\n",
    "        self.words_count = len(self.word2index)\n",
    "\n",
    "    def add_words(self, sentence):\n",
    "        for word in sentence.split(\" \"):\n",
    "            if word not in self.word2index:\n",
    "                self.word2index[word] = self.words_count\n",
    "                self.index2word[self.words_count] = word\n",
    "                self.words_count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "hidden_size = 500 # encoder and decoder hidden size\n",
    "batch_size = 50\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "knowledgebase = pd.read_excel('https://raw.githubusercontent.com/AndiAlifs/FLUENT-Chatbot-2023/main/KnowledgeBaseFilkom.xlsx', engine='openpyxl')\n",
    "knowledgebase.head()\n",
    "\n",
    "qa_paired = knowledgebase.drop(columns=knowledgebase.columns.drop(['Pertanyaan', 'Jawaban']))\n",
    "qa_paired.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_df = loadDF('data')\n",
    "# I will take only the first 5,000 Q&A to avoid CUDA out of memory error due to the large dataset\n",
    "# data_df = data_df.iloc[:5000, :]\n",
    "data_df = pd.DataFrame(columns=['Question', 'Answer'])\n",
    "data_df['Question'] = qa_paired['Pertanyaan']\n",
    "data_df['Answer'] = qa_paired['Jawaban']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">  email Fitra A. Bachtiar \n",
      "<   fitra.bachtiar[at]ub.ac.id \n",
      "\n",
      ">  NIK/NIP Fitra A. Bachtiar \n",
      "<  198406282019031006 \n",
      "\n",
      ">  nama lengkap Fitra A. Bachtiar \n",
      "<  Dr.Eng. Fitra A. Bachtiar \n",
      "\n",
      ">  Departemen Fitra A. Bachtiar \n",
      "<  Departemen Teknik Informatika \n",
      "\n",
      ">  Program Studi Fitra A. Bachtiar \n",
      "<  S2 Ilmu Komputer \n",
      "\n",
      ">  bidang penelitian Fitra A. Bachtiar \n",
      "<  Affective Computing, Affective Engineering, Intelligent System, Data Mining, Educational Data Mining \n",
      "\n",
      ">  nama awal Fakultas Ilmu Komputer (FILKOM) \n",
      "<  Program Teknologi Informasi dan Ilmu Komputer (PTIIK) \n",
      "\n",
      ">  rujukan surat keputusan SK Dikti dibentuk PTIIK \n",
      "<   SK Dikti No.163/KEP/DIKTI/2007  \n",
      "\n",
      ">  surat keputusan SK Rektor bentuk PTIIK \n",
      "<  Surat Keputusan Rektor Universitas Brawijaya Nomor: 516/SK/2011 \n",
      "\n",
      ">  tanggal dibentuk PTIIK \n",
      "<  27 Oktober 2011 \n",
      "\n",
      ">  program studi pembentuk PTIIK \n",
      "<  Teknik Perangkat Lunak dari Fakultas Teknik dan Ilmu Komputer dari Fakultas MIPA \n",
      "\n",
      ">  tanggal perubahan PTIIK menjadi FILKOM \n",
      "<  10 Desember 2014 \n",
      "\n",
      ">  surat keputusan SK Dikti FILKOM \n",
      "<  Surat Dikti No. 8073/EI/OT/2014 tentang Organisasi dan Tata Kerja Universitas Brawijaya \n",
      "\n",
      ">  visi FILKOM \n",
      "<  Menjadi Fakultas yang berdaya saing internasional dan berkontribusi kepada pengembangan Teknologi Informasi dan Ilmu Komputer untuk menunjang industri dan masyarakat dengan menyelaraskan pelaksanaan Pendidikan, Penelitian, dan Pengabdian kepada Masyarakat \n",
      "\n",
      ">  misi FILKOM \n",
      "<  Menyelenggarakan pendidikan di bidang Teknologi Informasi dan Ilmu Komputer yang berkualitas dan berstandar internasional secara berkelanjutan.\n",
      "Meningkatkan kemampuan sivitas akademika dalam pengembangan penelitian dan pengabdian yang selaras dengan kebutuhan industri dan masyarakat.\n",
      "Mengintegrasikan pengembangan pendidikan, penelitian, dan pengabdian kepada masyarakat yang ditunjang dengan tatakelola organisasi yang transparan, akuntabel, efektif, dan efisien.\n",
      "Mewujudkan kerja sama yang berkelanjutan di bidang pendidikan, penelitian, dan pengabdian kepada masyarakat dalam skala nasional dan internasional. \n",
      "\n",
      ">  apa tujuan filkom? \n",
      "<  Menghasilkan lulusan yang kompeten , profesional, berbudi pekerti luhur, berjiwa entrepreneur dan berdaya saing internasional.\n",
      "Menghasilkan sivitas akademika yang mampu mengembangkan penelitiandan pengabdian yang berorientasi pada pembaruan dan teknologi tepat guna untuk industri dan masyarakat.\n",
      "Terwujudnya suasana akademik yang kondusif dalam bidang pendidikan, penelitian, dan pengabdian kepada masyarakat yang berdaya saing unggul.\n",
      "Terwujudnya tata kelola organisasi yang transparan, akuntabel, efektif, dan efisien.\n",
      "Meningkatnya kuantitas dan kualitas kerja sama yang berkelanjutan di bidang pendidikan, penelitian, dan pengabdian kepada masyarakat dalam skala nasional dan internasional. \n",
      "\n",
      ">  sasaran pendidikan FILKOM \n",
      "<  1. Meningkatkan kompetensi dan kualifikasi pendidikan Dosen\n",
      "2. Meningkatkan sarana dan prasarana pembelajaran\n",
      "3. Mengembangkan kurikulum mengkuti perkembangan dan kebutuhan pemangku kepentingan\n",
      "4. Meningkatkan mutu lulusan yang berkualitas\n",
      "5. Mempercepat masa studi\n",
      "6. Meningkatkan kompetensi lulusan tersertifikasi bidang TIK\n",
      "7. Meningkatkan prestasi mahasiswa\n",
      "8. Meningkatkan mutu kelembagaan \n",
      "\n",
      ">  sasaran penelitian FILKOM \n",
      "<  1. Meningkatkan jumlah publikasi ilmiah\n",
      "2. Meningkatkan sarana dan prasarana penelitian\n",
      "3. Mengembangkan Grup Riset atau kelompok kajian \n",
      "\n",
      ">  sasaran pengabdian FILKOM \n",
      "<  1. Meningkatkan kualitas dan kuantitas pengabdian masyarakat\n",
      "2. Meningkatkan pemberdayaan masyarakat\n",
      "3. Menyediakan dana, sarana dan prasarana pengabdian masyarakat \n",
      "\n",
      ">  sasaran kerjasama FILKOM \n",
      "<  1. Mengadakan kerjasama pendidikan, penlitian dan pengabdian baik di tingkat Nasional maupun internasional\n",
      "2. Peningkatan kerjasama dengan industri dalam pengembangan produk \n",
      "\n",
      ">  Dekan Fakultas Ilmu Komputer FILKOM \n",
      "<  Prof. Ir. Wayan Firdaus Mahmudy, S.Si., MT., Ph.D. \n",
      "\n",
      ">  Wakil Dekan Bidang Akademik / Wakil Dekan 1 \n",
      "<  Dr. Eng. Ir. Herman Tolle, ST., MT. \n",
      "\n",
      ">  Wakil Dekan Bidang Umum, Keuangan, dan Sumber Daya / Wakil Dekan 2 \n",
      "<  Agus Wahyu Widodo, ST., M.Cs. \n",
      "\n",
      ">  Wakil Dekan Bidang Kemahasiswaan, Alumni, dan Kewirausahaan Mahasiswa / Wakil Dekan 3 \n",
      "<  Drs. Muh. Arif Rahman, M.Kom. \n",
      "\n",
      ">  Ketua Departemen Teknik Informatika \n",
      "<  Achmad Basuki, S.T., M.MG., Ph.D. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 25): # first 5 Q&A\n",
    "    print(\"> \", data_df.iloc[i,0], \"\\n< \", data_df.iloc[i,1], \"\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df['Question'] = data_df['Question'].apply(prepare_text)\n",
    "data_df['Answer'] = data_df['Answer'].apply(prepare_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = getPairs(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(290, 13)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_src, max_trg = getMaxLen(pairs)\n",
    "max_trg, max_src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_vocab = Vocab()\n",
    "A_vocab = Vocab()\n",
    "\n",
    "# build vocabularies for questions \"source\" and answers \"target\"\n",
    "for pair in pairs:\n",
    "    Q_vocab.add_words(pair[0])\n",
    "    A_vocab.add_words(pair[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_data = [toTensor(Q_vocab, pair[0]) for pair in pairs]\n",
    "target_data = [toTensor(A_vocab, pair[1]) for pair in pairs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/andialifs/fluent-tesis-24/e/FLUENT24-75\n",
      "1/100 Epoch  -  Training Loss = 7.7918  -  Validation Loss = 7.3463\n",
      "2/100 Epoch  -  Training Loss = 6.8273  -  Validation Loss = 6.7177\n",
      "3/100 Epoch  -  Training Loss = 6.4556  -  Validation Loss = 6.6382\n",
      "4/100 Epoch  -  Training Loss = 6.3122  -  Validation Loss = 6.7757\n",
      "5/100 Epoch  -  Training Loss = 6.0941  -  Validation Loss = 6.3723\n",
      "6/100 Epoch  -  Training Loss = 5.9660  -  Validation Loss = 6.2463\n",
      "7/100 Epoch  -  Training Loss = 5.8427  -  Validation Loss = 6.1582\n",
      "8/100 Epoch  -  Training Loss = 5.7419  -  Validation Loss = 6.0635\n",
      "9/100 Epoch  -  Training Loss = 5.6716  -  Validation Loss = 5.9831\n",
      "10/100 Epoch  -  Training Loss = 5.6131  -  Validation Loss = 5.9141\n",
      "11/100 Epoch  -  Training Loss = 5.7291  -  Validation Loss = 5.9113\n",
      "12/100 Epoch  -  Training Loss = 5.6688  -  Validation Loss = 6.1261\n",
      "13/100 Epoch  -  Training Loss = 5.6568  -  Validation Loss = 6.0274\n",
      "14/100 Epoch  -  Training Loss = 5.6286  -  Validation Loss = 5.9189\n",
      "15/100 Epoch  -  Training Loss = 5.5598  -  Validation Loss = 5.8353\n",
      "16/100 Epoch  -  Training Loss = 5.5052  -  Validation Loss = 5.7842\n",
      "17/100 Epoch  -  Training Loss = 5.4610  -  Validation Loss = 5.7782\n",
      "18/100 Epoch  -  Training Loss = 5.4387  -  Validation Loss = 5.7753\n",
      "19/100 Epoch  -  Training Loss = 5.4180  -  Validation Loss = 5.7645\n",
      "20/100 Epoch  -  Training Loss = 5.3983  -  Validation Loss = 5.7529\n",
      "21/100 Epoch  -  Training Loss = 5.3973  -  Validation Loss = 5.7297\n",
      "22/100 Epoch  -  Training Loss = 5.3976  -  Validation Loss = 5.7586\n",
      "23/100 Epoch  -  Training Loss = 5.3530  -  Validation Loss = 5.7430\n",
      "24/100 Epoch  -  Training Loss = 5.3666  -  Validation Loss = 5.7279\n",
      "25/100 Epoch  -  Training Loss = 5.3389  -  Validation Loss = 5.7186\n",
      "26/100 Epoch  -  Training Loss = 5.3202  -  Validation Loss = 5.7249\n",
      "27/100 Epoch  -  Training Loss = 5.3048  -  Validation Loss = 5.7157\n",
      "28/100 Epoch  -  Training Loss = 5.2891  -  Validation Loss = 5.7073\n",
      "29/100 Epoch  -  Training Loss = 5.2813  -  Validation Loss = 5.6856\n",
      "30/100 Epoch  -  Training Loss = 5.2639  -  Validation Loss = 5.6891\n",
      "31/100 Epoch  -  Training Loss = 5.2551  -  Validation Loss = 5.6933\n",
      "32/100 Epoch  -  Training Loss = 5.2479  -  Validation Loss = 5.6917\n",
      "33/100 Epoch  -  Training Loss = 5.2349  -  Validation Loss = 5.6772\n",
      "34/100 Epoch  -  Training Loss = 5.2221  -  Validation Loss = 5.6807\n",
      "35/100 Epoch  -  Training Loss = 5.2111  -  Validation Loss = 5.6604\n",
      "36/100 Epoch  -  Training Loss = 5.1995  -  Validation Loss = 5.6695\n",
      "37/100 Epoch  -  Training Loss = 5.1950  -  Validation Loss = 5.6760\n",
      "38/100 Epoch  -  Training Loss = 5.1870  -  Validation Loss = 5.6629\n",
      "39/100 Epoch  -  Training Loss = 5.1777  -  Validation Loss = 5.6570\n",
      "40/100 Epoch  -  Training Loss = 5.1707  -  Validation Loss = 5.6420\n",
      "41/100 Epoch  -  Training Loss = 5.1636  -  Validation Loss = 5.6394\n",
      "42/100 Epoch  -  Training Loss = 5.1557  -  Validation Loss = 5.6269\n",
      "43/100 Epoch  -  Training Loss = 5.1479  -  Validation Loss = 5.6172\n",
      "44/100 Epoch  -  Training Loss = 5.1463  -  Validation Loss = 5.6070\n",
      "45/100 Epoch  -  Training Loss = 5.1409  -  Validation Loss = 5.5990\n",
      "46/100 Epoch  -  Training Loss = 5.1381  -  Validation Loss = 5.5946\n",
      "47/100 Epoch  -  Training Loss = 5.1275  -  Validation Loss = 5.5957\n",
      "48/100 Epoch  -  Training Loss = 5.1167  -  Validation Loss = 5.5835\n",
      "49/100 Epoch  -  Training Loss = 5.1106  -  Validation Loss = 5.5751\n",
      "50/100 Epoch  -  Training Loss = 5.1010  -  Validation Loss = 5.5695\n",
      "51/100 Epoch  -  Training Loss = 5.0912  -  Validation Loss = 5.5675\n",
      "52/100 Epoch  -  Training Loss = 5.0833  -  Validation Loss = 5.5623\n",
      "53/100 Epoch  -  Training Loss = 5.0728  -  Validation Loss = 5.5496\n",
      "54/100 Epoch  -  Training Loss = 5.0630  -  Validation Loss = 5.5396\n",
      "55/100 Epoch  -  Training Loss = 5.0581  -  Validation Loss = 5.5542\n",
      "56/100 Epoch  -  Training Loss = 5.0500  -  Validation Loss = 5.5260\n",
      "57/100 Epoch  -  Training Loss = 5.0397  -  Validation Loss = 5.5180\n",
      "58/100 Epoch  -  Training Loss = 5.0296  -  Validation Loss = 5.5443\n",
      "59/100 Epoch  -  Training Loss = 5.0204  -  Validation Loss = 5.5510\n",
      "60/100 Epoch  -  Training Loss = 5.0087  -  Validation Loss = 5.5336\n",
      "61/100 Epoch  -  Training Loss = 5.0022  -  Validation Loss = 5.5584\n",
      "62/100 Epoch  -  Training Loss = 4.9918  -  Validation Loss = 5.5368\n",
      "63/100 Epoch  -  Training Loss = 4.9863  -  Validation Loss = 5.5388\n",
      "64/100 Epoch  -  Training Loss = 4.9802  -  Validation Loss = 5.5161\n",
      "65/100 Epoch  -  Training Loss = 4.9759  -  Validation Loss = 5.5207\n",
      "66/100 Epoch  -  Training Loss = 4.9768  -  Validation Loss = 5.5567\n",
      "67/100 Epoch  -  Training Loss = 4.9459  -  Validation Loss = 5.5012\n",
      "68/100 Epoch  -  Training Loss = 4.9563  -  Validation Loss = 5.5430\n",
      "69/100 Epoch  -  Training Loss = 4.9347  -  Validation Loss = 5.4703\n",
      "70/100 Epoch  -  Training Loss = 4.9263  -  Validation Loss = 5.5103\n",
      "71/100 Epoch  -  Training Loss = 4.9113  -  Validation Loss = 5.5002\n",
      "72/100 Epoch  -  Training Loss = 4.9088  -  Validation Loss = 5.5090\n",
      "73/100 Epoch  -  Training Loss = 4.8949  -  Validation Loss = 5.5227\n",
      "74/100 Epoch  -  Training Loss = 4.8847  -  Validation Loss = 5.5168\n",
      "75/100 Epoch  -  Training Loss = 4.8713  -  Validation Loss = 5.5096\n",
      "76/100 Epoch  -  Training Loss = 4.8617  -  Validation Loss = 5.4706\n",
      "77/100 Epoch  -  Training Loss = 4.8488  -  Validation Loss = 5.4997\n",
      "78/100 Epoch  -  Training Loss = 4.8369  -  Validation Loss = 5.4946\n",
      "79/100 Epoch  -  Training Loss = 4.8235  -  Validation Loss = 5.4833\n",
      "80/100 Epoch  -  Training Loss = 4.8153  -  Validation Loss = 5.4404\n",
      "81/100 Epoch  -  Training Loss = 4.8073  -  Validation Loss = 5.4364\n",
      "82/100 Epoch  -  Training Loss = 4.7971  -  Validation Loss = 5.4298\n",
      "83/100 Epoch  -  Training Loss = 4.7883  -  Validation Loss = 5.4131\n",
      "84/100 Epoch  -  Training Loss = 4.7807  -  Validation Loss = 5.4046\n",
      "85/100 Epoch  -  Training Loss = 4.7787  -  Validation Loss = 5.4078\n",
      "86/100 Epoch  -  Training Loss = 4.7674  -  Validation Loss = 5.4341\n",
      "87/100 Epoch  -  Training Loss = 4.7650  -  Validation Loss = 5.4349\n",
      "88/100 Epoch  -  Training Loss = 4.7592  -  Validation Loss = 5.4092\n",
      "89/100 Epoch  -  Training Loss = 4.7555  -  Validation Loss = 5.3964\n",
      "90/100 Epoch  -  Training Loss = 4.7397  -  Validation Loss = 5.3996\n",
      "91/100 Epoch  -  Training Loss = 4.7363  -  Validation Loss = 5.3863\n",
      "92/100 Epoch  -  Training Loss = 4.7299  -  Validation Loss = 5.3762\n",
      "93/100 Epoch  -  Training Loss = 4.7187  -  Validation Loss = 5.4164\n",
      "94/100 Epoch  -  Training Loss = 4.7062  -  Validation Loss = 5.3158\n",
      "95/100 Epoch  -  Training Loss = 4.6939  -  Validation Loss = 5.3750\n",
      "96/100 Epoch  -  Training Loss = 4.6827  -  Validation Loss = 5.3109\n",
      "97/100 Epoch  -  Training Loss = 4.6683  -  Validation Loss = 5.2556\n",
      "98/100 Epoch  -  Training Loss = 4.6589  -  Validation Loss = 5.2405\n",
      "99/100 Epoch  -  Training Loss = 4.6457  -  Validation Loss = 5.1194\n",
      "100/100 Epoch  -  Training Loss = 4.6343  -  Validation Loss = 5.1294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread NeptuneWebhooks:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/andyalyfsyah/.conda/envs/myenv/lib/python3.8/site-packages/websocket/_socket.py\", line 118, in recv\n",
      "Exception in thread NeptuneWebhooks:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/andyalyfsyah/.conda/envs/myenv/lib/python3.8/site-packages/websocket/_socket.py\", line 118, in recv\n",
      "    bytes_ = _recv()\n",
      "  File \"/home/andyalyfsyah/.conda/envs/myenv/lib/python3.8/site-packages/websocket/_socket.py\", line 97, in _recv\n",
      "    return sock.recv(bufsize)\n",
      "  File \"/home/andyalyfsyah/.conda/envs/myenv/lib/python3.8/ssl.py\", line 1226, in recv\n",
      "    bytes_ = _recv()\n",
      "  File \"/home/andyalyfsyah/.conda/envs/myenv/lib/python3.8/site-packages/websocket/_socket.py\", line 97, in _recv\n",
      "    return self.read(buflen)\n",
      "  File \"/home/andyalyfsyah/.conda/envs/myenv/lib/python3.8/ssl.py\", line 1101, in read\n",
      "    return sock.recv(bufsize)\n",
      "  File \"/home/andyalyfsyah/.conda/envs/myenv/lib/python3.8/ssl.py\", line 1226, in recv\n",
      "    return self._sslobj.read(len)\n",
      "TimeoutError: [Errno 110] Connection timed out\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/andyalyfsyah/.conda/envs/myenv/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "    return self.read(buflen)\n",
      "  File \"/home/andyalyfsyah/.conda/envs/myenv/lib/python3.8/ssl.py\", line 1101, in read\n",
      "    self.run()\n",
      "  File \"/home/andyalyfsyah/.conda/envs/myenv/lib/python3.8/site-packages/neptune/internal/threading/daemon.py\", line 96, in run\n",
      "    return self._sslobj.read(len)\n",
      "TimeoutError: [Errno 110] Connection timed out\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/andyalyfsyah/.conda/envs/myenv/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "    self.work()\n",
      "  File \"/home/andyalyfsyah/.conda/envs/myenv/lib/python3.8/site-packages/neptune/internal/websockets/websocket_signals_background_job.py\", line 84, in work\n",
      "Exception in thread NeptuneWebhooks:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/andyalyfsyah/.conda/envs/myenv/lib/python3.8/site-packages/websocket/_socket.py\", line 118, in recv\n",
      "    raw_message = self._ws_client.recv()\n",
      "  File \"/home/andyalyfsyah/.conda/envs/myenv/lib/python3.8/site-packages/neptune/common/websockets/reconnecting_websocket.py\", line 51, in recv\n",
      "    bytes_ = _recv()\n",
      "  File \"/home/andyalyfsyah/.conda/envs/myenv/lib/python3.8/site-packages/websocket/_socket.py\", line 97, in _recv\n",
      "    data = self.client.recv()\n",
      "  File \"/home/andyalyfsyah/.conda/envs/myenv/lib/python3.8/site-packages/neptune/common/websockets/websocket_client_adapter.py\", line 63, in recv\n",
      "    self.run()\n",
      "  File \"/home/andyalyfsyah/.conda/envs/myenv/lib/python3.8/site-packages/neptune/internal/threading/daemon.py\", line 96, in run\n",
      "    opcode, data = self._ws_client.recv_data()\n",
      "  File \"/home/andyalyfsyah/.conda/envs/myenv/lib/python3.8/site-packages/websocket/_core.py\", line 416, in recv_data\n",
      "    opcode, frame = self.recv_data_frame(control_frame)\n",
      "  File \"/home/andyalyfsyah/.conda/envs/myenv/lib/python3.8/site-packages/websocket/_core.py\", line 437, in recv_data_frame\n",
      "    frame = self.recv_frame()\n",
      "  File \"/home/andyalyfsyah/.conda/envs/myenv/lib/python3.8/site-packages/websocket/_core.py\", line 478, in recv_frame\n",
      "    return sock.recv(bufsize)\n",
      "  File \"/home/andyalyfsyah/.conda/envs/myenv/lib/python3.8/ssl.py\", line 1226, in recv\n",
      "    return self.frame_buffer.recv_frame()\n",
      "  File \"/home/andyalyfsyah/.conda/envs/myenv/lib/python3.8/site-packages/websocket/_abnf.py\", line 363, in recv_frame\n",
      "    self.work()\n",
      "  File \"/home/andyalyfsyah/.conda/envs/myenv/lib/python3.8/site-packages/neptune/internal/websockets/websocket_signals_background_job.py\", line 84, in work\n",
      "    raw_message = self._ws_client.recv()\n",
      "  File \"/home/andyalyfsyah/.conda/envs/myenv/lib/python3.8/site-packages/neptune/common/websockets/reconnecting_websocket.py\", line 51, in recv\n",
      "    self.recv_header()\n",
      "  File \"/home/andyalyfsyah/.conda/envs/myenv/lib/python3.8/site-packages/websocket/_abnf.py\", line 319, in recv_header\n",
      "    data = self.client.recv()\n",
      "  File \"/home/andyalyfsyah/.conda/envs/myenv/lib/python3.8/site-packages/neptune/common/websockets/websocket_client_adapter.py\", line 63, in recv\n",
      "    opcode, data = self._ws_client.recv_data()\n",
      "  File \"/home/andyalyfsyah/.conda/envs/myenv/lib/python3.8/site-packages/websocket/_core.py\", line 416, in recv_data\n",
      "    header = self.recv_strict(2)\n",
      "  File \"/home/andyalyfsyah/.conda/envs/myenv/lib/python3.8/site-packages/websocket/_abnf.py\", line 398, in recv_strict\n",
      "    opcode, frame = self.recv_data_frame(control_frame)\n",
      "  File \"/home/andyalyfsyah/.conda/envs/myenv/lib/python3.8/site-packages/websocket/_core.py\", line 437, in recv_data_frame\n",
      "    return self.read(buflen)\n",
      "  File \"/home/andyalyfsyah/.conda/envs/myenv/lib/python3.8/ssl.py\", line 1101, in read\n",
      "    bytes_ = self.recv(min(16384, shortage))\n",
      "  File \"/home/andyalyfsyah/.conda/envs/myenv/lib/python3.8/site-packages/websocket/_core.py\", line 563, in _recv\n",
      "    frame = self.recv_frame()\n",
      "  File \"/home/andyalyfsyah/.conda/envs/myenv/lib/python3.8/site-packages/websocket/_core.py\", line 478, in recv_frame\n",
      "    return recv(self.sock, bufsize)\n",
      "  File \"/home/andyalyfsyah/.conda/envs/myenv/lib/python3.8/site-packages/websocket/_socket.py\", line 120, in recv\n",
      "    return self._sslobj.read(len)\n",
      "TimeoutError: [Errno 110] Connection timed out\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/andyalyfsyah/.conda/envs/myenv/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "    raise WebSocketTimeoutException(\"Connection timed out\")\n",
      "websocket._exceptions.WebSocketTimeoutException: Connection timed out\n",
      "    self.run()\n",
      "  File \"/home/andyalyfsyah/.conda/envs/myenv/lib/python3.8/site-packages/neptune/internal/threading/daemon.py\", line 96, in run\n",
      "    self.work()\n",
      "  File \"/home/andyalyfsyah/.conda/envs/myenv/lib/python3.8/site-packages/neptune/internal/websockets/websocket_signals_background_job.py\", line 84, in work\n",
      "    return self.frame_buffer.recv_frame()\n",
      "  File \"/home/andyalyfsyah/.conda/envs/myenv/lib/python3.8/site-packages/websocket/_abnf.py\", line 363, in recv_frame\n",
      "    raw_message = self._ws_client.recv()\n",
      "  File \"/home/andyalyfsyah/.conda/envs/myenv/lib/python3.8/site-packages/neptune/common/websockets/reconnecting_websocket.py\", line 51, in recv\n",
      "    self.recv_header()\n",
      "  File \"/home/andyalyfsyah/.conda/envs/myenv/lib/python3.8/site-packages/websocket/_abnf.py\", line 319, in recv_header\n",
      "    header = self.recv_strict(2)\n",
      "  File \"/home/andyalyfsyah/.conda/envs/myenv/lib/python3.8/site-packages/websocket/_abnf.py\", line 398, in recv_strict\n",
      "    bytes_ = self.recv(min(16384, shortage))\n",
      "  File \"/home/andyalyfsyah/.conda/envs/myenv/lib/python3.8/site-packages/websocket/_core.py\", line 563, in _recv\n",
      "    return recv(self.sock, bufsize)\n",
      "  File \"/home/andyalyfsyah/.conda/envs/myenv/lib/python3.8/site-packages/websocket/_socket.py\", line 120, in recv\n",
      "    raise WebSocketTimeoutException(\"Connection timed out\")\n",
      "websocket._exceptions.WebSocketTimeoutException: Connection timed out\n",
      "    data = self.client.recv()\n",
      "  File \"/home/andyalyfsyah/.conda/envs/myenv/lib/python3.8/site-packages/neptune/common/websockets/websocket_client_adapter.py\", line 63, in recv\n",
      "    opcode, data = self._ws_client.recv_data()\n",
      "  File \"/home/andyalyfsyah/.conda/envs/myenv/lib/python3.8/site-packages/websocket/_core.py\", line 416, in recv_data\n",
      "    opcode, frame = self.recv_data_frame(control_frame)\n",
      "  File \"/home/andyalyfsyah/.conda/envs/myenv/lib/python3.8/site-packages/websocket/_core.py\", line 437, in recv_data_frame\n",
      "    frame = self.recv_frame()\n",
      "  File \"/home/andyalyfsyah/.conda/envs/myenv/lib/python3.8/site-packages/websocket/_core.py\", line 478, in recv_frame\n",
      "    return self.frame_buffer.recv_frame()\n",
      "  File \"/home/andyalyfsyah/.conda/envs/myenv/lib/python3.8/site-packages/websocket/_abnf.py\", line 363, in recv_frame\n",
      "    self.recv_header()\n",
      "  File \"/home/andyalyfsyah/.conda/envs/myenv/lib/python3.8/site-packages/websocket/_abnf.py\", line 319, in recv_header\n",
      "    header = self.recv_strict(2)\n",
      "  File \"/home/andyalyfsyah/.conda/envs/myenv/lib/python3.8/site-packages/websocket/_abnf.py\", line 398, in recv_strict\n",
      "    bytes_ = self.recv(min(16384, shortage))\n",
      "  File \"/home/andyalyfsyah/.conda/envs/myenv/lib/python3.8/site-packages/websocket/_core.py\", line 563, in _recv\n",
      "    return recv(self.sock, bufsize)\n",
      "  File \"/home/andyalyfsyah/.conda/envs/myenv/lib/python3.8/site-packages/websocket/_socket.py\", line 120, in recv\n",
      "    raise WebSocketTimeoutException(\"Connection timed out\")\n",
      "websocket._exceptions.WebSocketTimeoutException: Connection timed out\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[neptune] [warning] Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterConnectionError\n",
      "[neptune] [warning] Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterConnectionError\n",
      "[neptune] [warning] Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterConnectionError\n",
      "[neptune] [info   ] Communication with Neptune restored!\n",
      "[neptune] [info   ] Communication with Neptune restored!\n",
      "[neptune] [info   ] Communication with Neptune restored!\n"
     ]
    }
   ],
   "source": [
    "seq2seq = Seq2Seq(Q_vocab.words_count, hidden_size, A_vocab.words_count)\n",
    "\n",
    "train(source_data = source_data,\n",
    "      target_data = target_data,\n",
    "      model = seq2seq,\n",
    "      print_every = 1,\n",
    "      epochs = epochs,\n",
    "      learning_rate = learning_rate,\n",
    "      batch_size = batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(740, 500)\n",
       "    (input): Linear(in_features=740, out_features=500, bias=True)\n",
       "    (lstm): LSTM(500, 500)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(2655, 500)\n",
       "    (lstm): LSTM(500, 500)\n",
       "    (fc): Linear(in_features=500, out_features=2655, bias=True)\n",
       "    (softmax): LogSoftmax(dim=1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "model_path = 'seq2seq.pt'\n",
    "\n",
    "torch.save(seq2seq, model_path)\n",
    "\n",
    "seq2seq = torch.load(model_path, map_location=torch.device('cuda'))\n",
    "seq2seq.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type 'exit' to finish the chat.\n",
      " ------------------------------ \n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'evaluate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [77]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m src\u001b[38;5;241m.\u001b[39mstrip() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexit\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[43mevaluate\u001b[49m(src, Q_vocab, A_vocab, Q_seq2seq, max_trg)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'evaluate' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Type 'exit' to finish the chat.\\n\", \"-\"*30, '\\n')\n",
    "while (True):\n",
    "    src = input(\"> \")\n",
    "    if src.strip() == \"exit\":\n",
    "        break\n",
    "    evaluate(src, Q_vocab, A_vocab, Q_seq2seq, max_trg)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "(Starter Code) LSTM Bot",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "My Environment",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
